<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Blog | LAION</title><meta name="title" content="Blog | LAION"/><meta property="og:title" content="Blog | LAION"/><meta name="twitter:title" content="Blog | LAION"/><meta name="description" content="LAION, Large-scale Artificial Intelligence Open Network, is a non-profit organization making machine learning resources available to the general public."/><meta property="og:description" content="LAION, Large-scale Artificial Intelligence Open Network, is a non-profit organization making machine learning resources available to the general public."/><meta name="twitter:description" content="LAION, Large-scale Artificial Intelligence Open Network, is a non-profit organization making machine learning resources available to the general public."/><meta property="og:image" content="https://laion.ai/social.png"/><meta name="twitter:image" content="https://laion.ai/social.png"/><meta name="twitter:image:alt" content="The text: LAION. Large-scale Artificial Intelligence Open Network, TRULY OPEN AI. 100% NON-PROFIT. 100% FREE."/><meta property="og:type" content="website"/><meta property="og:url" content="https://laion.ai/blog-de"/><meta name="twitter:url" content="https://laion.ai/blog-de"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1D374E"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.png"/><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff2" as="font" type="font/woff2" crossorigin="true"/><meta name="next-head-count" content="25"/><link rel="stylesheet" href="/fonts/load.css"/><link rel="preload" href="/_next/static/css/5357c8cce67e7f29.css" as="style"/><link rel="stylesheet" href="/_next/static/css/5357c8cce67e7f29.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fb0512e25146295.js" defer=""></script><script src="/_next/static/chunks/286-30519d8a3e60551d.js" defer=""></script><script src="/_next/static/chunks/58-229d4b5119b416a2.js" defer=""></script><script src="/_next/static/chunks/pages/blog-de-71d7fe712644fb13.js" defer=""></script><script src="/_next/static/R99kgm_HfSIL7_rSdnHLT/_buildManifest.js" defer=""></script><script src="/_next/static/R99kgm_HfSIL7_rSdnHLT/_ssgManifest.js" defer=""></script><script src="/_next/static/R99kgm_HfSIL7_rSdnHLT/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-paper md:text-sky md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LAION</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/team/">Team</a><a href="/blog/">Blog</a><a href="/notes/">Notes</a><a href="/press/">Press</a><a href="/about/">About</a><a href="/faq/">FAQ</a><a href="/donations/">Donations</a><a href="/privacy-policy/">Privacy Policy</a><a href="/dataset-requests/">Dataset Requests</a><a href="/impressum/">Impressum</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1D374E;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/team/">Team</a></p><p><a href="/blog/">Blog</a></p><p><a href="/notes/">Notes</a></p><p><a href="/press/">Press</a></p><p><a href="/about/">About</a></p><p><a href="/faq/">FAQ</a></p><p><a href="/donations/">Donations</a></p><p><a href="/privacy-policy/">Privacy Policy</a></p><p><a href="/dataset-requests/">Dataset Requests</a></p><p><a href="/impressum/">Impressum</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-paper grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center pt-16 md:pt-5"><div class="container px-5"><h1 class="text-8xl md:text-8xl font-bold pb-2">BLOG</h1><div class="text-md pb-4">Willkommen in unserem LAION-Blog! Hier finden Sie Kommentare, Nachrichten und Updates zu unseren aktuellen Forschungsprojekten und Fortschritten im Bereich der KI-Forschung. Diese Blogbeitr√§ge sind nicht als vollst√§ndige wissenschaftliche Forschungsarbeiten gedacht, sondern als Arbeitsfortschritte, um weitere Forschungen/Diskussionen auf unserem Discord-Server und in der offenen wissenschaftlichen Gemeinschaft zu f√∂rdern.</div><hr class="mb-5 md:hidden"/><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Admin Bud-E V1.0 ‚Äì Datenschutzfreundliche KI-Assistenz f√ºr Schulen, Universit√§ten &amp; Unternehmen</p><p class="text-lg pb-1">by: <!-- -->Christoph Schuhmann, Robert Kaczmarczyk<!-- -->, <!-- -->09 Oct, 2025<!-- --></p><hr/><p class="pt-2">&amp;lt;img width=&amp;quot;1024&amp;quot; height=&amp;quot;1024&amp;quot; alt=&amp;quot;image&amp;quot; src=&amp;quot;https://github.com/user-attachments/assets/babd1bd7-cca5-470e-ab5e-335a49585f4d&amp;quot; /&amp;gt;
LAION verfolgt seit seiner Gr√ºndung ein klares Ziel: die Demokratisierung von K√ºnstlicher Intelligenz und zug√§ngliche, fa...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">LeoLM: Ein Impuls f√ºr Deutschsprachige LLM-Forschung</p><p class="text-lg pb-1">by: <!-- -->Bj√∂rn Pl√ºster<!-- -->, <!-- -->28 Sep, 2023<!-- --></p><hr/><p class="pt-2">Lernen Sie LeoLM kennen, das erste offen und kommerziell verf√ºgbare deutsche Foundation Language Model, das auf Llama-2 basiert.
Unsere Modelle erweitern die F√§higkeiten von Llama-2 durch ein fortgesetztes Training auf einem gro√üen Korpus von hochwertigen deutschen und gr√∂√ütenteils lokal spezifische...</p></div></div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"slug":"admin_bud-e","frontmatter":{"title":"Admin Bud-E V1.0 ‚Äì Datenschutzfreundliche KI-Assistenz f√ºr Schulen, Universit√§ten \u0026 Unternehmen","author":"Christoph Schuhmann, Robert Kaczmarczyk","date":"October 9 2025","previewImg":"/images/blog/admin-bud-e+.jpg"},"content":"\n\u003cimg width=\"1024\" height=\"1024\" alt=\"image\" src=\"https://github.com/user-attachments/assets/babd1bd7-cca5-470e-ab5e-335a49585f4d\" /\u003e\n\n**LAION** verfolgt seit seiner Gr√ºndung ein klares Ziel: die **Demokratisierung von K√ºnstlicher Intelligenz** und **zug√§ngliche, faire Bildung** f√ºr alle. Mit **[Bud-E 1.0](https://laion.ai/blog/bud-e-release/)** und **[School Bud-E 1.0](https://laion.ai/blog/bud-e-release/)** haben wir zu Jahresbeginn zwei **browserbasierte Sprachassistenten** bereitgestellt, die konsequent auf **Privatsph√§re**, **Offenheit** und **Datenhoheit** setzen. Gespr√§che bleiben **lokal im Browser**, die Architektur ist **modular**, und der Betrieb ist mit **selbst gehosteten** oder **DSGVO-konformen** APIs m√∂glich.\nGleichzeitig adressiert **Bud-E** einen Punkt, den viele kommerzielle Angebote (z. B. Fobizz, Telli Chat) bislang nur eingeschr√§nkt abdecken: **intuitive Spracheingabe** und **angenehme, nat√ºrlich klingende Sprachausgabe**. **Bud-E versteht gesprochene Sprache**, **antwortet schnell** und kann ‚Äì wenn gew√ºnscht ‚Äì **in einer gut verst√§ndlichen Stimme vorlesen**. So werden nicht nur Fakten vermittelt, sondern es entsteht eine **inspirierende Lernatmosph√§re**, die individuelle Lernst√§nde, Sprachniveaus und Vorwissen ber√ºcksichtigt und damit besonders auch Lernende unterst√ºtzt, die vom **Digital Divide** betroffen sind\u003csup id=\"cite-voice-divide\"\u003e\u003ca href=\"#ref-voice-divide\"\u003e[1]\u003c/a\u003e\u003c/sup\u003e. **Studien** zeigen, dass LLMs personalisiertes Feedback und motivierendere Lernumgebungen erleichtern ‚Äì das st√§rkt Selbstwirksamkeit und Lernfortschritt und entlastet Lehrkr√§fte\u003csup id=\"cite-llm-evidence\"\u003e\u003ca href=\"#ref-llm-evidence\"\u003e[2]\u003c/a\u003e\u003c/sup\u003e.\nMit **Admin Bud-E V1.0** komplettieren wir nun das System: Gemeinsam mit **School Bud-E** entsteht eine **durchg√§ngige L√∂sung** f√ºr Organisationen, die **volle Kontrolle √ºber Speicher- und Verarbeitungsort** ihrer Daten behalten ‚Äì und dabei **einen Gro√üteil der Kosten** im Vergleich zu klassischen, kommerziellen Angeboten einsparen.\n\n## Einleitung\n\n### Was ist Admin Bud-E?\n\n**Admin Bud-E** ist eine **Open-Source-Administrationsschicht** zwischen dem Frontend (zum Beispiel School Bud-E) und den von Ihnen gew√§hlten KI-Anbietern. Die Middleware nimmt Anfragen entgegen, leitet sie an die konfigurierten Dienste weiter, misst die Nutzung (zum Beispiel in Tokens, Zeichen oder Sekunden), bucht die entsprechenden Credits pro Nutzerin oder Nutzer und pro Projekt und erzeugt bei Bedarf Berichte. Das Frontend kann auf Ihrer eigenen Infrastruktur laufen oder von uns bereitgestellt werden. Die Gespr√§chsverl√§ufe verbleiben **ausschlie√ülich im Browser** der Anwenderinnen und Anwender. Auf dem Server wird **nichts dauerhaft gespeichert**.\n\n### Warum jetzt?\n\nViele Schultr√§ger, Hochschulen und Organisationen w√ºnschen sich **volle Datenkontrolle**, **sp√ºrbare Kostenvorteile** gegen√ºber geschlossenen Komplettl√∂sungen und ein **schnelles Onboarding** ganzer Gruppen ohne komplizierte Kontenerstellung. **Admin Bud-E** liefert genau das: Administratorinnen und Administratoren erzeugen **anonyme API-Schl√ºssel** in Sekunden, exportieren diese als **CSV**, verteilen **projektbezogene Budgets** und stellen das **Routing** zu unterschiedlichen Modellen und Anbietern mit wenigen Klicks ein ‚Äì **ohne** dass die Clients ge√§ndert werden m√ºssen. Zugleich profitieren alle Beteiligten von den **didaktischen St√§rken** der Sprachschnittstelle: **Sprechen, Zuh√∂ren, Nachfragen** ‚Äì Bud-E unterst√ºtzt Lernprozesse **interaktiv** und **niedrigschwellig** und kann bei Bedarf Quellenwissen aus **Wikipedia** und **ORKG** einbeziehen. Das unterscheidet die L√∂sung **sp√ºrbar** von reinen Text-Chat-Angeboten.\n\n## Datenschutz \u0026 Architektur\n\n* **Lokale Speicherung:** Sch√ºler- und Nutzerdaten bleiben im **Browser**; die Middleware leitet Anfragen **fl√ºchtig im RAM** weiter. Eine **persistente Server-Ablage** personenbezogener Inhalte findet nicht statt.\n* **TLS-gesicherte Verarbeitung:** Bei Cloud-Anbietern (z. B. Vertex, Mistral) erfolgt die √úbertragung **verschl√ºsselt**; es wird **kein Logging** personengebundener Inhalte erzwungen. Ein **DSGVO-konformer** Betrieb ist m√∂glich.\n* **Offen \u0026 modular:** Alle Bud-E-Varianten (School, Web, Desktop) basieren auf einer **flexiblen Client-Server-Architektur**. Komponenten wie **ASR**, **LLM**, **TTS** oder **Vision** sind austauschbar und k√∂nnen an Ihre Umgebung angepasst werden.\n\nAuf dieser Basis ‚Äì **lokale Speicherung**, **TLS-gesicherte Verarbeitung** und **offene, modulare Architektur** ‚Äì f√ºgt sich **Admin Bud-E** sauber in die Anforderungen der **Datenschutz-Grundverordnung (DSGVO)** und die **Leitlinien zur Nutzung von KI im Bildungsbereich** (LI Hamburg) ein. Weil viele kommerzielle Dienste **personenbezogene Daten** serverseitig **persistieren und weiterverarbeiten**, meiden Schulen und Beh√∂rden solche Systeme h√§ufig. **Admin Bud-E** l√§sst die Entscheidung **wo** Daten **gespeichert und verarbeitet** werden ‚Äì lokal, on-prem oder EU-Cloud ‚Äì in der **Organisation**. Das schafft **Vertrauen** und erm√∂glicht den **rechtskonformen** Einsatz von KI. Zudem kann die Schul-IT mit Admin Bud-E legal und dezentral handeln, statt aus Kostengr√ºnden auf nicht genehmigte Tools ohne API auszuweichen, wie es nach LI-Hamburg-Hinweisen mancherorts vorkommt.\n\n## Kernfunktionen in Admin Bud-E\n\n**Users (Nutzerverwaltung):**\nAdministratorinnen und Administratoren legen **Nutzerkonten** an, **aktivieren** sie und vergeben **individuelle Credits** ‚Äì also Nutzungsbudgets ‚Äì pro Person. **API-Schl√ºssel** lassen sich mit einem Klick **erzeugen**, **rotieren** und als **CSV** exportieren. Ein Klick auf eine Tabellenzeile √ºbernimmt die Daten bequem in die Eingabefelder, damit √Ñnderungen **schnell und fehlerarm** erfolgen.\n\n**Projects (Projekte \u0026 Budgets):**\nEin **Projekt** fasst eine organisatorische Einheit zusammen ‚Äì etwa eine **Abteilung**, eine **Fakult√§t** oder eine **Schule**. F√ºr jedes Projekt definieren Sie **Allowance-Intervalle** (t√§glich, w√∂chentlich oder monatlich), also wie oft sich pers√∂nliche Budgets automatisch erneuern. Optional aktivieren Sie den **Common Pool**: Nicht verbrauchte Credits von Personen, die wenig nutzen, flie√üen in einen **gemeinsamen Topf** und stehen dann anderen mit h√∂herem Bedarf **innerhalb desselben Projekts** zur Verf√ºgung. So entsteht **Fairness**, w√§hrend die **Gesamtausgaben** des Projekts **nie das definierte Budget √ºberschreiten**. Ein **Wizard** richtet auf Wunsch in einem Durchgang ein neues Projekt **inklusive N Nutzerinnen/Nutzern und Schl√ºsseln** ein. Exportfunktionen (z. B. `user_id, email, api_key`) und **Key-Rotation** sind integriert.\n\n**Pricing (Abrechnung \u0026 Einheiten):**\nDie Preislogik ist **klar und einheitlich**. **LLM/VLM-Dienste** werden pro **1 000 000 Tokens** gerechnet, wobei **Eingabe** und **Ausgabe** getrennt erfasst werden. **TTS** wird **pro Zeichen** abgerechnet, **ASR** vorzugsweise **pro Tokens** (Fallback: **pro Zeit**), sodass auch **Whisper-√§hnliche Endpunkte** korrekt erfasst sind. So ist f√ºr alle transparent, **wo Kosten entstehen** und **welche Stellschrauben** (Antwortl√§nge, TTS-Anteil) wirklich wirken.\n\n**Providers \u0026 Routes (Anbieter \u0026 Weiterschaltung):**\nSie hinterlegen **mehrere Anbieter** mit **Name**, **Basis-URL** und **API-Key**. F√ºr jeden Aufgabentyp ‚Äì **LLM/VLM**, **TTS**, **ASR** ‚Äì definieren Sie **Priorit√§tenketten**. F√§llt ein Dienst aus oder ist ausgelastet, greift automatisch ein **Failover** zur n√§chsten Route. Das Frontend kann einfach auf **‚Äûauto‚Äú** stehen. Ein **Reset-to-defaults** stellt die Standardkonfiguration wieder her. Merkhilfe: **Provider** sind die **Leitungen**, **Routes** die **Reihenfolge**, in der sie genutzt werden.\n\n**Usage (Nutzung \u0026 Exporte):**\nEine **transparente Nutzungs√ºbersicht** unterst√ºtzt **Audits** und **Kostenkontrolle**. Bei Bedarf exportieren oder restaurieren Sie den **SQLite-Datenstand** per Klick. Ein **Hard-Reset** ist m√∂glich, wenn eine Instanz vollst√§ndig neu aufgesetzt werden soll.\n\n\u003cimg width=\"1374\" height=\"588\" alt=\"image\" src=\"https://github.com/user-attachments/assets/6849a076-eead-4ed7-a4be-116436a1f950\" /\u003e\n\n## Kosten ‚Äì fair \u0026 planbar\n\nWenn eine Nachricht durch Bud-E l√§uft, passiert immer dasselbe in drei Schritten: Zuerst verwandelt die **Spracherkennung (ASR)** Gesprochenes in Text. Dann ‚Äûdenkt‚Äú das **Sprachmodell (LLM)** nach und formuliert eine Antwort. Auf Wunsch liest die **Vorlesestimme (TTS)** die Antwort h√∂rbar vor. Das **LLM** rechnet in **Tokens** ab ‚Äì kleine Textst√ºcke; ein deutsches Wort entspricht im Schnitt **1‚Äì2 Tokens**. **250 Tokens** sind grob **150‚Äì200 W√∂rter**, **500 Tokens** etwa **300‚Äì400 W√∂rter**. **TTS** rechnet **Zeichen** (Buchstaben) und wird bei langen Antworten zum **gr√∂√üten Kostenblock**, w√§hrend **ASR** sehr g√ºnstig bleibt und **LLM** moderat ist. Darum ist TTS im **Bud-E-Frontend** **standardm√§√üig aus**; stilles Lesen ist im Alltag h√§ufig schneller. F√ºr **kurze Hinweise** oder **motivierende R√ºckmeldungen** l√§sst sich TTS **gezielt einschalten**. Zur **Modellwahl**: Der Betrieb ist z. B. mit **Gemini Flash** und **Gemini Pro** √ºber die **Google Cloud (Vertex)** m√∂glich; Sie k√∂nnen EU-Regionen erzwingen und Trainingsspeicherung ausschlie√üen. Alternativ sind **Microsoft Azure**, **Mistral** oder **self-hosted** Modelle m√∂glich ‚Äì **Admin Bud-E** gibt die Wahl und die Steuerbarkeit.\n\n**Konkrete Gr√∂√üenordnung (Beispielrechnung mit Gemini + Chirp 3 HD):** Eine **kurze Antwort (~250 Tokens)** kostet **mit Vorlesen** etwa **$0.030‚Äì$0.034**; **ohne Vorlesen** fallen nur **ASR/LLM**-Anteile an (**‚âà $0.00019‚Äì$0.00351**, je nach Modellstufe). Auf **1 000 Nachrichten** pro Person umgerechnet: **ohne TTS** insgesamt **‚âà $0.19 (Flash-Lite)** bis **‚âà $3.51 (Pro)**; **mit TTS nur in 25 % der F√§lle** **‚âà $7.70‚Äì$11.00**. So bleiben die Kosten **transparent** und **steuerbar** ‚Äì √ºber **Antwortl√§nge** (Tokens) und **TTS-Anteil** (Zeichen).\n\n## Skalierung \u0026 Betrieb\n\n**Admin Bud-E** w√§chst mit Ihren Bed√ºrfnissen ‚Äì vom **Pilot mit zehn** Nutzenden bis zur **Instanz mit zehntausend**. Die Middleware bleibt **schlank**, da sie Anfragen √ºberwiegend **durchreicht**; oft gen√ºgt ein **einzelner, kosteng√ºnstiger EU-Server** (z. B. Hetzner). **Modelle und Anbieter** wechseln Sie bequem in der **Admin-UI**, **ohne** Client-Anpassungen ‚Äì so bleiben Sie technisch und finanziell **beweglich**.\n\n## Praxis: So l√§uft‚Äôs im Alltag\n\nDie IT setzt **Admin Bud-E** in einem **Nachmittag** auf, erzeugt **anonyme API-Schl√ºssel** und verteilt sie als **CSV** ‚Äì ohne pers√∂nliche Accounts. Nutzende f√ºgen den Schl√ºssel in den **Einstellungen** ein, laden **PDFs/Bilder** und erhalten **sofort Antworten** ‚Äì still lesbar oder auf Wunsch **vorgelesen**. **Allowance** (pers√∂nliches, automatisch erneuerbares Guthaben) und **Common Pool** (gemeinsamer Projekttopf) sorgen f√ºr **Fairness** und **Budgetstabilit√§t**. Ein praktischer Mehrwert: **Sprechen statt tippen**, **zuh√∂ren statt suchen** ‚Äì Bud-E nimmt **m√ºndliche Fragen** auf, versteht **Kontext** und meldet sich bei Bedarf **nat√ºrlich klingend** zur√ºck. Die Anbindung an **Wikipedia** und den **Open Research Knowledge Graph** unterst√ºtzt **Quellenarbeit** und **kriteriengeleitetes Recherchieren**.\n\n\u003cimg width=\"1484\" height=\"851\" alt=\"image\" src=\"https://github.com/user-attachments/assets/e2b2044c-ed0e-4dc8-a32e-0e88eb93d98e\" /\u003e\n\n## Ressourcen \u0026 Links\n\n* **School Bud-E Frontend (Live):** [https://school.bud-e.ai/](https://school.bud-e.ai/)\n* **Admin Bud-E Repository (GitHub):** [https://github.com/LAION-AI/Admin_Bud-E](https://github.com/LAION-AI/Admin_Bud-E)\n* **School Bud-E Frontend Repository (GitHub):** [https://github.com/LAION-AI/school-bud-e-frontend](https://github.com/LAION-AI/school-bud-e-frontend)\n* **Admin Bud-E Image (Hugging Face):** [https://huggingface.co/laion/Admin_Bud-E_image](https://huggingface.co/laion/Admin_Bud-E_image) ‚Äì fertiges Systemabbild f√ºr einen schnellen Start (z. B. bei [Contabo](https://contabo.com)).\n\n## Mitmachen\n\n\u003cimg width=\"1024\" height=\"1024\" alt=\"image\" src=\"https://github.com/user-attachments/assets/126d983a-febd-4afb-9808-ba02f4953631\" /\u003e\n\nWir laden **Schulen, Hochschulen, Unternehmen und √∂ffentliche Einrichtungen** ein, **Admin Bud-E** in der Praxis zu erproben, zu evaluieren und gemeinsam weiterzuentwickeln. Wir bieten **regelm√§√üig kostenlose Webinare** zum Einrichten von **Admin Bud-E** und **School Bud-E** f√ºr die eigene Organisation auf eigenen Servern an. Bei Interesse schreiben Sie uns bitte an **[contact@laion.ai](mailto:contact@laion.ai)**. Gemeinsam bringen wir **faire, offene und empathische KI-Assistenz** unter **Ihrer** Kontrolle in den Alltag.\n\n## Referenzen\n\n\u003col\u003e\n  \u003cli id=\"ref-voice-divide\"\u003e\n    Masina, F. et al. (2020). \u003cem\u003eInvestigating the Accessibility of Voice Assistants with Cognitive and Linguistic Tests\u003c/em\u003e. \u003cstrong\u003eJournal of Accessibility and Design for All\u003c/strong\u003e. \n    \u003ca href=\"https://doi.org/10.17411/jacces.v10i2.265\" target=\"_blank\" rel=\"noopener\"\u003ehttps://doi.org/10.17411/jacces.v10i2.265\u003c/a\u003e\u003cbr\u003e\n    Chemnad, K. et al. (2024). \u003cem\u003eDigital accessibility in the era of artificial intelligence ‚Äî a systematic review\u003c/em\u003e. \u003cstrong\u003ePLOS Digital Health\u003c/strong\u003e. \n    \u003ca href=\"https://doi.org/10.1371/journal.pdig.0000371\" target=\"_blank\" rel=\"noopener\"\u003ehttps://doi.org/10.1371/journal.pdig.0000371\u003c/a\u003e\n    \u0026nbsp;\u003ca href=\"#cite-voice-divide\"\u003e‚Ü©Ô∏é\u003c/a\u003e\n  \u003c/li\u003e\n  \u003cli id=\"ref-llm-evidence\"\u003e\n    Ma, W., Adesope, O. O., Nesbit, J. C., \u0026amp; Liu, Q. (2014). \u003cem\u003eIntelligent Tutoring Systems and Learning Outcomes: A Meta-Analysis\u003c/em\u003e. \u003cstrong\u003eReview of Educational Research\u003c/strong\u003e. \n    \u003ca href=\"https://doi.org/10.3102/0034654315581420\" target=\"_blank\" rel=\"noopener\"\u003ehttps://doi.org/10.3102/0034654315581420\u003c/a\u003e\u003cbr\u003e\n    Meyer, J. et al. (2024). \u003cem\u003eUsing LLMs to bring evidence-based feedback into the classroom\u003c/em\u003e. \u003cstrong\u003eComputers \u0026amp; Education: X\u003c/strong\u003e. (Preprint/open access, sofern verf√ºgbar.)\u003cbr\u003e\n    Zhang, K. et al. (2025). \u003cem\u003eEnhancing Critical Writing Through AI Feedback: A Randomized Controlled Trial\u003c/em\u003e. \u003cstrong\u003eBehavioral Sciences\u003c/strong\u003e, 15(5), 600.\n    \u0026nbsp;\u003ca href=\"#cite-llm-evidence\"\u003e‚Ü©Ô∏é\u003c/a\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n","date":1759968000000},{"slug":"leo-lm","frontmatter":{"title":"LeoLM: Ein Impuls f√ºr Deutschsprachige LLM-Forschung","author":"Bj√∂rn Pl√ºster","date":"September 28 2023","previewImg":"/images/blog/leolm-banner.jpg"},"content":"\nLernen Sie LeoLM kennen, das erste offen und kommerziell verf√ºgbare deutsche Foundation Language Model, das auf Llama-2 basiert.\nUnsere Modelle erweitern die F√§higkeiten von Llama-2 durch ein fortgesetztes Training auf einem gro√üen Korpus von hochwertigen deutschen und gr√∂√ütenteils lokal spezifischen Texten.\nDank eines Compute-Grants auf dem neuen Supercomputer **42** von [HessianAI](https://hessian.ai/) ver√∂ffentlichen wir zwei Foundation-Modelle, die mit einer Kontextl√§nge von 8k trainiert wurden,\n[`LeoLM/leo-hessianai-7b`](https://huggingface.co/LeoLM/leo-hessianai-7b) und [`LeoLM/leo-hessianai-13b`](https://huggingface.co/LeoLM/leo-hessianai-13b) (70b folgt auch bald! üëÄ) unter der [Llama-2 Community-Lizenz](https://ai.meta.com/llama/license/). Zus√§tzlich konstruieren wir einen Evaluierungssatz f√ºr Benchmarks zur √úberpr√ºfung der F√§higkeiten deutscher Sprachmodelle, um den Modellvergleich zu standardisieren, √§hnlich zu den weit verbreiteten auf Englisch basierten Evaluierungen, wie sie beispielsweise von [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) oder [LLM-Foundry](https://github.com/mosaicml/llm-foundry) bereitgestellt werden.\nMit dieser Ver√∂ffentlichung sind LAION und Hessian.AI bereit, die deutsche Open-Source und kommerzielle LLM-Forschung erheblich zu verbessern und so neue M√∂glichkeiten zu f√∂rdern und die weite Verbreitung zu beschleunigen.\n\nProbieren Sie [**LeoLM/leo-hessianai-7b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-7b-chat) und [**LeoLM/leo-hessianai-13b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-13b-chat) auf HuggingFace Spaces aus!\n\n*[[Auf Englisch lesen]](/blog/leo-lm)*\n\n## Einleitung\n\nSeit der Ver√∂ffentlichung der urspr√ºnglichen Llama Foundation Models \u003csup\u003e1\u003c/sup\u003e im Januar 2023 hat die Open-Source und wissenschaftliche Forschungsgemeinschaft\neine rasante Beschleunigung in der Entwicklung von immer f√§higeren Sprachmodellen erlebt. Die Fortschritte\nder letzten Wochen haben die leistungsf√§higsten Llama-2 \u003csup\u003e2\u003c/sup\u003e basierten Modelle n√§her an die Konkurrenz zu OpenAI's ChatGPT auf Basis von GPT-3.5 oder sogar dem st√§rkeren GPT4 gebracht.\nDennoch besteht eine bemerkenswerte Einschr√§nkung fort: Die Mehrheit dieser bahnbrechenden Fortschritte bleibt auf den Bereich der englischen Sprache beschr√§nkt.\nDiese Einschr√§nkung resultiert haupts√§chlich daraus, dass gro√üe Open-Source-Modelle √ºberwiegend auf monolingualen englischen Daten trainiert wurden. Obwohl es einige\nForschungen zum Zweitsprachen- oder Mehrsprachen-Finetuning gibt, sind die meisten resultierenden Modelle in ihren F√§higkeiten beschr√§nkt und leiden unter grammatikalischen Schw√§chen und der US-zentrischen Voreingenommenheit, die den englischen Daten inh√§rent ist.\n\nWir wollen diese Probleme im Fallbeispiel der deutschen Sprache durch die Anwendung vieler heutiger Spitzentechniken l√∂sen, um ein wirklich leistungsf√§higes,\nlokales und zweisprachiges LLM zu entwickeln.\nZu diesem Zweck pr√§sentieren wir LeoLM (**L**inguistisch **E**rweitertes **O**ffenes **L**anguage **M**odel), eine Suite von auf Llama-2 basierenden deutschen Foundation-\nModellen und eine Auswahl begleitender Feinabstimmungen.\nDes Weiteren pr√§sentieren wir GermanBench, eine Sammlung der relevantesten ins Deutsche √ºbersetzten englischen Benchmarks, die es uns erm√∂glichen in √§hnlichem Ausma√ü wie im Englischen, die F√§higkeiten von LeoLM gr√ºndlich zu bewerten.\n\n\u003csup\u003e1\u003c/sup\u003e: [Touvron et al. 2023a](https://arxiv.org/abs/2302.13971)\n\u003csup\u003e2\u003c/sup\u003e: [Touvron et al. 2023b](https://arxiv.org/abs/2307.09288)\n\n## Vorverarbeitung in Phase 2\n\nLlama-2-Modelle werden auf 2 Billionen Tokens √ºberwiegend englischen Textes vortrainiert. Um ihre Kompetenz in der deutschen Sprache zu erh√∂hen, verwenden wir ein fortgesetztes Vortraining, welches wir als \"Stage 2 Pretraining\" bezeichnen.\nWir initialisieren LeoLM mit Llama-2-Gewichten und setzen das Training des Modells auf einem gro√üen deutschen Textkorpus von 65 Milliarden Tokens fort, die rigoros gefiltert und dedupliziert wurden und gr√∂√ütenteils aus dem [OSCAR-2301-Korpus](https://huggingface.co/datasets/oscar-corpus/OSCAR-2301) stammen.\nEin wesentlicher Aspekt dieses Ansatzes besteht darin, das Vergessen oder den Verlust von zuvor erlerntem Wissen oder F√§higkeiten zu minimieren. Wir folgen den Erkenntnissen von [Gupta et al. (2023)](https://arxiv.org/abs/2308.04014) in unserer Wahl der Hyperparameter, um das Risiko des Vergessens zu minimieren.\nZus√§tzlich folgen wir der Arbeit von [Together](https://huggingface.co/togethercomputer/LLaMA-2-7B-32K) bei der Anwendung von [linearer RoPE-Skalierung](https://kaiokendev.github.io/til#extending-context-to-8k) und [Flash Attention 2](https://tridao.me/publications/flash2/flash2.pdf), um die Trainingseffizienz zu verbessern und die Kontextl√§nge auf 8k Tokens zu verdoppeln.\nSiehe Abbildung 1 f√ºr einen √úberblick √ºber alle Training-Hyperparameter.\n\n![training_parameters](/images/blog/training_params.png \"Training-Hyperparameters\")\n\n## Feinabstimmungsdatens√§tze\n\nEs gibt viel Diskussion dar√ºber, was ein guter Chat/Instruktionstuning-Datensatz bieten muss, was zur Entwicklung einer Vielzahl verschiedener, erfolgreicher Ans√§tze gef√ºhrt hat. Wir lassen uns von dieser Vielfalt inspirieren und √ºbersetzen, um √§hnliche F√§higkeiten auf Deutsch zu bringen, eine Auswahl hochwertiger Instruktionsdatens√§tze ins Deutsche mit OpenAI's `gpt-3.5-turbo` API. Die Verwendung von `gpt-3.5-turbo` stellt sicher, dass der Zusammenhang zwischen Aufforderungen und Antworten intakt bleibt und dass komplexe Anweisungen, die m√∂glicherweise Code, Gleichungen oder formatierte Daten enthalten, korrekt √ºbersetzt werden.\nAufbauend auf den Erkenntnissen der Community, w√§hlen wir eine Vielzahl von Datens√§tzen aus, die wir √ºbersetzen und f√ºr das Training unseres Chat-Modells verwenden.\nDie √ºbersetzten Datens√§tze sind:\n\n- [OpenPlatypus](https://huggingface.co/datasets/garage-bAInd/Open-Platypus) -\u003e [OpenSchnabeltier](https://huggingface.co/datasets/LeoLM/OpenSchnabeltier)\n- [OpenAssistant OASST1](https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25) -\u003e [OpenAssistant-DE](https://huggingface.co/datasets/OpenAssistant/OASST-DE)\n\nAu√üerdem verwenden wir [`FreedomIntelligence/evol-instruct-deutsch`](https://huggingface.co/datasets/FreedomIntelligence/evol-instruct-deutsch) und [`FreedomIntelligence/alpaca-gpt4-deutsch`](https://huggingface.co/datasets/FreedomIntelligence/alpaca-gpt4-deutsch) aus dem [MultilingualSIFT](https://github.com/FreedomIntelligence/MultilingualSIFT)-Projekt. Vielen Dank an die Autoren, dass sie ihre Daten geteilt haben!\nUm die zweisprachige Nutzung zu erleichtern, trainieren wir auch Modelle auf einer Kombination dieser √ºbersetzten Datens√§tze und ihren urspr√ºnglichen, englischen Gegenst√ºcken.\n\nSchlie√ülich erstellen wir, um Schw√§chen beim kreativen Schreiben und Reimen, die bei fr√ºhen Tests festgestellt wurden, auszugleichen, zwei weitere Datens√§tze:\n\n- [GPT4 Gedichte](https://huggingface.co/datasets/LeoLM/German_Poems): Eine Sammlung deutscher Gedichte zu verschiedenen Themen, geschrieben von GPT4\n- [GPT4 Lieder](https://huggingface.co/datasets/LeoLM/German_Songs): Eine Sammlung deutscher Lieder und nachfolgende Analysen, geschrieben von GPT4.\n\n## Evaluation und Ergebnisse\n\nDie Evaluierung der F√§higkeiten von LLMs, insbesondere von Chat-Modellen, ist komplex und die besten Methoden sind noch umstritten. Benchmarks, die auf Multiple-Choice basieren und anhand der Protokoll-Wahrscheinlichkeiten des Modells ausgewertet werden (wie im [Open LLM Leaderboard]()), sind eine derzeit beliebte Methode. Eine andere Methode bewertet Antworten automatisch mit GPT4, wie bei AlpacaEval oder MT-Bench. Dieser Ansatz richtet sich eher an Chat-Modelle, da er die Qualit√§t von Modellantworten in realen Aufgaben ber√ºcksichtigt. Um so vergleichbar wie m√∂glich zu sein, √ºbersetzen wir eine Reihe von englischen Benchmarks direkt ins Deutsche. Wir ver√∂ffentlichen diese Datens√§tze in unserer [HF-Organisation](https://huggingface.co/LeoLM) und mit ausf√ºhrlicher Dokumentation [auf GitHub](https://github.com/bjoernpl/GermanBenchmark), und Sie finden den entsprechende `lm-evaluation-harness`-Branch [hier](https://github.com/bjoernpl/lm-evaluation-harness-de/tree/mmlu_de) und den `FastEval`-Branch [hier](https://github.com/bjoernpl/FastEval).\n\nIn Abbildung 3 k√∂nnen Sie einen Vergleich von LeoLM gegen√ºber den Basis-Llama-2-Modellen auf einer Auswahl von Benchmarks mit sowohl der englischen Version (blau) als auch unserer √ºbersetzten Version (gr√ºn) sehen. Unser Trainging verbessert die Benchmark-Ergebnisse f√ºr die deutschen Aufgaben, w√§hrend die Ergebnisse f√ºr die englischen Aufgaben leicht reduziert werden. Bemerkenswert ist, dass der durchschnittliche Anstieg der deutschen Benchmark-Ergebnisse die durchschnittliche Abnahme der Leistung auf den englischen Benchmarks deutlich √ºberwiegt, was zeigt, dass unser Ansatz das Erlernen einer neuen Sprache erm√∂glicht, ohne zu vergessen, was zuvor gelernt wurde. Warum die Ergebnisse in Deutsch niedriger bleiben als in Englisch, ist eine offene Frage, kann aber zum Teil auf eine Qualit√§tsminderung bei der √úbersetzung zur√ºckgef√ºhrt werden.\n\n![](/images/blog/benchmarks.png)\n\nDie folgende Tabelle zeigt die Ergebnisse auf unserer √ºbersetzten Version von MT-Bench. MT-Bench ist ein Benchmark, der die Multi-Turn-Leistung auf einem kuratierten Satz von 80 Fragen aus mehreren Kategorien mit GPT-4 als Richter bewertet. Dabei bewertet GPT-4 die Aufforderungen auf einer Skala von 1-10 hinsichtlich der wahrgenommenen Hilfsbereitschaft, Relevanz, Genauigkeit, Tiefe, Kreativit√§t und Detailliertheit der Antwort. Das monolinguale Modell `leo-hessianai-13b-chat` schneidet insgesamt am besten ab und kommt sogar dem GPT-3.5 im Thema \"Geisteswissenschaften\" nahe. Es erzielt auffallend schlechte Ergebnisse in Mathematik und Codierung, was zu erwarten ist, da die Llama-2-Modelle in diesem Bereich ohne sehr explizites Finetuning von vornherein Schw√§chen aufweisen. Die zweisprachigen Modelle erzielen in einigen Kategorien wie Mathematik und Logik leicht unter ihren monolingualen Gegenst√ºcken, w√§hrend sie in Codierung und Extraktion √ºbertreffen.\n\n![](/images/blog/mt_bench.png)\nF√ºr eine detailliertere Evaluierung, bleiben Sie dran f√ºr unser Paper!\n\n## Qualitative Ergebnisse\n\nBenchmarks neigen dazu, ziemlich abstrakt zu sein. Um ein besseres Gef√ºhl f√ºr LeoLM's zu bekommen, schauen Sie sich unsere Demos an und probieren Sie es selbst aus: [**LeoLM/leo-hessianai-7b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-7b-chat) und den gr√∂√üeren Bruder [**LeoLM/leo-hessianai-13b-chat`**](https://huggingface.co/spaces/LeoLM/leo-hessianai-13b-chat). Alternativ k√∂nnen Sie das Modell selbst mit ü§óTransformers ausf√ºhren. Weitere Informationen zur Einrichtung finden Sie auf der [Modellkarte](https://huggingface.co/LeoLM/leo-hessianai-13b-chat).\n\n## Fazit\n\nUnsere Forschung hat mehrere Schl√ºsselbeitr√§ge:\n\n- Wir ver√∂ffentlichen eine Suite von deutschen Foundation-Sprachmodellen unter einer offenen Lizenz.\n- Wir √ºbertragen einen gr√ºndlichen und vielseitigen Evaluierungsansatz f√ºr Basis- und Chat-Modelle ins Deutsche.\n- Wir zeigen, dass eine gro√ü angelegte Fortbildung auch f√ºr datenges√§ttigte Modelle wie Llama-2 ohne signifikantes Vergessen oder Verlust von fr√ºheren F√§higkeiten m√∂glich ist.\n- Wir pr√§sentieren eine vielf√§ltige Suite von Instruktions-/Chat-Tuning-Datens√§tzen, die vom Englischen ins Deutsche √ºbersetzt wurden, um als Basis f√ºr die deutsche Open-Source-LLM-Forschungsgemeinschaft zu dienen.\n\nInsgesamt ist die LeoLM-Modellsuite ein Proof-of-Concept f√ºr den Spracherwerb f√ºr vortrainierte Modelle. Dar√ºber hinaus pr√§sentiert sie sich als das erste offen verf√ºgbare deutsche Foundation-Modell, das den heutigen Standards entspricht. Wir bei LAION hoffen, die deutsche Open-Source-Forschungsgemeinschaft ansto√üen zu k√∂nnen, um die Abh√§ngigkeit von geschlossenen kommerziellen Quellen wie OpenAI zu verringern. Viel Spa√ü mit LeoLM!\n\n## Danksagungen\n\nDieses Projekt wurde von Bj√∂rn Pl√ºster (Universit√§t Hamburg) geleitet mit Unterst√ºtzung, Anleitung und\nSupervision von Christoph Schuhmann (LAION), Patrick Schramowski (LAION, Hessian AI, DFKI) und Jenia Jitsev (LAION, Juelich Supercomputing Center, Helmholtz Research Center Juelich).\nWir danken den Leuten bei hessian.AI, insbesondere Wolfgang Stille, Kristian Kersting, Florian Kieser und Patrick Diem\nf√ºr ihre gro√üz√ºgige Rechenzeitvergabe √ºber das Hessian AI Innovation Lab (gef√∂rdet vom Hessischen Ministerium f√ºr Digitale Strategie und Entwicklung) und das AI Service Center hessian.AISC (gef√∂rdert vom Bundesministerium f√ºr Bildung und Forschung (BMBF)), und Unterst√ºtzung w√§hrend des gesamten Trainingsprozesses!  Vielen Dank auch an Jae Hee Lee von der Universit√§t Hamburg, Andreas K√∂pf von Open-Assistant und Community-Mitglieder bei LAION f√ºr ihr hilfreiches Feedback. Weiterhin danken wir dem hessian.AI's Innovationslabor, dem AI Servicezentrum hessian.AISC und dem Deutschen Zentrum f√ºr K√ºnstliche Intelligenz (DFKI) f√ºr ihre Unterst√ºtzung.\n\nAu√üerdem danken wir den Gruppen, die die Datens√§tze f√ºr unser Chat-finetuning urspr√ºnglich erstellt haben: [OpenAssistant](https://huggingface.co/OpenAssistant), [FreedomIntelligence](https://huggingface.co/FreedomIntelligence), [garage-bAInd](https://huggingface.co/garage-bAInd), [WizardLM](https://huggingface.co/WizardLM) and [Instruction-Tuning-with-GPT-4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM).\n\n---\n\n### An Example\n\n**User:**  Warum ist open-source besonders in der KI-Forschung wichtig?\n\n**LeoLM:**\nOpen-Source spielt eine wichtige Rolle in der KI-Forschung, da sie Zusammenarbeit, Innovation und Zugang zur Technologie f√∂rdert. Hier sind einige Gr√ºnde, warum Open-Source in der KI-Forschung besonders wichtig ist:\n\n- Zusammenarbeit: Durch Open-Source k√∂nnen Forscher gemeinsam an Projekten arbeiten, Ideen austauschen und voneinander lernen. Dies f√∂rdert die Zusammenarbeit zwischen Forschern und kann zu schnelleren und effizienteren Fortschritten in der KI-Forschung f√ºhren.\n\n- Transparenz: Open-Source erm√∂glicht Transparenz und Reproduzierbarkeit in der KI-Forschung. Forscher k√∂nnen den Code und die Methoden anderer sehen, reproduzieren und darauf aufbauen, was die Validierung von Ergebnissen und die Identifizierung potenzieller Schwachstellen erleichtert.\n\n- Zugang zur Technologie: Open-Source macht KI-Technologien und -Werkzeuge einem breiteren Publikum zug√§nglich. Dies erm√∂glicht kleineren Forschungseinrichtungen und Einzelpersonen den Zugang zu KI-Werkzeugen, die ihnen sonst m√∂glicherweise nicht zur Verf√ºgung st√ºnden. Dadurch kann Innovation und Forschung auch au√üerhalb traditioneller Zentren gef√∂rdert werden.\n\n- Innovation: Open-Source f√∂rdert Innovation in der KI-Forschung, indem sie Raum f√ºr Experimente und neue Ideen bietet. Da der Code offen und f√ºr alle zug√§nglich ist, k√∂nnen Forscher ihn an ihre spezifischen Bed√ºrfnisse anpassen und neue KI-Werkzeuge und -Technologien entwickeln.\n\n- Lernen: Open-Source erm√∂glicht es Forschern, voneinander zu lernen. Durch die gemeinsame Nutzung von Code und Methoden k√∂nnen Forscher aus den Erfahrungen anderer lernen und ihre eigenen Forschungsprojekte verbessern.\n\nZusammenfassend ist Open-Source ein wichtiger Aspekt der KI-Forschung, da er Zusammenarbeit, Transparenz, Zugang zur Technologie, Innovation und Lernen f√∂rdert. Dadurch wird die KI-Forschung effizienter, innovativer und zug√§nglicher f√ºr eine breitere Palette von Forschern und Institutionen\n\n| [Link zu einer PDF mit mehr Beispielen](/documents/leo-lm.pdf) |\n|----------|\n","date":1695859200000}]},"__N_SSG":true},"page":"/blog-de","query":{},"buildId":"R99kgm_HfSIL7_rSdnHLT","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>