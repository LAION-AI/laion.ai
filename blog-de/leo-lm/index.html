<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>LeoLM: Ein Impuls für Deutschsprachige LLM-Forschung | LAION</title><meta name="title" content="LeoLM: Ein Impuls für Deutschsprachige LLM-Forschung | LAION"/><meta property="og:title" content="LeoLM: Ein Impuls für Deutschsprachige LLM-Forschung | LAION"/><meta name="twitter:title" content="LeoLM: Ein Impuls für Deutschsprachige LLM-Forschung | LAION"/><meta name="description" content="&lt;p&gt;Lernen Sie LeoLM kennen, das erste offen und kommerziell verfügbare deutsche Foundation Language Model, das auf Llama-2 basiert.
Unsere Modelle erweitern ..."/><meta property="og:description" content="&lt;p&gt;Lernen Sie LeoLM kennen, das erste offen und kommerziell verfügbare deutsche Foundation Language Model, das auf Llama-2 basiert.
Unsere Modelle erweitern ..."/><meta name="twitter:description" content="&lt;p&gt;Lernen Sie LeoLM kennen, das erste offen und kommerziell verfügbare deutsche Foundation Language Model, das auf Llama-2 basiert.
Unsere Modelle erweitern ..."/><meta property="og:image" content="https://laion.ai/images/blog/leolm-banner.jpg"/><meta name="twitter:image" content="https://laion.ai/images/blog/leolm-banner.jpg"/><meta name="twitter:image:alt" content="The text: LAION. Large-scale Artificial Intelligence Open Network, TRULY OPEN AI. 100% NON-PROFIT. 100% FREE."/><meta property="og:type" content="website"/><meta property="og:url" content="https://laion.ai/blog-de/leo-lm"/><meta name="twitter:url" content="https://laion.ai/blog-de/leo-lm"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1D374E"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.png"/><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff2" as="font" type="font/woff2" crossorigin="true"/><meta name="next-head-count" content="25"/><link rel="stylesheet" href="/fonts/load.css"/><link rel="preload" href="/_next/static/css/d887f485c70dd7b7.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d887f485c70dd7b7.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fb0512e25146295.js" defer=""></script><script src="/_next/static/chunks/286-30519d8a3e60551d.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/pages/blog-de/%5Bslug%5D-9b92a19caa39d7d8.js" defer=""></script><script src="/_next/static/aiqu4Uyy9_dWymlMLbsHv/_buildManifest.js" defer=""></script><script src="/_next/static/aiqu4Uyy9_dWymlMLbsHv/_ssgManifest.js" defer=""></script><script src="/_next/static/aiqu4Uyy9_dWymlMLbsHv/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-paper md:text-sky md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LAION</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/team/">Team</a><a href="/blog/">Blog</a><a href="/notes/">Notes</a><a href="/press/">Press</a><a href="/about/">About</a><a href="/faq/">FAQ</a><a href="/donations/">Donations</a><a href="/privacy-policy/">Privacy Policy</a><a href="/dataset-requests/">Dataset Requests</a><a href="/impressum/">Impressum</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1D374E;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/team/">Team</a></p><p><a href="/blog/">Blog</a></p><p><a href="/notes/">Notes</a></p><p><a href="/press/">Press</a></p><p><a href="/about/">About</a></p><p><a href="/faq/">FAQ</a></p><p><a href="/donations/">Donations</a></p><p><a href="/privacy-policy/">Privacy Policy</a></p><p><a href="/dataset-requests/">Dataset Requests</a></p><p><a href="/impressum/">Impressum</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-paper grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-8xl md:text-8xl w-full font-bold title-flow break-words">LEOLM: EIN IMPULS FÜR DEUTSCHSPRACHIGE LLM-FORSCHUNG</h1><p class="text-2xl pb-2">by: <!-- -->Björn Plüster<!-- -->,<!-- --> <!-- -->28 Sep, 2023<!-- --></p><hr/><div class="pt-2 article"><p>Lernen Sie LeoLM kennen, das erste offen und kommerziell verfügbare deutsche Foundation Language Model, das auf Llama-2 basiert.
Unsere Modelle erweitern die Fähigkeiten von Llama-2 durch ein fortgesetztes Training auf einem großen Korpus von hochwertigen deutschen und größtenteils lokal spezifischen Texten.
Dank eines Compute-Grants auf dem neuen Supercomputer <strong>42</strong> von <a href="https://hessian.ai/">HessianAI</a> veröffentlichen wir zwei Foundation-Modelle, die mit einer Kontextlänge von 8k trainiert wurden,
<a href="https://huggingface.co/LeoLM/leo-hessianai-7b"><code>LeoLM/leo-hessianai-7b</code></a> und <a href="https://huggingface.co/LeoLM/leo-hessianai-13b"><code>LeoLM/leo-hessianai-13b</code></a> (70b folgt auch bald! 👀) unter der <a href="https://ai.meta.com/llama/license/">Llama-2 Community-Lizenz</a>. Zusätzlich konstruieren wir einen Evaluierungssatz für Benchmarks zur Überprüfung der Fähigkeiten deutscher Sprachmodelle, um den Modellvergleich zu standardisieren, ähnlich zu den weit verbreiteten auf Englisch basierten Evaluierungen, wie sie beispielsweise von <a href="https://github.com/EleutherAI/lm-evaluation-harness">lm-evaluation-harness</a> oder <a href="https://github.com/mosaicml/llm-foundry">LLM-Foundry</a> bereitgestellt werden.
Mit dieser Veröffentlichung sind LAION und Hessian.AI bereit, die deutsche Open-Source und kommerzielle LLM-Forschung erheblich zu verbessern und so neue Möglichkeiten zu fördern und die weite Verbreitung zu beschleunigen.</p>
<p>Probieren Sie <a href="https://huggingface.co/spaces/LeoLM/leo-hessianai-7b-chat"><strong>LeoLM/leo-hessianai-7b-chat</strong></a> und <a href="https://huggingface.co/spaces/LeoLM/leo-hessianai-13b-chat"><strong>LeoLM/leo-hessianai-13b-chat</strong></a> auf HuggingFace Spaces aus!</p>
<p><em><a href="/blog/leo-lm">[Auf Englisch lesen]</a></em></p>
<h2><a id="einleitung" class="anchor" href="#einleitung" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Einleitung</h2>
<p>Seit der Veröffentlichung der ursprünglichen Llama Foundation Models <sup>1</sup> im Januar 2023 hat die Open-Source und wissenschaftliche Forschungsgemeinschaft
eine rasante Beschleunigung in der Entwicklung von immer fähigeren Sprachmodellen erlebt. Die Fortschritte
der letzten Wochen haben die leistungsfähigsten Llama-2 <sup>2</sup> basierten Modelle näher an die Konkurrenz zu OpenAI's ChatGPT auf Basis von GPT-3.5 oder sogar dem stärkeren GPT4 gebracht.
Dennoch besteht eine bemerkenswerte Einschränkung fort: Die Mehrheit dieser bahnbrechenden Fortschritte bleibt auf den Bereich der englischen Sprache beschränkt.
Diese Einschränkung resultiert hauptsächlich daraus, dass große Open-Source-Modelle überwiegend auf monolingualen englischen Daten trainiert wurden. Obwohl es einige
Forschungen zum Zweitsprachen- oder Mehrsprachen-Finetuning gibt, sind die meisten resultierenden Modelle in ihren Fähigkeiten beschränkt und leiden unter grammatikalischen Schwächen und der US-zentrischen Voreingenommenheit, die den englischen Daten inhärent ist.</p>
<p>Wir wollen diese Probleme im Fallbeispiel der deutschen Sprache durch die Anwendung vieler heutiger Spitzentechniken lösen, um ein wirklich leistungsfähiges,
lokales und zweisprachiges LLM zu entwickeln.
Zu diesem Zweck präsentieren wir LeoLM (<strong>L</strong>inguistisch <strong>E</strong>rweitertes <strong>O</strong>ffenes <strong>L</strong>anguage <strong>M</strong>odel), eine Suite von auf Llama-2 basierenden deutschen Foundation-
Modellen und eine Auswahl begleitender Feinabstimmungen.
Des Weiteren präsentieren wir GermanBench, eine Sammlung der relevantesten ins Deutsche übersetzten englischen Benchmarks, die es uns ermöglichen in ähnlichem Ausmaß wie im Englischen, die Fähigkeiten von LeoLM gründlich zu bewerten.</p>
<p><sup>1</sup>: <a href="https://arxiv.org/abs/2302.13971">Touvron et al. 2023a</a>
<sup>2</sup>: <a href="https://arxiv.org/abs/2307.09288">Touvron et al. 2023b</a></p>
<h2><a id="vorverarbeitung-in-phase-2" class="anchor" href="#vorverarbeitung-in-phase-2" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Vorverarbeitung in Phase 2</h2>
<p>Llama-2-Modelle werden auf 2 Billionen Tokens überwiegend englischen Textes vortrainiert. Um ihre Kompetenz in der deutschen Sprache zu erhöhen, verwenden wir ein fortgesetztes Vortraining, welches wir als &quot;Stage 2 Pretraining&quot; bezeichnen.
Wir initialisieren LeoLM mit Llama-2-Gewichten und setzen das Training des Modells auf einem großen deutschen Textkorpus von 65 Milliarden Tokens fort, die rigoros gefiltert und dedupliziert wurden und größtenteils aus dem <a href="https://huggingface.co/datasets/oscar-corpus/OSCAR-2301">OSCAR-2301-Korpus</a> stammen.
Ein wesentlicher Aspekt dieses Ansatzes besteht darin, das Vergessen oder den Verlust von zuvor erlerntem Wissen oder Fähigkeiten zu minimieren. Wir folgen den Erkenntnissen von <a href="https://arxiv.org/abs/2308.04014">Gupta et al. (2023)</a> in unserer Wahl der Hyperparameter, um das Risiko des Vergessens zu minimieren.
Zusätzlich folgen wir der Arbeit von <a href="https://huggingface.co/togethercomputer/LLaMA-2-7B-32K">Together</a> bei der Anwendung von <a href="https://kaiokendev.github.io/til#extending-context-to-8k">linearer RoPE-Skalierung</a> und <a href="https://tridao.me/publications/flash2/flash2.pdf">Flash Attention 2</a>, um die Trainingseffizienz zu verbessern und die Kontextlänge auf 8k Tokens zu verdoppeln.
Siehe Abbildung 1 für einen Überblick über alle Training-Hyperparameter.</p>
<p><img src="/images/blog/training_params.png" alt="training_parameters" title="Training-Hyperparameters"></p>
<h2><a id="feinabstimmungsdatensätze" class="anchor" href="#feinabstimmungsdatensätze" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Feinabstimmungsdatensätze</h2>
<p>Es gibt viel Diskussion darüber, was ein guter Chat/Instruktionstuning-Datensatz bieten muss, was zur Entwicklung einer Vielzahl verschiedener, erfolgreicher Ansätze geführt hat. Wir lassen uns von dieser Vielfalt inspirieren und übersetzen, um ähnliche Fähigkeiten auf Deutsch zu bringen, eine Auswahl hochwertiger Instruktionsdatensätze ins Deutsche mit OpenAI's <code>gpt-3.5-turbo</code> API. Die Verwendung von <code>gpt-3.5-turbo</code> stellt sicher, dass der Zusammenhang zwischen Aufforderungen und Antworten intakt bleibt und dass komplexe Anweisungen, die möglicherweise Code, Gleichungen oder formatierte Daten enthalten, korrekt übersetzt werden.
Aufbauend auf den Erkenntnissen der Community, wählen wir eine Vielzahl von Datensätzen aus, die wir übersetzen und für das Training unseres Chat-Modells verwenden.
Die übersetzten Datensätze sind:</p>
<ul>
<li><a href="https://huggingface.co/datasets/garage-bAInd/Open-Platypus">OpenPlatypus</a> -&gt; <a href="https://huggingface.co/datasets/LeoLM/OpenSchnabeltier">OpenSchnabeltier</a></li>
<li><a href="https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25">OpenAssistant OASST1</a> -&gt; <a href="https://huggingface.co/datasets/OpenAssistant/OASST-DE">OpenAssistant-DE</a></li>
</ul>
<p>Außerdem verwenden wir <a href="https://huggingface.co/datasets/FreedomIntelligence/evol-instruct-deutsch"><code>FreedomIntelligence/evol-instruct-deutsch</code></a> und <a href="https://huggingface.co/datasets/FreedomIntelligence/alpaca-gpt4-deutsch"><code>FreedomIntelligence/alpaca-gpt4-deutsch</code></a> aus dem <a href="https://github.com/FreedomIntelligence/MultilingualSIFT">MultilingualSIFT</a>-Projekt. Vielen Dank an die Autoren, dass sie ihre Daten geteilt haben!
Um die zweisprachige Nutzung zu erleichtern, trainieren wir auch Modelle auf einer Kombination dieser übersetzten Datensätze und ihren ursprünglichen, englischen Gegenstücken.</p>
<p>Schließlich erstellen wir, um Schwächen beim kreativen Schreiben und Reimen, die bei frühen Tests festgestellt wurden, auszugleichen, zwei weitere Datensätze:</p>
<ul>
<li><a href="https://huggingface.co/datasets/LeoLM/German_Poems">GPT4 Gedichte</a>: Eine Sammlung deutscher Gedichte zu verschiedenen Themen, geschrieben von GPT4</li>
<li><a href="https://huggingface.co/datasets/LeoLM/German_Songs">GPT4 Lieder</a>: Eine Sammlung deutscher Lieder und nachfolgende Analysen, geschrieben von GPT4.</li>
</ul>
<h2><a id="evaluation-und-ergebnisse" class="anchor" href="#evaluation-und-ergebnisse" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Evaluation und Ergebnisse</h2>
<p>Die Evaluierung der Fähigkeiten von LLMs, insbesondere von Chat-Modellen, ist komplex und die besten Methoden sind noch umstritten. Benchmarks, die auf Multiple-Choice basieren und anhand der Protokoll-Wahrscheinlichkeiten des Modells ausgewertet werden (wie im <a href="">Open LLM Leaderboard</a>), sind eine derzeit beliebte Methode. Eine andere Methode bewertet Antworten automatisch mit GPT4, wie bei AlpacaEval oder MT-Bench. Dieser Ansatz richtet sich eher an Chat-Modelle, da er die Qualität von Modellantworten in realen Aufgaben berücksichtigt. Um so vergleichbar wie möglich zu sein, übersetzen wir eine Reihe von englischen Benchmarks direkt ins Deutsche. Wir veröffentlichen diese Datensätze in unserer <a href="https://huggingface.co/LeoLM">HF-Organisation</a> und mit ausführlicher Dokumentation <a href="https://github.com/bjoernpl/GermanBenchmark">auf GitHub</a>, und Sie finden den entsprechende <code>lm-evaluation-harness</code>-Branch <a href="https://github.com/bjoernpl/lm-evaluation-harness-de/tree/mmlu_de">hier</a> und den <code>FastEval</code>-Branch <a href="https://github.com/bjoernpl/FastEval">hier</a>.</p>
<p>In Abbildung 3 können Sie einen Vergleich von LeoLM gegenüber den Basis-Llama-2-Modellen auf einer Auswahl von Benchmarks mit sowohl der englischen Version (blau) als auch unserer übersetzten Version (grün) sehen. Unser Trainging verbessert die Benchmark-Ergebnisse für die deutschen Aufgaben, während die Ergebnisse für die englischen Aufgaben leicht reduziert werden. Bemerkenswert ist, dass der durchschnittliche Anstieg der deutschen Benchmark-Ergebnisse die durchschnittliche Abnahme der Leistung auf den englischen Benchmarks deutlich überwiegt, was zeigt, dass unser Ansatz das Erlernen einer neuen Sprache ermöglicht, ohne zu vergessen, was zuvor gelernt wurde. Warum die Ergebnisse in Deutsch niedriger bleiben als in Englisch, ist eine offene Frage, kann aber zum Teil auf eine Qualitätsminderung bei der Übersetzung zurückgeführt werden.</p>
<p><img src="/images/blog/benchmarks.png" alt=""></p>
<p>Die folgende Tabelle zeigt die Ergebnisse auf unserer übersetzten Version von MT-Bench. MT-Bench ist ein Benchmark, der die Multi-Turn-Leistung auf einem kuratierten Satz von 80 Fragen aus mehreren Kategorien mit GPT-4 als Richter bewertet. Dabei bewertet GPT-4 die Aufforderungen auf einer Skala von 1-10 hinsichtlich der wahrgenommenen Hilfsbereitschaft, Relevanz, Genauigkeit, Tiefe, Kreativität und Detailliertheit der Antwort. Das monolinguale Modell <code>leo-hessianai-13b-chat</code> schneidet insgesamt am besten ab und kommt sogar dem GPT-3.5 im Thema &quot;Geisteswissenschaften&quot; nahe. Es erzielt auffallend schlechte Ergebnisse in Mathematik und Codierung, was zu erwarten ist, da die Llama-2-Modelle in diesem Bereich ohne sehr explizites Finetuning von vornherein Schwächen aufweisen. Die zweisprachigen Modelle erzielen in einigen Kategorien wie Mathematik und Logik leicht unter ihren monolingualen Gegenstücken, während sie in Codierung und Extraktion übertreffen.</p>
<p><img src="/images/blog/mt_bench.png" alt="">
Für eine detailliertere Evaluierung, bleiben Sie dran für unser Paper!</p>
<h2><a id="qualitative-ergebnisse" class="anchor" href="#qualitative-ergebnisse" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Qualitative Ergebnisse</h2>
<p>Benchmarks neigen dazu, ziemlich abstrakt zu sein. Um ein besseres Gefühl für LeoLM's zu bekommen, schauen Sie sich unsere Demos an und probieren Sie es selbst aus: <a href="https://huggingface.co/spaces/LeoLM/leo-hessianai-7b-chat"><strong>LeoLM/leo-hessianai-7b-chat</strong></a> und den größeren Bruder <a href="https://huggingface.co/spaces/LeoLM/leo-hessianai-13b-chat"><strong>LeoLM/leo-hessianai-13b-chat`</strong></a>. Alternativ können Sie das Modell selbst mit 🤗Transformers ausführen. Weitere Informationen zur Einrichtung finden Sie auf der <a href="https://huggingface.co/LeoLM/leo-hessianai-13b-chat">Modellkarte</a>.</p>
<h2><a id="fazit" class="anchor" href="#fazit" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fazit</h2>
<p>Unsere Forschung hat mehrere Schlüsselbeiträge:</p>
<ul>
<li>Wir veröffentlichen eine Suite von deutschen Foundation-Sprachmodellen unter einer offenen Lizenz.</li>
<li>Wir übertragen einen gründlichen und vielseitigen Evaluierungsansatz für Basis- und Chat-Modelle ins Deutsche.</li>
<li>Wir zeigen, dass eine groß angelegte Fortbildung auch für datengesättigte Modelle wie Llama-2 ohne signifikantes Vergessen oder Verlust von früheren Fähigkeiten möglich ist.</li>
<li>Wir präsentieren eine vielfältige Suite von Instruktions-/Chat-Tuning-Datensätzen, die vom Englischen ins Deutsche übersetzt wurden, um als Basis für die deutsche Open-Source-LLM-Forschungsgemeinschaft zu dienen.</li>
</ul>
<p>Insgesamt ist die LeoLM-Modellsuite ein Proof-of-Concept für den Spracherwerb für vortrainierte Modelle. Darüber hinaus präsentiert sie sich als das erste offen verfügbare deutsche Foundation-Modell, das den heutigen Standards entspricht. Wir bei LAION hoffen, die deutsche Open-Source-Forschungsgemeinschaft anstoßen zu können, um die Abhängigkeit von geschlossenen kommerziellen Quellen wie OpenAI zu verringern. Viel Spaß mit LeoLM!</p>
<h2><a id="danksagungen" class="anchor" href="#danksagungen" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Danksagungen</h2>
<p>Dieses Projekt wurde von Björn Plüster (Universität Hamburg) geleitet mit Unterstützung, Anleitung und
Supervision von Christoph Schuhmann (LAION), Patrick Schramowski (LAION, Hessian AI, DFKI) und Jenia Jitsev (LAION, Juelich Supercomputing Center, Helmholtz Research Center Juelich).
Wir danken den Leuten bei hessian.AI, insbesondere Wolfgang Stille, Kristian Kersting, Florian Kieser und Patrick Diem
für ihre großzügige Rechenzeitvergabe über das Hessian AI Innovation Lab (gefördet vom Hessischen Ministerium für Digitale Strategie und Entwicklung) und das AI Service Center hessian.AISC (gefördert vom Bundesministerium für Bildung und Forschung (BMBF)), und Unterstützung während des gesamten Trainingsprozesses!  Vielen Dank auch an Jae Hee Lee von der Universität Hamburg, Andreas Köpf von Open-Assistant und Community-Mitglieder bei LAION für ihr hilfreiches Feedback. Weiterhin danken wir dem hessian.AI's Innovationslabor, dem AI Servicezentrum hessian.AISC und dem Deutschen Zentrum für Künstliche Intelligenz (DFKI) für ihre Unterstützung.</p>
<p>Außerdem danken wir den Gruppen, die die Datensätze für unser Chat-finetuning ursprünglich erstellt haben: <a href="https://huggingface.co/OpenAssistant">OpenAssistant</a>, <a href="https://huggingface.co/FreedomIntelligence">FreedomIntelligence</a>, <a href="https://huggingface.co/garage-bAInd">garage-bAInd</a>, <a href="https://huggingface.co/WizardLM">WizardLM</a> and <a href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM">Instruction-Tuning-with-GPT-4</a>.</p>
<hr>
<h3><a id="an-example" class="anchor" href="#an-example" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>An Example</h3>
<p><strong>User:</strong>  Warum ist open-source besonders in der KI-Forschung wichtig?</p>
<p><strong>LeoLM:</strong>
Open-Source spielt eine wichtige Rolle in der KI-Forschung, da sie Zusammenarbeit, Innovation und Zugang zur Technologie fördert. Hier sind einige Gründe, warum Open-Source in der KI-Forschung besonders wichtig ist:</p>
<ul>
<li>
<p>Zusammenarbeit: Durch Open-Source können Forscher gemeinsam an Projekten arbeiten, Ideen austauschen und voneinander lernen. Dies fördert die Zusammenarbeit zwischen Forschern und kann zu schnelleren und effizienteren Fortschritten in der KI-Forschung führen.</p>
</li>
<li>
<p>Transparenz: Open-Source ermöglicht Transparenz und Reproduzierbarkeit in der KI-Forschung. Forscher können den Code und die Methoden anderer sehen, reproduzieren und darauf aufbauen, was die Validierung von Ergebnissen und die Identifizierung potenzieller Schwachstellen erleichtert.</p>
</li>
<li>
<p>Zugang zur Technologie: Open-Source macht KI-Technologien und -Werkzeuge einem breiteren Publikum zugänglich. Dies ermöglicht kleineren Forschungseinrichtungen und Einzelpersonen den Zugang zu KI-Werkzeugen, die ihnen sonst möglicherweise nicht zur Verfügung stünden. Dadurch kann Innovation und Forschung auch außerhalb traditioneller Zentren gefördert werden.</p>
</li>
<li>
<p>Innovation: Open-Source fördert Innovation in der KI-Forschung, indem sie Raum für Experimente und neue Ideen bietet. Da der Code offen und für alle zugänglich ist, können Forscher ihn an ihre spezifischen Bedürfnisse anpassen und neue KI-Werkzeuge und -Technologien entwickeln.</p>
</li>
<li>
<p>Lernen: Open-Source ermöglicht es Forschern, voneinander zu lernen. Durch die gemeinsame Nutzung von Code und Methoden können Forscher aus den Erfahrungen anderer lernen und ihre eigenen Forschungsprojekte verbessern.</p>
</li>
</ul>
<p>Zusammenfassend ist Open-Source ein wichtiger Aspekt der KI-Forschung, da er Zusammenarbeit, Transparenz, Zugang zur Technologie, Innovation und Lernen fördert. Dadurch wird die KI-Forschung effizienter, innovativer und zugänglicher für eine breitere Palette von Forschern und Institutionen</p>
<table>
<thead>
<tr>
<th><a href="/documents/leo-lm.pdf">Link zu einer PDF mit mehr Beispielen</a></th>
</tr>
</thead>
</table>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"LeoLM: Ein Impuls für Deutschsprachige LLM-Forschung","author":"Björn Plüster","date":"September 28 2023","previewImg":"/images/blog/leolm-banner.jpg"},"content":"\nLernen Sie LeoLM kennen, das erste offen und kommerziell verfügbare deutsche Foundation Language Model, das auf Llama-2 basiert.\nUnsere Modelle erweitern die Fähigkeiten von Llama-2 durch ein fortgesetztes Training auf einem großen Korpus von hochwertigen deutschen und größtenteils lokal spezifischen Texten.\nDank eines Compute-Grants auf dem neuen Supercomputer **42** von [HessianAI](https://hessian.ai/) veröffentlichen wir zwei Foundation-Modelle, die mit einer Kontextlänge von 8k trainiert wurden,\n[`LeoLM/leo-hessianai-7b`](https://huggingface.co/LeoLM/leo-hessianai-7b) und [`LeoLM/leo-hessianai-13b`](https://huggingface.co/LeoLM/leo-hessianai-13b) (70b folgt auch bald! 👀) unter der [Llama-2 Community-Lizenz](https://ai.meta.com/llama/license/). Zusätzlich konstruieren wir einen Evaluierungssatz für Benchmarks zur Überprüfung der Fähigkeiten deutscher Sprachmodelle, um den Modellvergleich zu standardisieren, ähnlich zu den weit verbreiteten auf Englisch basierten Evaluierungen, wie sie beispielsweise von [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) oder [LLM-Foundry](https://github.com/mosaicml/llm-foundry) bereitgestellt werden.\nMit dieser Veröffentlichung sind LAION und Hessian.AI bereit, die deutsche Open-Source und kommerzielle LLM-Forschung erheblich zu verbessern und so neue Möglichkeiten zu fördern und die weite Verbreitung zu beschleunigen.\n\nProbieren Sie [**LeoLM/leo-hessianai-7b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-7b-chat) und [**LeoLM/leo-hessianai-13b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-13b-chat) auf HuggingFace Spaces aus!\n\n*[[Auf Englisch lesen]](/blog/leo-lm)*\n\n## Einleitung\n\nSeit der Veröffentlichung der ursprünglichen Llama Foundation Models \u003csup\u003e1\u003c/sup\u003e im Januar 2023 hat die Open-Source und wissenschaftliche Forschungsgemeinschaft\neine rasante Beschleunigung in der Entwicklung von immer fähigeren Sprachmodellen erlebt. Die Fortschritte\nder letzten Wochen haben die leistungsfähigsten Llama-2 \u003csup\u003e2\u003c/sup\u003e basierten Modelle näher an die Konkurrenz zu OpenAI's ChatGPT auf Basis von GPT-3.5 oder sogar dem stärkeren GPT4 gebracht.\nDennoch besteht eine bemerkenswerte Einschränkung fort: Die Mehrheit dieser bahnbrechenden Fortschritte bleibt auf den Bereich der englischen Sprache beschränkt.\nDiese Einschränkung resultiert hauptsächlich daraus, dass große Open-Source-Modelle überwiegend auf monolingualen englischen Daten trainiert wurden. Obwohl es einige\nForschungen zum Zweitsprachen- oder Mehrsprachen-Finetuning gibt, sind die meisten resultierenden Modelle in ihren Fähigkeiten beschränkt und leiden unter grammatikalischen Schwächen und der US-zentrischen Voreingenommenheit, die den englischen Daten inhärent ist.\n\nWir wollen diese Probleme im Fallbeispiel der deutschen Sprache durch die Anwendung vieler heutiger Spitzentechniken lösen, um ein wirklich leistungsfähiges,\nlokales und zweisprachiges LLM zu entwickeln.\nZu diesem Zweck präsentieren wir LeoLM (**L**inguistisch **E**rweitertes **O**ffenes **L**anguage **M**odel), eine Suite von auf Llama-2 basierenden deutschen Foundation-\nModellen und eine Auswahl begleitender Feinabstimmungen.\nDes Weiteren präsentieren wir GermanBench, eine Sammlung der relevantesten ins Deutsche übersetzten englischen Benchmarks, die es uns ermöglichen in ähnlichem Ausmaß wie im Englischen, die Fähigkeiten von LeoLM gründlich zu bewerten.\n\n\u003csup\u003e1\u003c/sup\u003e: [Touvron et al. 2023a](https://arxiv.org/abs/2302.13971)\n\u003csup\u003e2\u003c/sup\u003e: [Touvron et al. 2023b](https://arxiv.org/abs/2307.09288)\n\n## Vorverarbeitung in Phase 2\n\nLlama-2-Modelle werden auf 2 Billionen Tokens überwiegend englischen Textes vortrainiert. Um ihre Kompetenz in der deutschen Sprache zu erhöhen, verwenden wir ein fortgesetztes Vortraining, welches wir als \"Stage 2 Pretraining\" bezeichnen.\nWir initialisieren LeoLM mit Llama-2-Gewichten und setzen das Training des Modells auf einem großen deutschen Textkorpus von 65 Milliarden Tokens fort, die rigoros gefiltert und dedupliziert wurden und größtenteils aus dem [OSCAR-2301-Korpus](https://huggingface.co/datasets/oscar-corpus/OSCAR-2301) stammen.\nEin wesentlicher Aspekt dieses Ansatzes besteht darin, das Vergessen oder den Verlust von zuvor erlerntem Wissen oder Fähigkeiten zu minimieren. Wir folgen den Erkenntnissen von [Gupta et al. (2023)](https://arxiv.org/abs/2308.04014) in unserer Wahl der Hyperparameter, um das Risiko des Vergessens zu minimieren.\nZusätzlich folgen wir der Arbeit von [Together](https://huggingface.co/togethercomputer/LLaMA-2-7B-32K) bei der Anwendung von [linearer RoPE-Skalierung](https://kaiokendev.github.io/til#extending-context-to-8k) und [Flash Attention 2](https://tridao.me/publications/flash2/flash2.pdf), um die Trainingseffizienz zu verbessern und die Kontextlänge auf 8k Tokens zu verdoppeln.\nSiehe Abbildung 1 für einen Überblick über alle Training-Hyperparameter.\n\n![training_parameters](/images/blog/training_params.png \"Training-Hyperparameters\")\n\n## Feinabstimmungsdatensätze\n\nEs gibt viel Diskussion darüber, was ein guter Chat/Instruktionstuning-Datensatz bieten muss, was zur Entwicklung einer Vielzahl verschiedener, erfolgreicher Ansätze geführt hat. Wir lassen uns von dieser Vielfalt inspirieren und übersetzen, um ähnliche Fähigkeiten auf Deutsch zu bringen, eine Auswahl hochwertiger Instruktionsdatensätze ins Deutsche mit OpenAI's `gpt-3.5-turbo` API. Die Verwendung von `gpt-3.5-turbo` stellt sicher, dass der Zusammenhang zwischen Aufforderungen und Antworten intakt bleibt und dass komplexe Anweisungen, die möglicherweise Code, Gleichungen oder formatierte Daten enthalten, korrekt übersetzt werden.\nAufbauend auf den Erkenntnissen der Community, wählen wir eine Vielzahl von Datensätzen aus, die wir übersetzen und für das Training unseres Chat-Modells verwenden.\nDie übersetzten Datensätze sind:\n\n- [OpenPlatypus](https://huggingface.co/datasets/garage-bAInd/Open-Platypus) -\u003e [OpenSchnabeltier](https://huggingface.co/datasets/LeoLM/OpenSchnabeltier)\n- [OpenAssistant OASST1](https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25) -\u003e [OpenAssistant-DE](https://huggingface.co/datasets/OpenAssistant/OASST-DE)\n\nAußerdem verwenden wir [`FreedomIntelligence/evol-instruct-deutsch`](https://huggingface.co/datasets/FreedomIntelligence/evol-instruct-deutsch) und [`FreedomIntelligence/alpaca-gpt4-deutsch`](https://huggingface.co/datasets/FreedomIntelligence/alpaca-gpt4-deutsch) aus dem [MultilingualSIFT](https://github.com/FreedomIntelligence/MultilingualSIFT)-Projekt. Vielen Dank an die Autoren, dass sie ihre Daten geteilt haben!\nUm die zweisprachige Nutzung zu erleichtern, trainieren wir auch Modelle auf einer Kombination dieser übersetzten Datensätze und ihren ursprünglichen, englischen Gegenstücken.\n\nSchließlich erstellen wir, um Schwächen beim kreativen Schreiben und Reimen, die bei frühen Tests festgestellt wurden, auszugleichen, zwei weitere Datensätze:\n\n- [GPT4 Gedichte](https://huggingface.co/datasets/LeoLM/German_Poems): Eine Sammlung deutscher Gedichte zu verschiedenen Themen, geschrieben von GPT4\n- [GPT4 Lieder](https://huggingface.co/datasets/LeoLM/German_Songs): Eine Sammlung deutscher Lieder und nachfolgende Analysen, geschrieben von GPT4.\n\n## Evaluation und Ergebnisse\n\nDie Evaluierung der Fähigkeiten von LLMs, insbesondere von Chat-Modellen, ist komplex und die besten Methoden sind noch umstritten. Benchmarks, die auf Multiple-Choice basieren und anhand der Protokoll-Wahrscheinlichkeiten des Modells ausgewertet werden (wie im [Open LLM Leaderboard]()), sind eine derzeit beliebte Methode. Eine andere Methode bewertet Antworten automatisch mit GPT4, wie bei AlpacaEval oder MT-Bench. Dieser Ansatz richtet sich eher an Chat-Modelle, da er die Qualität von Modellantworten in realen Aufgaben berücksichtigt. Um so vergleichbar wie möglich zu sein, übersetzen wir eine Reihe von englischen Benchmarks direkt ins Deutsche. Wir veröffentlichen diese Datensätze in unserer [HF-Organisation](https://huggingface.co/LeoLM) und mit ausführlicher Dokumentation [auf GitHub](https://github.com/bjoernpl/GermanBenchmark), und Sie finden den entsprechende `lm-evaluation-harness`-Branch [hier](https://github.com/bjoernpl/lm-evaluation-harness-de/tree/mmlu_de) und den `FastEval`-Branch [hier](https://github.com/bjoernpl/FastEval).\n\nIn Abbildung 3 können Sie einen Vergleich von LeoLM gegenüber den Basis-Llama-2-Modellen auf einer Auswahl von Benchmarks mit sowohl der englischen Version (blau) als auch unserer übersetzten Version (grün) sehen. Unser Trainging verbessert die Benchmark-Ergebnisse für die deutschen Aufgaben, während die Ergebnisse für die englischen Aufgaben leicht reduziert werden. Bemerkenswert ist, dass der durchschnittliche Anstieg der deutschen Benchmark-Ergebnisse die durchschnittliche Abnahme der Leistung auf den englischen Benchmarks deutlich überwiegt, was zeigt, dass unser Ansatz das Erlernen einer neuen Sprache ermöglicht, ohne zu vergessen, was zuvor gelernt wurde. Warum die Ergebnisse in Deutsch niedriger bleiben als in Englisch, ist eine offene Frage, kann aber zum Teil auf eine Qualitätsminderung bei der Übersetzung zurückgeführt werden.\n\n![](/images/blog/benchmarks.png)\n\nDie folgende Tabelle zeigt die Ergebnisse auf unserer übersetzten Version von MT-Bench. MT-Bench ist ein Benchmark, der die Multi-Turn-Leistung auf einem kuratierten Satz von 80 Fragen aus mehreren Kategorien mit GPT-4 als Richter bewertet. Dabei bewertet GPT-4 die Aufforderungen auf einer Skala von 1-10 hinsichtlich der wahrgenommenen Hilfsbereitschaft, Relevanz, Genauigkeit, Tiefe, Kreativität und Detailliertheit der Antwort. Das monolinguale Modell `leo-hessianai-13b-chat` schneidet insgesamt am besten ab und kommt sogar dem GPT-3.5 im Thema \"Geisteswissenschaften\" nahe. Es erzielt auffallend schlechte Ergebnisse in Mathematik und Codierung, was zu erwarten ist, da die Llama-2-Modelle in diesem Bereich ohne sehr explizites Finetuning von vornherein Schwächen aufweisen. Die zweisprachigen Modelle erzielen in einigen Kategorien wie Mathematik und Logik leicht unter ihren monolingualen Gegenstücken, während sie in Codierung und Extraktion übertreffen.\n\n![](/images/blog/mt_bench.png)\nFür eine detailliertere Evaluierung, bleiben Sie dran für unser Paper!\n\n## Qualitative Ergebnisse\n\nBenchmarks neigen dazu, ziemlich abstrakt zu sein. Um ein besseres Gefühl für LeoLM's zu bekommen, schauen Sie sich unsere Demos an und probieren Sie es selbst aus: [**LeoLM/leo-hessianai-7b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-7b-chat) und den größeren Bruder [**LeoLM/leo-hessianai-13b-chat`**](https://huggingface.co/spaces/LeoLM/leo-hessianai-13b-chat). Alternativ können Sie das Modell selbst mit 🤗Transformers ausführen. Weitere Informationen zur Einrichtung finden Sie auf der [Modellkarte](https://huggingface.co/LeoLM/leo-hessianai-13b-chat).\n\n## Fazit\n\nUnsere Forschung hat mehrere Schlüsselbeiträge:\n\n- Wir veröffentlichen eine Suite von deutschen Foundation-Sprachmodellen unter einer offenen Lizenz.\n- Wir übertragen einen gründlichen und vielseitigen Evaluierungsansatz für Basis- und Chat-Modelle ins Deutsche.\n- Wir zeigen, dass eine groß angelegte Fortbildung auch für datengesättigte Modelle wie Llama-2 ohne signifikantes Vergessen oder Verlust von früheren Fähigkeiten möglich ist.\n- Wir präsentieren eine vielfältige Suite von Instruktions-/Chat-Tuning-Datensätzen, die vom Englischen ins Deutsche übersetzt wurden, um als Basis für die deutsche Open-Source-LLM-Forschungsgemeinschaft zu dienen.\n\nInsgesamt ist die LeoLM-Modellsuite ein Proof-of-Concept für den Spracherwerb für vortrainierte Modelle. Darüber hinaus präsentiert sie sich als das erste offen verfügbare deutsche Foundation-Modell, das den heutigen Standards entspricht. Wir bei LAION hoffen, die deutsche Open-Source-Forschungsgemeinschaft anstoßen zu können, um die Abhängigkeit von geschlossenen kommerziellen Quellen wie OpenAI zu verringern. Viel Spaß mit LeoLM!\n\n## Danksagungen\n\nDieses Projekt wurde von Björn Plüster (Universität Hamburg) geleitet mit Unterstützung, Anleitung und\nSupervision von Christoph Schuhmann (LAION), Patrick Schramowski (LAION, Hessian AI, DFKI) und Jenia Jitsev (LAION, Juelich Supercomputing Center, Helmholtz Research Center Juelich).\nWir danken den Leuten bei hessian.AI, insbesondere Wolfgang Stille, Kristian Kersting, Florian Kieser und Patrick Diem\nfür ihre großzügige Rechenzeitvergabe über das Hessian AI Innovation Lab (gefördet vom Hessischen Ministerium für Digitale Strategie und Entwicklung) und das AI Service Center hessian.AISC (gefördert vom Bundesministerium für Bildung und Forschung (BMBF)), und Unterstützung während des gesamten Trainingsprozesses!  Vielen Dank auch an Jae Hee Lee von der Universität Hamburg, Andreas Köpf von Open-Assistant und Community-Mitglieder bei LAION für ihr hilfreiches Feedback. Weiterhin danken wir dem hessian.AI's Innovationslabor, dem AI Servicezentrum hessian.AISC und dem Deutschen Zentrum für Künstliche Intelligenz (DFKI) für ihre Unterstützung.\n\nAußerdem danken wir den Gruppen, die die Datensätze für unser Chat-finetuning ursprünglich erstellt haben: [OpenAssistant](https://huggingface.co/OpenAssistant), [FreedomIntelligence](https://huggingface.co/FreedomIntelligence), [garage-bAInd](https://huggingface.co/garage-bAInd), [WizardLM](https://huggingface.co/WizardLM) and [Instruction-Tuning-with-GPT-4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM).\n\n---\n\n### An Example\n\n**User:**  Warum ist open-source besonders in der KI-Forschung wichtig?\n\n**LeoLM:**\nOpen-Source spielt eine wichtige Rolle in der KI-Forschung, da sie Zusammenarbeit, Innovation und Zugang zur Technologie fördert. Hier sind einige Gründe, warum Open-Source in der KI-Forschung besonders wichtig ist:\n\n- Zusammenarbeit: Durch Open-Source können Forscher gemeinsam an Projekten arbeiten, Ideen austauschen und voneinander lernen. Dies fördert die Zusammenarbeit zwischen Forschern und kann zu schnelleren und effizienteren Fortschritten in der KI-Forschung führen.\n\n- Transparenz: Open-Source ermöglicht Transparenz und Reproduzierbarkeit in der KI-Forschung. Forscher können den Code und die Methoden anderer sehen, reproduzieren und darauf aufbauen, was die Validierung von Ergebnissen und die Identifizierung potenzieller Schwachstellen erleichtert.\n\n- Zugang zur Technologie: Open-Source macht KI-Technologien und -Werkzeuge einem breiteren Publikum zugänglich. Dies ermöglicht kleineren Forschungseinrichtungen und Einzelpersonen den Zugang zu KI-Werkzeugen, die ihnen sonst möglicherweise nicht zur Verfügung stünden. Dadurch kann Innovation und Forschung auch außerhalb traditioneller Zentren gefördert werden.\n\n- Innovation: Open-Source fördert Innovation in der KI-Forschung, indem sie Raum für Experimente und neue Ideen bietet. Da der Code offen und für alle zugänglich ist, können Forscher ihn an ihre spezifischen Bedürfnisse anpassen und neue KI-Werkzeuge und -Technologien entwickeln.\n\n- Lernen: Open-Source ermöglicht es Forschern, voneinander zu lernen. Durch die gemeinsame Nutzung von Code und Methoden können Forscher aus den Erfahrungen anderer lernen und ihre eigenen Forschungsprojekte verbessern.\n\nZusammenfassend ist Open-Source ein wichtiger Aspekt der KI-Forschung, da er Zusammenarbeit, Transparenz, Zugang zur Technologie, Innovation und Lernen fördert. Dadurch wird die KI-Forschung effizienter, innovativer und zugänglicher für eine breitere Palette von Forschern und Institutionen\n\n| [Link zu einer PDF mit mehr Beispielen](/documents/leo-lm.pdf) |\n|----------|\n","slug":"leo-lm"},"__N_SSG":true},"page":"/blog-de/[slug]","query":{"slug":"leo-lm"},"buildId":"aiqu4Uyy9_dWymlMLbsHv","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>