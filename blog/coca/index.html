<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Training Contrastive Captioners | LAION</title><meta name="title" content="Training Contrastive Captioners | LAION"/><meta property="og:title" content="Training Contrastive Captioners | LAION"/><meta name="twitter:title" content="Training Contrastive Captioners | LAION"/><meta name="description" content="&lt;p&gt;We introduce a new model type to &lt;a href=&quot;https://github.com/mlfoundations/open_clip&quot;&gt;OpenClip&lt;/a&gt; Contrastive Captioners (CoCa) [1]. This model adds an a..."/><meta property="og:description" content="&lt;p&gt;We introduce a new model type to &lt;a href=&quot;https://github.com/mlfoundations/open_clip&quot;&gt;OpenClip&lt;/a&gt; Contrastive Captioners (CoCa) [1]. This model adds an a..."/><meta name="twitter:description" content="&lt;p&gt;We introduce a new model type to &lt;a href=&quot;https://github.com/mlfoundations/open_clip&quot;&gt;OpenClip&lt;/a&gt; Contrastive Captioners (CoCa) [1]. This model adds an a..."/><meta property="og:image" content="https://laion.ai/images/blog/eval_coca_clip.jpg"/><meta name="twitter:image" content="https://laion.ai/images/blog/eval_coca_clip.jpg"/><meta name="twitter:image:alt" content="The text: LAION. Large-scale Artificial Intelligence Open Network, TRULY OPEN AI. 100% NON-PROFIT. 100% FREE."/><meta property="og:type" content="website"/><meta property="og:url" content="https://laion.ai/blog/coca"/><meta name="twitter:url" content="https://laion.ai/blog/coca"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1D374E"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.png"/><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff" as="font" type="font/woff"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff2" as="font" type="font/woff2"/><link rel="preload" href="/fonts/Dinish-Regular.woff" as="font" type="font/woff"/><link rel="preload" href="/fonts/Dinish-Regular.woff2" as="font" type="font/woff2"/><link rel="preload" href="/fonts/Dinish-Italic.woff" as="font" type="font/woff"/><link rel="preload" href="/fonts/Dinish-Italic.woff2" as="font" type="font/woff2"/><meta name="next-head-count" content="25"/><link rel="stylesheet" href="/fonts/load.css"/><link rel="preload" href="/_next/static/css/6182a1940c49bb84.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6182a1940c49bb84.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-663923589dd6c477.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-891d88d46bc1d783.js" defer=""></script><script src="/_next/static/vg5WVpnXmqXi2d2CnAhly/_buildManifest.js" defer=""></script><script src="/_next/static/vg5WVpnXmqXi2d2CnAhly/_ssgManifest.js" defer=""></script><script src="/_next/static/vg5WVpnXmqXi2d2CnAhly/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-paper md:text-sky md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LAION</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/team/">Team</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/faq/">FAQ</a><a href="/privacy-policy/">Privacy Policy</a><a href="/dataset-requests/">Dataset Requests</a><a href="/impressum/">Impressum</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1D374E;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/team/">Team</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/faq/">FAQ</a></p><p><a href="/privacy-policy/">Privacy Policy</a></p><p><a href="/dataset-requests/">Dataset Requests</a></p><p><a href="/impressum/">Impressum</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-paper grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-8xl md:text-8xl w-full font-bold title-flow break-words">TRAINING CONTRASTIVE CAPTIONERS</h1><p class="text-2xl pb-2">by: <!-- -->Giovanni Puccetti, Maciej Kilian, Romain Beaumont<!-- -->,<!-- --> <!-- -->02 Feb, 2023<!-- --></p><hr/><div class="pt-2 article"><p>We introduce a new model type to <a href="https://github.com/mlfoundations/open_clip">OpenClip</a> Contrastive Captioners (CoCa) [1]. This model adds an autoregressive objective (generation) on top of the CLIP contrastive one. The architecture is composed of three parts, the first two are similar to those composing a CLIP model and the third is a text decoder that stands on top of the text encoder. The additional decoder takes as input the encoded images (through cross-attention) and the previous tokens to predict the next most probable one. One of the few architecture changes, compared to CLIP, is attentional pooling [2], used to aggregate image representations and pass them to both the contrastive loss and the decoder cross-attention.</p>
<p>This is interesting for several reasons:</p>
<ul>
<li>We believe there is no openly available trained model with this architecture;</li>
<li>Adding a generative task appears to help the contrastive task with minimal computational impact;</li>
<li>The model is easily adaptable to a large number of tasks, on top of all those CLIP is suited for. CoCa models can (with relatively cheap fine-tuning) perform Image Captioning, Visual Question Answering, Multimodal Understanding, and more;</li>
<li>CoCa gives captioning models an intermediate contrastive latent space for minimal training cost increase.</li>
</ul>
<h2><a id="benchmarks" class="anchor" href="#benchmarks" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Benchmarks</h2>
<p>On a comparable model size and with the same training data available, CoCa outperforms a CLIP model on several zero-shot tasks (Figure 1). Most notably on <em>imagenet1k</em> CoCa achieves 75.5 and CLIP 73.1 (2.6% improvement).</p>
<table>
<thead>
<tr>
<th style="text-align:left">(a) <img src="/images/blog/eval_coca_clip.jpg" alt=""></th>
<th style="text-align:left">(b) <img src="/images/blog/eval_coca_clip_diff.jpg" alt=""></th>
</tr>
</thead>
</table>
<p><em>Figure 1:</em> Scores achieved by <em>coca_ViT-L-14</em> and <em>ViT-L-14</em> on several zeroshot classification tasks <strong>(a)</strong>, together with the performance gap between the two models, in the same tasks sorted by magnitude <strong>(b)</strong>.</p>
<p>Table 2 shows the results achieved on Text to Image and Image to Text retrieval by both CoCa and CLIP. In this case too, CoCa outperforms CLIP on all tasks with differences ranging from 0.3 to 1.3.</p>
<table>
  <tr>
   <td colspan="4" align="center" > Text to Image Retrieval Recall@5
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td>flickr30k
   </td>
   <td>flickr8k
   </td>
   <td>Mscoco captions
   </td>
  </tr>
  <tr>
   <td>coca_ViT-L-14
   </td>
   <td>92.0
   </td>
   <td>70.1
   </td>
   <td>70.5
   </td>
  </tr>
  <tr>
   <td>ViT-L-14
   </td>
   <td>91.7
   </td>
   <td>69.0
   </td>
   <td>69.2
   </td>
  </tr>
  <tr>
   <td colspan="4" align="center"> Image to Text Retrieval Recall@5
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td>flickr30k
   </td>
   <td>flickr8k
   </td>
   <td>Mscoco captions
   </td>
  </tr>
  <tr>
   <td>coca_ViT-L-14
   </td>
   <td>99.3
   </td>
   <td>81.7
   </td>
   <td>83.6
   </td>
  </tr>
  <tr>
   <td>ViT-L-14
   </td>
   <td>98.4
   </td>
   <td>81.2
   </td>
   <td>83.0
   </td>
  </tr>
</table>
<p><em>Table 2:</em> Text to Image and Image to Text retrieval <strong>Recall@5</strong> on <em>flickr30k</em>, <em>flickr8k</em> and <em>Mscoco captions</em>.</p>
<h2><a id="released-checkpoint" class="anchor" href="#released-checkpoint" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Released Checkpoint</h2>
<p>We release checkpoints for two model configs, <em>coca_ViT-B-32</em> and <em>coca_ViT-L-14</em>. We also release the MSCOCO finetunes of those models which are much better at captioning but unfortunately lose their contrastive capabilities during fine tuning.</p>
<p>Try generation in this <a href="https://huggingface.co/spaces/laion/CoCa">Space</a> or in this <a href="https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb">colab notebook</a>!</p>
<table>
  <tr>
   <td>
   </td>
   <td>L/14
   </td>
   <td>B/32
   </td>
   <td>CoCa (from paper)
   </td>
  </tr>
  <tr>
  <td># Params Image Encoder
   </td>
   <td>306.72M
   </td>
   <td>89.16M
   </td>
   <td>
    1B
   </td>
  </tr>
  <tr>
   <td># Params Text Encoder
   </td>
   <td>123.65M
   </td>
   <td>63.42M
   </td>
   <td rowspan="2">
    1.1B
   </td>
  </tr>
  <tr>
   <td># Params Text Decoder
   </td>
   <td>208.07M
   </td>
   <td>100.96M
   </td>
  </tr>
</table>
<p><em>Table 3:</em> Number of parameters for each encoder/decoder component for <em>coca_ViT-L-14</em>, <em>coca_ViT-B-32</em> and the <em>CoCa</em> model from the original paper (M=millions, B=billions).</p>
<h2><a id="training-notes" class="anchor" href="#training-notes" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Training Notes</h2>
<h3><a id="pretraining" class="anchor" href="#pretraining" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Pretraining</h3>
<p>We train both model configurations on 13B samples seen from <a href="https://laion.ai/blog/laion-5b/">LAION-2B</a> [3] with a batch size of 90k, learning rate of 1e-3, and a cosine decay learning rate schedule. Experiments were performed on 384 A100’s and over the course of training we maintained 75.5 samples/s/gpu (~29k samples/s in total).</p>
<p>When it comes to cost, even though CoCa has more capabilities than single-task captioning models there’s a minimal increase ~20% (as reported by Table 8b of the paper). This is due to the fact that the first half of the text decoder (i.e. the text encoder) is unimodal and is computed in parallel to the image encoder, once the encoders are done we simply continue the forward pass of the text embeddings through the text decoder and also include the image embeddings via cross attention. The trainig report can be found <a href="https://wandb.ai/iejmac/open-clip/reports/CoCa-L-14--VmlldzozNDEwMDIx">here</a>.</p>
<h3><a id="fine-tuning" class="anchor" href="#fine-tuning" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fine-tuning</h3>
<p>For image captioning tasks fine-tuning is a straightforward extension of pretraining with few hyper parameters changes. The crucial one is contrastive loss weight, which has to be set to zero to let the backward pass only account for the generative loss, besides  there are no additional fine-tuning oriented components nor changes in the loss. We use a batch size of 128 with a learning rate of 1e-5 and a cosine learning rate schedule. Experiments are performed on 4 A100's. Table 4 shows the language generation scores achieved by <em>coca_ViT-L-14</em> and by CoCa in the original paper, _coca_ViT-L-14 performance is still far from the original CoCa model one.</p>
<p>It is noteworthy that (in our experiments) after fine-tuning with a generative only loss these models lose their contrastive skills entirely.</p>
<table>
  <tr>
   <td>
   </td>
   <td>Bleu@4
   </td>
   <td>METEOR
   </td>
   <td>CIDEr
   </td>
   <td>Spice
   </td>
  </tr>
  <tr>
    <td colspan="5" align="center">
    coca_ViT-L-14
    </td>
  </tr>
  <tr>
   <td>Karpathy val
   </td>
   <td>35.6
   </td>
   <td>29.8
   </td>
   <td>125.3
   </td>
   <td>23.4
   </td>
  </tr>
  <tr>
   <td>NoCaps
   </td>
   <td>39.9
   </td>
   <td>29.1
   </td>
   <td>106.5
   </td>
   <td>14.7
   </td>
  </tr>
  <tr>
    <td colspan="5" align="center">
    Original CoCa (from paper)
    </td>
  </tr>
  <tr>
   <td>Karpathy val
   </td>
   <td>40.9
   </td>
   <td>33.9
   </td>
   <td>143.6
   </td>
   <td>24.7
   </td>
  </tr>
  <tr>
   <td>NoCaps
   </td>
   <td> -
   </td>
   <td>-
   </td>
   <td>122.4
   </td>
   <td>15.5
   </td>
  </tr>
</table>
<p><em>Table 4:</em> Visual captioning scores achieved with <em>coca_ViT-L-14</em> on <em>karpathy</em> validation set and <em>NoCaps</em>.</p>
<h2><a id="captioning-examples" class="anchor" href="#captioning-examples" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Captioning Examples</h2>
<table>
<thead>
<tr>
<th style="text-align:left"><img src="/images/blog/ipod_apple.png" alt="cao" width="500"></th>
<th style="text-align:left"><img src="/images/blog/space_raccoon.png" alt="cao" width="500"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">An apple sitting on top of a wooden table.</td>
<td style="text-align:left">A painting of a raccoon in a space suit.</td>
</tr>
</tbody>
</table>
<h2><a id="whats-next" class="anchor" href="#whats-next" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What’s Next</h2>
<ul>
<li>Unimodal Text Pretraining - One of the shortcomings of CoCa is that it can have trouble with zero-shot captioning because the noisy web text it was trained on isn’t as rich as unimodal text data. To this end we can look into methods that provide CoCa models with this rich text understanding either via initializing the weights of the decoder with some pretrained unimodal text decoder or perhaps alternating between multimodal and unimodal losses that use different data.</li>
<li>Fine tuning on more tasks VQA, multimodal reasoning, and more.</li>
<li>Image Decoder - CoCa adds a multimodal text decoder on top of CLIP and shows this multi-task learning can benefit both tasks. Why not also add a multimodal image decoder?</li>
</ul>
<h2><a id="contributions-and-acknowledgements" class="anchor" href="#contributions-and-acknowledgements" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Contributions and acknowledgements</h2>
<p>Thanks to</p>
<ul>
<li><a href="https://gpucce.github.io/">gpucce</a> and <a href="https://github.com/iejMac">iejMac</a> for implementation into open_clip and training the models.</li>
<li><a href="https://github.com/lucidrains">lucidrains</a> for <a href="https://github.com/lucidrains/CoCa-pytorch">initial implementation</a>.</li>
<li><a href="https://github.com/rom1504">Romain Beaumont</a> and <a href="https://github.com/rwightman">Ross Wightman</a> for advice, reviews, and engineering support.</li>
<li><a href="https://github.com/Soonhwan-Kwon">Soonhwan-Kwon</a> for implementing beam search.</li>
</ul>
<p>Huge thanks to <a href="https://twitter.com/EMostaque">Emad</a> and <a href="https://stability.ai/">StabilityAI</a> for providing the compute resources required to train these models.</p>
<h2><a id="references" class="anchor" href="#references" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>References</h2>
<p>[1] Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., &amp; Wu, Y. (2022). CoCa: Contrastive Captioners are Image-Text Foundation Models. <em>ArXiv, abs/2205.01917</em>.</p>
<p>[2] Lee, J., Lee, Y., Kim, J., Kosiorek, A.R., Choi, S., &amp; Teh, Y.W. (2018). Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. <em>International Conference on Machine Learning</em>.</p>
<p>[3] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., &amp; Jitsev, J. (2022). LAION-5B: An open large-scale dataset for training next generation image-text models. <em>ArXiv, abs/2210.08402</em>.</p>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Training Contrastive Captioners","author":"Giovanni Puccetti, Maciej Kilian, Romain Beaumont","date":"Feb 2 2023","previewImg":"/images/blog/eval_coca_clip.jpg"},"content":"\n\nWe introduce a new model type to [OpenClip](https://github.com/mlfoundations/open_clip) Contrastive Captioners (CoCa) [1]. This model adds an autoregressive objective (generation) on top of the CLIP contrastive one. The architecture is composed of three parts, the first two are similar to those composing a CLIP model and the third is a text decoder that stands on top of the text encoder. The additional decoder takes as input the encoded images (through cross-attention) and the previous tokens to predict the next most probable one. One of the few architecture changes, compared to CLIP, is attentional pooling [2], used to aggregate image representations and pass them to both the contrastive loss and the decoder cross-attention.\n\nThis is interesting for several reasons:\n\n* We believe there is no openly available trained model with this architecture;\n* Adding a generative task appears to help the contrastive task with minimal computational impact;\n* The model is easily adaptable to a large number of tasks, on top of all those CLIP is suited for. CoCa models can (with relatively cheap fine-tuning) perform Image Captioning, Visual Question Answering, Multimodal Understanding, and more;\n* CoCa gives captioning models an intermediate contrastive latent space for minimal training cost increase.\n\n\n## Benchmarks\n\nOn a comparable model size and with the same training data available, CoCa outperforms a CLIP model on several zero-shot tasks (Figure 1). Most notably on _imagenet1k_ CoCa achieves 75.5 and CLIP 73.1 (2.6% improvement).\n\n\n|(a) ![](/images/blog/eval_coca_clip.jpg) |(b) ![](/images/blog/eval_coca_clip_diff.jpg) |\n|:-|:-|\n\n\n_Figure 1:_ Scores achieved by _coca_ViT-L-14_ and _ViT-L-14_ on several zeroshot classification tasks **(a)**, together with the performance gap between the two models, in the same tasks sorted by magnitude **(b)**.\n\n\n\n\nTable 2 shows the results achieved on Text to Image and Image to Text retrieval by both CoCa and CLIP. In this case too, CoCa outperforms CLIP on all tasks with differences ranging from 0.3 to 1.3.\n\n\n\u003ctable\u003e\n  \u003ctr\u003e\n   \u003ctd colspan=\"4\" align=\"center\" \u003e Text to Image Retrieval Recall@5\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e\n   \u003c/td\u003e\n   \u003ctd\u003eflickr30k\n   \u003c/td\u003e\n   \u003ctd\u003eflickr8k\n   \u003c/td\u003e\n   \u003ctd\u003eMscoco captions\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003ecoca_ViT-L-14\n   \u003c/td\u003e\n   \u003ctd\u003e92.0\n   \u003c/td\u003e\n   \u003ctd\u003e70.1\n   \u003c/td\u003e\n   \u003ctd\u003e70.5\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eViT-L-14\n   \u003c/td\u003e\n   \u003ctd\u003e91.7\n   \u003c/td\u003e\n   \u003ctd\u003e69.0\n   \u003c/td\u003e\n   \u003ctd\u003e69.2\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd colspan=\"4\" align=\"center\"\u003e Image to Text Retrieval Recall@5\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e\n   \u003c/td\u003e\n   \u003ctd\u003eflickr30k\n   \u003c/td\u003e\n   \u003ctd\u003eflickr8k\n   \u003c/td\u003e\n   \u003ctd\u003eMscoco captions\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003ecoca_ViT-L-14\n   \u003c/td\u003e\n   \u003ctd\u003e99.3\n   \u003c/td\u003e\n   \u003ctd\u003e81.7\n   \u003c/td\u003e\n   \u003ctd\u003e83.6\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eViT-L-14\n   \u003c/td\u003e\n   \u003ctd\u003e98.4\n   \u003c/td\u003e\n   \u003ctd\u003e81.2\n   \u003c/td\u003e\n   \u003ctd\u003e83.0\n   \u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n_Table 2:_ Text to Image and Image to Text retrieval **Recall@5** on _flickr30k_, _flickr8k_ and _Mscoco captions_.\n\n## Released Checkpoint\n\nWe release checkpoints for two model configs, _coca_ViT-B-32_ and _coca_ViT-L-14_. We also release the MSCOCO finetunes of those models which are much better at captioning but unfortunately lose their contrastive capabilities during fine tuning.\n\nTry generation in this [Space](https://huggingface.co/spaces/laion/CoCa) or in this [colab notebook](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb)!\n\n\n\u003ctable\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e\n   \u003c/td\u003e\n   \u003ctd\u003eL/14\n   \u003c/td\u003e\n   \u003ctd\u003eB/32\n   \u003c/td\u003e\n   \u003ctd\u003eCoCa (from paper)\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e# Params Image Encoder\n   \u003c/td\u003e\n   \u003ctd\u003e306.72M\n   \u003c/td\u003e\n   \u003ctd\u003e89.16M\n   \u003c/td\u003e\n   \u003ctd\u003e\n    1B\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e# Params Text Encoder\n   \u003c/td\u003e\n   \u003ctd\u003e123.65M\n   \u003c/td\u003e\n   \u003ctd\u003e63.42M\n   \u003c/td\u003e\n   \u003ctd rowspan=\"2\"\u003e\n    1.1B\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e# Params Text Decoder\n   \u003c/td\u003e\n   \u003ctd\u003e208.07M\n   \u003c/td\u003e\n   \u003ctd\u003e100.96M\n   \u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n_Table 3:_ Number of parameters for each encoder/decoder component for _coca_ViT-L-14_, _coca_ViT-B-32_ and the _CoCa_ model from the original paper (M=millions, B=billions).\n\n\n\n## Training Notes\n\n\n### Pretraining\n\nWe train both model configurations on 13B samples seen from [LAION-2B](https://laion.ai/blog/laion-5b/) [3] with a batch size of 90k, learning rate of 1e-3, and a cosine decay learning rate schedule. Experiments were performed on 384 A100’s and over the course of training we maintained 75.5 samples/s/gpu (~29k samples/s in total).\n\nWhen it comes to cost, even though CoCa has more capabilities than single-task captioning models there’s a minimal increase ~20% (as reported by Table 8b of the paper). This is due to the fact that the first half of the text decoder (i.e. the text encoder) is unimodal and is computed in parallel to the image encoder, once the encoders are done we simply continue the forward pass of the text embeddings through the text decoder and also include the image embeddings via cross attention. The trainig report can be found [here](https://wandb.ai/iejmac/open-clip/reports/CoCa-L-14--VmlldzozNDEwMDIx).\n\n\n### Fine-tuning\n\nFor image captioning tasks fine-tuning is a straightforward extension of pretraining with few hyper parameters changes. The crucial one is contrastive loss weight, which has to be set to zero to let the backward pass only account for the generative loss, besides  there are no additional fine-tuning oriented components nor changes in the loss. We use a batch size of 128 with a learning rate of 1e-5 and a cosine learning rate schedule. Experiments are performed on 4 A100's. Table 4 shows the language generation scores achieved by _coca_ViT-L-14_ and by CoCa in the original paper, _coca_ViT-L-14 performance is still far from the original CoCa model one.\n\nIt is noteworthy that (in our experiments) after fine-tuning with a generative only loss these models lose their contrastive skills entirely.\n\n\n\u003ctable\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e\n   \u003c/td\u003e\n   \u003ctd\u003eBleu@4\n   \u003c/td\u003e\n   \u003ctd\u003eMETEOR\n   \u003c/td\u003e\n   \u003ctd\u003eCIDEr\n   \u003c/td\u003e\n   \u003ctd\u003eSpice\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd colspan=\"5\" align=\"center\"\u003e\n    coca_ViT-L-14\n    \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eKarpathy val\n   \u003c/td\u003e\n   \u003ctd\u003e35.6\n   \u003c/td\u003e\n   \u003ctd\u003e29.8\n   \u003c/td\u003e\n   \u003ctd\u003e125.3\n   \u003c/td\u003e\n   \u003ctd\u003e23.4\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eNoCaps\n   \u003c/td\u003e\n   \u003ctd\u003e39.9\n   \u003c/td\u003e\n   \u003ctd\u003e29.1\n   \u003c/td\u003e\n   \u003ctd\u003e106.5\n   \u003c/td\u003e\n   \u003ctd\u003e14.7\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd colspan=\"5\" align=\"center\"\u003e\n    Original CoCa (from paper)\n    \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eKarpathy val\n   \u003c/td\u003e\n   \u003ctd\u003e40.9\n   \u003c/td\u003e\n   \u003ctd\u003e33.9\n   \u003c/td\u003e\n   \u003ctd\u003e143.6\n   \u003c/td\u003e\n   \u003ctd\u003e24.7\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eNoCaps\n   \u003c/td\u003e\n   \u003ctd\u003e -\n   \u003c/td\u003e\n   \u003ctd\u003e-\n   \u003c/td\u003e\n   \u003ctd\u003e122.4\n   \u003c/td\u003e\n   \u003ctd\u003e15.5\n   \u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n_Table 4:_ Visual captioning scores achieved with _coca_ViT-L-14_ on _karpathy_ validation set and _NoCaps_.\n\n\n\n## Captioning Examples\n\n\n\n|\u003cimg src=\"/images/blog/ipod_apple.png\" alt=\"cao\" width=\"500\"\u003e|\u003cimg src=\"/images/blog/space_raccoon.png\" alt=\"cao\" width=\"500\"\u003e|\n|:-|:-|\n|An apple sitting on top of a wooden table.|A painting of a raccoon in a space suit.|\n\n\n\n\n\n\n\n\n\n\n\n\n## What’s Next\n\n\n\n* Unimodal Text Pretraining - One of the shortcomings of CoCa is that it can have trouble with zero-shot captioning because the noisy web text it was trained on isn’t as rich as unimodal text data. To this end we can look into methods that provide CoCa models with this rich text understanding either via initializing the weights of the decoder with some pretrained unimodal text decoder or perhaps alternating between multimodal and unimodal losses that use different data.\n* Fine tuning on more tasks VQA, multimodal reasoning, and more.\n* Image Decoder - CoCa adds a multimodal text decoder on top of CLIP and shows this multi-task learning can benefit both tasks. Why not also add a multimodal image decoder?\n\n\n## Contributions and acknowledgements\n\nThanks to\n\n\n\n* [gpucce](https://gpucce.github.io/) and [iejMac](https://github.com/iejMac) for implementation into open_clip and training the models.\n* [lucidrains](https://github.com/lucidrains) for [initial implementation](https://github.com/lucidrains/CoCa-pytorch).\n* [Romain Beaumont](https://github.com/rom1504) and [Ross Wightman](https://github.com/rwightman) for advice, reviews, and engineering support.\n* [Soonhwan-Kwon](https://github.com/Soonhwan-Kwon) for implementing beam search.\n\nHuge thanks to [Emad](https://twitter.com/EMostaque) and [StabilityAI](https://stability.ai/) for providing the compute resources required to train these models.\n\n\n## References\n\n[1] Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., \u0026 Wu, Y. (2022). CoCa: Contrastive Captioners are Image-Text Foundation Models. _ArXiv, abs/2205.01917_.\n\n[2] Lee, J., Lee, Y., Kim, J., Kosiorek, A.R., Choi, S., \u0026 Teh, Y.W. (2018). Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. _International Conference on Machine Learning_.\n\n[3] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., \u0026 Jitsev, J. (2022). LAION-5B: An open large-scale dataset for training next generation image-text models. _ArXiv, abs/2210.08402_.","slug":"coca"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"coca"},"buildId":"vg5WVpnXmqXi2d2CnAhly","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>