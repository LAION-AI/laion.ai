<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Announcing OpenFlamingo: An open-source framework for training vision-language models with in-context learning | LAION</title><meta name="title" content="Announcing OpenFlamingo: An open-source framework for training vision-language models with in-context learning | LAION"/><meta property="og:title" content="Announcing OpenFlamingo: An open-source framework for training vision-language models with in-context learning | LAION"/><meta name="twitter:title" content="Announcing OpenFlamingo: An open-source framework for training vision-language models with in-context learning | LAION"/><meta name="description" content="&lt;p&gt;&lt;strong&gt;Overview.&lt;/strong&gt;
We are thrilled to announce the release of OpenFlamingo, an open-source reproduction of DeepMind&#x27;s Flamingo model. At its core,..."/><meta property="og:description" content="&lt;p&gt;&lt;strong&gt;Overview.&lt;/strong&gt;
We are thrilled to announce the release of OpenFlamingo, an open-source reproduction of DeepMind&#x27;s Flamingo model. At its core,..."/><meta name="twitter:description" content="&lt;p&gt;&lt;strong&gt;Overview.&lt;/strong&gt;
We are thrilled to announce the release of OpenFlamingo, an open-source reproduction of DeepMind&#x27;s Flamingo model. At its core,..."/><meta property="og:image" content="https://laion.ai/images/blog/flamingo-logo.png"/><meta name="twitter:image" content="https://laion.ai/images/blog/flamingo-logo.png"/><meta name="twitter:image:alt" content="The text: LAION. Large-scale Artificial Intelligence Open Network, TRULY OPEN AI. 100% NON-PROFIT. 100% FREE."/><meta property="og:type" content="website"/><meta property="og:url" content="https://laion.ai/blog/open-flamingo"/><meta name="twitter:url" content="https://laion.ai/blog/open-flamingo"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1D374E"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.png"/><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff2" as="font" type="font/woff2" crossorigin="true"/><meta name="next-head-count" content="25"/><link rel="stylesheet" href="/fonts/load.css"/><link rel="preload" href="/_next/static/css/5357c8cce67e7f29.css" as="style"/><link rel="stylesheet" href="/_next/static/css/5357c8cce67e7f29.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fb0512e25146295.js" defer=""></script><script src="/_next/static/chunks/286-30519d8a3e60551d.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-44fac0971625f498.js" defer=""></script><script src="/_next/static/zdSCNtiOyCDn7dVCkW2TW/_buildManifest.js" defer=""></script><script src="/_next/static/zdSCNtiOyCDn7dVCkW2TW/_ssgManifest.js" defer=""></script><script src="/_next/static/zdSCNtiOyCDn7dVCkW2TW/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-paper md:text-sky md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LAION</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/team/">Team</a><a href="/blog/">Blog</a><a href="/notes/">Notes</a><a href="/press/">Press</a><a href="/about/">About</a><a href="/faq/">FAQ</a><a href="/donations/">Donations</a><a href="/privacy-policy/">Privacy Policy</a><a href="/dataset-requests/">Dataset Requests</a><a href="/impressum/">Impressum</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1D374E;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/team/">Team</a></p><p><a href="/blog/">Blog</a></p><p><a href="/notes/">Notes</a></p><p><a href="/press/">Press</a></p><p><a href="/about/">About</a></p><p><a href="/faq/">FAQ</a></p><p><a href="/donations/">Donations</a></p><p><a href="/privacy-policy/">Privacy Policy</a></p><p><a href="/dataset-requests/">Dataset Requests</a></p><p><a href="/impressum/">Impressum</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-paper grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-8xl md:text-8xl w-full font-bold title-flow break-words">ANNOUNCING OPENFLAMINGO: AN OPEN-SOURCE FRAMEWORK FOR TRAINING VISION-LANGUAGE MODELS WITH IN-CONTEXT LEARNING</h1><p class="text-2xl pb-2">by: <!-- -->Anas Awadalla and Irena Gao<!-- -->,<!-- --> <!-- -->28 Mar, 2023<!-- --></p><hr/><div class="pt-2 article"><p><strong>Overview.</strong>
We are thrilled to announce the release of OpenFlamingo, an open-source reproduction of DeepMind's Flamingo model. At its core, OpenFlamingo is a framework that enables training and evaluation of large multimodal models (LMMs). Check out our <a href="https://github.com/mlfoundations/open_flamingo">GitHub repository</a> and <a href="https://7164d2142d11.ngrok.app">demo</a> to get started!</p>
<p>For this first release, our contributions are as follows:</p>
<ul>
<li>üèãÔ∏è A Python framework to train Flamingo-style LMMs (based on Lucidrains' <a href="https://github.com/lucidrains/flamingo-pytorch">flamingo implementation</a> and David Hansmair's <a href="https://github.com/dhansmair/flamingo-mini">flamingo-mini repository</a>).</li>
<li>ü™Ö A large-scale multimodal dataset with interleaved image and text sequences.</li>
<li>üß™ An in-context learning evaluation benchmark for vision-language tasks.</li>
<li>ü§ñ A first version of our OpenFlamingo-9B model based on LLaMA, with much better models to come!</li>
</ul>
<p>The recent progress in open-source LMMs with the release of <a href="https://arxiv.org/abs/2301.12597">BLIP-2</a> and <a href="https://jykoh.com/fromage">FROMAGe</a> has shown the exciting potential of multimodal systems. We hope that OpenFlamingo will help drive progress in multimodal machine learning, and we have more exciting contributions in the pipeline, so stay tuned!</p>
<p><strong>Goal.</strong>
Our goal with OpenFlamingo is to develop a multimodal system that can tackle a diverse range of vision-language tasks. Ultimately, we aim to match the power and versatility of GPT-4 in handling visual and text input. To achieve this goal, we are creating an open-source version of <a href="https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model">DeepMind's Flamingo</a> model, a LMM capable of processing and reasoning about images, videos, and text. We are committed to build fully open-source models, and believe this transparency is essential for fostering collaboration, accelerating progress, and democratizing access to state-of-the-art LMMs. Our release is the first step towards this goal.</p>
<p>We are sharing the first checkpoint of our OpenFlamingo-9B model. While the model is not yet fully optimized, it demonstrates the potential of this project. By working together and receiving feedback from the community, we can train better LMMs. We encourage the community to participate in the development process by providing feedback and contributing to the repository.</p>
<p><strong>Technical Details.</strong>
Our implementation largely follows that of <a href="https://arxiv.org/abs/2204.14198">Flamingo</a>. Flamingo models are trained on large-scale web corpora containing interleaved text and images, which is crucial for endowing them with in-context few-shot learning capabilities. OpenFlamingo implements the same architecture (Perceiver resamplers, cross-attention layers) proposed in the original Flamingo paper. However, since the training data for Flamingo is not available to the public, we use open-source datasets for training our models. Specifically, the released OpenFlamingo-9B checkpoint is trained on 5M samples from our new Multimodal C4 dataset and 10M samples from <a href="https://huggingface.co/datasets/laion/laion2B-en">LAION-2B</a>.</p>
<h2><a id="multimodal-c4" class="anchor" href="#multimodal-c4" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Multimodal C4</strong></h2>
<p>The Multimodal-C4 dataset is an expansion of the text-only <a href="https://www.tensorflow.org/datasets/catalog/c4">C4 dataset</a>, which was used to train  <a href="https://arxiv.org/abs/1910.10683">T5 models</a>. This dataset is built by our collaborators <a href="https://jmhessel.com">Jack Hessel</a> and <a href="https://wanrong-zhu.com">Wanrong Zhu</a> at the Allen Institute for AI. For each document in the <a href="https://www.tensorflow.org/datasets/catalog/c4#c4en_default_config">C4 en.clean</a> dataset, we retrieve the original webpage from <a href="https://commoncrawl.org/">Common Crawl</a>, then collect the downloadable images. Data cleaning is carried out through deduplication and content filtering, which aims to eliminate non-safe for work (NSFW) and unrelated images, such as advertisements. Additionally, we run face detection and discard images with positive identifications. Finally, images and sentences are interleaved using bipartite matching within a document: CLIP ViT/L-14 image-text similarities serve as edge weights. Multimodal-C4 consists of approximately 75 million documents, encompassing around 400M images and 38B tokens. A full release with more detail is coming soon.</p>
<p><img src="/images/blog/mmc4-example.png" alt=""></p>
<h2><a id="benchmark" class="anchor" href="#benchmark" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Benchmark</strong></h2>
<p>To measure the performance of OpenFlamingo, we evaluate on a diverse set of downstream tasks. Our aim is to eventually build an open-source version of Flamingo‚Äôs benchmark and extend past that to standardize vision-language task evaluation. Currently we support visual question-answering (<a href="https://visualqa.org/index.html">VQAv2</a>, <a href="https://okvqa.allenai.org">OK-VQA</a>), captioning (<a href="https://cocodataset.org/#home">COCO</a>, <a href="https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset">Flickr30k</a>), and image classification (<a href="https://image-net.org/index.php">ImageNet</a>) tasks. Expect us to add many more evaluation sets that probe model reasoning, biases, and more! You can access the benchmark on the OpenFlamingo repo.</p>
<h2><a id="model-release" class="anchor" href="#model-release" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Model release</strong></h2>
<p><img src="/images/blog/flamingo-llama.png" alt=""></p>
<p>As part of our release, we are also providing a checkpoint from our under-development OpenFlamingo-9B, a LMM built on top of <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">LLaMA 7B</a> and <a href="https://openai.com/research/clip">CLIP ViT/L-14</a>. This model is still a work in progress but it can already bring a lot of value to the community. For instance,</p>
<p><img src="/images/blog/flamingo-9B-sample-one.png" alt="">
<img src="/images/blog/flamingo-9B-sample-two.png" alt=""></p>
<p><strong>Performance</strong></p>
<p>We evaluated our checkpoint on COCO and VQAv2. Here we report the validation performance using a different number of shots.</p>
<p>COCO (CIDEr)</p>
<table>
  <tr>
   <td>
   </td>
   <td>0-shot
   </td>
   <td>4-shot
   </td>
   <td>8-shot
   </td>
   <td>16-shot
   </td>
   <td>32-shot
   </td>
  </tr>
  <tr>
   <td>OpenFlamingo-9B*
   </td>
   <td>65.5
   </td>
   <td>74.3
   </td>
   <td>79.3
   </td>
   <td>81.8
   </td>
   <td>84.5
   </td>
  </tr>
  <tr>
   <td>DeepMind Flamingo-9B
   </td>
   <td>79.4
   </td>
   <td>93.1
   </td>
   <td>99.0
   </td>
   <td>102.2
   </td>
   <td>106.3
   </td>
  </tr>
</table>
<hr>
<p>VQAv2 (VQA accuracy)</p>
<table>
  <tr>
   <td>
   </td>
   <td>0-shot
   </td>
   <td>4-shot
   </td>
   <td>8-shot
   </td>
   <td>16-shot
   </td>
   <td>32-shot
   </td>
  </tr>
  <tr>
   <td>OpenFlamingo-9B*
   </td>
   <td>43.5
   </td>
   <td>44.0
   </td>
   <td>47.5
   </td>
   <td>48.9
   </td>
   <td>50.3
   </td>
  </tr>
  <tr>
   <td>DeepMind Flamingo-9B
   </td>
   <td>51.8
   </td>
   <td>56.3
   </td>
   <td>58.0
   </td>
   <td>59.4
   </td>
   <td>60.4
   </td>
  </tr>
</table>
<p>*Note that we report validation performance (using the same setup outlined in Flamingo paper) for OpenFlamingo-9B while DeepMind Flamingo-9B performance is on test data.</p>
<p><strong>Safety and ethical considerations</strong></p>
<p>As OpenFlamingo-9B is built on top of frozen <a href="https://arxiv.org/abs/2302.13971">LLaMA</a> and <a href="https://arxiv.org/abs/2103.00020">CLIP</a> models, you can expect OpenFlamingo to inherit the harms of the parent models. We understand that by releasing these models, they may be used in harmful ways. However, it is important for the research community to study the harms of large multimodal models, and we believe that open-sourcing these models will enable the community to develop better ways to mitigate these harms in future models.</p>
<p>We emphasize that OpenFlamingo-9B is a research artifact and not a finished product. It can produce unintended, inappropriate, offensive, and/or inaccurate results. We thus advocate for caution and thorough evaluations before using our models in any real applications.</p>
<h3><a id="contributions" class="anchor" href="#contributions" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Contributions</h3>
<p><strong>Thanks to:</strong></p>
<ul>
<li><a href="https://homes.cs.washington.edu/~jpgard/">Josh Gardner</a> and <a href="https://yonatanbitton.github.io/">Yonatan Bitton</a> for implementing the evaluation benchmark.</li>
<li><a href="https://kalyani7195.github.io/">Kalyani Marathe</a> for implementing the data pipeline and improving code quality.</li>
<li><a href="https://www.linkedin.com/in/yusufhanafy/">Yusuf Hanafy</a> for working on the demo.</li>
<li><a href="https://wanrong-zhu.com/">Wanrong Zhu</a>, <a href="https://jmhessel.com/">Jack Hessel</a>, and <a href="https://sagadre.github.io/">Samir Gadre</a> for building the Multimodal C4 dataset.</li>
<li><a href="https://scholar.google.de/citations?user=p1FuAMkAAAAJ&amp;hl=en">Jenia Jitsev</a> for helping us with large scale training.</li>
<li><a href="https://mitchellnw.github.io/">Mitchell Wortsman</a>, <a href="https://gabrielilharco.com/">Gabriel Ilharco</a>, <a href="https://simonster.com/">Simon Kornblith</a>, <a href="https://koh.pw/">Pang Wei Koh</a> for technical discussions and for feedback on this blog.</li>
<li><a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt</a> for being our main advisor on this project and for their support.</li>
</ul>
<h3><a id="acknowledgements" class="anchor" href="#acknowledgements" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgements</h3>
<p>This code is based on Lucidrains' <a href="https://github.com/lucidrains/flamingo-pytorch">flamingo implementation</a> and David Hansmair's <a href="https://github.com/dhansmair/flamingo-mini">flamingo-mini repo</a>. Thank you for making your code public! We also thank the <a href="https://github.com/mlfoundations/open_clip">OpenCLIP</a> team as we use their data loading code and take inspiration from their library design.</p>
<p>We would like to thank <a href="https://www.jbalayrac.com/">Jean-Baptiste Alayrac</a> and <a href="https://antoine77340.github.io/">Antoine Miech</a> for their advice, <a href="https://www.rohantaori.com/">Rohan Taori</a>, <a href="https://nicholasschiefer.com/">Nicholas Schiefer</a>, <a href="https://hai.stanford.edu/people/deep-ganguli">Deep Ganguli</a>, <a href="https://thomasliao.com/">Thomas Liao</a>, <a href="https://thashim.github.io/">Tatsunori Hashimoto</a>, and <a href="https://nicholas.carlini.com/">Nicholas Carlini</a> for their help with assessing the safety risks of our release. This research is supported in part by NSF Institute on the Foundations of Machine Learning (IFML). Thanks to <a href="https://stability.ai">Stability AI</a> for providing us with compute resources to train these models!</p>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Announcing OpenFlamingo: An open-source framework for training vision-language models with in-context learning","author":"Anas Awadalla and Irena Gao","date":"Mar 28 2023","previewImg":"/images/blog/flamingo-logo.png"},"content":"\n**Overview.**\nWe are thrilled to announce the release of OpenFlamingo, an open-source reproduction of DeepMind's Flamingo model. At its core, OpenFlamingo is a framework that enables training and evaluation of large multimodal models (LMMs). Check out our [GitHub repository](https://github.com/mlfoundations/open_flamingo) and [demo](https://7164d2142d11.ngrok.app) to get started!\n\nFor this first release, our contributions are as follows:\n\n* üèãÔ∏è A Python framework to train Flamingo-style LMMs (based on Lucidrains' [flamingo implementation](https://github.com/lucidrains/flamingo-pytorch) and David Hansmair's [flamingo-mini repository](https://github.com/dhansmair/flamingo-mini)).\n* ü™Ö A large-scale multimodal dataset with interleaved image and text sequences.\n* üß™ An in-context learning evaluation benchmark for vision-language tasks.\n* ü§ñ A first version of our OpenFlamingo-9B model based on LLaMA, with much better models to come!\n\n\nThe recent progress in open-source LMMs with the release of [BLIP-2](https://arxiv.org/abs/2301.12597) and [FROMAGe](https://jykoh.com/fromage) has shown the exciting potential of multimodal systems. We hope that OpenFlamingo will help drive progress in multimodal machine learning, and we have more exciting contributions in the pipeline, so stay tuned! \n\n\n**Goal.**\nOur goal with OpenFlamingo is to develop a multimodal system that can tackle a diverse range of vision-language tasks. Ultimately, we aim to match the power and versatility of GPT-4 in handling visual and text input. To achieve this goal, we are creating an open-source version of [DeepMind's Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model) model, a LMM capable of processing and reasoning about images, videos, and text. We are committed to build fully open-source models, and believe this transparency is essential for fostering collaboration, accelerating progress, and democratizing access to state-of-the-art LMMs. Our release is the first step towards this goal.\n\nWe are sharing the first checkpoint of our OpenFlamingo-9B model. While the model is not yet fully optimized, it demonstrates the potential of this project. By working together and receiving feedback from the community, we can train better LMMs. We encourage the community to participate in the development process by providing feedback and contributing to the repository. \n\n\n**Technical Details.**\nOur implementation largely follows that of [Flamingo](https://arxiv.org/abs/2204.14198). Flamingo models are trained on large-scale web corpora containing interleaved text and images, which is crucial for endowing them with in-context few-shot learning capabilities. OpenFlamingo implements the same architecture (Perceiver resamplers, cross-attention layers) proposed in the original Flamingo paper. However, since the training data for Flamingo is not available to the public, we use open-source datasets for training our models. Specifically, the released OpenFlamingo-9B checkpoint is trained on 5M samples from our new Multimodal C4 dataset and 10M samples from [LAION-2B](https://huggingface.co/datasets/laion/laion2B-en). \n\n\n## **Multimodal C4**\n\nThe Multimodal-C4 dataset is an expansion of the text-only [C4 dataset](https://www.tensorflow.org/datasets/catalog/c4), which was used to train  [T5 models](https://arxiv.org/abs/1910.10683). This dataset is built by our collaborators [Jack Hessel](https://jmhessel.com) and [Wanrong Zhu](https://wanrong-zhu.com) at the Allen Institute for AI. For each document in the [C4 en.clean](https://www.tensorflow.org/datasets/catalog/c4#c4en_default_config) dataset, we retrieve the original webpage from [Common Crawl](https://commoncrawl.org/), then collect the downloadable images. Data cleaning is carried out through deduplication and content filtering, which aims to eliminate non-safe for work (NSFW) and unrelated images, such as advertisements. Additionally, we run face detection and discard images with positive identifications. Finally, images and sentences are interleaved using bipartite matching within a document: CLIP ViT/L-14 image-text similarities serve as edge weights. Multimodal-C4 consists of approximately 75 million documents, encompassing around 400M images and 38B tokens. A full release with more detail is coming soon.\n\n![](/images/blog/mmc4-example.png)\n\n## **Benchmark**\n\nTo measure the performance of OpenFlamingo, we evaluate on a diverse set of downstream tasks. Our aim is to eventually build an open-source version of Flamingo‚Äôs benchmark and extend past that to standardize vision-language task evaluation. Currently we support visual question-answering ([VQAv2](https://visualqa.org/index.html), [OK-VQA](https://okvqa.allenai.org)), captioning ([COCO](https://cocodataset.org/#home), [Flickr30k](https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset)), and image classification ([ImageNet](https://image-net.org/index.php)) tasks. Expect us to add many more evaluation sets that probe model reasoning, biases, and more! You can access the benchmark on the OpenFlamingo repo. \n\n\n## **Model release**\n\n![](/images/blog/flamingo-llama.png)\n\nAs part of our release, we are also providing a checkpoint from our under-development OpenFlamingo-9B, a LMM built on top of [LLaMA 7B](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) and [CLIP ViT/L-14](https://openai.com/research/clip). This model is still a work in progress but it can already bring a lot of value to the community. For instance,\n\n![](/images/blog/flamingo-9B-sample-one.png)\n![](/images/blog/flamingo-9B-sample-two.png)\n\n**Performance**\n\nWe evaluated our checkpoint on COCO and VQAv2. Here we report the validation performance using a different number of shots. \n\nCOCO (CIDEr)\n\u003ctable\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e\n   \u003c/td\u003e\n   \u003ctd\u003e0-shot\n   \u003c/td\u003e\n   \u003ctd\u003e4-shot\n   \u003c/td\u003e\n   \u003ctd\u003e8-shot\n   \u003c/td\u003e\n   \u003ctd\u003e16-shot\n   \u003c/td\u003e\n   \u003ctd\u003e32-shot\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eOpenFlamingo-9B*\n   \u003c/td\u003e\n   \u003ctd\u003e65.5\n   \u003c/td\u003e\n   \u003ctd\u003e74.3\n   \u003c/td\u003e\n   \u003ctd\u003e79.3\n   \u003c/td\u003e\n   \u003ctd\u003e81.8\n   \u003c/td\u003e\n   \u003ctd\u003e84.5\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eDeepMind Flamingo-9B\n   \u003c/td\u003e\n   \u003ctd\u003e79.4\n   \u003c/td\u003e\n   \u003ctd\u003e93.1\n   \u003c/td\u003e\n   \u003ctd\u003e99.0\n   \u003c/td\u003e\n   \u003ctd\u003e102.2\n   \u003c/td\u003e\n   \u003ctd\u003e106.3\n   \u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n---\n\nVQAv2 (VQA accuracy)\n\u003ctable\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e\n   \u003c/td\u003e\n   \u003ctd\u003e0-shot\n   \u003c/td\u003e\n   \u003ctd\u003e4-shot\n   \u003c/td\u003e\n   \u003ctd\u003e8-shot\n   \u003c/td\u003e\n   \u003ctd\u003e16-shot\n   \u003c/td\u003e\n   \u003ctd\u003e32-shot\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eOpenFlamingo-9B*\n   \u003c/td\u003e\n   \u003ctd\u003e43.5\n   \u003c/td\u003e\n   \u003ctd\u003e44.0\n   \u003c/td\u003e\n   \u003ctd\u003e47.5\n   \u003c/td\u003e\n   \u003ctd\u003e48.9\n   \u003c/td\u003e\n   \u003ctd\u003e50.3\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eDeepMind Flamingo-9B\n   \u003c/td\u003e\n   \u003ctd\u003e51.8\n   \u003c/td\u003e\n   \u003ctd\u003e56.3\n   \u003c/td\u003e\n   \u003ctd\u003e58.0\n   \u003c/td\u003e\n   \u003ctd\u003e59.4\n   \u003c/td\u003e\n   \u003ctd\u003e60.4\n   \u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n\n*Note that we report validation performance (using the same setup outlined in Flamingo paper) for OpenFlamingo-9B while DeepMind Flamingo-9B performance is on test data.\n\n**Safety and ethical considerations**\n\nAs OpenFlamingo-9B is built on top of frozen [LLaMA](https://arxiv.org/abs/2302.13971) and [CLIP](https://arxiv.org/abs/2103.00020) models, you can expect OpenFlamingo to inherit the harms of the parent models. We understand that by releasing these models, they may be used in harmful ways. However, it is important for the research community to study the harms of large multimodal models, and we believe that open-sourcing these models will enable the community to develop better ways to mitigate these harms in future models.\n\nWe emphasize that OpenFlamingo-9B is a research artifact and not a finished product. It can produce unintended, inappropriate, offensive, and/or inaccurate results. We thus advocate for caution and thorough evaluations before using our models in any real applications.\n\n\n### Contributions\n\n**Thanks to:**\n\n* [Josh Gardner](https://homes.cs.washington.edu/~jpgard/) and [Yonatan Bitton](https://yonatanbitton.github.io/) for implementing the evaluation benchmark.\n* [Kalyani Marathe](https://kalyani7195.github.io/) for implementing the data pipeline and improving code quality.\n* [Yusuf Hanafy](https://www.linkedin.com/in/yusufhanafy/) for working on the demo.\n* [Wanrong Zhu](https://wanrong-zhu.com/), [Jack Hessel](https://jmhessel.com/), and [Samir Gadre](https://sagadre.github.io/) for building the Multimodal C4 dataset.\n* [Jenia Jitsev](https://scholar.google.de/citations?user=p1FuAMkAAAAJ\u0026hl=en) for helping us with large scale training.\n* [Mitchell Wortsman](https://mitchellnw.github.io/), [Gabriel Ilharco](https://gabrielilharco.com/), [Simon Kornblith](https://simonster.com/), [Pang Wei Koh](https://koh.pw/) for technical discussions and for feedback on this blog.\n* [Ludwig Schmidt](https://people.csail.mit.edu/ludwigs/) for being our main advisor on this project and for their support.\n\n\n### Acknowledgements\n\nThis code is based on Lucidrains' [flamingo implementation](https://github.com/lucidrains/flamingo-pytorch) and David Hansmair's [flamingo-mini repo](https://github.com/dhansmair/flamingo-mini). Thank you for making your code public! We also thank the [OpenCLIP](https://github.com/mlfoundations/open_clip) team as we use their data loading code and take inspiration from their library design.\n\nWe would like to thank [Jean-Baptiste Alayrac](https://www.jbalayrac.com/) and [Antoine Miech](https://antoine77340.github.io/) for their advice, [Rohan Taori](https://www.rohantaori.com/), [Nicholas Schiefer](https://nicholasschiefer.com/), [Deep Ganguli](https://hai.stanford.edu/people/deep-ganguli), [Thomas Liao](https://thomasliao.com/), [Tatsunori Hashimoto](https://thashim.github.io/), and [Nicholas Carlini](https://nicholas.carlini.com/) for their help with assessing the safety risks of our release. This research is supported in part by NSF Institute on the Foundations of Machine Learning (IFML). Thanks to [Stability AI](https://stability.ai) for providing us with compute resources to train these models!\n","slug":"open-flamingo"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"open-flamingo"},"buildId":"zdSCNtiOyCDn7dVCkW2TW","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>