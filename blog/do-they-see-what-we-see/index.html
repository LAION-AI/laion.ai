<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Do They See What We See? | LAION</title><meta name="title" content="Do They See What We See? | LAION"/><meta property="og:title" content="Do They See What We See? | LAION"/><meta name="twitter:title" content="Do They See What We See? | LAION"/><meta name="description" content="&lt;p&gt;Building Emotionally Intelligent AI with EmoNet, a suite of open tools and resources developed by the LAION community&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A LAION &amp;amp; Intel Colla..."/><meta property="og:description" content="&lt;p&gt;Building Emotionally Intelligent AI with EmoNet, a suite of open tools and resources developed by the LAION community&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A LAION &amp;amp; Intel Colla..."/><meta name="twitter:description" content="&lt;p&gt;Building Emotionally Intelligent AI with EmoNet, a suite of open tools and resources developed by the LAION community&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A LAION &amp;amp; Intel Colla..."/><meta property="og:image" content="https://laion.ai/images/blog/do_they_see/image4.png"/><meta name="twitter:image" content="https://laion.ai/images/blog/do_they_see/image4.png"/><meta name="twitter:image:alt" content="The text: LAION. Large-scale Artificial Intelligence Open Network, TRULY OPEN AI. 100% NON-PROFIT. 100% FREE."/><meta property="og:type" content="website"/><meta property="og:url" content="https://laion.ai/blog/do-they-see-what-we-see"/><meta name="twitter:url" content="https://laion.ai/blog/do-they-see-what-we-see"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1D374E"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.png"/><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff2" as="font" type="font/woff2" crossorigin="true"/><meta name="next-head-count" content="25"/><link rel="stylesheet" href="/fonts/load.css"/><link rel="preload" href="/_next/static/css/5357c8cce67e7f29.css" as="style"/><link rel="stylesheet" href="/_next/static/css/5357c8cce67e7f29.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fb0512e25146295.js" defer=""></script><script src="/_next/static/chunks/286-30519d8a3e60551d.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-44fac0971625f498.js" defer=""></script><script src="/_next/static/_SM4P8X5pqC39g-fsfkhJ/_buildManifest.js" defer=""></script><script src="/_next/static/_SM4P8X5pqC39g-fsfkhJ/_ssgManifest.js" defer=""></script><script src="/_next/static/_SM4P8X5pqC39g-fsfkhJ/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-paper md:text-sky md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LAION</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/team/">Team</a><a href="/blog/">Blog</a><a href="/notes/">Notes</a><a href="/press/">Press</a><a href="/about/">About</a><a href="/faq/">FAQ</a><a href="/donations/">Donations</a><a href="/privacy-policy/">Privacy Policy</a><a href="/dataset-requests/">Dataset Requests</a><a href="/impressum/">Impressum</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1D374E;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/team/">Team</a></p><p><a href="/blog/">Blog</a></p><p><a href="/notes/">Notes</a></p><p><a href="/press/">Press</a></p><p><a href="/about/">About</a></p><p><a href="/faq/">FAQ</a></p><p><a href="/donations/">Donations</a></p><p><a href="/privacy-policy/">Privacy Policy</a></p><p><a href="/dataset-requests/">Dataset Requests</a></p><p><a href="/impressum/">Impressum</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-paper grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-8xl md:text-8xl w-full font-bold title-flow break-words">DO THEY SEE WHAT WE SEE?</h1><p class="text-2xl pb-2">by: <!-- -->LAION<!-- -->,<!-- --> <!-- -->19 Jun, 2025<!-- --></p><hr/><div class="pt-2 article"><p>Building Emotionally Intelligent AI with EmoNet, a suite of open tools and resources developed by the LAION community</p>
<p><em>A LAION &amp; Intel Collaboration</em></p>
<h2><a id="authors" class="anchor" href="#authors" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Authors</h2>
<p><strong>LAION e.V.</strong><br>
Christoph Schuhmann*, Robert Kaczmarczyk*, Gollam Rabby, Felix Friedrich, Maurice Kraus, Kourosh Nadi, Desmond Grealy, Huu Nguyen, Cahya Wirawan, Krishna Kalyan, Kristian Kersting, Sören Auer</p>
<p><strong>Intel Corporation</strong><br>
Jayaraman Mahalingam</p>
<p><img src="/images/blog/do_they_see/image4.png" alt=""></p>
<p>An exciting frontier in technology today is the quest for artificial intelligence that truly understands and interacts with humans on a deeper level. While AI has made remarkable progress in language processing and complex problem-solving, one critical dimension has yet to be fully realized: true emotional intelligence.</p>
<p>Can our AI systems perceive the subtle joy in a crinkled eye, the faint tremor of anxiety in a voice, or the complex blend of emotions that color our everyday interactions? We believe this is not just a fascinating academic pursuit but a fundamental necessity for the future of human-AI collaboration.</p>
<p>Today, we're proud to release <strong>EmoNet</strong> – a suite of new, open and freely available models and tools designed to support global research and innovation in the emerging field of emotionally intelligent AI. Our contributions are multi-faceted, addressing critical gaps in current research and providing powerful new tools for the global AI community.</p>
<p>Thank you to our partner Intel. LAION and Intel have been collaborating on fostering empathic, thoughtful and productive human-AI interaction for several years.</p>
<h2><a id="voice-acting-samples-demo" class="anchor" href="#voice-acting-samples-demo" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Voice Acting Samples Demo</h2>
<div align="center">
<div style="position: relative; width: 100%; padding-bottom: 56.25%; height: 0; margin-bottom: 1em;">
  <iframe 
    src="https://www.youtube.com/embed/TsTVKCmqHhk" 
    title="Voice Acting Samples Demo"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen
    style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;">
  </iframe>
</div>
</div>
<h2><a id="our-empathic-computing-contributions" class="anchor" href="#our-empathic-computing-contributions" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Our Empathic Computing Contributions</h2>
<h3><a id="1-emonet-face-benchmark" class="anchor" href="#1-emonet-face-benchmark" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>1. EMONET-FACE Benchmark</h3>
<p>A novel, expert-annotated benchmark for fine-grained facial emotion estimation, featuring a comprehensive 40-category emotion taxonomy and large-scale, demographically diverse synthetic image datasets (EMONET-FACE BIG, BINARY, and HQ).</p>
<blockquote>
<p><strong>📊 Dataset:</strong> <a href="https://huggingface.co/datasets/laion/EmoNet-Face_Big">EMONET-FACE hosted by Hugging Face</a></p>
</blockquote>
<p><img src="/images/blog/do_they_see/image5.png" alt=""></p>
<p><strong>Figure 2:</strong> Samples from our EmoNet-Face datasets generated with different SOTA T2I models.</p>
<h3><a id="2-emonet-voice-benchmark" class="anchor" href="#2-emonet-voice-benchmark" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>2. EMONET-VOICE Benchmark</h3>
<p>A similarly fine-grained, expert-verified benchmark for speech emotion detection. Built upon our 40-category taxonomy and leveraging state-of-the-art synthetic voice generation for privacy and diversity. It includes 4,692 high-agreement audio samples.</p>
<p><img src="/images/blog/do_they_see/image7.png" alt=""></p>
<p><strong>Table 1:</strong> Comparison of SER datasets. Key aspects include licensing, scale, emotional range, speaker diversity, synthetic origin, multilingual support and defining characteristics.</p>
<h3><a id="3-empathic-insight-face-model" class="anchor" href="#3-empathic-insight-face-model" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>3. EMPATHIC INSIGHT-FACE Model</h3>
<p>A state-of-the-art model for facial emotion estimation trained on our EMONET-FACE suite, surpassing the performance of leading models like Gemini 2.5 Pro and proprietary APIs like Hume AI on our benchmarks.</p>
<p><img src="/images/blog/do_they_see/image6.png" alt=""></p>
<p><strong>Figure 3:</strong> Mean Spearman's Rho correlation between various model annotators and human annotations.</p>
<p>This figure highlights the strength of facial emotion recognition correlation between the EmoNet Empathic Insights models and actual emotions compared to other models. The bar heights represent the mean of these per-emotion Spearman's Rho values calculated across all emotions for each model. Error bars indicate bootstrap 95% confidence intervals (N=1000 bootstraps) for these means. Model annotator groups, including our trained models (Empathic-Insight-Face), VLMs with multi-shot or zero-shot prompting, proprietary models (HumeFace), and a random baseline, are distinguished by patterns as detailed in the legend.</p>
<blockquote>
<p><strong>🔗 Models:</strong></p>
<ul>
<li><a href="https://huggingface.co/laion/Empathic-Insight-Face-Large">Large Empathic-Insight-Face Model</a></li>
<li><a href="https://huggingface.co/laion/Empathic-Insight-Face-Small">Small Empathic-Insight-Face Model</a></li>
</ul>
</blockquote>
<p><img src="/images/blog/do_they_see/image9.png" alt=""></p>
<p><strong>Figure 4:</strong> EMPATHIC INSIGHT-FACE Model prediction example</p>
<blockquote>
<p><strong>💻 Try it:</strong> <a href="https://colab.research.google.com/drive/11oUMo2HX0OuD9dx5ZM4ltNvoYxbI65hu?usp=sharing">EMPATHIC INSIGHT-FACE Model Colab</a></p>
</blockquote>
<h3><a id="4-empathic-insight-voice-model" class="anchor" href="#4-empathic-insight-voice-model" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>4. EMPATHIC INSIGHT-VOICE Model</h3>
<p>A state-of-the-art model for speech emotion estimation, setting a new benchmark for nuanced understanding of vocal emotional cues, similarly outperforming established systems on our EMONET-VOICE benchmark.</p>
<p><img src="/images/blog/do_they_see/image8.png" alt=""></p>
<p><strong>Table 2:</strong> Performance Comparison of Audio Language Models on the EmoNet-Voice Benchmark.</p>
<blockquote>
<p><strong>🔗 Models:</strong></p>
<ul>
<li><a href="https://huggingface.co/laion/Empathic-Insight-Voice-Large">Large Empathic-Insight-Voice Model</a></li>
<li><a href="https://huggingface.co/laion/Empathic-Insight-Voice-Small">Small Empathic-Insight-Voice Model</a></li>
</ul>
</blockquote>
<h3><a id="5-bud-e-whisper-better-understanding-of-emotion-whisper" class="anchor" href="#5-bud-e-whisper-better-understanding-of-emotion-whisper" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>5. BUD-E Whisper (Better Understanding of Emotion Whisper)</h3>
<p>A suite of fine-tuned Whisper models for advanced emotional speech captioning, going beyond mere transcription to describe emotional tone, vocal bursts, and speaker traits.</p>
<blockquote>
<p><strong>🔗 Resources:</strong></p>
<ul>
<li><a href="https://huggingface.co/laion/BUD-E-Whisper">BUD-E Whisper Model</a></li>
<li><a href="https://colab.research.google.com/drive/1VoAtmNhY1hI5Yzv1_dppHTcYky82OCDK?usp=sharing">BUD-E Whisper Colab</a></li>
</ul>
</blockquote>
<h3><a id="6-laions-got-talent-dataset" class="anchor" href="#6-laions-got-talent-dataset" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>6. LAION's Got Talent Dataset</h3>
<p>An extensive synthetic voice-acting dataset, forming the foundation for EMONET-VOICE, featuring over <strong>5,000 hours of speech</strong> across 11 synthetic voices, 40 emotions, and 4 languages. The cumulative playtime of this dataset is more than the cumulative playtime of all movies shown in US cinemas from 2021 to 2024, putting its sheer scale into perspective.</p>
<blockquote>
<p><strong>📊 Datasets:</strong></p>
<ul>
<li><a href="https://huggingface.co/datasets/laion/laions_got_talent_enhanced_flash_annotations_and_long_captions">Enhanced Version with Gemini Flash 2.0 Annotations</a></li>
<li><a href="https://huggingface.co/datasets/laion/laions_got_talent_raw">Raw Dataset</a></li>
</ul>
</blockquote>
<h2><a id="introducing-emonet-face--emonet-voice-a-new-foundation" class="anchor" href="#introducing-emonet-face--emonet-voice-a-new-foundation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introducing EMONET-FACE &amp; EMONET-VOICE: A New Foundation</h2>
<p>To address these challenges, we developed the EMONET suites. At their core is a <strong>novel 40-category emotion taxonomy</strong>, meticulously derived from an extensive analysis of the &quot;Handbook of Emotions&quot; and refined through consultation with psychologists. This taxonomy moves far beyond basic emotions, encompassing a rich spectrum of positive and negative affective states, cognitive states (e.g., <em>Concentration, Confusion, Doubt</em>), physical states (e.g., <em>Pain, Fatigue, Intoxication</em>), and socially mediated emotions (e.g., <em>Embarrassment, Shame, Pride, Teasing</em>). This granularity is crucial for building AI that can appreciate the finer details of human emotional life.</p>
<h3><a id="emonet-face" class="anchor" href="#emonet-face" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>EMONET-FACE</h3>
<p>EMONET-FACE provides a rich resource for visual emotion understanding:</p>
<ul>
<li><strong>EMONET-FACE BIG</strong> (over 203,000 synthetic images) offers a vast dataset for pre-training models.</li>
<li><strong>EMONET-FACE BINARY</strong> (approx. 20,000 images) is designed for fine-tuning and features over 62,000 binary (present/absent) emotion annotations from human experts. These annotations underwent a rigorous multi-stage process, requiring triple positive agreement for affirmative labels and a contrastive batch to ensure high-quality true negatives.</li>
<li><strong>EMONET-FACE HQ</strong> (2,500 images) serves as our gold-standard evaluation benchmark. Each image was meticulously rated by multiple psychology experts on a continuous 0-7 intensity scale across all 40 emotion categories, resulting in 10,000 expert annotations.</li>
</ul>
<p>The synthetic images were generated using state-of-the-art text-to-image models with explicit prompts to ensure diverse demographic representation (across ethnicity, age, and gender) and clear, full-face expressions. This approach not only allows for controlled diversity but also sidesteps the ethical concerns associated with using real individuals' images.</p>
<p><img src="/images/blog/do_they_see/image2.png" alt=""></p>
<p><strong>Figure 5:</strong> Mapping Facial Expression Emotional Understanding</p>
<h3><a id="emonet-voice" class="anchor" href="#emonet-voice" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>EMONET-VOICE</h3>
<p>EMONET-VOICE tackles the auditory domain with similar rigor:</p>
<ul>
<li>EMONET-VOICE curates <strong>4,692 high-agreement audio samples</strong> from <strong>LAION's Got Talent</strong>.</li>
<li>Each snippet simulates actors portraying scenes designed to evoke specific emotions.</li>
<li>Crucially, each snippet underwent rigorous validation by human experts with psychology degrees. They assigned perceived intensity labels (<em>Not Present, Mildly Present, Intensely Present</em>) based on a strict 3-annotator consensus protocol, focusing on estimating the <em>presence and intensity</em> of emotions rather than assuming a single definitive label.</li>
</ul>
<p>This privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets.</p>
<h2><a id="why-emotion-in-speech-and-face-matters-the-vision-of-universal-voice-actors" class="anchor" href="#why-emotion-in-speech-and-face-matters-the-vision-of-universal-voice-actors" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why Emotion in Speech and Face Matters: The Vision of Universal Voice Actors</h2>
<p>Effective communication transcends mere words. It's woven with the rich threads of emotion, conveyed through the subtle shifts in our facial expressions and the intricate nuances of our voice. Capturing these expressions enables AI assistants to become more empathetic, engaging, and supportive; qualities crucial for transformative applications in education, mental health, companionship, and beyond.</p>
<p>We envision a future where multimodal foundation models evolve into &quot;omni-models&quot; with sophisticated audio-in/audio-out capabilities. Soon, every new foundation model on platforms like Hugging Face could be capable of performing voice acting like Robert De Niro or Scarlett Johansson. These AI systems will function like world-class voice actors, capable of being prompted not just by text, but also by voice, to adopt any persona. Imagine an AI that can embody an empathetic educator adapting to a student's confusion, a thrilling storyteller captivating an audience, or a knowledgeable research assistant explaining complex concepts with clarity and appropriate gravitas. This level of seamless and inspiring human-AI interaction is our ultimate goal.</p>
<h2><a id="the-imperative-for-better-benchmarks-seeing-and-hearing-the-nuance" class="anchor" href="#the-imperative-for-better-benchmarks-seeing-and-hearing-the-nuance" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Imperative for Better Benchmarks: Seeing and Hearing the Nuance</h2>
<p>The journey to emotionally intelligent AI begins with data. Existing datasets for emotion recognition, while valuable, often present significant limitations. Facial emotion datasets might rely on a narrow range of &quot;basic&quot; emotions, use images with occlusions or poor lighting, or lack demographic diversity, leading to biased models that perform poorly across different populations. Similarly, speech emotion datasets can be constrained by coarse emotion taxonomies, privacy concerns tied to real user data, or an over-reliance on acted portrayals that don't capture the subtlety of spontaneous emotional expression.</p>
<p>The <strong>Theory of Constructed Emotion (TCE)</strong>, a prominent psychological framework (<a href="https://pubmed.ncbi.nlm.nih.gov/27798257/">research link</a>), posits that emotions are not universal, pre-programmed entities that we simply &quot;recognize.&quot; Instead, they are constructed by our brains based on a combination of interoceptive signals (like valence – pleasantness/unpleasantness, and arousal – activation/deactivation), learned concepts, and contextual information. This means there isn't a single, definitive facial expression or vocal intonation for &quot;joy&quot; or &quot;sadness&quot; that is universally and unambiguously displayed. Rather, emotional expression is a complex, dynamic, and often ambiguous signal.</p>
<p>This understanding underscores the need for <strong>emotion estimation</strong> rather than simple recognition. We need AI that can assess the <em>likelihood</em> and <em>intensity</em> of various emotions being present, rather than forcing a single label onto a complex human state.</p>
<p><img src="/images/blog/do_they_see/image10.png" alt=""></p>
<p><strong>Figure 6:</strong> Mapping Voice Emotional Understanding</p>
<h2><a id="motivation-and-taxonomy-development-capturing-the-full-spectrum" class="anchor" href="#motivation-and-taxonomy-development-capturing-the-full-spectrum" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Motivation and Taxonomy Development: Capturing the Full Spectrum</h2>
<p>Our approach to defining the emotional landscape for these datasets was systematic and grounded in psychological theory, while also addressing practical needs for AI applications:</p>
<ol>
<li><strong>Theoretical Foundations &amp; Gap Analysis:</strong> We analyzed established psychological frameworks, such as Plutchik's Wheel of Emotions. While foundational, these often neglect states critical for real-world AI, like <em>Bitterness</em> (often linked to social exclusion), physical states like <em>Pain</em> and <em>Fatigue</em>, or altered states like <em>Intoxication</em>. Though not always strictly classified as &quot;emotions&quot; in psychology, their accurate perception is vital for empathetic AI.</li>
<li><strong>&quot;Handbook of Emotions&quot; Extraction:</strong> We processed the comprehensive &quot;Handbook of Emotions&quot; (edited by Lisa Feldman Barrett et al.) in 500-word chunks, using a large language model (LLM) to extract emotion-related nouns and adjectives.</li>
<li><strong>Refinement to 40 Categories:</strong> After automatic clustering of these terms, manual refinement by our team, in consultation with psychology experts, yielded our final 40-category taxonomy. This taxonomy, shared by both EMONET-FACE and EMONET-VOICE, is detailed below.</li>
</ol>
<h3><a id="full-emotion-taxonomy" class="anchor" href="#full-emotion-taxonomy" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Full Emotion Taxonomy</h3>
<p><img src="/images/blog/do_they_see/image13.png" alt=""></p>
<p><strong>Table 3:</strong> Taxonomy Listing of Emotions Captured in Datasets</p>
<h2><a id="the-emonet-voice-benchmark-a-gold-standard-for-speech-emotion" class="anchor" href="#the-emonet-voice-benchmark-a-gold-standard-for-speech-emotion" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The EMONET-VOICE Benchmark: A Gold Standard for Speech Emotion</h2>
<p>Leveraging the rich LAION's Got Talent dataset, we are establishing the <strong>EMONET-VOICE Benchmark</strong>. This isn't just another dataset; it's a meticulously curated training and evaluation corpus for our 40-category emotion taxonomy.</p>
<ul>
<li><strong>Intensity Annotation:</strong> Each audio snippet is being annotated for emotional expression intensity. For most emotions, a three-level scale is used: non-existent, slightly expressed, and strongly/fully expressed. For certain high-intensity or inherently ambiguous categories, a two-level distinction (non-existent vs. present) is applied.</li>
<li><strong>Expert Psychological Annotators:</strong> To ensure the highest reliability, labeling is conducted by annotators with formal psychological training (typically at least a bachelor's degree).</li>
<li><strong>Strict Consensus Protocol:</strong> Only labels achieving exact agreement among three independent expert annotators (no disagreement on the intensity level) are included. This conservative approach ensures exceptional inter-rater reliability, making the annotations a true gold standard.</li>
</ul>
<p>Unlike existing benchmarks often limited to basic emotions or binary states, EMONET-VOICE offers a structured, expert-validated evaluation across 40 nuanced categories, applicable to both naturalistic and synthetic speech. This will enable objective, reproducible, and fine-grained assessment of models like BUD-E Whisper and future multimodal systems.</p>
<p><img src="/images/blog/do_they_see/image11.png" alt=""></p>
<h3><a id="emonet-face-paralleling-progress-in-visual-emotion-recognition" class="anchor" href="#emonet-face-paralleling-progress-in-visual-emotion-recognition" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>EMONET-FACE: Paralleling Progress in Visual Emotion Recognition</h3>
<p>Our efforts in speech emotion are mirrored by our work in facial emotion recognition with the <strong>EMONET-FACE</strong> suite. This includes:</p>
<ul>
<li><strong>EMONET-FACE BIG:</strong> Over 203,000 synthetic images for pre-training.</li>
<li><strong>EMONET-FACE BINARY:</strong> Approx. 20,000 images with 65,000+ human expert binary emotion annotations for fine-tuning.</li>
<li><strong>EMONET-FACE HQ:</strong> 2,500 images with 10,000 continuous (0-7 scale) expert annotations across all 40 emotions for high-fidelity evaluation.</li>
</ul>
<h3><a id="empathic-insight-models-setting-new-state-of-the-art-performance" class="anchor" href="#empathic-insight-models-setting-new-state-of-the-art-performance" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>EMPATHIC INSIGHT Models: Setting New State-of-the-Art Performance</h3>
<p>The power of these benchmarks is demonstrated by our <strong>EMPATHIC INSIGHT</strong> models, which showcase the capabilities unlocked by our datasets and taxonomies:</p>
<ul>
<li><strong>EMPATHIC INSIGHT-FACE</strong> achieves human-expert-level performance on EMONET-FACE HQ, outperforming models like Gemini 2.5 Pro and proprietary APIs. (More technical details on the model architecture and training can be found in our accompanying paper/technical report).</li>
<li><strong>EMPATHIC INSIGHT-VOICE</strong>, trained on LAION's Got Talent and EMONET-VOICE, similarly sets a new SOTA for nuanced speech emotion estimation. (Further technical specifics on this model are also available).</li>
</ul>
<p>These models, are permissively licensed (Creative Commons for the models, Apache 2.0 for the code), prove that with focused dataset construction and careful modeling, AI can indeed learn to &quot;see&quot; and &quot;hear&quot; emotions with a level of nuance approaching human perception.</p>
<p><img src="/images/blog/do_they_see/image12.png" alt="">
<img src="/images/blog/do_they_see/image14.png" alt="">
<img src="/images/blog/do_they_see/image15.png" alt=""></p>
<p><strong>Figure 7:</strong> More EMPATHIC INSIGHT-FACE Model prediction examples</p>
<p><img src="/images/blog/do_they_see/image16.png" alt=""></p>
<p><strong>Figure 8:</strong> LAION's Empathic Insights Face Models Closely Track with Human Annotators</p>
<h2><a id="introducing-bud-e-whisper-beyond-transcription-to-emotional-understanding" class="anchor" href="#introducing-bud-e-whisper-beyond-transcription-to-emotional-understanding" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introducing BUD-E Whisper: Beyond Transcription to Emotional Understanding</h2>
<p>To truly unlock the emotional content within speech, transcription alone is insufficient. This led us to develop <strong>BUD-E Whisper</strong>, a suite of fine-tuned variants of OpenAI's Whisper model. BUD-E Whisper is specifically adapted for advanced emotion captioning. These models go beyond simply converting speech to text; they generate structured descriptions of:</p>
<ul>
<li><strong>Emotional Tone:</strong> Identifying the perceived emotions from our 40-category taxonomy.</li>
<li><strong>Vocal Bursts:</strong> Recognizing non-lexical expressions like laughter, sighs, gasps, etc.</li>
<li><strong>Speaker Traits:</strong> Inferring characteristics like age, gender, and even speech style.</li>
</ul>
<p>The training of BUD-E Whisper was a journey of iterative refinement. We utilized a diverse dataset including the <strong>LAION's Got Talent</strong> voice-acting data and approximately <strong>5,000 hours of audio</strong> from public vlogs, online diaries, and cinematic dialogue, with Voice Activity Detection (VAD) used to isolate speech segments. Gemini Flash 2.0 was then employed to annotate these samples along our 40-category emotion taxonomy.</p>
<p>Initial experiments attempting direct regression from Whisper's architecture to scalar emotion intensity values (0-5 scale) proved challenging, as the autoregressive nature of Whisper isn't inherently suited for stable numerical output. We then shifted to a captioning approach. First, we used procedurally generated emotion summaries – templated sentences describing emotions, intensity, and speaker traits. While an improvement, these templates led to syntactic predictability and overfitting. The breakthrough came when we used LLMs to <em>paraphrase</em> these procedural captions. This introduced crucial syntactic variability while preserving semantic consistency. Training on these diverse, paraphrased captions enabled BUD-E Whisper to produce fluid, context-sensitive, and highly interpretable emotion descriptions.</p>
<p>The result is a robust system capable of identifying and describing nuanced emotional signals in speech, representing a significant step towards more emotionally aware voice assistants. BUD-E Whisper is particularly useful for generating rich captions for audio, preparing data for training text-to-speech and foundation models.</p>
<h2><a id="laions-got-talent-a-rich-tapestry-of-synthetic-voices" class="anchor" href="#laions-got-talent-a-rich-tapestry-of-synthetic-voices" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>LAION's Got Talent: A Rich Tapestry of Synthetic Voices</h2>
<p>At the heart of our speech emotion initiative is the <strong>LAION's Got Talent</strong> dataset. This comprehensive resource, created via the HyperLab API leveraging OpenAI's GPT-4 Audio model, comprises:</p>
<ul>
<li><strong>11 distinct voices:</strong> Offering a variety of vocal timbres and characteristics.</li>
<li><strong>40 meticulously curated emotion categories:</strong> (Listed in detail above)</li>
<li><strong>Four languages:</strong> English (approx. 2,156 hours), German (approx. 716 hours), Spanish (approx. 888 hours), and French (approx. 881 hours).</li>
<li><strong>Acting Challenges (English + German):</strong> Approximately 111 hours of specialized scenarios.</li>
<li><strong>Diverse English Accent Distribution:</strong> Including Louisiana (~133h), Valley Girl (~159h), British (~132h), and even English spoken with simulated Chinese, French, German, Indian, Italian, Mexican, Russian, Spanish, and Texan accents, plus a &quot;Vulgar Street English&quot; category (~149h), and English without specific accent specified (~391h), ensuring a broad representation.</li>
</ul>
<blockquote>
<p><strong>📊 Download the LAION's Got Talent Datasets:</strong></p>
<ul>
<li><a href="https://huggingface.co/datasets/laion/laions_got_talent_enhanced_flash_annotations_and_long_captions">Enhanced Version with Gemini Flash 2.0 Annotations</a></li>
<li><a href="https://huggingface.co/datasets/laion/laions_got_talent_raw">Raw Dataset</a></li>
</ul>
</blockquote>
<h2><a id="the-power-of-synthetic-data-ethical-and-diverse" class="anchor" href="#the-power-of-synthetic-data-ethical-and-diverse" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Power of Synthetic Data: Ethical and Diverse</h2>
<p>A cornerstone of our initiative is the strategic use of synthetic data. This approach offers several key advantages:</p>
<ul>
<li><strong>Privacy:</strong> It entirely bypasses the ethical complexities and privacy risks associated with collecting and annotating real human emotional expressions, especially for sensitive states.</li>
<li><strong>Diversity and Control:</strong> We can programmatically ensure demographic diversity in our datasets, controlling for age, gender, and ethnicity in facial images, and voice characteristics in speech. This is crucial for building fairer and less biased AI systems.</li>
<li><strong>Scale and Scope:</strong> Synthetic generation allows us to create datasets of a scale and emotional breadth that would be prohibitively expensive or logistically impossible to achieve with human-acted or in-the-wild data.</li>
</ul>
<h2><a id="investing-in-open-standards-based-ai-innovation" class="anchor" href="#investing-in-open-standards-based-ai-innovation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Investing in Open Standards-based AI Innovation</h2>
<p>Intel is excited to collaborate with AI innovators like LAION, setting the groundwork for the use of AI in LLLMs and Visual computing for education, healthcare, and agentic assistive AI for all age groups and needs. It is innovative creators like those at LAION bringing empathy and emotional intelligence to AI that embody the positive social impact that responsible AI can deliver for people.</p>
<p>Intel has a strong commitment to open standards and open-source innovation, particularly in AI, software, and hardware ecosystems, fostering collaboration, accessibility, and interoperability. Intel supports open standards in AI through initiatives like the Linux Foundation's AI &amp; Data projects, ensuring AI frameworks and tools are hardware-agnostic and widely adoptable. This aligns with its <a href="https://huggingface.co/Intel">Hugging Face collaboration</a>, where Intel optimizes models for <a href="https://onnx.ai/">ONNX</a>-compatible inference on its hardware. This collaboration goes back to 2021 in order to optimize AI model performance on Intel hardware, focusing on accelerating training and inference for transformer-based models. Their partnership aims to democratize AI by making it faster, more efficient, and accessible through open-source tools.</p>
<p>The mission of Intel's COE with LAION, Germany, established in 2024, is <a href="https://www.intel.com/content/www/us/en/developer/articles/technical/bud-e-ai-assisted-education-for-all.html">to advance the development of BUD-E</a>, an open-source, empathetic AI education assistant that aims to democratize personalized learning worldwide. LAION is proud to work with Intel, famous for the <a href="https://en.wikipedia.org/wiki/International_Science_and_Engineering_Fair">International Science and Engineering Fair</a>, founded by former Intel CEO Gordon Moore. Hugging Face's Datasets, Models, and Collections are core components of its ecosystem, hosted on the Hugging Face Hub, designed to facilitate AI development and deployment. <strong>As further steps</strong>, these core components can be enhanced by using Intel-optimized datasets and models, integrated with Intel libraries, and curating collections to showcase these resources for tasks like generative AI or image classification, all aligned with its open-source commitment.</p>
<h3><a id="try-intels-ai-tools" class="anchor" href="#try-intels-ai-tools" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Try Intel's AI Tools</h3>
<p>If you'd like to sample the same technological innovations that LAION has access to, give these a try:</p>
<ul>
<li><a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html">PyTorch Optimizations from Intel</a></li>
<li><a href="https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html">OpenVINO™ Toolkit</a></li>
<li><a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html">Intel® oneAPI Toolkits</a></li>
</ul>
<p>See which leading research institutions, universities, and innovative startup companies are joint in efforts just like this as part of the <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/training/academic-program/centers-of-excellence.html">Intel® AI and oneAPI Center of Excellence</a> program.</p>
<h2><a id="the-future-reasoning-about-emotions-and-the-dawn-of-universal-voice-actors" class="anchor" href="#the-future-reasoning-about-emotions-and-the-dawn-of-universal-voice-actors" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Future: Reasoning About Emotions, and the Dawn of Universal Voice Actors</h2>
<p>The ability to accurately estimate emotions is a critical first step. The next frontier is to enable AI systems to <em>reason</em> about these emotions in context. We are convinced that in the very near future, foundation models will be multimodal, taking not only text but also audio natively in and natively out. These will be the &quot;universal voice actors&quot; we envision – capable of understanding, embodying, and expressing a vast range of human personas and emotions.</p>
<p>Imagine prompting an AI: &quot;Speak like a caring nurse comforting a worried patient,&quot; or &quot;Tell this story as a slightly grumpy but lovable grandpa.&quot; <strong>LAION's Got Talent</strong> and <strong>EMONET-VOICE</strong> are paving the way for such capabilities. Furthermore, the rich, multi-label, intensity-aware annotations in our EMONET suites provide the kind of data needed for training advanced reasoning models (like OpenAI's O-family or DeepSeek's R1) to understand the <em>implications</em> of emotional states and predict human future actions or outcomes based on observed cues from mental models, moving beyond simple recognition to true comprehension.</p>
<p>To truly democratize this field, LAION, with Intel's support, is committed to annotating <strong>millions of permissively licensed audio samples</strong> using our EMPATHIC INSIGHT-VOICE model. This will create an unparalleled public resource, fueling further research and development in self-supervised and multi-modal emotion learning.</p>
<p>Looking ahead, our next ambitious goal is to create a massive, permissively licensed multilingual speech dataset exceeding 500,000 hours. This monumental undertaking is powered by the Intel® Tiber AI Cloud, where we are leveraging its high-performance, 192-core CPU instances to process and curate this unparalleled resource. This will further democratize and accelerate research, paving the way for the next generation of emotionally aware AI.</p>
<h2><a id="try-it-and-collaborate-join-our-journey" class="anchor" href="#try-it-and-collaborate-join-our-journey" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Try It and Collaborate: Join Our Journey</h2>
<p>The development of emotionally intelligent AI is a collaborative endeavor. We invite the global AI community – researchers, developers, ethicists, and enthusiasts – to explore our work and contribute to this exciting field.</p>
<h3><a id="-read-the-papers" class="anchor" href="#-read-the-papers" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>📚 Read the Papers</h3>
<ul>
<li><strong>EmoNet Face:</strong> <a href="https://arxiv.org/abs/2505.20033">https://arxiv.org/abs/2505.20033</a></li>
<li><strong>EmoNet Voice:</strong> <a href="https://arxiv.org/abs/2506.09827">https://arxiv.org/abs/2506.09827</a></li>
</ul>
<h3><a id="-explore-the-datasets" class="anchor" href="#-explore-the-datasets" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>📊 Explore the Datasets</h3>
<ul>
<li><strong>LAION's Got Talent:</strong>
<ul>
<li><a href="https://huggingface.co/datasets/laion/laions_got_talent_enhanced_flash_annotations_and_long_captions">Enhanced Version</a></li>
<li><a href="https://huggingface.co/datasets/laion/laions_got_talent_raw">Raw Dataset</a></li>
</ul>
</li>
<li><strong>EMONET-FACE:</strong> <a href="https://huggingface.co/collections/t1a5anu-anon/emonet-face-6825a1dd6c6ea537cecba7b8">Collection</a></li>
<li><strong>EMONET-VOICE:</strong> <a href="https://huggingface.co/collections/t1a5anu-anon/emonet-voice-6825a1f3daac6ea7b37c26fb">Collection</a></li>
</ul>
<h3><a id="-experiment-with-the-models" class="anchor" href="#-experiment-with-the-models" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>🤖 Experiment with the Models</h3>
<p><strong>BUD-E Whisper:</strong></p>
<ul>
<li><a href="https://huggingface.co/laion/BUD-E-Whisper">Model</a></li>
</ul>
<p><strong>Empathic Insight Face:</strong></p>
<ul>
<li><a href="https://huggingface.co/laion/Empathic-Insight-Face-Small">Small Model</a> | <a href="https://colab.research.google.com/drive/1aLkBFncxBEdC2y0OcXbISd98Dc5MFq29?usp=sharing">Colab</a></li>
<li><a href="https://huggingface.co/laion/Empathic-Insight-Face-Large">Large Model</a> | <a href="https://colab.research.google.com/drive/11oUMo2HX0OuD9dx5ZM4ltNvoYxbI65hu?usp=sharing">Colab</a></li>
</ul>
<p><strong>Empathic Insight Voice:</strong></p>
<ul>
<li><a href="https://huggingface.co/laion/Empathic-Insight-Voice-Small">Model</a> | <a href="https://colab.research.google.com/drive/1WR-B6j--Y5RdhIyRGF_tJ3YdFF8BkUA2">Colab</a></li>
</ul>
<h3><a id="-engage-with-our-community" class="anchor" href="#-engage-with-our-community" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>🌐 Engage with our Community</h3>
<ul>
<li><strong>GitHub:</strong> <a href="https://github.com/laion-ai">https://github.com/laion-ai</a></li>
<li><strong>Discord:</strong> <a href="https://discord.gg/xBPBXfcFHd">https://discord.gg/xBPBXfcFHd</a></li>
</ul>
<p>Stay tuned for the official release of EMONET-VOICE and upcoming publications detailing our methodologies and findings.</p>
<h2><a id="acknowledgements" class="anchor" href="#acknowledgements" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgements</h2>
<p>This ambitious undertaking would not be possible without the incredible support of our partners. We extend our deepest gratitude to the Technical University Darmstadt, DFKI, Hessian AI, TIB-Leibniz Information Centre for Science and Technology, University Hannover, NOUS Research, Camb AI and especially Intel for their indispensable support, resources, and shared vision for advancing open and responsible AI research. Their commitment is instrumental in our journey to create AI that not only understands us but truly cares.</p>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Do They See What We See?","author":"LAION","date":"Jun 19 2025","previewImg":"/images/blog/do_they_see/image4.png"},"content":"\nBuilding Emotionally Intelligent AI with EmoNet, a suite of open tools and resources developed by the LAION community\n\n*A LAION \u0026 Intel Collaboration*\n\n## Authors\n\n**LAION e.V.**  \nChristoph Schuhmann*, Robert Kaczmarczyk*, Gollam Rabby, Felix Friedrich, Maurice Kraus, Kourosh Nadi, Desmond Grealy, Huu Nguyen, Cahya Wirawan, Krishna Kalyan, Kristian Kersting, Sören Auer\n\n**Intel Corporation**  \nJayaraman Mahalingam\n\n![][image4]\n\nAn exciting frontier in technology today is the quest for artificial intelligence that truly understands and interacts with humans on a deeper level. While AI has made remarkable progress in language processing and complex problem-solving, one critical dimension has yet to be fully realized: true emotional intelligence.\n\nCan our AI systems perceive the subtle joy in a crinkled eye, the faint tremor of anxiety in a voice, or the complex blend of emotions that color our everyday interactions? We believe this is not just a fascinating academic pursuit but a fundamental necessity for the future of human-AI collaboration.\n\nToday, we're proud to release **EmoNet** – a suite of new, open and freely available models and tools designed to support global research and innovation in the emerging field of emotionally intelligent AI. Our contributions are multi-faceted, addressing critical gaps in current research and providing powerful new tools for the global AI community.\n\nThank you to our partner Intel. LAION and Intel have been collaborating on fostering empathic, thoughtful and productive human-AI interaction for several years.\n\n## Voice Acting Samples Demo\n\n\u003cdiv align=\"center\"\u003e\n\n\u003cdiv style=\"position: relative; width: 100%; padding-bottom: 56.25%; height: 0; margin-bottom: 1em;\"\u003e\n  \u003ciframe \n    src=\"https://www.youtube.com/embed/TsTVKCmqHhk\" \n    title=\"Voice Acting Samples Demo\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    allowfullscreen\n    style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\"\u003e\n  \u003c/iframe\u003e\n\u003c/div\u003e\n\n\u003c/div\u003e\n\n## Our Empathic Computing Contributions\n\n### 1. EMONET-FACE Benchmark\n\nA novel, expert-annotated benchmark for fine-grained facial emotion estimation, featuring a comprehensive 40-category emotion taxonomy and large-scale, demographically diverse synthetic image datasets (EMONET-FACE BIG, BINARY, and HQ).\n\n\u003e **📊 Dataset:** [EMONET-FACE hosted by Hugging Face](https://huggingface.co/datasets/laion/EmoNet-Face_Big)\n\n![][image5]\n\n**Figure 2:** Samples from our EmoNet-Face datasets generated with different SOTA T2I models.\n\n### 2. EMONET-VOICE Benchmark\n\nA similarly fine-grained, expert-verified benchmark for speech emotion detection. Built upon our 40-category taxonomy and leveraging state-of-the-art synthetic voice generation for privacy and diversity. It includes 4,692 high-agreement audio samples.\n\n![][image7]\n\n**Table 1:** Comparison of SER datasets. Key aspects include licensing, scale, emotional range, speaker diversity, synthetic origin, multilingual support and defining characteristics.\n\n### 3. EMPATHIC INSIGHT-FACE Model\n\nA state-of-the-art model for facial emotion estimation trained on our EMONET-FACE suite, surpassing the performance of leading models like Gemini 2.5 Pro and proprietary APIs like Hume AI on our benchmarks.\n\n![][image6]\n\n**Figure 3:** Mean Spearman's Rho correlation between various model annotators and human annotations.\n\nThis figure highlights the strength of facial emotion recognition correlation between the EmoNet Empathic Insights models and actual emotions compared to other models. The bar heights represent the mean of these per-emotion Spearman's Rho values calculated across all emotions for each model. Error bars indicate bootstrap 95% confidence intervals (N=1000 bootstraps) for these means. Model annotator groups, including our trained models (Empathic-Insight-Face), VLMs with multi-shot or zero-shot prompting, proprietary models (HumeFace), and a random baseline, are distinguished by patterns as detailed in the legend.\n\n\u003e **🔗 Models:**\n\u003e - [Large Empathic-Insight-Face Model](https://huggingface.co/laion/Empathic-Insight-Face-Large)\n\u003e - [Small Empathic-Insight-Face Model](https://huggingface.co/laion/Empathic-Insight-Face-Small)\n\n![][image9]\n\n**Figure 4:** EMPATHIC INSIGHT-FACE Model prediction example\n\n\u003e **💻 Try it:** [EMPATHIC INSIGHT-FACE Model Colab](https://colab.research.google.com/drive/11oUMo2HX0OuD9dx5ZM4ltNvoYxbI65hu?usp=sharing)\n\n### 4. EMPATHIC INSIGHT-VOICE Model\n\nA state-of-the-art model for speech emotion estimation, setting a new benchmark for nuanced understanding of vocal emotional cues, similarly outperforming established systems on our EMONET-VOICE benchmark.\n\n![][image8]\n\n**Table 2:** Performance Comparison of Audio Language Models on the EmoNet-Voice Benchmark.\n\n\u003e **🔗 Models:**\n\u003e - [Large Empathic-Insight-Voice Model](https://huggingface.co/laion/Empathic-Insight-Voice-Large)\n\u003e - [Small Empathic-Insight-Voice Model](https://huggingface.co/laion/Empathic-Insight-Voice-Small)\n\n### 5. BUD-E Whisper (Better Understanding of Emotion Whisper)\n\nA suite of fine-tuned Whisper models for advanced emotional speech captioning, going beyond mere transcription to describe emotional tone, vocal bursts, and speaker traits.\n\n\u003e **🔗 Resources:**\n\u003e - [BUD-E Whisper Model](https://huggingface.co/laion/BUD-E-Whisper)\n\u003e - [BUD-E Whisper Colab](https://colab.research.google.com/drive/1VoAtmNhY1hI5Yzv1_dppHTcYky82OCDK?usp=sharing)\n\n### 6. LAION's Got Talent Dataset\n\nAn extensive synthetic voice-acting dataset, forming the foundation for EMONET-VOICE, featuring over **5,000 hours of speech** across 11 synthetic voices, 40 emotions, and 4 languages. The cumulative playtime of this dataset is more than the cumulative playtime of all movies shown in US cinemas from 2021 to 2024, putting its sheer scale into perspective.\n\n\u003e **📊 Datasets:**\n\u003e - [Enhanced Version with Gemini Flash 2.0 Annotations](https://huggingface.co/datasets/laion/laions_got_talent_enhanced_flash_annotations_and_long_captions)\n\u003e - [Raw Dataset](https://huggingface.co/datasets/laion/laions_got_talent_raw)\n\n## Introducing EMONET-FACE \u0026 EMONET-VOICE: A New Foundation\n\nTo address these challenges, we developed the EMONET suites. At their core is a **novel 40-category emotion taxonomy**, meticulously derived from an extensive analysis of the \"Handbook of Emotions\" and refined through consultation with psychologists. This taxonomy moves far beyond basic emotions, encompassing a rich spectrum of positive and negative affective states, cognitive states (e.g., *Concentration, Confusion, Doubt*), physical states (e.g., *Pain, Fatigue, Intoxication*), and socially mediated emotions (e.g., *Embarrassment, Shame, Pride, Teasing*). This granularity is crucial for building AI that can appreciate the finer details of human emotional life.\n\n### EMONET-FACE\n\nEMONET-FACE provides a rich resource for visual emotion understanding:\n\n- **EMONET-FACE BIG** (over 203,000 synthetic images) offers a vast dataset for pre-training models.\n- **EMONET-FACE BINARY** (approx. 20,000 images) is designed for fine-tuning and features over 62,000 binary (present/absent) emotion annotations from human experts. These annotations underwent a rigorous multi-stage process, requiring triple positive agreement for affirmative labels and a contrastive batch to ensure high-quality true negatives.\n- **EMONET-FACE HQ** (2,500 images) serves as our gold-standard evaluation benchmark. Each image was meticulously rated by multiple psychology experts on a continuous 0-7 intensity scale across all 40 emotion categories, resulting in 10,000 expert annotations.\n\nThe synthetic images were generated using state-of-the-art text-to-image models with explicit prompts to ensure diverse demographic representation (across ethnicity, age, and gender) and clear, full-face expressions. This approach not only allows for controlled diversity but also sidesteps the ethical concerns associated with using real individuals' images.\n\n![][image2]\n\n**Figure 5:** Mapping Facial Expression Emotional Understanding\n\n### EMONET-VOICE\n\nEMONET-VOICE tackles the auditory domain with similar rigor:\n\n- EMONET-VOICE curates **4,692 high-agreement audio samples** from **LAION's Got Talent**.\n- Each snippet simulates actors portraying scenes designed to evoke specific emotions.\n- Crucially, each snippet underwent rigorous validation by human experts with psychology degrees. They assigned perceived intensity labels (*Not Present, Mildly Present, Intensely Present*) based on a strict 3-annotator consensus protocol, focusing on estimating the *presence and intensity* of emotions rather than assuming a single definitive label.\n\nThis privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets.\n\n## Why Emotion in Speech and Face Matters: The Vision of Universal Voice Actors\n\nEffective communication transcends mere words. It's woven with the rich threads of emotion, conveyed through the subtle shifts in our facial expressions and the intricate nuances of our voice. Capturing these expressions enables AI assistants to become more empathetic, engaging, and supportive; qualities crucial for transformative applications in education, mental health, companionship, and beyond.\n\nWe envision a future where multimodal foundation models evolve into \"omni-models\" with sophisticated audio-in/audio-out capabilities. Soon, every new foundation model on platforms like Hugging Face could be capable of performing voice acting like Robert De Niro or Scarlett Johansson. These AI systems will function like world-class voice actors, capable of being prompted not just by text, but also by voice, to adopt any persona. Imagine an AI that can embody an empathetic educator adapting to a student's confusion, a thrilling storyteller captivating an audience, or a knowledgeable research assistant explaining complex concepts with clarity and appropriate gravitas. This level of seamless and inspiring human-AI interaction is our ultimate goal.\n\n## The Imperative for Better Benchmarks: Seeing and Hearing the Nuance\n\nThe journey to emotionally intelligent AI begins with data. Existing datasets for emotion recognition, while valuable, often present significant limitations. Facial emotion datasets might rely on a narrow range of \"basic\" emotions, use images with occlusions or poor lighting, or lack demographic diversity, leading to biased models that perform poorly across different populations. Similarly, speech emotion datasets can be constrained by coarse emotion taxonomies, privacy concerns tied to real user data, or an over-reliance on acted portrayals that don't capture the subtlety of spontaneous emotional expression.\n\nThe **Theory of Constructed Emotion (TCE)**, a prominent psychological framework ([research link](https://pubmed.ncbi.nlm.nih.gov/27798257/)), posits that emotions are not universal, pre-programmed entities that we simply \"recognize.\" Instead, they are constructed by our brains based on a combination of interoceptive signals (like valence – pleasantness/unpleasantness, and arousal – activation/deactivation), learned concepts, and contextual information. This means there isn't a single, definitive facial expression or vocal intonation for \"joy\" or \"sadness\" that is universally and unambiguously displayed. Rather, emotional expression is a complex, dynamic, and often ambiguous signal.\n\nThis understanding underscores the need for **emotion estimation** rather than simple recognition. We need AI that can assess the *likelihood* and *intensity* of various emotions being present, rather than forcing a single label onto a complex human state.\n\n![][image10]\n\n**Figure 6:** Mapping Voice Emotional Understanding\n\n## Motivation and Taxonomy Development: Capturing the Full Spectrum\n\nOur approach to defining the emotional landscape for these datasets was systematic and grounded in psychological theory, while also addressing practical needs for AI applications:\n\n1. **Theoretical Foundations \u0026 Gap Analysis:** We analyzed established psychological frameworks, such as Plutchik's Wheel of Emotions. While foundational, these often neglect states critical for real-world AI, like *Bitterness* (often linked to social exclusion), physical states like *Pain* and *Fatigue*, or altered states like *Intoxication*. Though not always strictly classified as \"emotions\" in psychology, their accurate perception is vital for empathetic AI.\n2. **\"Handbook of Emotions\" Extraction:** We processed the comprehensive \"Handbook of Emotions\" (edited by Lisa Feldman Barrett et al.) in 500-word chunks, using a large language model (LLM) to extract emotion-related nouns and adjectives.\n3. **Refinement to 40 Categories:** After automatic clustering of these terms, manual refinement by our team, in consultation with psychology experts, yielded our final 40-category taxonomy. This taxonomy, shared by both EMONET-FACE and EMONET-VOICE, is detailed below.\n\n### Full Emotion Taxonomy\n\n![][image13]\n\n**Table 3:** Taxonomy Listing of Emotions Captured in Datasets\n\n## The EMONET-VOICE Benchmark: A Gold Standard for Speech Emotion\n\nLeveraging the rich LAION's Got Talent dataset, we are establishing the **EMONET-VOICE Benchmark**. This isn't just another dataset; it's a meticulously curated training and evaluation corpus for our 40-category emotion taxonomy.\n\n- **Intensity Annotation:** Each audio snippet is being annotated for emotional expression intensity. For most emotions, a three-level scale is used: non-existent, slightly expressed, and strongly/fully expressed. For certain high-intensity or inherently ambiguous categories, a two-level distinction (non-existent vs. present) is applied.\n- **Expert Psychological Annotators:** To ensure the highest reliability, labeling is conducted by annotators with formal psychological training (typically at least a bachelor's degree).\n- **Strict Consensus Protocol:** Only labels achieving exact agreement among three independent expert annotators (no disagreement on the intensity level) are included. This conservative approach ensures exceptional inter-rater reliability, making the annotations a true gold standard.\n\nUnlike existing benchmarks often limited to basic emotions or binary states, EMONET-VOICE offers a structured, expert-validated evaluation across 40 nuanced categories, applicable to both naturalistic and synthetic speech. This will enable objective, reproducible, and fine-grained assessment of models like BUD-E Whisper and future multimodal systems.\n\n![][image11]\n\n### EMONET-FACE: Paralleling Progress in Visual Emotion Recognition\n\nOur efforts in speech emotion are mirrored by our work in facial emotion recognition with the **EMONET-FACE** suite. This includes:\n\n- **EMONET-FACE BIG:** Over 203,000 synthetic images for pre-training.\n- **EMONET-FACE BINARY:** Approx. 20,000 images with 65,000+ human expert binary emotion annotations for fine-tuning.\n- **EMONET-FACE HQ:** 2,500 images with 10,000 continuous (0-7 scale) expert annotations across all 40 emotions for high-fidelity evaluation.\n\n### EMPATHIC INSIGHT Models: Setting New State-of-the-Art Performance\n\nThe power of these benchmarks is demonstrated by our **EMPATHIC INSIGHT** models, which showcase the capabilities unlocked by our datasets and taxonomies:\n\n- **EMPATHIC INSIGHT-FACE** achieves human-expert-level performance on EMONET-FACE HQ, outperforming models like Gemini 2.5 Pro and proprietary APIs. (More technical details on the model architecture and training can be found in our accompanying paper/technical report).\n- **EMPATHIC INSIGHT-VOICE**, trained on LAION's Got Talent and EMONET-VOICE, similarly sets a new SOTA for nuanced speech emotion estimation. (Further technical specifics on this model are also available).\n\nThese models, are permissively licensed (Creative Commons for the models, Apache 2.0 for the code), prove that with focused dataset construction and careful modeling, AI can indeed learn to \"see\" and \"hear\" emotions with a level of nuance approaching human perception.\n\n![][image12]\n![][image14]\n![][image15]\n\n**Figure 7:** More EMPATHIC INSIGHT-FACE Model prediction examples\n\n![][image16]\n\n**Figure 8:** LAION's Empathic Insights Face Models Closely Track with Human Annotators\n\n## Introducing BUD-E Whisper: Beyond Transcription to Emotional Understanding\n\nTo truly unlock the emotional content within speech, transcription alone is insufficient. This led us to develop **BUD-E Whisper**, a suite of fine-tuned variants of OpenAI's Whisper model. BUD-E Whisper is specifically adapted for advanced emotion captioning. These models go beyond simply converting speech to text; they generate structured descriptions of:\n\n- **Emotional Tone:** Identifying the perceived emotions from our 40-category taxonomy.\n- **Vocal Bursts:** Recognizing non-lexical expressions like laughter, sighs, gasps, etc.\n- **Speaker Traits:** Inferring characteristics like age, gender, and even speech style.\n\nThe training of BUD-E Whisper was a journey of iterative refinement. We utilized a diverse dataset including the **LAION's Got Talent** voice-acting data and approximately **5,000 hours of audio** from public vlogs, online diaries, and cinematic dialogue, with Voice Activity Detection (VAD) used to isolate speech segments. Gemini Flash 2.0 was then employed to annotate these samples along our 40-category emotion taxonomy.\n\nInitial experiments attempting direct regression from Whisper's architecture to scalar emotion intensity values (0-5 scale) proved challenging, as the autoregressive nature of Whisper isn't inherently suited for stable numerical output. We then shifted to a captioning approach. First, we used procedurally generated emotion summaries – templated sentences describing emotions, intensity, and speaker traits. While an improvement, these templates led to syntactic predictability and overfitting. The breakthrough came when we used LLMs to *paraphrase* these procedural captions. This introduced crucial syntactic variability while preserving semantic consistency. Training on these diverse, paraphrased captions enabled BUD-E Whisper to produce fluid, context-sensitive, and highly interpretable emotion descriptions.\n\nThe result is a robust system capable of identifying and describing nuanced emotional signals in speech, representing a significant step towards more emotionally aware voice assistants. BUD-E Whisper is particularly useful for generating rich captions for audio, preparing data for training text-to-speech and foundation models.\n\n## LAION's Got Talent: A Rich Tapestry of Synthetic Voices\n\nAt the heart of our speech emotion initiative is the **LAION's Got Talent** dataset. This comprehensive resource, created via the HyperLab API leveraging OpenAI's GPT-4 Audio model, comprises:\n\n- **11 distinct voices:** Offering a variety of vocal timbres and characteristics.\n- **40 meticulously curated emotion categories:** (Listed in detail above)\n- **Four languages:** English (approx. 2,156 hours), German (approx. 716 hours), Spanish (approx. 888 hours), and French (approx. 881 hours).\n- **Acting Challenges (English + German):** Approximately 111 hours of specialized scenarios.\n- **Diverse English Accent Distribution:** Including Louisiana (~133h), Valley Girl (~159h), British (~132h), and even English spoken with simulated Chinese, French, German, Indian, Italian, Mexican, Russian, Spanish, and Texan accents, plus a \"Vulgar Street English\" category (~149h), and English without specific accent specified (~391h), ensuring a broad representation.\n\n\u003e **📊 Download the LAION's Got Talent Datasets:**\n\u003e - [Enhanced Version with Gemini Flash 2.0 Annotations](https://huggingface.co/datasets/laion/laions_got_talent_enhanced_flash_annotations_and_long_captions)\n\u003e - [Raw Dataset](https://huggingface.co/datasets/laion/laions_got_talent_raw)\n\n## The Power of Synthetic Data: Ethical and Diverse\n\nA cornerstone of our initiative is the strategic use of synthetic data. This approach offers several key advantages:\n\n- **Privacy:** It entirely bypasses the ethical complexities and privacy risks associated with collecting and annotating real human emotional expressions, especially for sensitive states.\n- **Diversity and Control:** We can programmatically ensure demographic diversity in our datasets, controlling for age, gender, and ethnicity in facial images, and voice characteristics in speech. This is crucial for building fairer and less biased AI systems.\n- **Scale and Scope:** Synthetic generation allows us to create datasets of a scale and emotional breadth that would be prohibitively expensive or logistically impossible to achieve with human-acted or in-the-wild data.\n\n## Investing in Open Standards-based AI Innovation\n\nIntel is excited to collaborate with AI innovators like LAION, setting the groundwork for the use of AI in LLLMs and Visual computing for education, healthcare, and agentic assistive AI for all age groups and needs. It is innovative creators like those at LAION bringing empathy and emotional intelligence to AI that embody the positive social impact that responsible AI can deliver for people.\n\nIntel has a strong commitment to open standards and open-source innovation, particularly in AI, software, and hardware ecosystems, fostering collaboration, accessibility, and interoperability. Intel supports open standards in AI through initiatives like the Linux Foundation's AI \u0026 Data projects, ensuring AI frameworks and tools are hardware-agnostic and widely adoptable. This aligns with its [Hugging Face collaboration](https://huggingface.co/Intel), where Intel optimizes models for [ONNX](https://onnx.ai/)-compatible inference on its hardware. This collaboration goes back to 2021 in order to optimize AI model performance on Intel hardware, focusing on accelerating training and inference for transformer-based models. Their partnership aims to democratize AI by making it faster, more efficient, and accessible through open-source tools.\n\nThe mission of Intel's COE with LAION, Germany, established in 2024, is [to advance the development of BUD-E](https://www.intel.com/content/www/us/en/developer/articles/technical/bud-e-ai-assisted-education-for-all.html), an open-source, empathetic AI education assistant that aims to democratize personalized learning worldwide. LAION is proud to work with Intel, famous for the [International Science and Engineering Fair](https://en.wikipedia.org/wiki/International_Science_and_Engineering_Fair), founded by former Intel CEO Gordon Moore. Hugging Face's Datasets, Models, and Collections are core components of its ecosystem, hosted on the Hugging Face Hub, designed to facilitate AI development and deployment. **As further steps**, these core components can be enhanced by using Intel-optimized datasets and models, integrated with Intel libraries, and curating collections to showcase these resources for tasks like generative AI or image classification, all aligned with its open-source commitment.\n\n### Try Intel's AI Tools\n\nIf you'd like to sample the same technological innovations that LAION has access to, give these a try:\n\n- [PyTorch Optimizations from Intel](https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html)\n- [OpenVINO™ Toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html)\n- [Intel® oneAPI Toolkits](https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html)\n\nSee which leading research institutions, universities, and innovative startup companies are joint in efforts just like this as part of the [Intel® AI and oneAPI Center of Excellence](https://www.intel.com/content/www/us/en/developer/tools/oneapi/training/academic-program/centers-of-excellence.html) program.\n\n## The Future: Reasoning About Emotions, and the Dawn of Universal Voice Actors\n\nThe ability to accurately estimate emotions is a critical first step. The next frontier is to enable AI systems to *reason* about these emotions in context. We are convinced that in the very near future, foundation models will be multimodal, taking not only text but also audio natively in and natively out. These will be the \"universal voice actors\" we envision – capable of understanding, embodying, and expressing a vast range of human personas and emotions.\n\nImagine prompting an AI: \"Speak like a caring nurse comforting a worried patient,\" or \"Tell this story as a slightly grumpy but lovable grandpa.\" **LAION's Got Talent** and **EMONET-VOICE** are paving the way for such capabilities. Furthermore, the rich, multi-label, intensity-aware annotations in our EMONET suites provide the kind of data needed for training advanced reasoning models (like OpenAI's O-family or DeepSeek's R1) to understand the *implications* of emotional states and predict human future actions or outcomes based on observed cues from mental models, moving beyond simple recognition to true comprehension.\n\nTo truly democratize this field, LAION, with Intel's support, is committed to annotating **millions of permissively licensed audio samples** using our EMPATHIC INSIGHT-VOICE model. This will create an unparalleled public resource, fueling further research and development in self-supervised and multi-modal emotion learning.\n\nLooking ahead, our next ambitious goal is to create a massive, permissively licensed multilingual speech dataset exceeding 500,000 hours. This monumental undertaking is powered by the Intel® Tiber AI Cloud, where we are leveraging its high-performance, 192-core CPU instances to process and curate this unparalleled resource. This will further democratize and accelerate research, paving the way for the next generation of emotionally aware AI.\n\n## Try It and Collaborate: Join Our Journey\n\nThe development of emotionally intelligent AI is a collaborative endeavor. We invite the global AI community – researchers, developers, ethicists, and enthusiasts – to explore our work and contribute to this exciting field.\n\n### 📚 Read the Papers\n\n- **EmoNet Face:** [https://arxiv.org/abs/2505.20033](https://arxiv.org/abs/2505.20033)\n- **EmoNet Voice:** [https://arxiv.org/abs/2506.09827](https://arxiv.org/abs/2506.09827)\n\n### 📊 Explore the Datasets\n\n- **LAION's Got Talent:** \n  - [Enhanced Version](https://huggingface.co/datasets/laion/laions_got_talent_enhanced_flash_annotations_and_long_captions)\n  - [Raw Dataset](https://huggingface.co/datasets/laion/laions_got_talent_raw)\n- **EMONET-FACE:** [Collection](https://huggingface.co/collections/t1a5anu-anon/emonet-face-6825a1dd6c6ea537cecba7b8)\n- **EMONET-VOICE:** [Collection](https://huggingface.co/collections/t1a5anu-anon/emonet-voice-6825a1f3daac6ea7b37c26fb)\n\n### 🤖 Experiment with the Models\n\n**BUD-E Whisper:**\n- [Model](https://huggingface.co/laion/BUD-E-Whisper)\n\n**Empathic Insight Face:**\n- [Small Model](https://huggingface.co/laion/Empathic-Insight-Face-Small) | [Colab](https://colab.research.google.com/drive/1aLkBFncxBEdC2y0OcXbISd98Dc5MFq29?usp=sharing)\n- [Large Model](https://huggingface.co/laion/Empathic-Insight-Face-Large) | [Colab](https://colab.research.google.com/drive/11oUMo2HX0OuD9dx5ZM4ltNvoYxbI65hu?usp=sharing)\n\n**Empathic Insight Voice:**\n- [Model](https://huggingface.co/laion/Empathic-Insight-Voice-Small) | [Colab](https://colab.research.google.com/drive/1WR-B6j--Y5RdhIyRGF_tJ3YdFF8BkUA2)\n\n### 🌐 Engage with our Community\n\n- **GitHub:** [https://github.com/laion-ai](https://github.com/laion-ai)\n- **Discord:** [https://discord.gg/xBPBXfcFHd](https://discord.gg/xBPBXfcFHd)\n\nStay tuned for the official release of EMONET-VOICE and upcoming publications detailing our methodologies and findings.\n\n## Acknowledgements\n\nThis ambitious undertaking would not be possible without the incredible support of our partners. We extend our deepest gratitude to the Technical University Darmstadt, DFKI, Hessian AI, TIB-Leibniz Information Centre for Science and Technology, University Hannover, NOUS Research, Camb AI and especially Intel for their indispensable support, resources, and shared vision for advancing open and responsible AI research. Their commitment is instrumental in our journey to create AI that not only understands us but truly cares.\n\n[image1]: /images/blog/do_they_see/image1.jpg\n[image2]: /images/blog/do_they_see/image2.png\n[image4]: /images/blog/do_they_see/image4.png\n[image5]: /images/blog/do_they_see/image5.png\n[image6]: /images/blog/do_they_see/image6.png\n[image7]: /images/blog/do_they_see/image7.png\n[image8]: /images/blog/do_they_see/image8.png\n[image9]: /images/blog/do_they_see/image9.png\n[image10]: /images/blog/do_they_see/image10.png\n[image11]: /images/blog/do_they_see/image11.png\n[image12]: /images/blog/do_they_see/image12.png\n[image13]: /images/blog/do_they_see/image13.png\n[image14]: /images/blog/do_they_see/image14.png\n[image15]: /images/blog/do_they_see/image15.png\n[image16]: /images/blog/do_they_see/image16.png\n","slug":"do-they-see-what-we-see"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"do-they-see-what-we-see"},"buildId":"_SM4P8X5pqC39g-fsfkhJ","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>