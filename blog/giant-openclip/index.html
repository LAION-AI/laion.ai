<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Reaching 80% zero-shot accuracy with OpenCLIP: ViT-G/14 trained on LAION-2B | LAION</title><meta name="title" content="Reaching 80% zero-shot accuracy with OpenCLIP: ViT-G/14 trained on LAION-2B | LAION"/><meta property="og:title" content="Reaching 80% zero-shot accuracy with OpenCLIP: ViT-G/14 trained on LAION-2B | LAION"/><meta name="twitter:title" content="Reaching 80% zero-shot accuracy with OpenCLIP: ViT-G/14 trained on LAION-2B | LAION"/><meta name="description" content="&lt;p&gt;We have trained a new &lt;a href=&quot;https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k&quot;&gt;ViT-G/14 CLIP&lt;/a&gt; model with &lt;a href=&quot;https://github.com/m..."/><meta property="og:description" content="&lt;p&gt;We have trained a new &lt;a href=&quot;https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k&quot;&gt;ViT-G/14 CLIP&lt;/a&gt; model with &lt;a href=&quot;https://github.com/m..."/><meta name="twitter:description" content="&lt;p&gt;We have trained a new &lt;a href=&quot;https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k&quot;&gt;ViT-G/14 CLIP&lt;/a&gt; model with &lt;a href=&quot;https://github.com/m..."/><meta property="og:image" content="https://laion.ai/images/blog/scaling_vit_giant.png"/><meta name="twitter:image" content="https://laion.ai/images/blog/scaling_vit_giant.png"/><meta name="twitter:image:alt" content="The text: LAION. Large-scale Artificial Intelligence Open Network, TRULY OPEN AI. 100% NON-PROFIT. 100% FREE."/><meta property="og:type" content="website"/><meta property="og:url" content="https://laion.ai/blog/giant-openclip"/><meta name="twitter:url" content="https://laion.ai/blog/giant-openclip"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1D374E"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.png"/><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff" as="font" type="font/woff"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff2" as="font" type="font/woff2"/><link rel="preload" href="/fonts/Dinish-Regular.woff" as="font" type="font/woff"/><link rel="preload" href="/fonts/Dinish-Regular.woff2" as="font" type="font/woff2"/><link rel="preload" href="/fonts/Dinish-Italic.woff" as="font" type="font/woff"/><link rel="preload" href="/fonts/Dinish-Italic.woff2" as="font" type="font/woff2"/><meta name="next-head-count" content="25"/><link rel="stylesheet" href="/fonts/load.css"/><link rel="preload" href="/_next/static/css/d9c1099a3d2e24ef.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d9c1099a3d2e24ef.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-663923589dd6c477.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-334b72c72516adee.js" defer=""></script><script src="/_next/static/_Y9IB71-ysOqNeQa3ppj_/_buildManifest.js" defer=""></script><script src="/_next/static/_Y9IB71-ysOqNeQa3ppj_/_ssgManifest.js" defer=""></script><script src="/_next/static/_Y9IB71-ysOqNeQa3ppj_/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-paper md:text-sky md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LAION</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/team/">Team</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/faq/">FAQ</a><a href="/privacy-policy/">Privacy Policy</a><a href="/dataset-requests/">Dataset Requests</a><a href="/impressum/">Impressum</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1D374E;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/team/">Team</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/faq/">FAQ</a></p><p><a href="/privacy-policy/">Privacy Policy</a></p><p><a href="/dataset-requests/">Dataset Requests</a></p><p><a href="/impressum/">Impressum</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-paper grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-8xl md:text-8xl w-full font-bold title-flow break-words">REACHING 80% ZERO-SHOT ACCURACY WITH OPENCLIP: VIT-G/14 TRAINED ON LAION-2B</h1><p class="text-2xl pb-2">by: <!-- -->Mitchell Wortsman<!-- -->,<!-- --> <!-- -->1 Jan, 2023<!-- --></p><hr/><div class="pt-2 article"><p>We have trained a new <a href="https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k">ViT-G/14 CLIP</a> model with <a href="https://github.com/mlfoundations/open_clip">OpenCLIP</a> which achieves <strong>80.1%</strong> zero-shot accuracy on ImageNet and <strong>74.9%</strong> zero-shot image retrieval (Recall@5) on MS COCO. As of January 2023, this is the best open source CLIP model.</p>
<p>We believe this is interesting because:</p>
<ul>
<li>CLIP models are useful for zero-shot classification, retrieval, and for guidance/conditioning in generative models (OpenCLIP is used in Stable Diffusion V2 and currently the third most downloaded model on HuggingFace is a CLIP model). The approach underlying CLIP—self supervised learning on a large, heterogeneous dataset—has been shown to produce models which are more <a href="https://openai.com/blog/clip/">robust</a> and <a href="https://ai.facebook.com/blog/seer-10b-better-fairer-computer-vision-through-self-supervised-learning-training-on-diverse-datasets/">fair</a>.</li>
<li>Our new ViT-G model achieves the highest zero-shot ImageNet accuracy among models that use only naturally occurring image-text pairs as training data, and without explicit labels, pseudo-labels, or any pretrained image or text encoders.</li>
<li>Our training run utilized multiple new techniques, including <a href="https://arxiv.org/abs/2212.00794">FLIP</a> to accelerate training and <a href="https://arxiv.org/abs/2203.05482">model soups</a> to surpass 80% accuracy.</li>
</ul>
<h2>Main Results</h2>
<p>The following results are with image resolution 224x224 except for CoCa which uses 576x576.</p>
<table>
<thead>
<tr>
<th>Model name</th>
<th style="text-align:center">Batch size</th>
<th style="text-align:center">Samples seen</th>
<th style="text-align:center">Text Params</th>
<th style="text-align:center">Image params</th>
<th style="text-align:center">ImageNet top1</th>
<th style="text-align:center">Mscoco image retrieval at 5</th>
<th style="text-align:center">Flickr30k image retrieval at 5</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenAI CLIP L/14</td>
<td style="text-align:center">32k</td>
<td style="text-align:center">13B</td>
<td style="text-align:center">123.65M</td>
<td style="text-align:center">303.97M</td>
<td style="text-align:center">75.4%</td>
<td style="text-align:center">61.0%</td>
<td style="text-align:center">87.0%</td>
</tr>
<tr>
<td>OpenCLIP H/14</td>
<td style="text-align:center">79k</td>
<td style="text-align:center">32B (16 epochs of laion2B)</td>
<td style="text-align:center">354.0M</td>
<td style="text-align:center">632.08M</td>
<td style="text-align:center">78.0%</td>
<td style="text-align:center">73.4%</td>
<td style="text-align:center">94%</td>
</tr>
<tr>
<td>OpenCLIP G/14</td>
<td style="text-align:center">160k</td>
<td style="text-align:center">32B +unmasked fine-tune (details below)</td>
<td style="text-align:center">694.7M</td>
<td style="text-align:center">1844.9M</td>
<td style="text-align:center">80.1%*</td>
<td style="text-align:center">74.9%</td>
<td style="text-align:center">94.9%</td>
</tr>
<tr>
<td>CoCa</td>
<td style="text-align:center">66k</td>
<td style="text-align:center">33B</td>
<td style="text-align:center">1100M</td>
<td style="text-align:center">1000M</td>
<td style="text-align:center">86.3%**</td>
<td style="text-align:center">74.2</td>
<td style="text-align:center">95.7</td>
</tr>
</tbody>
</table>
<p>* When using <a href="https://arxiv.org/abs/2209.03320">CuPL</a> prompts instead of the standard prompts from OpenAI, the zero-shot accuracy is 80.3%. When evaluating at 280x280 and changing resize to squash, Ross Wightman found the model achieves 80.4%.</p>
<p>** In addition to natural language supervision, <a href="https://arxiv.org/abs/2205.01917">CoCa</a> uses synthetic captions constructed with the labels from the JFT-3B dataset. In addition to natural language supervision, CoCa uses synthetic captions constructed with the labels from the JFT-3B dataset. 973 of the 1,000 ImageNet classes have a corresponding class in JFT (e.g., see here sec C.7.2).</p>
<p>Also see the figure below (figure code by Ross) and our analysis of scaling trends for OpenCLIP model <a href="https://arxiv.org/abs/2212.07143">here</a>.</p>
<p><img src="/images/blog/scaling_vit_giant.png" alt=""></p>
<h2>Released Checkpoints</h2>
<p>We release the checkpoint through <a href="https://github.com/mlfoundations/open_clip">OpenCLIP</a> and in the <a href="https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k">HuggingFace hub</a>.</p>
<h2>Notes on scaling up</h2>
<p>To scale up model size while reducing compute we used <a href="https://arxiv.org/abs/2212.00794">Fast Language-Image Pre-training (FLIP)</a> with patch dropout 0.5. Similar to <a href="https://arxiv.org/abs/2111.06377">Masked Autoencoders (MAE)</a>, FLIP drops out patches during training. FLIP also requires a short “unmasked tuning” phase, which we discuss in training notes below. In addition to reducing Giga multiply–accumulate operations (GMACs) for each forward/backward pass, FLIP allowed us to use a larger per-GPU batch size. Without FLIP, gradient accumulation was necessary to maintain a large batch size. Keeping batch size and number of GPUs consistent (at 160k and 512, respectively) but switching to unmasked fine-tuning resulted in a drop from 46.9 to 20.4 samples per second per GPU. For reference, OpenCLIP H/14 with global batch size 79k across 824 GPUs without patch dropout trained at 42 samples/s/GPU.</p>
<p>To scale up the batch size to 160k, we used <a href="https://arxiv.org/abs/1604.06174v2">gradient checkpointing</a> and 80GM VRAM A100s. For the unmasked tuning portion, we also used gradient accumulation (see our implementation for the contrastive objective <a href="https://github.com/mlfoundations/open_clip/pull/267">here</a>). Finally, we used a 2x higher learning rate of 2e-3 compared to our experiments with batch size 80k. The combination of scaling up model, batch size, and learning rate resulted in training instability during the warmup phase. Accordingly, we increased warm-up to 13k steps, trained with layer scale, and used AdamW beta2 0.95. All runs used AMP bfloat16, after previously switching from float16 in prior experiments with L/14 and H/14.</p>
<h2>Training notes</h2>
<h3>Phase 1: Patch dropout</h3>
<p>For phase 1 we trained ViT-G with <a href="https://arxiv.org/abs/2212.00794">patch dropout</a> 0.5 on LAION-2B for 32B samples seen. We used batch size 160k, learning rate 2e-3, and a cosine decay schedule. After this phase the model reached 79.07 zero-shot top1 accuracy on ImageNet.</p>
<p>Training was mainly done on 512 to 760 A100s depending on availability. When changing the number of GPUs, local batch size was also modified so that the global batch size remained at 160k. When using 512 GPUs we set local batch size to 313 and observed roughly 24k samples per second or 46.9 samples/s/GPU. When using 760 GPUs we set local batch size 211 and observed roughly 33k samples per second or 43.4 samples/s/GPU.</p>
<h3>Phase 2: Unmasked tuning + Model soups</h3>
<p>For phase 2 we followed <a href="https://arxiv.org/abs/2212.00794">FLIP</a> in conducting a short unmasked tuning phase. We fell short of 80% in our first unmasked fine-tuning phase, reaching only 79.43%. So we tried twice more with different settings (described below) to obtain 79.45% and 79.2%, respectively. Next, we followed <a href="https://arxiv.org/abs/2203.05482">model soups</a> and averaged the weights of three checkpoints produced by these runs to achieve our final accuracy of 80.1%. <a href="https://arxiv.org/abs/2206.02770">LIMoE</a> and <a href="https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html">PaLI</a> also used model soups for better pre-training.</p>
<p>For our first unmasked fine-tuning run we did not modify the learning rate schedule, but instead doubled the base LR and extended the number of iterations so that the run would proceed for an additional 2B samples seen. LR started at 3.8e-5. For the second run we used LR 5.5e-5 with a full cosine schedule (warmup for roughly 200M samples and a total of 4B samples). The third run had identical hyperparameters to the first but used the LAION-A subset of LAION-2B. LAION-A is a 900M subset of LAION-2B filtered with aesthetic V2 4.5+ and pHash deduplicated. Instead of waiting for the third run to complete we use the checkpoint after approximately 700M samples which, when “<a href="https://arxiv.org/abs/2203.05482">souped</a>” with the final checkpoints from the two proceeding runs, already allowed us to surpass our goal of 80% accuracy. This indiviual checkpoint achieved 79.2%.</p>
<p>Unmasked fine-tuning was done on 512 A100 GPUs at a speed of roughly 10,450 samples/s or 20.4 samples/s/GPU.</p>
<p>The following plot shows the loss curve for phase 1.</p>
<p><img src="/images/blog/loss_vit_giant.png" alt=""></p>
<h2>More results</h2>
<p>Zero-shot accuracies at resolution 224x224 computed with <a href="https://github.com/LAION-AI/CLIP_benchmark">CLIP Benchmark</a>.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>OpenCLIP H/14</th>
<th>OpenCLIP G/14</th>
</tr>
</thead>
<tbody>
<tr>
<td>ImageNet</td>
<td>78.0</td>
<td>80.1</td>
</tr>
<tr>
<td>ImageNet-V2</td>
<td>70.8</td>
<td>73.6</td>
</tr>
<tr>
<td>ImageNet-R</td>
<td>89.3</td>
<td>92.1</td>
</tr>
<tr>
<td>ImageNet-Sketch</td>
<td>66.6</td>
<td>68.9</td>
</tr>
<tr>
<td>ObjectNet</td>
<td>69.7</td>
<td>73.0</td>
</tr>
<tr>
<td>ImageNet-A</td>
<td>59.2</td>
<td>69.3</td>
</tr>
<tr>
<td>CIFAR-10</td>
<td>97.4</td>
<td>98.2</td>
</tr>
<tr>
<td>CIFAR-100</td>
<td>84.7</td>
<td>87.5</td>
</tr>
<tr>
<td>MNIST</td>
<td>72.9</td>
<td>71.6</td>
</tr>
<tr>
<td>SVHN</td>
<td>56.1</td>
<td>62.5</td>
</tr>
<tr>
<td>Caltech-101</td>
<td>85.0</td>
<td>86.4</td>
</tr>
<tr>
<td>SUN397</td>
<td>75.2</td>
<td>74.5</td>
</tr>
<tr>
<td>FGVC Aircraft</td>
<td>42.8</td>
<td>49.7</td>
</tr>
<tr>
<td>Country211</td>
<td>30.0</td>
<td>33.8</td>
</tr>
<tr>
<td>Cars</td>
<td>93.5</td>
<td>94.6</td>
</tr>
</tbody>
</table>
<p>Here is a summary figure comparing G/14 and H/14 made with evals by Romain Beaumont.</p>
<p><img src="/images/blog/summary_vit_giant.png" alt=""></p>
<h2>What’s Next?</h2>
<p>In the future, we may fine-tune the model to enable multilingual capabilities, or fine-tune at higher resolution. Also, <a href="https://github.com/mlfoundations/open_clip/pull/358">FSDP is coming to OpenCLIP</a> which will allow even larger models, as is <a href="https://github.com/mlfoundations/open_clip/pull/308">CoCa</a> which will allow new openclip models to also be captioners. More contributions to OpenCLIP are always welcome!</p>
<h2>Contributions and acknowledgements</h2>
<p>Thanks to:</p>
<ul>
<li><a href="https://github.com/rom1504">Romain Beaumont</a>, <a href="https://github.com/rwightman">Ross Wightman</a>, <a href="https://github.com/mehdidc">Mehdi Cherti</a>, <a href="https://gabrielilharco.com/">Gabriel Ilharco</a>, and <a href="https://github.com/JeniaJitsev">Jenia Jitsev</a> for providing extensive ideas, advice, engineering support, evaluating the model, and maintaining the openclip repository used for model training.</li>
<li><a href="https://github.com/christophschuhmann">Christoph Schuhmann</a> for encouragement and support</li>
<li><a href="https://github.com/rvencu">Richard Vencu</a> for cluster support</li>
<li><a href="https://github.com/lucidrains">Phil Wang</a> and <a href="https://haoqifan.github.io/">Haoqi Fan</a> for the implementation and discussion regarding patch dropout</li>
<li><a href="https://www.shoyaida.com/">Sho Yaida</a>, <a href="https://jongwook.kim/">Jong Wook Kim</a>, <a href="http://www.arimorcos.com/">Ari Morcos</a> and <a href="https://www.sainingxie.com/">Saining Xie</a> for helpful remarks regarding hyperparameters</li>
<li><a href="https://sarahpratt.github.io/">Sarah Pratt</a> for implementing CuPL</li>
<li><a href="https://github.com/ludwigschmidt">Ludwig Schmidt</a> and <a href="https://homes.cs.washington.edu/~ali/">Ali Farhadi</a> for helpful discussions, and to the <a href="https://raivn.cs.washington.edu/">RAIVN</a> and <a href="https://github.com/mlfoundations/">EFML</a> labs at the University of Washington</li>
</ul>
<p>And of course thanks to <a href="https://twitter.com/EMostaque">Emad</a> and <a href="https://stability.ai/">Stability AI</a> for providing the compute resources used for these experiments.</p>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Reaching 80% zero-shot accuracy with OpenCLIP: ViT-G/14 trained on LAION-2B","author":"Mitchell Wortsman","date":"Jan 24 2023","previewImg":"/images/blog/scaling_vit_giant.png"},"content":"\nWe have trained a new [ViT-G/14 CLIP](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k) model with [OpenCLIP](https://github.com/mlfoundations/open_clip) which achieves **80.1%** zero-shot accuracy on ImageNet and **74.9%** zero-shot image retrieval (Recall@5) on MS COCO. As of January 2023, this is the best open source CLIP model.\n\nWe believe this is interesting because:\n* CLIP models are useful for zero-shot classification, retrieval, and for guidance/conditioning in generative models (OpenCLIP is used in Stable Diffusion V2 and currently the third most downloaded model on HuggingFace is a CLIP model). The approach underlying CLIP—self supervised learning on a large, heterogeneous dataset—has been shown to produce models which are more [robust](https://openai.com/blog/clip/) and [fair](https://ai.facebook.com/blog/seer-10b-better-fairer-computer-vision-through-self-supervised-learning-training-on-diverse-datasets/).\n* Our new ViT-G model achieves the highest zero-shot ImageNet accuracy among models that use only naturally occurring image-text pairs as training data, and without explicit labels, pseudo-labels, or any pretrained image or text encoders.\n* Our training run utilized multiple new techniques, including [FLIP](https://arxiv.org/abs/2212.00794) to accelerate training and [model soups](https://arxiv.org/abs/2203.05482) to surpass 80% accuracy.\n\n## Main Results\nThe following results are with image resolution 224x224 except for CoCa which uses 576x576.\n\n| Model name       | Batch size |               Samples seen              | Text Params | Image params | ImageNet top1 | Mscoco image retrieval at 5 | Flickr30k image retrieval at 5 |\n|------------------|:----------:|:---------------------------------------:|:-----------:|:------------:|:-------------:|:---------------------------:|:------------------------------:|\n| OpenAI CLIP L/14 | 32k        | 13B                                     | 123.65M     | 303.97M      | 75.4%         | 61.0%                         | 87.0%                            |\n| OpenCLIP H/14    | 79k        | 32B (16 epochs of laion2B)              | 354.0M      | 632.08M      | 78.0%         | 73.4%                       | 94%                            |\n| OpenCLIP G/14    | 160k       | 32B +unmasked fine-tune (details below) | 694.7M      | 1844.9M      | 80.1%*        | 74.9%                       | 94.9%                          |\n| CoCa            | 66k        | 33B                                     | 1100M       | 1000M        | 86.3%**       | 74.2                        | 95.7                           |\n\n\\* When using [CuPL](https://arxiv.org/abs/2209.03320) prompts instead of the standard prompts from OpenAI, the zero-shot accuracy is 80.3%. When evaluating at 280x280 and changing resize to squash, Ross Wightman found the model achieves 80.4%.\n\n** In addition to natural language supervision, [CoCa](https://arxiv.org/abs/2205.01917) uses synthetic captions constructed with the labels from the JFT-3B dataset. In addition to natural language supervision, CoCa uses synthetic captions constructed with the labels from the JFT-3B dataset. 973 of the 1,000 ImageNet classes have a corresponding class in JFT (e.g., see here sec C.7.2).\n\nAlso see the figure below (figure code by Ross) and our analysis of scaling trends for OpenCLIP model [here](https://arxiv.org/abs/2212.07143).\n\n![](/images/blog/scaling_vit_giant.png)\n\n## Released Checkpoints\n\nWe release the checkpoint through [OpenCLIP](https://github.com/mlfoundations/open_clip) and in the [HuggingFace hub](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k).\n\n## Notes on scaling up\n\nTo scale up model size while reducing compute we used [Fast Language-Image Pre-training (FLIP)](https://arxiv.org/abs/2212.00794) with patch dropout 0.5. Similar to [Masked Autoencoders (MAE)](https://arxiv.org/abs/2111.06377), FLIP drops out patches during training. FLIP also requires a short “unmasked tuning” phase, which we discuss in training notes below. In addition to reducing Giga multiply–accumulate operations (GMACs) for each forward/backward pass, FLIP allowed us to use a larger per-GPU batch size. Without FLIP, gradient accumulation was necessary to maintain a large batch size. Keeping batch size and number of GPUs consistent (at 160k and 512, respectively) but switching to unmasked fine-tuning resulted in a drop from 46.9 to 20.4 samples per second per GPU. For reference, OpenCLIP H/14 with global batch size 79k across 824 GPUs without patch dropout trained at 42 samples/s/GPU.\n\nTo scale up the batch size to 160k, we used [gradient checkpointing](https://arxiv.org/abs/1604.06174v2) and 80GM VRAM A100s. For the unmasked tuning portion, we also used gradient accumulation (see our implementation for the contrastive objective [here](https://github.com/mlfoundations/open_clip/pull/267)). Finally, we used a 2x higher learning rate of 2e-3 compared to our experiments with batch size 80k. The combination of scaling up model, batch size, and learning rate resulted in training instability during the warmup phase. Accordingly, we increased warm-up to 13k steps, trained with layer scale, and used AdamW beta2 0.95. All runs used AMP bfloat16, after previously switching from float16 in prior experiments with L/14 and H/14.\n\n## Training notes\n\n### Phase 1: Patch dropout\n\nFor phase 1 we trained ViT-G with [patch dropout](https://arxiv.org/abs/2212.00794) 0.5 on LAION-2B for 32B samples seen. We used batch size 160k, learning rate 2e-3, and a cosine decay schedule. After this phase the model reached 79.07 zero-shot top1 accuracy on ImageNet.\n\nTraining was mainly done on 512 to 760 A100s depending on availability. When changing the number of GPUs, local batch size was also modified so that the global batch size remained at 160k. When using 512 GPUs we set local batch size to 313 and observed roughly 24k samples per second or 46.9 samples/s/GPU. When using 760 GPUs we set local batch size 211 and observed roughly 33k samples per second or 43.4 samples/s/GPU.\n\n### Phase 2: Unmasked tuning + Model soups\n\nFor phase 2 we followed [FLIP](https://arxiv.org/abs/2212.00794) in conducting a short unmasked tuning phase. We fell short of 80% in our first unmasked fine-tuning phase, reaching only 79.43%. So we tried twice more with different settings (described below) to obtain 79.45% and 79.2%, respectively. Next, we followed [model soups](https://arxiv.org/abs/2203.05482) and averaged the weights of three checkpoints produced by these runs to achieve our final accuracy of 80.1%. [LIMoE](https://arxiv.org/abs/2206.02770) and [PaLI](https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html) also used model soups for better pre-training.\n\nFor our first unmasked fine-tuning run we did not modify the learning rate schedule, but instead doubled the base LR and extended the number of iterations so that the run would proceed for an additional 2B samples seen. LR started at 3.8e-5. For the second run we used LR 5.5e-5 with a full cosine schedule (warmup for roughly 200M samples and a total of 4B samples). The third run had identical hyperparameters to the first but used the LAION-A subset of LAION-2B. LAION-A is a 900M subset of LAION-2B filtered with aesthetic V2 4.5+ and pHash deduplicated. Instead of waiting for the third run to complete we use the checkpoint after approximately 700M samples which, when “[souped](https://arxiv.org/abs/2203.05482)” with the final checkpoints from the two proceeding runs, already allowed us to surpass our goal of 80% accuracy. This indiviual checkpoint achieved 79.2%.\n\nUnmasked fine-tuning was done on 512 A100 GPUs at a speed of roughly 10,450 samples/s or 20.4 samples/s/GPU.\n\nThe following plot shows the loss curve for phase 1.\n\n![](/images/blog/loss_vit_giant.png)\n\n## More results\n\nZero-shot accuracies at resolution 224x224 computed with [CLIP Benchmark](https://github.com/LAION-AI/CLIP_benchmark).\n\n| Dataset         | OpenCLIP H/14 | OpenCLIP G/14 |\n|-----------------|---------------|---------------|\n| ImageNet        | 78.0         | 80.1         |\n| ImageNet-V2     | 70.8         | 73.6         |\n| ImageNet-R      | 89.3         | 92.1         |\n| ImageNet-Sketch | 66.6         | 68.9         |\n| ObjectNet       | 69.7         | 73.0         |\n| ImageNet-A      | 59.2         | 69.3         |\n| CIFAR-10        | 97.4         | 98.2         |\n| CIFAR-100       | 84.7         | 87.5         |\n| MNIST           | 72.9         | 71.6         |\n| SVHN            | 56.1         | 62.5         |\n| Caltech-101     | 85.0         | 86.4         |\n| SUN397          | 75.2         | 74.5         |\n| FGVC Aircraft   | 42.8         | 49.7         |\n| Country211      | 30.0         | 33.8         |\n| Cars            | 93.5         | 94.6         |\n\nHere is a summary figure comparing G/14 and H/14 made with evals by Romain Beaumont.\n\n![](/images/blog/summary_vit_giant.png)\n\n## What’s Next?\n\nIn the future, we may fine-tune the model to enable multilingual capabilities, or fine-tune at higher resolution. Also, [FSDP is coming to OpenCLIP](https://github.com/mlfoundations/open_clip/pull/358) which will allow even larger models, as is [CoCa](https://github.com/mlfoundations/open_clip/pull/308) which will allow new openclip models to also be captioners. More contributions to OpenCLIP are always welcome!\n\n## Contributions and acknowledgements\n\nThanks to:\n* [Romain Beaumont](https://github.com/rom1504), [Ross Wightman](https://github.com/rwightman), [Mehdi Cherti](https://github.com/mehdidc), [Gabriel Ilharco](https://gabrielilharco.com/), and [Jenia Jitsev](https://github.com/JeniaJitsev) for providing extensive ideas, advice, engineering support, evaluating the model, and maintaining the openclip repository used for model training.\n* [Christoph Schuhmann](https://github.com/christophschuhmann) for encouragement and support\n* [Richard Vencu](https://github.com/rvencu) for cluster support\n* [Phil Wang](https://github.com/lucidrains) and [Haoqi Fan](https://haoqifan.github.io/) for the implementation and discussion regarding patch dropout\n* [Sho Yaida](https://www.shoyaida.com/), [Jong Wook Kim](https://jongwook.kim/), [Ari Morcos](http://www.arimorcos.com/) and [Saining Xie](https://www.sainingxie.com/) for helpful remarks regarding hyperparameters\n* [Sarah Pratt](https://sarahpratt.github.io/) for implementing CuPL\n* [Ludwig Schmidt](https://github.com/ludwigschmidt) and [Ali Farhadi](https://homes.cs.washington.edu/~ali/) for helpful discussions, and to the [RAIVN](https://raivn.cs.washington.edu/) and [EFML](https://github.com/mlfoundations/) labs at the University of Washington\n\nAnd of course thanks to [Emad](https://twitter.com/EMostaque) and [Stability AI](https://stability.ai/) for providing the compute resources used for these experiments.\n","slug":"giant-openclip"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"giant-openclip"},"buildId":"_Y9IB71-ysOqNeQa3ppj_","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>