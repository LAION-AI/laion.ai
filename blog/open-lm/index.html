<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Introducing OpenLM | LAION</title><meta name="title" content="Introducing OpenLM | LAION"/><meta property="og:title" content="Introducing OpenLM | LAION"/><meta name="twitter:title" content="Introducing OpenLM | LAION"/><meta name="description" content="&lt;p&gt;&lt;img src=&quot;/images/blog/openlm_teaser_wide.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id=&quot;introduction&quot; class=&quot;anchor&quot; href=&quot;#introduction&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidde..."/><meta property="og:description" content="&lt;p&gt;&lt;img src=&quot;/images/blog/openlm_teaser_wide.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id=&quot;introduction&quot; class=&quot;anchor&quot; href=&quot;#introduction&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidde..."/><meta name="twitter:description" content="&lt;p&gt;&lt;img src=&quot;/images/blog/openlm_teaser_wide.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id=&quot;introduction&quot; class=&quot;anchor&quot; href=&quot;#introduction&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidde..."/><meta property="og:image" content="https://laion.ai/images/blog/new_openlm_teaser.png"/><meta name="twitter:image" content="https://laion.ai/images/blog/new_openlm_teaser.png"/><meta name="twitter:image:alt" content="The text: LAION. Large-scale Artificial Intelligence Open Network, TRULY OPEN AI. 100% NON-PROFIT. 100% FREE."/><meta property="og:type" content="website"/><meta property="og:url" content="https://laion.ai/blog/open-lm"/><meta name="twitter:url" content="https://laion.ai/blog/open-lm"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1D374E"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.png"/><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff2" as="font" type="font/woff2" crossorigin="true"/><meta name="next-head-count" content="25"/><link rel="stylesheet" href="/fonts/load.css"/><link rel="preload" href="/_next/static/css/5357c8cce67e7f29.css" as="style"/><link rel="stylesheet" href="/_next/static/css/5357c8cce67e7f29.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fb0512e25146295.js" defer=""></script><script src="/_next/static/chunks/286-30519d8a3e60551d.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-44fac0971625f498.js" defer=""></script><script src="/_next/static/lHKLVRzIdSQcB-3GZe2dC/_buildManifest.js" defer=""></script><script src="/_next/static/lHKLVRzIdSQcB-3GZe2dC/_ssgManifest.js" defer=""></script><script src="/_next/static/lHKLVRzIdSQcB-3GZe2dC/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-paper md:text-sky md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LAION</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/team/">Team</a><a href="/blog/">Blog</a><a href="/notes/">Notes</a><a href="/press/">Press</a><a href="/about/">About</a><a href="/faq/">FAQ</a><a href="/donations/">Donations</a><a href="/privacy-policy/">Privacy Policy</a><a href="/dataset-requests/">Dataset Requests</a><a href="/impressum/">Impressum</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1D374E;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/team/">Team</a></p><p><a href="/blog/">Blog</a></p><p><a href="/notes/">Notes</a></p><p><a href="/press/">Press</a></p><p><a href="/about/">About</a></p><p><a href="/faq/">FAQ</a></p><p><a href="/donations/">Donations</a></p><p><a href="/privacy-policy/">Privacy Policy</a></p><p><a href="/dataset-requests/">Dataset Requests</a></p><p><a href="/impressum/">Impressum</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-paper grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-8xl md:text-8xl w-full font-bold title-flow break-words">INTRODUCING OPENLM</h1><p class="text-2xl pb-2">by: <!-- -->OpenLM team<!-- -->,<!-- --> <!-- -->26 Sep, 2023<!-- --></p><hr/><div class="pt-2 article"><p><img src="/images/blog/openlm_teaser_wide.png" alt=""></p>
<h2><a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h2>
<p>We release <a href="https://github.com/mlfoundations/open_lm">OpenLM</a> a simple and minimalist PyTorch codebase for training medium-sized language models. OpenLM is designed to maximize GPU utilization and training speed, and is easy to modify for new language model research and applications.</p>
<p>We validate OpenLM by training two language models, OpenLM-1B and OpenLM-7B, on 1.6T and 1.25T tokens of text, respectively. We evaluate these models on standard zero-shot text classification and multiple choice tasks and find that OpenLM-1B outperforms many popular, similarly sized models such as OPT-1.3B and Pythia-1B.  OpenLM-7B achieves similar performance to LLAMA-7B and MPT-7B.</p>
<p>In this blogpost, we briefly describe the training data, model, evaluation setup, and overall results. We also describe exciting future work we plan to pursue with these models and our OpenLM framework.</p>
<h2><a id="model-and-data-release" class="anchor" href="#model-and-data-release" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model and Data Release</h2>
<p>All models and training data (tokenized and shuffled) are available on Huggingface at the following links:</p>
<ul>
<li><a href="https://huggingface.co/mlfoundations/open_lm_1B">OpenLM-1B</a></li>
<li><a href="https://huggingface.co/mlfoundations/open_lm_7B_1.25T">OpenLM-7B</a></li>
<li><a href="https://huggingface.co/datasets/mlfoundations/open_lm_example_data">Training and validation data</a></li>
</ul>
<p>We are working on releasing intermediate checkpoints.</p>
<h2><a id="data" class="anchor" href="#data" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data</h2>
<p>We train our models on a collection of text totaling 1.6T tokens. The training data comes from the following sources:</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Tokens</th>
<th>Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td>RedPajama</td>
<td>1157.3B</td>
<td>72.6%</td>
</tr>
<tr>
<td>Pile</td>
<td>336.2B</td>
<td>21.1%</td>
</tr>
<tr>
<td>S2ORC</td>
<td>48.9B</td>
<td>3.1%</td>
</tr>
<tr>
<td>Pile of Law</td>
<td>27.1B</td>
<td>1.7%</td>
</tr>
<tr>
<td>RealNews</td>
<td>25.0B</td>
<td>1.6%</td>
</tr>
<tr>
<td>Total</td>
<td>1594.5B</td>
<td>100%</td>
</tr>
</tbody>
</table>
<p>We do not perform additional preprocessing on the text, and take the data as is from the original sources. To train our model on these data sources, we simply use the following data mix: 72.6% on RedPajama, 27.4% everything else. This follows the given distribution of data in the table above.</p>
<h2><a id="models" class="anchor" href="#models" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Models</h2>
<p>The models we train follow the basic architecture proposed by LLaMA. The two differences are that we use the  <strong>GPT-NeoX tokenizer</strong>, which we found to be effective in early experiments, and we use LayerNorm instead of RMSNorm, because we haven’t yet added a fused RMSNorm operation.</p>
<p>The 1B model is trained with AdamW (LR 1e-3, weight decay 0.1) on 128 A100 40GB GPUs, with a global batch size of 2M tokens.</p>
<p>The 7B model is trained with AdamW (LR 3e-4, weight decay 0.1) on 256 A100 40GB GPUs, with a global batch size of 4M tokens.</p>
<p>The training speed for the 7B model is 2300 tokens/s/GPU. For model parallelism we use PyTorch FSDP.</p>
<p>Aside from the model, the codebase closely follows OpenCLIP which has been tested on around 1,000 GPUs.</p>
<h2><a id="evaluation-setup" class="anchor" href="#evaluation-setup" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Evaluation Setup</h2>
<p>During training, we track validation loss using a held out subset of recent papers from the authors of the OpenLM library, breaking news at the time of development, and the OpenLM codebase.</p>
<p>After training, we use the LLM-foundry to  evaluate model performance on the 13 zero-shot tasks used to evaluate MPT-7B and LLaMA 7B in the MPT-7B release. We additionally evaluate 5-shot MMLU performance.</p>
<h2><a id="results" class="anchor" href="#results" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Results</h2>
<h3><a id="validation-loss" class="anchor" href="#validation-loss" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Validation Loss</h3>
<p>Here, we display the validation loss for up to 1T tokens of training for both the OpenLM-1B and 7B models:</p>
<p><img src="/images/blog/1B_loss.png" alt="validation loss of 1b model"></p>
<p><img src="/images/blog/7B_loss.png" alt="validation loss of 7b model"></p>
<h2><a id="downstream-evaluations" class="anchor" href="#downstream-evaluations" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Downstream Evaluations</h2>
<p>Here, we display the zero-shot evaluation results of OpenLM-1B throughout training:</p>
<table>
<thead>
<tr>
<th><strong>OpenLM-1B</strong></th>
<th><strong>250B  tokens</strong></th>
<th><strong>500B tokens</strong></th>
<th><strong>750B tokens</strong></th>
<th><strong>1T tokens</strong></th>
<th><strong>1.25T tokens</strong></th>
<th><strong>1.5T tokens</strong></th>
<th><strong>1.6T tokens</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Training progress</td>
<td>16% complete</td>
<td>31% complete</td>
<td>47% complete</td>
<td>63% complete</td>
<td>78% complete</td>
<td>94% complete</td>
<td>100% complete</td>
</tr>
<tr>
<td>arc_challenge</td>
<td>0.27</td>
<td>0.28</td>
<td>0.29</td>
<td>0.28</td>
<td>0.29</td>
<td>0.31</td>
<td>0.31</td>
</tr>
<tr>
<td>arc_easy</td>
<td>0.49</td>
<td>0.50</td>
<td>0.51</td>
<td>0.53</td>
<td>0.54</td>
<td>0.56</td>
<td>0.56</td>
</tr>
<tr>
<td>boolq</td>
<td>0.60</td>
<td>0.61</td>
<td>0.62</td>
<td>0.62</td>
<td>0.65</td>
<td>0.64</td>
<td>0.65</td>
</tr>
<tr>
<td>copa</td>
<td>0.71</td>
<td>0.70</td>
<td>0.70</td>
<td>0.78</td>
<td>0.71</td>
<td>0.73</td>
<td>0.70</td>
</tr>
<tr>
<td>hellaswag</td>
<td>0.50</td>
<td>0.54</td>
<td>0.54</td>
<td>0.57</td>
<td>0.59</td>
<td>0.61</td>
<td>0.61</td>
</tr>
<tr>
<td>lambada_openai</td>
<td>0.56</td>
<td>0.57</td>
<td>0.61</td>
<td>0.61</td>
<td>0.65</td>
<td>0.65</td>
<td>0.66</td>
</tr>
<tr>
<td>piqa</td>
<td>0.70</td>
<td>0.70</td>
<td>0.71</td>
<td>0.72</td>
<td>0.73</td>
<td>0.74</td>
<td>0.74</td>
</tr>
<tr>
<td>winogrande</td>
<td>0.55</td>
<td>0.57</td>
<td>0.58</td>
<td>0.59</td>
<td>0.61</td>
<td>0.60</td>
<td>0.60</td>
</tr>
<tr>
<td>MMLU</td>
<td>0.24</td>
<td>0.24</td>
<td>0.24</td>
<td>0.23</td>
<td>0.26</td>
<td>0.24</td>
<td>0.25</td>
</tr>
<tr>
<td>Jeopardy</td>
<td>0.01</td>
<td>0.02</td>
<td>0.01</td>
<td>0.01</td>
<td>0.04</td>
<td>0.09</td>
<td>0.10</td>
</tr>
<tr>
<td>Winograd</td>
<td>0.75</td>
<td>0.77</td>
<td>0.77</td>
<td>0.79</td>
<td>0.81</td>
<td>0.80</td>
<td>0.79</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Average</strong></td>
<td><strong>0.49</strong></td>
<td><strong>0.50</strong></td>
<td><strong>0.51</strong></td>
<td><strong>0.52</strong></td>
<td><strong>0.53</strong></td>
<td><strong>0.54</strong></td>
<td><strong>0.54</strong></td>
</tr>
</tbody>
</table>
<p>As a comparison, here are the zero-shot results of similarly sized baselines. Our model achieves similar performance to OPT-IML-1.3B, an instruction-tuned model.</p>
<table>
<thead>
<tr>
<th><strong>1B Baselines</strong></th>
<th style="text-align:right"><strong>OPT-1.3B</strong></th>
<th style="text-align:right"><strong>Pythia-1B</strong></th>
<th style="text-align:right"><strong>Neox-1.3B</strong></th>
<th style="text-align:right"><strong>OPT-IML-1.3B</strong></th>
<th style="text-align:right"><strong>OpenLM-1B</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_challenge</td>
<td style="text-align:right">0.27</td>
<td style="text-align:right">0.26</td>
<td style="text-align:right">0.26</td>
<td style="text-align:right">0.30</td>
<td style="text-align:right">0.31</td>
</tr>
<tr>
<td>arc_easy</td>
<td style="text-align:right">0.49</td>
<td style="text-align:right">0.51</td>
<td style="text-align:right">0.47</td>
<td style="text-align:right">0.58</td>
<td style="text-align:right">0.56</td>
</tr>
<tr>
<td>boolq</td>
<td style="text-align:right">0.58</td>
<td style="text-align:right">0.61</td>
<td style="text-align:right">0.62</td>
<td style="text-align:right">0.72</td>
<td style="text-align:right">0.65</td>
</tr>
<tr>
<td>copa</td>
<td style="text-align:right">0.75</td>
<td style="text-align:right">0.68</td>
<td style="text-align:right">0.72</td>
<td style="text-align:right">0.73</td>
<td style="text-align:right">0.70</td>
</tr>
<tr>
<td>hellaswag</td>
<td style="text-align:right">0.54</td>
<td style="text-align:right">0.49</td>
<td style="text-align:right">0.48</td>
<td style="text-align:right">0.54</td>
<td style="text-align:right">0.61</td>
</tr>
<tr>
<td>lambada_openai</td>
<td style="text-align:right">0.59</td>
<td style="text-align:right">0.58</td>
<td style="text-align:right">0.57</td>
<td style="text-align:right">0.57</td>
<td style="text-align:right">0.66</td>
</tr>
<tr>
<td>piqa</td>
<td style="text-align:right">0.72</td>
<td style="text-align:right">0.70</td>
<td style="text-align:right">0.72</td>
<td style="text-align:right">0.73</td>
<td style="text-align:right">0.74</td>
</tr>
<tr>
<td>winogrande</td>
<td style="text-align:right">0.59</td>
<td style="text-align:right">0.53</td>
<td style="text-align:right">0.55</td>
<td style="text-align:right">0.59</td>
<td style="text-align:right">0.60</td>
</tr>
<tr>
<td>MMLU</td>
<td style="text-align:right">0.25</td>
<td style="text-align:right">0.26</td>
<td style="text-align:right">0.26</td>
<td style="text-align:right">0.30</td>
<td style="text-align:right">0.25</td>
</tr>
<tr>
<td>Jeopardy</td>
<td style="text-align:right">0.01</td>
<td style="text-align:right">0.00</td>
<td style="text-align:right">0.00</td>
<td style="text-align:right">0.12</td>
<td style="text-align:right">0.10</td>
</tr>
<tr>
<td>Winograd</td>
<td style="text-align:right">0.74</td>
<td style="text-align:right">0.71</td>
<td style="text-align:right">0.75</td>
<td style="text-align:right">0.73</td>
<td style="text-align:right">0.79</td>
</tr>
<tr>
<td><strong>Average</strong></td>
<td style="text-align:right"><strong>0.50</strong></td>
<td style="text-align:right"><strong>0.48</strong></td>
<td style="text-align:right"><strong>0.49</strong></td>
<td style="text-align:right"><strong>0.54</strong></td>
<td style="text-align:right"><strong>0.54</strong></td>
</tr>
</tbody>
</table>
<p>Next, we display the zero-shot evaluation results of OpenLM-7B throughout training:</p>
<table>
<thead>
<tr>
<th><strong>OpenLM-7B</strong></th>
<th><strong>275B tokens</strong></th>
<th><strong>500B tokens</strong></th>
<th><strong>675B tokens</strong></th>
<th><strong>775B tokens</strong></th>
<th><strong>1T tokens</strong></th>
<th><strong>1.25T tokens</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Training progress</td>
<td>17% complete</td>
<td>31% complete</td>
<td>42% complete</td>
<td>48% complete</td>
<td>63% complete</td>
<td>78% complete</td>
</tr>
<tr>
<td>arc_challenge</td>
<td>0.35</td>
<td>0.35</td>
<td>0.36</td>
<td>0.37</td>
<td>0.39</td>
<td>0.39</td>
</tr>
<tr>
<td>arc_easy</td>
<td>0.60</td>
<td>0.61</td>
<td>0.62</td>
<td>0.62</td>
<td>0.63</td>
<td>0.66</td>
</tr>
<tr>
<td>boolq</td>
<td>0.67</td>
<td>0.66</td>
<td>0.69</td>
<td>0.69</td>
<td>0.70</td>
<td>0.70</td>
</tr>
<tr>
<td>copa</td>
<td>0.75</td>
<td>0.79</td>
<td>0.75</td>
<td>0.80</td>
<td>0.80</td>
<td>0.78</td>
</tr>
<tr>
<td>hellaswag</td>
<td>0.64</td>
<td>0.67</td>
<td>0.68</td>
<td>0.68</td>
<td>0.69</td>
<td>0.70</td>
</tr>
<tr>
<td>lambada_openai</td>
<td>0.67</td>
<td>0.68</td>
<td>0.69</td>
<td>0.70</td>
<td>0.70</td>
<td>0.70</td>
</tr>
<tr>
<td>piqa</td>
<td>0.75</td>
<td>0.76</td>
<td>0.76</td>
<td>0.76</td>
<td>0.77</td>
<td>0.77</td>
</tr>
<tr>
<td>winogrande</td>
<td>0.62</td>
<td>0.65</td>
<td>0.65</td>
<td>0.65</td>
<td>0.67</td>
<td>0.67</td>
</tr>
<tr>
<td>MMLU-0 shot</td>
<td>0.25</td>
<td>0.25</td>
<td>0.27</td>
<td>0.27</td>
<td>0.28</td>
<td>0.30</td>
</tr>
<tr>
<td>Jeopardy</td>
<td>0.15</td>
<td>0.18</td>
<td>0.23</td>
<td>0.22</td>
<td>0.16</td>
<td>0.21</td>
</tr>
<tr>
<td>Winograd</td>
<td>0.82</td>
<td>0.81</td>
<td>0.84</td>
<td>0.84</td>
<td>0.85</td>
<td>0.86</td>
</tr>
<tr>
<td><strong>Average</strong></td>
<td><strong>0.57</strong></td>
<td><strong>0.58</strong></td>
<td><strong>0.60</strong></td>
<td><strong>0.60</strong></td>
<td><strong>0.60</strong></td>
<td><strong>0.61</strong></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Task</th>
<th><strong>OpenLM-7B</strong></th>
<th><strong>LLAMA-7B</strong></th>
<th><strong>MPT-7B</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>arc_challenge</td>
<td>0.39</td>
<td>0.41</td>
<td>0.39</td>
</tr>
<tr>
<td>arc_easy</td>
<td>0.66</td>
<td>0.65</td>
<td>0.67</td>
</tr>
<tr>
<td>boolq</td>
<td>0.70</td>
<td>0.77</td>
<td>0.75</td>
</tr>
<tr>
<td>copa</td>
<td>0.78</td>
<td>0.78</td>
<td>0.81</td>
</tr>
<tr>
<td>hellaswag</td>
<td>0.70</td>
<td>0.75</td>
<td>0.76</td>
</tr>
<tr>
<td>lambada_openai</td>
<td>0.70</td>
<td>0.74</td>
<td>0.70</td>
</tr>
<tr>
<td>piqa</td>
<td>0.77</td>
<td>0.79</td>
<td>0.80</td>
</tr>
<tr>
<td>winogrande</td>
<td>0.67</td>
<td>0.68</td>
<td>0.68</td>
</tr>
<tr>
<td>MMLU-0 shot</td>
<td>0.30</td>
<td>0.30</td>
<td>0.30</td>
</tr>
<tr>
<td>Jeopardy</td>
<td>0.21</td>
<td>0.33</td>
<td>0.31</td>
</tr>
<tr>
<td>Winograd</td>
<td>0.86</td>
<td>0.81</td>
<td>0.88</td>
</tr>
<tr>
<td><strong>Average</strong></td>
<td><strong>0.61</strong></td>
<td><strong>0.64</strong></td>
<td><strong>0.64</strong></td>
</tr>
<tr>
<td><strong>MMLU-5 shot</strong></td>
<td><strong>0.34</strong></td>
<td><strong>0.34</strong></td>
<td></td>
</tr>
</tbody>
</table>
<p>Consistent with the validation loss, our models continue to improve in zero-shot performance even late in training. At 1.25T tokens, OpenLM-7B matches or outperforms LLaMA-7B or MPT-7B on 7 out of 11 tasks.</p>
<h2><a id="future-work" class="anchor" href="#future-work" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Future Work</h2>
<p>OpenLM has already enabled new language modeling research, for example in the development of <a href="https://arxiv.org/abs/2308.04430">low-risk language models trained on permissively licensed text</a>. We plan to use OpenLM to support a variety of new research directions, including multimodal models, mixture of experts, and dataset composition. We also plan to scale up OpenLM so it supports training larger models.</p>
<h2><a id="team-and-acknowledgements" class="anchor" href="#team-and-acknowledgements" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Team and acknowledgements</h2>
<p>The OpenLM team currently consists of: Suchin Gururangan*, Mitchell Wortsman*, Samir Yitzhak Gadre, Achal Dave, Maciej Kilian, Weijia Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard Heckel, Alex Dimakis, Ali Farhadi, Vaishaal Shankar, Ludwig Schmidt.</p>
<p>Code is based heavily on <a href="https://github.com/mlfoundations/open_clip">open-clip</a> developed by a team including Ross Wightman, Romain Beaumont, Cade Gordon, Mehdi Cherti, Jenia Jitsev, and <a href="https://github.com/mlfoundations/open_flamingo">open-flamingo</a>, developed by a team including Anas Awadalla and Irena Gao. Additional inspiration is from <a href="https://github.com/Lightning-AI/lit-llama">lit-llama</a>.</p>
<p>We thank Stability AI for providing the compute for this project, the RedPajama team for their dataset, Sarah Pratt for logo design, <a href="https://www.ifml.institute/">IFML</a>, and Toyota Research Institute. We also thank the following people for helpful advice and feedback throughout the project: Jonathan Frankle, Daniel King, Luca Soldaini.</p>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Introducing OpenLM","author":"OpenLM team","date":"September 26 2023","previewImg":"/images/blog/new_openlm_teaser.png"},"content":"![](/images/blog/openlm_teaser_wide.png)\n## Introduction\n\nWe release [OpenLM](https://github.com/mlfoundations/open_lm) a simple and minimalist PyTorch codebase for training medium-sized language models. OpenLM is designed to maximize GPU utilization and training speed, and is easy to modify for new language model research and applications.\n\n\nWe validate OpenLM by training two language models, OpenLM-1B and OpenLM-7B, on 1.6T and 1.25T tokens of text, respectively. We evaluate these models on standard zero-shot text classification and multiple choice tasks and find that OpenLM-1B outperforms many popular, similarly sized models such as OPT-1.3B and Pythia-1B.  OpenLM-7B achieves similar performance to LLAMA-7B and MPT-7B.\n\nIn this blogpost, we briefly describe the training data, model, evaluation setup, and overall results. We also describe exciting future work we plan to pursue with these models and our OpenLM framework. \n\n## Model and Data Release\n\nAll models and training data (tokenized and shuffled) are available on Huggingface at the following links:\n\n* [OpenLM-1B](https://huggingface.co/mlfoundations/open_lm_1B)\n* [OpenLM-7B](https://huggingface.co/mlfoundations/open_lm_7B_1.25T)\n* [Training and validation data](https://huggingface.co/datasets/mlfoundations/open_lm_example_data)\n\nWe are working on releasing intermediate checkpoints.\n  \n## Data\nWe train our models on a collection of text totaling 1.6T tokens. The training data comes from the following sources:\n\n\n| Dataset       | Tokens  | Percentage |\n|---------------|---------|------------|\n| RedPajama    | 1157.3B | 72.6% |\n| Pile         | 336.2B  | 21.1% |\n| S2ORC         | 48.9B   | 3.1% |\n| Pile of Law   | 27.1B   | 1.7% |\n| RealNews     | 25.0B   | 1.6% |\n| Total         | 1594.5B | 100% |\n\n\nWe do not perform additional preprocessing on the text, and take the data as is from the original sources. To train our model on these data sources, we simply use the following data mix: 72.6% on RedPajama, 27.4% everything else. This follows the given distribution of data in the table above.\n\n## Models\n\nThe models we train follow the basic architecture proposed by LLaMA. The two differences are that we use the  **GPT-NeoX tokenizer**, which we found to be effective in early experiments, and we use LayerNorm instead of RMSNorm, because we haven’t yet added a fused RMSNorm operation. \n\nThe 1B model is trained with AdamW (LR 1e-3, weight decay 0.1) on 128 A100 40GB GPUs, with a global batch size of 2M tokens. \n\nThe 7B model is trained with AdamW (LR 3e-4, weight decay 0.1) on 256 A100 40GB GPUs, with a global batch size of 4M tokens. \n\nThe training speed for the 7B model is 2300 tokens/s/GPU. For model parallelism we use PyTorch FSDP. \n\nAside from the model, the codebase closely follows OpenCLIP which has been tested on around 1,000 GPUs.\n\n## Evaluation Setup\n\nDuring training, we track validation loss using a held out subset of recent papers from the authors of the OpenLM library, breaking news at the time of development, and the OpenLM codebase.\n\nAfter training, we use the LLM-foundry to  evaluate model performance on the 13 zero-shot tasks used to evaluate MPT-7B and LLaMA 7B in the MPT-7B release. We additionally evaluate 5-shot MMLU performance.\n\n## Results\n\n### Validation Loss\n\nHere, we display the validation loss for up to 1T tokens of training for both the OpenLM-1B and 7B models:\n\n![validation loss of 1b model](/images/blog/1B_loss.png)\n\n\n![validation loss of 7b model](/images/blog/7B_loss.png)\n\n\n## Downstream Evaluations\n\nHere, we display the zero-shot evaluation results of OpenLM-1B throughout training:\n\n\n| **OpenLM-1B** | **250B  tokens** | **500B tokens** | **750B tokens** | **1T tokens** | **1.25T tokens** | **1.5T tokens** | **1.6T tokens** |\n|----------------|-----------------|-----------------|-----------------|---------------|------------------|-----------------|-----------------|\n|Training progress | 16% complete | 31% complete | 47% complete | 63% complete | 78% complete | 94% complete | 100% complete |\n| arc_challenge  |            0.27 |            0.28 |            0.29 |          0.28 |             0.29 |            0.31 |            0.31 |\n| arc_easy       |            0.49 |            0.50 |            0.51 |          0.53 |             0.54 |            0.56 |            0.56 |\n| boolq          |            0.60 |            0.61 |            0.62 |          0.62 |             0.65 |            0.64 |            0.65 |\n| copa           |            0.71 |            0.70 |            0.70 |          0.78 |             0.71 |            0.73 |            0.70 |\n| hellaswag      |            0.50 |            0.54 |            0.54 |          0.57 |             0.59 |            0.61 |            0.61 |\n| lambada_openai |            0.56 |            0.57 |            0.61 |          0.61 |             0.65 |            0.65 |            0.66 |\n| piqa           |            0.70 |            0.70 |            0.71 |          0.72 |             0.73 |            0.74 |            0.74 |\n| winogrande     |            0.55 |            0.57 |            0.58 |          0.59 |             0.61 |            0.60 |            0.60 |\n| MMLU           |            0.24 |            0.24 |            0.24 |          0.23 |             0.26 |            0.24 |            0.25 |\n| Jeopardy       |            0.01 |            0.02 |            0.01 |          0.01 |             0.04 |            0.09 |            0.10 |\n| Winograd       |            0.75 |            0.77 |            0.77 |          0.79 |             0.81 |            0.80 |            0.79 |\n|                |                 |                 |                 |               |                  |                 |                 |\n| **Average**    |        **0.49** |        **0.50** |        **0.51** |      **0.52** |         **0.53** |        **0.54** |        **0.54** |\n\n\nAs a comparison, here are the zero-shot results of similarly sized baselines. Our model achieves similar performance to OPT-IML-1.3B, an instruction-tuned model.\n\n| **1B Baselines** | **OPT-1.3B** | **Pythia-1B** | **Neox-1.3B** | **OPT-IML-1.3B** | **OpenLM-1B** |\n|------------------|-------------:|--------------:|--------------:|-----------------:|-----------------:|\n| arc_challenge    |         0.27 |          0.26 |          0.26 |             0.30 |             0.31 |\n| arc_easy         |         0.49 |          0.51 |          0.47 |             0.58 |             0.56 |\n| boolq            |         0.58 |          0.61 |          0.62 |             0.72 |             0.65 |\n| copa             |         0.75 |          0.68 |          0.72 |             0.73 |             0.70 |\n| hellaswag        |         0.54 |          0.49 |          0.48 |             0.54 |             0.61 |\n| lambada_openai   |         0.59 |          0.58 |          0.57 |             0.57 |             0.66 |\n| piqa             |         0.72 |          0.70 |          0.72 |             0.73 |             0.74 |\n| winogrande       |         0.59 |          0.53 |          0.55 |             0.59 |             0.60 |\n| MMLU             |         0.25 |          0.26 |          0.26 |             0.30 |             0.25 |\n| Jeopardy         |         0.01 |          0.00 |          0.00 |             0.12 |             0.10 |\n| Winograd         |         0.74 |          0.71 |          0.75 |             0.73 |             0.79 |\n| **Average**      |     **0.50** |      **0.48** |      **0.49** |         **0.54** |         **0.54** |\n\nNext, we display the zero-shot evaluation results of OpenLM-7B throughout training:\n\n | **OpenLM-7B**  | **275B tokens** | **500B tokens** | **675B tokens** | **775B tokens** | **1T tokens** | **1.25T tokens** |\n|-----------------|-----------------|-----------------|-----------------|-----------------|---------------|------------------|\n| Training progress | 17% complete | 31% complete | 42% complete | 48% complete | 63% complete | 78% complete |              |\n| arc_challenge   |            0.35 |            0.35 |            0.36 |            0.37 |          0.39 |             0.39   |\n| arc_easy        |            0.60 |            0.61 |            0.62 |            0.62 |          0.63 |             0.66  |\n| boolq           |            0.67 |            0.66 |            0.69 |            0.69 |          0.70 |             0.70    |\n| copa            |            0.75 |            0.79 |            0.75 |            0.80 |          0.80 |             0.78     |\n| hellaswag       |            0.64 |            0.67 |            0.68 |            0.68 |          0.69 |             0.70     |\n| lambada_openai  |            0.67 |            0.68 |            0.69 |            0.70 |          0.70 |             0.70       |\n| piqa            |            0.75 |            0.76 |            0.76 |            0.76 |          0.77 |             0.77      |\n| winogrande      |            0.62 |            0.65 |            0.65 |            0.65 |          0.67 |             0.67       |\n| MMLU-0 shot     |            0.25 |            0.25 |            0.27 |            0.27 |          0.28 |             0.30       |\n| Jeopardy        |            0.15 |            0.18 |            0.23 |            0.22 |          0.16 |             0.21       |\n| Winograd        |            0.82 |            0.81 |            0.84 |            0.84 |          0.85 |             0.86        |\n| **Average**     |        **0.57** |        **0.58** |        **0.60** |        **0.60** |      **0.60** |         **0.61**      |\n\n\n\n|Task | **OpenLM-7B** | **LLAMA-7B** | **MPT-7B** |\n|-----------------|-----------------|-----------------|-----------------|\n| arc_challenge   |              0.39  |         0.41 |       0.39 |\n| arc_easy        |            0.66  |         0.65 |       0.67 |\n| boolq           |               0.70   |         0.77 |       0.75 |\n| copa            |                      0.78      |         0.78 |       0.81 |\n| hellaswag       |                   0.70    |         0.75 |       0.76 |\n| lambada_openai  |                0.70      |         0.74 |       0.70 |\n| piqa            |                   0.77      |         0.79 |       0.80 |\n| winogrande      |                 0.67      |         0.68 |       0.68 |\n| MMLU-0 shot     |                    0.30        |         0.30 |       0.30 |\n| Jeopardy        |                       0.21        |         0.33 |       0.31 |\n| Winograd        |                       0.86              |         0.81 |       0.88 |\n| **Average**     |              **0.61**      |     **0.64** |   **0.64** |\n| **MMLU-5 shot** |                      **0.34**          |     **0.34** |            |\n\n\nConsistent with the validation loss, our models continue to improve in zero-shot performance even late in training. At 1.25T tokens, OpenLM-7B matches or outperforms LLaMA-7B or MPT-7B on 7 out of 11 tasks.\n\n## Future Work\n\nOpenLM has already enabled new language modeling research, for example in the development of [low-risk language models trained on permissively licensed text](https://arxiv.org/abs/2308.04430). We plan to use OpenLM to support a variety of new research directions, including multimodal models, mixture of experts, and dataset composition. We also plan to scale up OpenLM so it supports training larger models.\n\n## Team and acknowledgements\n\nThe OpenLM team currently consists of: Suchin Gururangan*, Mitchell Wortsman*, Samir Yitzhak Gadre, Achal Dave, Maciej Kilian, Weijia Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard Heckel, Alex Dimakis, Ali Farhadi, Vaishaal Shankar, Ludwig Schmidt.\n\nCode is based heavily on [open-clip](https://github.com/mlfoundations/open_clip) developed by a team including Ross Wightman, Romain Beaumont, Cade Gordon, Mehdi Cherti, Jenia Jitsev, and [open-flamingo](https://github.com/mlfoundations/open_flamingo), developed by a team including Anas Awadalla and Irena Gao. Additional inspiration is from [lit-llama](https://github.com/Lightning-AI/lit-llama).\n\nWe thank Stability AI for providing the compute for this project, the RedPajama team for their dataset, Sarah Pratt for logo design, [IFML](https://www.ifml.institute/), and Toyota Research Institute. We also thank the following people for helpful advice and feedback throughout the project: Jonathan Frankle, Daniel King, Luca Soldaini.","slug":"open-lm"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"open-lm"},"buildId":"lHKLVRzIdSQcB-3GZe2dC","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>