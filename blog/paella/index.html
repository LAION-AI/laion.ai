<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>A new Paella: Simple &amp; Efficient Text-To-Image generation | LAION</title><meta name="title" content="A new Paella: Simple &amp; Efficient Text-To-Image generation | LAION"/><meta property="og:title" content="A new Paella: Simple &amp; Efficient Text-To-Image generation | LAION"/><meta name="twitter:title" content="A new Paella: Simple &amp; Efficient Text-To-Image generation | LAION"/><meta name="description" content="&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/61938694/231021615-38df0a0a-d97e-4f7a-99d9-99952357b4b1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id=&quot;overview&quot; class=&quot;a..."/><meta property="og:description" content="&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/61938694/231021615-38df0a0a-d97e-4f7a-99d9-99952357b4b1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id=&quot;overview&quot; class=&quot;a..."/><meta name="twitter:description" content="&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/61938694/231021615-38df0a0a-d97e-4f7a-99d9-99952357b4b1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id=&quot;overview&quot; class=&quot;a..."/><meta property="og:image" content="https://laion.ai/images/blog/paella.png"/><meta name="twitter:image" content="https://laion.ai/images/blog/paella.png"/><meta name="twitter:image:alt" content="The text: LAION. Large-scale Artificial Intelligence Open Network, TRULY OPEN AI. 100% NON-PROFIT. 100% FREE."/><meta property="og:type" content="website"/><meta property="og:url" content="https://laion.ai/blog/paella"/><meta name="twitter:url" content="https://laion.ai/blog/paella"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1D374E"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.png"/><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff" as="font" type="font/woff"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff2" as="font" type="font/woff2"/><link rel="preload" href="/fonts/Dinish-Regular.woff" as="font" type="font/woff"/><link rel="preload" href="/fonts/Dinish-Regular.woff2" as="font" type="font/woff2"/><link rel="preload" href="/fonts/Dinish-Italic.woff" as="font" type="font/woff"/><link rel="preload" href="/fonts/Dinish-Italic.woff2" as="font" type="font/woff2"/><meta name="next-head-count" content="25"/><link rel="stylesheet" href="/fonts/load.css"/><link rel="preload" href="/_next/static/css/574fd6c60e30c5ab.css" as="style"/><link rel="stylesheet" href="/_next/static/css/574fd6c60e30c5ab.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-8a36306e7f94c275.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-891d88d46bc1d783.js" defer=""></script><script src="/_next/static/0vx7Q9bPLAi_abw6tpUib/_buildManifest.js" defer=""></script><script src="/_next/static/0vx7Q9bPLAi_abw6tpUib/_ssgManifest.js" defer=""></script><script src="/_next/static/0vx7Q9bPLAi_abw6tpUib/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-paper md:text-sky md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LAION</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/team/">Team</a><a href="/blog/">Blog</a><a href="/notes/">Notes</a><a href="/press/">Press</a><a href="/about/">About</a><a href="/faq/">FAQ</a><a href="/donations/">Donations</a><a href="/privacy-policy/">Privacy Policy</a><a href="/dataset-requests/">Dataset Requests</a><a href="/impressum/">Impressum</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1D374E;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/team/">Team</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/faq/">FAQ</a></p><p><a href="/privacy-policy/">Privacy Policy</a></p><p><a href="/dataset-requests/">Dataset Requests</a></p><p><a href="/impressum/">Impressum</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-paper grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-8xl md:text-8xl w-full font-bold title-flow break-words">A NEW PAELLA: SIMPLE &amp; EFFICIENT TEXT-TO-IMAGE GENERATION</h1><p class="text-2xl pb-2">by: <!-- -->Dominic Rampas and Pablo Pernias<!-- -->,<!-- --> <!-- -->15 Apr, 2023<!-- --></p><hr/><div class="pt-2 article"><p><img src="https://user-images.githubusercontent.com/61938694/231021615-38df0a0a-d97e-4f7a-99d9-99952357b4b1.png" alt=""></p>
<h3><a id="overview" class="anchor" href="#overview" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Overview.</h3>
<p>We are releasing a new Paella model which builds on top of our initial paper https://arxiv.org/abs/2211.07292.
Paella is a text-to-image model that works in a quantized latent space and learns similarly to MUSE and Diffusion models.
Paella is similar to MUSE as it also works on discrete tokens, but is different in the way tokens are noised as well as
the architecture. MUSE uses a transformer, whereas we use a CNN, which comes with many benefits. There are also subtle
differences in the conditioning Paella uses as well how images are sampled. And on the other hand, it can also be seen
as a discrete diffusion process, which noises images during training and iteratively removes noise during sampling.
Since the paper-release we worked intensively to bring Paella to a similar level as other
state-of-the-art models. With this release we are coming a step closer to that goal. However, our main intention is not
to make the greatest text-to-image model out there (at least for now), it is to bring text-to-image models closer
to people outside the field on a technical basis. For example, many models have codebases with many thousand lines of
code, that make it pretty hard for people to dive into the code and easily understand it. And that is our proudest
achievement with Paella. The training and sampling code for Paella is minimalistic and can be understood in
a few minutes, making further extensions, quick tests, idea testing etc. extremely fast. For instance, the entire
sampling code can be written in just <strong>12 lines</strong> of code.
In this blog post we will talk about how Paella works in short, give technical details and release the model.</p>
<h3><a id="how-does-paella-work" class="anchor" href="#how-does-paella-work" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How does Paella work?</h3>
<p>Paella works in a quantized latent space, just like StableDiffusion etc., to reduce the computational power needed.
Images are encoded to a smaller latent space and converted to visual tokens of shape <em>h x w</em>. During training,
these visual tokens are noised, by replacing a random amount of tokens with other randomly selected tokens
from the codebook of the VQGAN. The noised image are given to the model, along with a timestep and the conditional
information, which is text in our case. The model is tasked to predict the un-noised version of the tokens.
And that's it. The model is optimized with the CrossEntropy loss between the original tokens and the predicted tokens.
The amount of noise added during the training is just a linear schedule, meaning that we uniformly sample a percentage
between 0 and 100% and noise that amount of tokens.<br><br></p>
<figure>
  <img src="https://user-images.githubusercontent.com/61938694/231248435-d21170c1-57b4-4a8f-90a6-62cf3e7effcd.png" width="400">
  <figcaption>Images are noised and then fed to the model during training.</figcaption>
</figure>
<p>Sampling is also extremely simple, we start with the entire image being random tokens. Then we feed the latent image,
the timestep and the condition into the model and let it predict the final image. The models outputs a distribution
over every token, which we sample from with standard multinomial sampling.<br>
Since there are infinite possibilities for the result to look like, just doing a single step results in very basic
shapes without any details. That is why we add noise to the image again and feed it back to the model. And we repeat
that process for a number of times, with less noise being added every time, and slowly get our final image.
You can see how images emerge <a href="https://user-images.githubusercontent.com/61938694/231252449-d9ac4d15-15ef-4aed-a0de-91fa8746a415.png">here</a>.<br>
The following is the entire sampling code needed to generate images:</p>
<pre><code class="language-python">def sample(model_inputs, latent_shape, unconditional_inputs, steps=12, renoise_steps=11, temperature=(0.7, 0.3), cfg=8.0):
    with torch.inference_mode():
        sampled = torch.randint(low=0, high=model.num_labels, size=latent_shape)
        initial_noise = sampled.clone()
        timesteps = torch.linspace(1.0, 0.0, steps+1)
        temperatures = torch.linspace(temperature[0], temperature[1], steps)
        for i, t in enumerate(timesteps[:steps]):
            t = torch.ones(latent_shape[0]) * t

            logits = model(sampled, t, **model_inputs)
            if cfg:
                logits = logits * cfg + model(sampled, t, **unconditional_inputs) * (1-cfg)
            sampled = logits.div(temperatures[i]).softmax(dim=1).permute(0, 2, 3, 1).reshape(-1, logits.size(1))
            sampled = torch.multinomial(sampled, 1)[:, 0].view(logits.size(0), *logits.shape[2:])

            if i &lt; renoise_steps:
                t_next = torch.ones(latent_shape[0]) * timesteps[i+1]
                sampled = model.add_noise(sampled, t_next, random_x=initial_noise)[0]
    return sampled
</code></pre>
<h3><a id="results" class="anchor" href="#results" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Results</h3>
<img src="https://user-images.githubusercontent.com/61938694/231598512-2410c172-5a9d-43f4-947c-6ff7eaee77e7.png">
Since Paella is also conditioned on CLIP image embeddings the following things are also possible:<br><br>
<img src="https://user-images.githubusercontent.com/61938694/231278319-16551a8d-bfd1-49c9-b604-c6da3955a6d4.png">
<img src="https://user-images.githubusercontent.com/61938694/231287637-acd0b9b2-90c7-4518-9b9e-d7edefc6c3af.png">
<img src="https://user-images.githubusercontent.com/61938694/231287119-42fe496b-e737-4dc5-8e53-613bdba149da.png">
<h3><a id="technical-details" class="anchor" href="#technical-details" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Technical Details.</h3>
<p>Model-Architecture: U-Net (Mix of ConvNeXt, DiT etc.) <br>
Dataset: Laion-A, Laion Aesthetic &gt; 6.0 <br>
Training Steps: 1.3M <br>
Batch Size: 2048 <br>
Resolution: 256 <br>
VQGAN Compression: f4 <br>
Condition: ByT5-XL (95%), CLIP-H Image Embedding (10%), CLIP-H Text Embedding (10%)
Optimizer: AdamW
Hardware: 128 A100 @ 80GB <br>
Training Time: ~3 weeks <br>
Learning Rate: 1e-4 <br>
More details on the approach, training and sampling can be found in paper and on GitHub.</p>
<h3><a id="paper-model-code-release" class="anchor" href="#paper-model-code-release" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Paper, Model, Code Release</h3>
<p>Paper: https://arxiv.org/abs/2211.07292 <br>
Code: https://github.com/dome272/Paella <br>
Model: https://huggingface.co/dome272/Paella <br></p>
<h3><a id="limitations--conclusion" class="anchor" href="#limitations--conclusion" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Limitations &amp; Conclusion</h3>
<p>There are still many things to improve for Paella to get on par with standard diffusion models or to even outperform
them. One primary thing we notice is that even though we only condition the model on CLIP image embedding 10% of the
time, during inference the model heavily relies on the generated image embeddings by a prior model (mapping clip text
embeddings to image embeddings as proposed in Dalle2). We counteract this by decreasing the importance of the image
embeddings by reweighing the attention scores. There probably is a way to avoid this happening already in training.
Other limitations such as lack of composition, text depiction, unawareness of concepts etc. could also be reduced by
continuing the training for longer. As a reference, Paella has only seen as many images as SD 1.4 and due to concerns
in regard to training collapse (which later turned to be negligible), trained with a 10x lower learning rate for the
first 700k steps. To conclude, this is still work in progress, but our first model that works reasonably well and
a million times better than the first versions we trained months ago.</p>
<p>It is noteworthy that the design choices for Paella were based on trying to make a simple architecture and
model for text-to-image synthesis, drawing inspiration from existing techniques such as MaskGIT. Furthermore, this
approach eliminates the need for hyperparameters such as alpha, beta, and alpha_cum_prod, which are typically required
in diffusion models. As a result, this methodology is particularly well-suited for individuals who are new to the field
of generative artificial intelligence. Our aim is to lay the groundwork for future research in this domain, fostering
a landscape where AI is accessible and comprehensible to a broader audience. We encourage further exploration of this
approach, as we are confident in its potential to contribute useful insights and potentially advance the state of the
art in text-to-image synthesis.</p>
<h3><a id="contributions" class="anchor" href="#contributions" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Contributions</h3>
<p><strong>Thanks to:</strong></p>
<ul>
<li><a href="https://github.com/rom1504/">Romain Beaumont</a> and <a href="https://github.com/christophschuhmann">Christoph Schuhmann</a>
for constant help on datasets and giving useful advice.</li>
<li><a href="https://scholar.google.de/citations?user=p1FuAMkAAAAJ&amp;hl=en">Jenia Jitsev</a> for help on writing the blog post and
useful discussions.</li>
<li><a href="https://github.com/rvencu">Richard Vencu</a> for an incredible amount of help regarding hardware issues.</li>
<li><a href="https://stability.ai/">StabilityAI</a> for providing GPU-Cluster access and faith in Paella.</li>
</ul>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"A new Paella: Simple \u0026 Efficient Text-To-Image generation","author":"Dominic Rampas and Pablo Pernias","date":"April 15, 2023","previewImg":"/images/blog/paella.png"},"content":"![](https://user-images.githubusercontent.com/61938694/231021615-38df0a0a-d97e-4f7a-99d9-99952357b4b1.png)\n### Overview.\nWe are releasing a new Paella model which builds on top of our initial paper https://arxiv.org/abs/2211.07292.\nPaella is a text-to-image model that works in a quantized latent space and learns similarly to MUSE and Diffusion models.\nPaella is similar to MUSE as it also works on discrete tokens, but is different in the way tokens are noised as well as\nthe architecture. MUSE uses a transformer, whereas we use a CNN, which comes with many benefits. There are also subtle\ndifferences in the conditioning Paella uses as well how images are sampled. And on the other hand, it can also be seen\nas a discrete diffusion process, which noises images during training and iteratively removes noise during sampling.\nSince the paper-release we worked intensively to bring Paella to a similar level as other \nstate-of-the-art models. With this release we are coming a step closer to that goal. However, our main intention is not\nto make the greatest text-to-image model out there (at least for now), it is to bring text-to-image models closer\nto people outside the field on a technical basis. For example, many models have codebases with many thousand lines of \ncode, that make it pretty hard for people to dive into the code and easily understand it. And that is our proudest\nachievement with Paella. The training and sampling code for Paella is minimalistic and can be understood in \na few minutes, making further extensions, quick tests, idea testing etc. extremely fast. For instance, the entire\nsampling code can be written in just **12 lines** of code.\nIn this blog post we will talk about how Paella works in short, give technical details and release the model.\n\n### How does Paella work?\nPaella works in a quantized latent space, just like StableDiffusion etc., to reduce the computational power needed.\nImages are encoded to a smaller latent space and converted to visual tokens of shape *h x w*. During training,\nthese visual tokens are noised, by replacing a random amount of tokens with other randomly selected tokens\nfrom the codebook of the VQGAN. The noised image are given to the model, along with a timestep and the conditional\ninformation, which is text in our case. The model is tasked to predict the un-noised version of the tokens. \nAnd that's it. The model is optimized with the CrossEntropy loss between the original tokens and the predicted tokens.\nThe amount of noise added during the training is just a linear schedule, meaning that we uniformly sample a percentage \nbetween 0 and 100% and noise that amount of tokens.\u003cbr\u003e\u003cbr\u003e\n\n\u003cfigure\u003e\n  \u003cimg src=\"https://user-images.githubusercontent.com/61938694/231248435-d21170c1-57b4-4a8f-90a6-62cf3e7effcd.png\" width=\"400\"\u003e\n  \u003cfigcaption\u003eImages are noised and then fed to the model during training.\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\nSampling is also extremely simple, we start with the entire image being random tokens. Then we feed the latent image, \nthe timestep and the condition into the model and let it predict the final image. The models outputs a distribution\nover every token, which we sample from with standard multinomial sampling.  \nSince there are infinite possibilities for the result to look like, just doing a single step results in very basic \nshapes without any details. That is why we add noise to the image again and feed it back to the model. And we repeat\nthat process for a number of times, with less noise being added every time, and slowly get our final image.\nYou can see how images emerge [here](https://user-images.githubusercontent.com/61938694/231252449-d9ac4d15-15ef-4aed-a0de-91fa8746a415.png).\u003cbr\u003e\nThe following is the entire sampling code needed to generate images:\n```python\ndef sample(model_inputs, latent_shape, unconditional_inputs, steps=12, renoise_steps=11, temperature=(0.7, 0.3), cfg=8.0):\n    with torch.inference_mode():\n        sampled = torch.randint(low=0, high=model.num_labels, size=latent_shape)\n        initial_noise = sampled.clone()\n        timesteps = torch.linspace(1.0, 0.0, steps+1)\n        temperatures = torch.linspace(temperature[0], temperature[1], steps)\n        for i, t in enumerate(timesteps[:steps]):\n            t = torch.ones(latent_shape[0]) * t\n\n            logits = model(sampled, t, **model_inputs)\n            if cfg:\n                logits = logits * cfg + model(sampled, t, **unconditional_inputs) * (1-cfg)\n            sampled = logits.div(temperatures[i]).softmax(dim=1).permute(0, 2, 3, 1).reshape(-1, logits.size(1))\n            sampled = torch.multinomial(sampled, 1)[:, 0].view(logits.size(0), *logits.shape[2:])\n\n            if i \u003c renoise_steps:\n                t_next = torch.ones(latent_shape[0]) * timesteps[i+1]\n                sampled = model.add_noise(sampled, t_next, random_x=initial_noise)[0]\n    return sampled\n```\n\n### Results\n\u003cimg src=\"https://user-images.githubusercontent.com/61938694/231598512-2410c172-5a9d-43f4-947c-6ff7eaee77e7.png\"\u003e\nSince Paella is also conditioned on CLIP image embeddings the following things are also possible:\u003cbr\u003e\u003cbr\u003e\n\u003cimg src=\"https://user-images.githubusercontent.com/61938694/231278319-16551a8d-bfd1-49c9-b604-c6da3955a6d4.png\"\u003e\n\u003cimg src=\"https://user-images.githubusercontent.com/61938694/231287637-acd0b9b2-90c7-4518-9b9e-d7edefc6c3af.png\"\u003e\n\u003cimg src=\"https://user-images.githubusercontent.com/61938694/231287119-42fe496b-e737-4dc5-8e53-613bdba149da.png\"\u003e\n\n### Technical Details.\nModel-Architecture: U-Net (Mix of ConvNeXt, DiT etc.) \u003cbr\u003e\nDataset: Laion-A, Laion Aesthetic \u003e 6.0 \u003cbr\u003e\nTraining Steps: 1.3M \u003cbr\u003e\nBatch Size: 2048 \u003cbr\u003e\nResolution: 256 \u003cbr\u003e\nVQGAN Compression: f4 \u003cbr\u003e\nCondition: ByT5-XL (95%), CLIP-H Image Embedding (10%), CLIP-H Text Embedding (10%)\nOptimizer: AdamW\nHardware: 128 A100 @ 80GB \u003cbr\u003e\nTraining Time: ~3 weeks \u003cbr\u003e\nLearning Rate: 1e-4 \u003cbr\u003e\nMore details on the approach, training and sampling can be found in paper and on GitHub.\n\n### Paper, Model, Code Release\nPaper: https://arxiv.org/abs/2211.07292 \u003cbr\u003e\nCode: https://github.com/dome272/Paella \u003cbr\u003e\nModel: https://huggingface.co/dome272/Paella \u003cbr\u003e\n\n\n### Limitations \u0026 Conclusion\nThere are still many things to improve for Paella to get on par with standard diffusion models or to even outperform\nthem. One primary thing we notice is that even though we only condition the model on CLIP image embedding 10% of the\ntime, during inference the model heavily relies on the generated image embeddings by a prior model (mapping clip text\nembeddings to image embeddings as proposed in Dalle2). We counteract this by decreasing the importance of the image\nembeddings by reweighing the attention scores. There probably is a way to avoid this happening already in training.\nOther limitations such as lack of composition, text depiction, unawareness of concepts etc. could also be reduced by\ncontinuing the training for longer. As a reference, Paella has only seen as many images as SD 1.4 and due to concerns \nin regard to training collapse (which later turned to be negligible), trained with a 10x lower learning rate for the \nfirst 700k steps. To conclude, this is still work in progress, but our first model that works reasonably well and\na million times better than the first versions we trained months ago.\n\nIt is noteworthy that the design choices for Paella were based on trying to make a simple architecture and \nmodel for text-to-image synthesis, drawing inspiration from existing techniques such as MaskGIT. Furthermore, this \napproach eliminates the need for hyperparameters such as alpha, beta, and alpha_cum_prod, which are typically required \nin diffusion models. As a result, this methodology is particularly well-suited for individuals who are new to the field \nof generative artificial intelligence. Our aim is to lay the groundwork for future research in this domain, fostering\na landscape where AI is accessible and comprehensible to a broader audience. We encourage further exploration of this\napproach, as we are confident in its potential to contribute useful insights and potentially advance the state of the \nart in text-to-image synthesis.\n\n\n### Contributions\n\n**Thanks to:**\n\n* [Romain Beaumont](https://github.com/rom1504/) and [Christoph Schuhmann](https://github.com/christophschuhmann) \nfor constant help on datasets and giving useful advice.\n* [Jenia Jitsev](https://scholar.google.de/citations?user=p1FuAMkAAAAJ\u0026hl=en) for help on writing the blog post and\nuseful discussions.\n* [Richard Vencu](https://github.com/rvencu) for an incredible amount of help regarding hardware issues.\n* [StabilityAI](https://stability.ai/) for providing GPU-Cluster access and faith in Paella.\n\n","slug":"paella"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"paella"},"buildId":"0vx7Q9bPLAi_abw6tpUib","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>