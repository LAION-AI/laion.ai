<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Blog | LAION</title><meta name="title" content="Blog | LAION"/><meta property="og:title" content="Blog | LAION"/><meta name="twitter:title" content="Blog | LAION"/><meta name="description" content="LAION, Large-scale Artificial Intelligence Open Network, is a non-profit organization making machine learning resources available to the general public."/><meta property="og:description" content="LAION, Large-scale Artificial Intelligence Open Network, is a non-profit organization making machine learning resources available to the general public."/><meta name="twitter:description" content="LAION, Large-scale Artificial Intelligence Open Network, is a non-profit organization making machine learning resources available to the general public."/><meta property="og:image" content="https://laion.ai/social.png"/><meta name="twitter:image" content="https://laion.ai/social.png"/><meta name="twitter:image:alt" content="The text: LAION. Large-scale Artificial Intelligence Open Network, TRULY OPEN AI. 100% NON-PROFIT. 100% FREE."/><meta property="og:type" content="website"/><meta property="og:url" content="https://laion.ai/blog"/><meta name="twitter:url" content="https://laion.ai/blog"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1D374E"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.png"/><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff2" as="font" type="font/woff2" crossorigin="true"/><meta name="next-head-count" content="25"/><link rel="stylesheet" href="/fonts/load.css"/><link rel="preload" href="/_next/static/css/5357c8cce67e7f29.css" as="style"/><link rel="stylesheet" href="/_next/static/css/5357c8cce67e7f29.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fb0512e25146295.js" defer=""></script><script src="/_next/static/chunks/286-30519d8a3e60551d.js" defer=""></script><script src="/_next/static/chunks/58-229d4b5119b416a2.js" defer=""></script><script src="/_next/static/chunks/pages/blog-21908775d8c17a89.js" defer=""></script><script src="/_next/static/B54m5e8vIbpJDP9geUnol/_buildManifest.js" defer=""></script><script src="/_next/static/B54m5e8vIbpJDP9geUnol/_ssgManifest.js" defer=""></script><script src="/_next/static/B54m5e8vIbpJDP9geUnol/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-paper md:text-sky md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LAION</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/team/">Team</a><a href="/blog/">Blog</a><a href="/notes/">Notes</a><a href="/press/">Press</a><a href="/about/">About</a><a href="/faq/">FAQ</a><a href="/donations/">Donations</a><a href="/privacy-policy/">Privacy Policy</a><a href="/dataset-requests/">Dataset Requests</a><a href="/impressum/">Impressum</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1D374E;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/team/">Team</a></p><p><a href="/blog/">Blog</a></p><p><a href="/notes/">Notes</a></p><p><a href="/press/">Press</a></p><p><a href="/about/">About</a></p><p><a href="/faq/">FAQ</a></p><p><a href="/donations/">Donations</a></p><p><a href="/privacy-policy/">Privacy Policy</a></p><p><a href="/dataset-requests/">Dataset Requests</a></p><p><a href="/impressum/">Impressum</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-paper grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center pt-16 md:pt-5"><div class="container px-5"><h1 class="text-8xl md:text-8xl font-bold pb-2">BLOG</h1><div class="text-md pb-4">Welcome to our LAION blog! Here, you will find commentaries, news, and updates on our current research projects and progress in the field of AI research. These blog posts are not meant to be full scientific research papers, but work in progress to encourage further research / discussions on our discord server and the open scientific community.</div><hr class="mb-5 md:hidden"/><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Do They See What We See?</p><p class="text-lg pb-1">by: <!-- -->LAION<!-- -->, <!-- -->19 Jun, 2025<!-- --></p><hr/><p class="pt-2">Building Emotionally Intelligent AI with EmoNet, a suite of open tools and resources developed by the LAION community
A LAION &amp;amp; Intel Collaboration
Authors
LAION e.V.
Christoph Schuhmann*, Robert Kaczmarczyk*, Gollam Rabby, Felix Friedrich, Maurice Kraus, Kourosh Nadi, Desmond Grealy, Huu Nguyen...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Introducing BUD-E 1.0: AI-Assisted Education for Everyone</p><p class="text-lg pb-1">by: <!-- -->LAION<!-- -->, <!-- -->20 Jan, 2025<!-- --></p><hr/><p class="pt-2">Today marks a milestone in our journey towards democratizing education and empathy through technology. LAION e.V. is thrilled to announce the release of BUD-E version 1.0, an open-source, privacy-compliant AI education assistant framework.
BUD-E, which stands for Buddy for Understanding and Digital ...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">LAION-DISCO-12M</p><p class="text-lg pb-1">by: <!-- -->LAION e.V.<!-- -->, <!-- -->17 Nov, 2024<!-- --></p><hr/><p class="pt-2">LAION announces the LAION-DISCO-12M - a collection of 12 million links to publicly available YouTube samples paired with metadata to support basic machine learning research in foundation models for generic audio, music information retrieval, and audio dataset analysis. We collect metadata for 12,648...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">LAION AI/oneAPI Center of Excellence for Personalized AI Education</p><p class="text-lg pb-1">by: <!-- -->LAION e.V.<!-- -->, <!-- -->10 Sep, 2024<!-- --></p><hr/><p class="pt-2">Large-scale Artificial Intelligence Open Network (LAION) is honored to become an Intel AI/oneAPI Center of Excellence to advance the development of BUD-E (Buddy for Understanding and Digital Empathy), an open source, empathetic AI education assistant aimed at democratizing personalized learning worl...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Releasing Re-LAION 5B: transparent iteration on LAION-5B with additional safety fixes</p><p class="text-lg pb-1">by: <!-- -->LAION e.V.<!-- -->, <!-- -->30 Aug, 2024<!-- --></p><hr/><p class="pt-2">Today, following a safety revision procedure, we announce Re-LAION-5B, an updated version of LAION-5B, that is the first web-scale, text-link to images pair dataset to be thoroughly cleaned of known links to suspected CSAM.
Highlights

Re-LAION-5B fixes the issues as reported by Stanford Internet Ob...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">BUD-E: Enhancing AI Voice Assistants’ Conversational Quality, Naturalness and Empathy</p><p class="text-lg pb-1">by: <!-- -->LAION<!-- -->, <!-- -->08 Feb, 2024<!-- --></p><hr/><p class="pt-2">AI voice assistants have revolutionized our interaction with technology, answering queries, performing tasks, and making life easier. However, the stilted, mechanical nature of their responses is a barrier to truly immersive conversational experiences. Unlike human conversation partners, they often ...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">LAION POP: 600,000 high-resolution images with detailed descriptions</p><p class="text-lg pb-1">by: <!-- -->Christoph Schuhmann, Peter Bevan<!-- -->, <!-- -->17 Nov, 2023<!-- --></p><hr/><p class="pt-2">LAION POP is a subset of LAION 5B: This subset comprises 600,000 high-resolution images, each equipped with detailed descriptions. The selection of images was based on 10,000 different concepts popular on the image generation site &amp;quot;Midjourney&amp;quot;.



SampleOverview




LAION-POP Dataset on Hu...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Open Empathic Launch</p><p class="text-lg pb-1">by: <!-- -->Christoph, Knoriy, Robert<!-- -->, <!-- -->22 Oct, 2023<!-- --></p><hr/><p class="pt-2">We are thrilled to present Open Empathic, a pioneering open-source project initiated by our non-profit organization, LAION.
Open Empathic aims to equip open-source AI systems with empathy and emotional intelligence. We hope that methods and tools developed within the framework of this project, toget...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Strategic Game Datasets for Enhancing AI Planning: An Invitation for Collaborative Research</p><p class="text-lg pb-1">by: <!-- -->Christoph Schuhmann &amp; Qi Sun<!-- -->, <!-- -->18 Oct, 2023<!-- --></p><hr/><p class="pt-2">Recent advancements in artificial intelligence (AI) underscore the progress of reasoning and planning shown by recent generalist machine learning (ML) models. The progress can be boosted by datasets that can further boost these generic capabilities when used for training foundation models of various...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">CLARA: Advancing Machines in Understanding Speech Nuances</p><p class="text-lg pb-1">by: <!-- -->Knoriy, Christoph, Robert<!-- -->, <!-- -->16 Oct, 2023<!-- --></p><hr/><p class="pt-2">Voices carry not only words but also convey emotions, emphasis, and nuance through aspects like tone and accent. However, existing speech technology only partially comprehends these intricate components of human speech.
Introducing CLARA (Multilingual Contrastive Learning for Audio Representation Ac...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">LeoLM: Igniting German-Language LLM Research</p><p class="text-lg pb-1">by: <!-- -->Björn Plüster<!-- -->, <!-- -->28 Sep, 2023<!-- --></p><hr/><p class="pt-2">We proudly introduce LeoLM (Linguistically Enhanced Open Language Model), the first comprehensive suite of German-language Foundation Language Models trained in collaboration with HessianAI on their new supercomputer 42! Built on Llama-2 and trained on a large-scale, high-quality German text corpus,...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Introducing OpenLM</p><p class="text-lg pb-1">by: <!-- -->OpenLM team<!-- -->, <!-- -->26 Sep, 2023<!-- --></p><hr/><p class="pt-2">
Introduction
We release OpenLM a simple and minimalist PyTorch codebase for training medium-sized language models. OpenLM is designed to maximize GPU utilization and training speed, and is easy to modify for new language model research and applications.
We validate OpenLM by training two language m...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Towards a transparent AI Future: The Call for less regulatory hurdles on Open-Source AI in Europe</p><p class="text-lg pb-1">by: <!-- -->LAION<!-- -->, <!-- -->21 Sep, 2023<!-- --></p><hr/><p class="pt-2">Following our previous open letter to the European Parliament on the significance of open-source AI, LAION, backed by European Laboratory for Learning and Intelligent Systems (ELLIS) and a long list of very impactful AI researchers, we submit this new open letter to the European Parliament:



Link ...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">LAION Triumphs at the Falling Walls Science Breakthrough of the Year 2023 Awards</p><p class="text-lg pb-1">by: <!-- -->Christoph, Jenia, Robert<!-- -->, <!-- -->14 Sep, 2023<!-- --></p><hr/><p class="pt-2">We happily announce that we, LAION, won the Falling Walls Science Breakthrough of the Year 2023 Award in the category Science and Innovation Management for &amp;quot;democratizing AI research by providing open access to advanced AI models, tools, and datasets, fostering public engagement and awareness, ...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Introducing VisIT-Bench, a new benchmark for instruction-following vision-language models inspired by real-world use</p><p class="text-lg pb-1">by: <!-- -->Yonatan Bitton<!-- -->, <!-- -->15 Aug, 2023<!-- --></p><hr/><p class="pt-2">[Paper] [Code] [Dataset] [Leaderboard]
We are thrilled to introduce VisIT-Bench, a benchmark for evaluating instruction-following vision-language models (VLMs). The central goal of VisIT-Bench is to provide a more accurate and meaningful assessment of VLMs, particularly in the context of human-chatb...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Objaverse-XL: An Open Dataset of Over 10 Million 3D Objects</p><p class="text-lg pb-1">by: <!-- -->Matt Deitke<!-- -->, <!-- -->11 Jul, 2023<!-- --></p><hr/><p class="pt-2">
We are thrilled to announce Objaverse-XL, an open dataset of over 10 million 3D objects! Using it, we train Zero123-XL, a foundation model for 3D that displays remarkable generalization abilities. In the landscape of AI, scale has been paramount to recent advances. Over the past decade, we have obs...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">video2dataset: A simple tool for large video dataset curation</p><p class="text-lg pb-1">by: <!-- -->Maciej Kilian<!-- -->, <!-- -->10 Jul, 2023<!-- --></p><hr/><p class="pt-2">[GitHub]
Within only two years large foundational models like CLIP, Stable Diffusion, and Flamingo have fundamentally transformed multimodal deep learning. Because of such models and their impressive capabilities to either create stunning, high-resolution imagery or to solve complex downstream tasks...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">OpenFlamingo v2: New Models and Enhanced Training Setup</p><p class="text-lg pb-1">by: <!-- -->Anas Awadalla* and Irena Gao*<!-- -->, <!-- -->28 Jun, 2023<!-- --></p><hr/><p class="pt-2">[GitHub] [Demo] [Models]
About three months ago, we announced OpenFlamingo, an open-source effort to replicate DeepMind&#x27;s Flamingo models.
Today, we are excited to release five trained OpenFlamingo models across the 3B, 4B, and 9B scales. These models are based on Mosaic’s MPT-1B and 7B and Together...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Announcing DataComp: In search of the next generation of multimodal datasets</p><p class="text-lg pb-1">by: <!-- -->Gabriel Ilharco<!-- -->, <!-- -->27 Apr, 2023<!-- --></p><hr/><p class="pt-2">[ Paper ] [ Code ] [ Website ]
About a year ago, we released LAION-5B, a billion-scale open-source image-text dataset. Since then, LAION-5B has become a staple in the open-source machine learning ecosystem, powering open-source models like OpenCLIP, OpenFlamingo, and Stable Diffusion. From the begin...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">A new Paella: Simple &amp; Efficient Text-To-Image generation</p><p class="text-lg pb-1">by: <!-- -->Dominic Rampas and Pablo Pernias<!-- -->, <!-- -->15 Apr, 2023<!-- --></p><hr/><p class="pt-2">
Overview.
We are releasing a new Paella model which builds on top of our initial paper https://arxiv.org/abs/2211.07292.
Paella is a text-to-image model that works in a quantized latent space and learns similarly to MUSE and Diffusion models.
Paella is similar to MUSE as it also works on discrete t...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Petition for keeping up the progress tempo on AI research while securing its transparency and safety.</p><p class="text-lg pb-1">by: <!-- -->LAION.ai<!-- -->, <!-- -->29 Mar, 2023<!-- --></p><hr/><p class="pt-2">LINK TO OUR PETITION
Authors: Christoph Schuhmann, Huu Nguyen, Robert Kaczmarczyk, Jenia Jitsev &amp;amp; LAION community
Securing Our Digital Future: Calling for CERN like international organization to transparently coordinate and progress on large-scale AI research and its safety
In an era of unparall...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Announcing OpenFlamingo: An open-source framework for training vision-language models with in-context learning</p><p class="text-lg pb-1">by: <!-- -->Anas Awadalla and Irena Gao<!-- -->, <!-- -->28 Mar, 2023<!-- --></p><hr/><p class="pt-2">Overview.
We are thrilled to announce the release of OpenFlamingo, an open-source reproduction of DeepMind&#x27;s Flamingo model. At its core, OpenFlamingo is a framework that enables training and evaluation of large multimodal models (LMMs). Check out our GitHub repository and demo to get started!
For t...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">The OIG Dataset</p><p class="text-lg pb-1">by: <!-- -->By Huu Nguyen -  Ontocord.ai, Sameer Suri, Ken Tsui , Shahules786, Together.xyz team, and Christoph Schuhmann - LAION.ai<!-- -->, <!-- -->10 Mar, 2023<!-- --></p><hr/><p class="pt-2">The Open Instruction Generalist (OIG) dataset is a large open source instruction dataset that currently contains ~43M instructions.
OIG is one of many chatbot datasets that LAION, along with its volunteers, Ontocord, Together and other members of the open source community, will be releasing and is i...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Training Contrastive Captioners</p><p class="text-lg pb-1">by: <!-- -->Giovanni Puccetti, Maciej Kilian, Romain Beaumont<!-- -->, <!-- -->02 Feb, 2023<!-- --></p><hr/><p class="pt-2">We introduce a new model type to OpenClip Contrastive Captioners (CoCa) [1]. This model adds an autoregressive objective (generation) on top of the CLIP contrastive one. The architecture is composed of three parts, the first two are similar to those composing a CLIP model and the third is a text dec...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Clip-Retrieval Update: H-14 Index &amp; SLURM Inference</p><p class="text-lg pb-1">by: <!-- -->no usr<!-- -->, <!-- -->31 Jan, 2023<!-- --></p><hr/><p class="pt-2">Today we release a KNN index for LAION-5B that allows for fast queries of the dataset with the open clip ViT-H-14 CLIP model. This means that users can search through billions of samples quickly and easily, making it a powerful tool for various applications such as image and text retrieval, data fil...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Reaching 80% zero-shot accuracy with OpenCLIP: ViT-G/14 trained on LAION-2B</p><p class="text-lg pb-1">by: <!-- -->Mitchell Wortsman<!-- -->, <!-- -->24 Jan, 2023<!-- --></p><hr/><p class="pt-2">We have trained a new ViT-G/14 CLIP model with OpenCLIP which achieves 80.1% zero-shot accuracy on ImageNet and 74.9% zero-shot image retrieval (Recall@5) on MS COCO. As of January 2023, this is the best open source CLIP model.
We believe this is interesting because:

CLIP models are useful for zero...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Collaboration between LAION and the Stable Horde</p><p class="text-lg pb-1">by: <!-- -->Konstantinos Thoukydidis, hlky<!-- -->, <!-- -->08 Jan, 2023<!-- --></p><hr/><p class="pt-2">Author: Konstantinos Thoukydidis, hlky
We are happy to announce that LAION will be assisted by the Stable Horde to provide aesthetic ratings for existing datasets and a completely new dataset of Stable Diffusion generations, which will also be rated by their community.
We wrote in the past about LAI...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Laion coco: 600M synthetic captions from Laion2B-en</p><p class="text-lg pb-1">by: <!-- -->Christoph Schuhmann, Andreas Köpf, Richard Vencu, Theo Coombes, Romain Beaumont<!-- -->, <!-- -->15 Sep, 2022<!-- --></p><hr/><p class="pt-2">Author: Christoph Schuhmann, Andreas Köpf , Theo Coombes, Richard Vencu, Benjamin Trom , Romain Beaumont
We present LAION-COCO, the world’s largest dataset of 600M generated high-quality captions for publicly available web-images
Laion5B has five billion natural captions. They provide a lot of infor...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Laion translated: 3B captions translated to English from laion5B</p><p class="text-lg pb-1">by: <!-- -->Marianna Nezhurina, Romain Beaumont, Richard Vencu and Christoph Schuhmann<!-- -->, <!-- -->15 Sep, 2022<!-- --></p><hr/><p class="pt-2">Author: Marianna Nezhurina Romain Beaumont Richard Vencu Christoph Schuhmann
Laion5B dataset was automatically collected from a section of the human web (common crawl). Can models generate different and interesting data compared to what humans write?
That’s a question we are interested in investigat...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Large scale openCLIP: L/14, H/14 and g/14 trained on LAION-2B</p><p class="text-lg pb-1">by: <!-- -->Romain Beaumont<!-- -->, <!-- -->15 Sep, 2022<!-- --></p><hr/><p class="pt-2">We trained three large CLIP models with OpenCLIP: ViT-L/14, ViT-H/14 and ViT-g/14 (ViT-g/14 was trained only for about a third the epochs compared to the rest). The H/14 model achieves 78.0% zero shot top-1 accuracy on ImageNet and 73.4% on zero-shot image retrieval at Recall@5 on MS COCO. As of Sep...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">LAION-Aesthetics</p><p class="text-lg pb-1">by: <!-- -->Christoph Schuhmann<!-- -->, <!-- -->16 Aug, 2022<!-- --></p><hr/><p class="pt-2">We present LAION-Aesthetics, several collections of subsets from LAION 5B with high visual quality.

To create LAION-Aesthetics we trained several lightweight models that predict the rating people gave when they were asked “How much do you like this image on a scale from 1 to 10?”.
LAION-Aesthetics ...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">LAION-5B: A NEW ERA OF OPEN LARGE-SCALE MULTI-MODAL DATASETS</p><p class="text-lg pb-1">by: <!-- -->Romain Beaumont<!-- -->, <!-- -->31 Mar, 2022<!-- --></p><hr/><p class="pt-2">We present a dataset of 5,85 billion CLIP-filtered image-text pairs, 14x bigger than LAION-400M, previously the biggest openly accessible image-text dataset in the world - see also our NeurIPS2022 paper
See our update on the LAION-5B dataset.
Large image-text models like ALIGN, BASIC, Turing Bletchl...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">LAION-400-MILLION OPEN DATASET</p><p class="text-lg pb-1">by: <!-- -->Christoph Schuhmann<!-- -->, <!-- -->20 Aug, 2021<!-- --></p><hr/><p class="pt-2">We present LAION-400M: 400M English (image, text) pairs - see also our Data Centric AI NeurIPS Workshop 2021 paper
Concept and Content
The LAION-400M dataset is entirely openly, freely accessible.
WARNING: be aware that this large-scale dataset is non-curated. It was built for research purposes to e...</p></div></div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"slug":"do-they-see-what-we-see","frontmatter":{"title":"Do They See What We See?","author":"LAION","date":"Jun 19 2025","previewImg":"/images/blog/do_they_see/image4.png"},"content":"\nBuilding Emotionally Intelligent AI with EmoNet, a suite of open tools and resources developed by the LAION community\n\n*A LAION \u0026 Intel Collaboration*\n\n## Authors\n\n**LAION e.V.**  \nChristoph Schuhmann*, Robert Kaczmarczyk*, Gollam Rabby, Felix Friedrich, Maurice Kraus, Kourosh Nadi, Desmond Grealy, Huu Nguyen, Cahya Wirawan, Krishna Kalyan, Kristian Kersting, Sören Auer\n\n**Intel Corporation**  \nJayaraman Mahalingam\n\n![][image4]\n\nAn exciting frontier in technology today is the quest for artificial intelligence that truly understands and interacts with humans on a deeper level. While AI has made remarkable progress in language processing and complex problem-solving, one critical dimension has yet to be fully realized: true emotional intelligence.\n\nCan our AI systems perceive the subtle joy in a crinkled eye, the faint tremor of anxiety in a voice, or the complex blend of emotions that color our everyday interactions? We believe this is not just a fascinating academic pursuit but a fundamental necessity for the future of human-AI collaboration.\n\nToday, we're proud to release **EmoNet** – a suite of new, open and freely available models and tools designed to support global research and innovation in the emerging field of emotionally intelligent AI. Our contributions are multi-faceted, addressing critical gaps in current research and providing powerful new tools for the global AI community.\n\nThank you to our partner Intel. LAION and Intel have been collaborating on fostering empathic, thoughtful and productive human-AI interaction for several years.\n\n## Voice Acting Samples Demo\n\n\u003cdiv align=\"center\"\u003e\n\n\u003cdiv style=\"position: relative; width: 100%; padding-bottom: 56.25%; height: 0; margin-bottom: 1em;\"\u003e\n  \u003ciframe \n    src=\"https://www.youtube.com/embed/TsTVKCmqHhk\" \n    title=\"Voice Acting Samples Demo\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    allowfullscreen\n    style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\"\u003e\n  \u003c/iframe\u003e\n\u003c/div\u003e\n\n\u003c/div\u003e\n\n## Our Empathic Computing Contributions\n\n### 1. EMONET-FACE Benchmark\n\nA novel, expert-annotated benchmark for fine-grained facial emotion estimation, featuring a comprehensive 40-category emotion taxonomy and large-scale, demographically diverse synthetic image datasets (EMONET-FACE BIG, BINARY, and HQ).\n\n\u003e **📊 Dataset:** [EMONET-FACE hosted by Hugging Face](https://huggingface.co/datasets/laion/EmoNet-Face_Big)\n\n![][image3]\n\n**Figure 2:** Samples from our EmoNet-Face datasets generated with different SOTA T2I models.\n\n### 2. EMONET-VOICE Benchmark\n\nA similarly fine-grained, expert-verified benchmark for speech emotion detection. Built upon our 40-category taxonomy and leveraging state-of-the-art synthetic voice generation for privacy and diversity. It includes 4,692 high-agreement audio samples.\n\n**Table 1:** Comparison of SER datasets. Key aspects include licensing, scale, emotional range, speaker diversity, synthetic origin, multilingual support and defining characteristics.\n\n### 3. EMPATHIC INSIGHT-FACE Model\n\nA state-of-the-art model for facial emotion estimation trained on our EMONET-FACE suite, surpassing the performance of leading models like Gemini 2.5 Pro and proprietary APIs like Hume AI on our benchmarks.\n\n![][image5]\n\n**Figure 3:** Mean Spearman's Rho correlation between various model annotators and human annotations.\n\nThis figure highlights the strength of facial emotion recognition correlation between the EmoNet Empathic Insights models and actual emotions compared to other models. The bar heights represent the mean of these per-emotion Spearman's Rho values calculated across all emotions for each model. Error bars indicate bootstrap 95% confidence intervals (N=1000 bootstraps) for these means. Model annotator groups, including our trained models (Empathic-Insight-Face), VLMs with multi-shot or zero-shot prompting, proprietary models (HumeFace), and a random baseline, are distinguished by patterns as detailed in the legend.\n\n\u003e **🔗 Models:**\n\u003e - [Large Empathic-Insight-Face Model](https://huggingface.co/laion/Empathic-Insight-Face-Large)\n\u003e - [Small Empathic-Insight-Face Model](https://huggingface.co/laion/Empathic-Insight-Face-Small)\n\n![][image6]\n\n**Figure 4:** EMPATHIC INSIGHT-FACE Model prediction example\n\n\u003e **💻 Try it:** [EMPATHIC INSIGHT-FACE Model Colab](https://colab.research.google.com/drive/11oUMo2HX0OuD9dx5ZM4ltNvoYxbI65hu?usp=sharing)\n\n### 4. EMPATHIC INSIGHT-VOICE Model\n\nA state-of-the-art model for speech emotion estimation, setting a new benchmark for nuanced understanding of vocal emotional cues, similarly outperforming established systems on our EMONET-VOICE benchmark.\n\n![][image7]\n\n**Table 2:** Performance Comparison of Audio Language Models on the EmoNet-Voice Benchmark.\n\n\u003e **🔗 Models:**\n\u003e - [Large Empathic-Insight-Voice Model](https://huggingface.co/laion/Empathic-Insight-Voice-Large)\n\u003e - [Small Empathic-Insight-Voice Model](https://huggingface.co/laion/Empathic-Insight-Voice-Small)\n\n### 5. BUD-E Whisper (Better Understanding of Emotion Whisper)\n\nA suite of fine-tuned Whisper models for advanced emotional speech captioning, going beyond mere transcription to describe emotional tone, vocal bursts, and speaker traits.\n\n\u003e **🔗 Resources:**\n\u003e - [BUD-E Whisper Model](https://huggingface.co/laion/BUD-E-Whisper)\n\u003e - [BUD-E Whisper Colab](https://colab.research.google.com/drive/1VoAtmNhY1hI5Yzv1_dppHTcYky82OCDK?usp=sharing)\n\n### 6. LAION's Got Talent Dataset\n\nAn extensive synthetic voice-acting dataset, forming the foundation for EMONET-VOICE, featuring over **5,000 hours of speech** across 11 synthetic voices, 40 emotions, and 4 languages. The cumulative playtime of this dataset is more than the cumulative playtime of all movies shown in US cinemas from 2021 to 2024, putting its sheer scale into perspective.\n\n\u003e **📊 Datasets:**\n\u003e - [Enhanced Version with Gemini Flash 2.0 Annotations](https://huggingface.co/datasets/laion/laions_got_talent_enhanced_flash_annotations_and_long_captions)\n\u003e - [Raw Dataset](https://huggingface.co/datasets/laion/laions_got_talent_raw)\n\n## Introducing EMONET-FACE \u0026 EMONET-VOICE: A New Foundation\n\nTo address these challenges, we developed the EMONET suites. At their core is a **novel 40-category emotion taxonomy**, meticulously derived from an extensive analysis of the \"Handbook of Emotions\" and refined through consultation with psychologists. This taxonomy moves far beyond basic emotions, encompassing a rich spectrum of positive and negative affective states, cognitive states (e.g., *Concentration, Confusion, Doubt*), physical states (e.g., *Pain, Fatigue, Intoxication*), and socially mediated emotions (e.g., *Embarrassment, Shame, Pride, Teasing*). This granularity is crucial for building AI that can appreciate the finer details of human emotional life.\n\n### EMONET-FACE\n\nEMONET-FACE provides a rich resource for visual emotion understanding:\n\n- **EMONET-FACE BIG** (over 203,000 synthetic images) offers a vast dataset for pre-training models.\n- **EMONET-FACE BINARY** (approx. 20,000 images) is designed for fine-tuning and features over 62,000 binary (present/absent) emotion annotations from human experts. These annotations underwent a rigorous multi-stage process, requiring triple positive agreement for affirmative labels and a contrastive batch to ensure high-quality true negatives.\n- **EMONET-FACE HQ** (2,500 images) serves as our gold-standard evaluation benchmark. Each image was meticulously rated by multiple psychology experts on a continuous 0-7 intensity scale across all 40 emotion categories, resulting in 10,000 expert annotations.\n\nThe synthetic images were generated using state-of-the-art text-to-image models with explicit prompts to ensure diverse demographic representation (across ethnicity, age, and gender) and clear, full-face expressions. This approach not only allows for controlled diversity but also sidesteps the ethical concerns associated with using real individuals' images.\n\n![][image8]\n\n**Figure 5:** Mapping Facial Expression Emotional Understanding\n\n### EMONET-VOICE\n\nEMONET-VOICE tackles the auditory domain with similar rigor:\n\n- EMONET-VOICE curates **4,692 high-agreement audio samples** from **LAION's Got Talent**.\n- Each snippet simulates actors portraying scenes designed to evoke specific emotions.\n- Crucially, each snippet underwent rigorous validation by human experts with psychology degrees. They assigned perceived intensity labels (*Not Present, Mildly Present, Intensely Present*) based on a strict 3-annotator consensus protocol, focusing on estimating the *presence and intensity* of emotions rather than assuming a single definitive label.\n\nThis privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets.\n\n## Why Emotion in Speech and Face Matters: The Vision of Universal Voice Actors\n\nEffective communication transcends mere words. It's woven with the rich threads of emotion, conveyed through the subtle shifts in our facial expressions and the intricate nuances of our voice. Capturing these expressions enables AI assistants to become more empathetic, engaging, and supportive; qualities crucial for transformative applications in education, mental health, companionship, and beyond.\n\nWe envision a future where multimodal foundation models evolve into \"omni-models\" with sophisticated audio-in/audio-out capabilities. Soon, every new foundation model on platforms like Hugging Face could be capable of performing voice acting like Robert De Niro or Scarlett Johansson. These AI systems will function like world-class voice actors, capable of being prompted not just by text, but also by voice, to adopt any persona. Imagine an AI that can embody an empathetic educator adapting to a student's confusion, a thrilling storyteller captivating an audience, or a knowledgeable research assistant explaining complex concepts with clarity and appropriate gravitas. This level of seamless and inspiring human-AI interaction is our ultimate goal.\n\n## The Imperative for Better Benchmarks: Seeing and Hearing the Nuance\n\nThe journey to emotionally intelligent AI begins with data. Existing datasets for emotion recognition, while valuable, often present significant limitations. Facial emotion datasets might rely on a narrow range of \"basic\" emotions, use images with occlusions or poor lighting, or lack demographic diversity, leading to biased models that perform poorly across different populations. Similarly, speech emotion datasets can be constrained by coarse emotion taxonomies, privacy concerns tied to real user data, or an over-reliance on acted portrayals that don't capture the subtlety of spontaneous emotional expression.\n\nThe **Theory of Constructed Emotion (TCE)**, a prominent psychological framework ([research link](https://pubmed.ncbi.nlm.nih.gov/27798257/)), posits that emotions are not universal, pre-programmed entities that we simply \"recognize.\" Instead, they are constructed by our brains based on a combination of interoceptive signals (like valence – pleasantness/unpleasantness, and arousal – activation/deactivation), learned concepts, and contextual information. This means there isn't a single, definitive facial expression or vocal intonation for \"joy\" or \"sadness\" that is universally and unambiguously displayed. Rather, emotional expression is a complex, dynamic, and often ambiguous signal.\n\nThis understanding underscores the need for **emotion estimation** rather than simple recognition. We need AI that can assess the *likelihood* and *intensity* of various emotions being present, rather than forcing a single label onto a complex human state.\n\n![][image9]\n\n**Figure 6:** Mapping Voice Emotional Understanding\n\n## Motivation and Taxonomy Development: Capturing the Full Spectrum\n\nOur approach to defining the emotional landscape for these datasets was systematic and grounded in psychological theory, while also addressing practical needs for AI applications:\n\n1. **Theoretical Foundations \u0026 Gap Analysis:** We analyzed established psychological frameworks, such as Plutchik's Wheel of Emotions. While foundational, these often neglect states critical for real-world AI, like *Bitterness* (often linked to social exclusion), physical states like *Pain* and *Fatigue*, or altered states like *Intoxication*. Though not always strictly classified as \"emotions\" in psychology, their accurate perception is vital for empathetic AI.\n2. **\"Handbook of Emotions\" Extraction:** We processed the comprehensive \"Handbook of Emotions\" (edited by Lisa Feldman Barrett et al.) in 500-word chunks, using a large language model (LLM) to extract emotion-related nouns and adjectives.\n3. **Refinement to 40 Categories:** After automatic clustering of these terms, manual refinement by our team, in consultation with psychology experts, yielded our final 40-category taxonomy. This taxonomy, shared by both EMONET-FACE and EMONET-VOICE, is detailed below.\n\n### Full Emotion Taxonomy\n\n![][image10]\n\n**Table 3:** Taxonomy Listing of Emotions Captured in Datasets\n\n## The EMONET-VOICE Benchmark: A Gold Standard for Speech Emotion\n\nLeveraging the rich LAION's Got Talent dataset, we are establishing the **EMONET-VOICE Benchmark**. This isn't just another dataset; it's a meticulously curated training and evaluation corpus for our 40-category emotion taxonomy.\n\n- **Intensity Annotation:** Each audio snippet is being annotated for emotional expression intensity. For most emotions, a three-level scale is used: non-existent, slightly expressed, and strongly/fully expressed. For certain high-intensity or inherently ambiguous categories, a two-level distinction (non-existent vs. present) is applied.\n- **Expert Psychological Annotators:** To ensure the highest reliability, labeling is conducted by annotators with formal psychological training (typically at least a bachelor's degree).\n- **Strict Consensus Protocol:** Only labels achieving exact agreement among three independent expert annotators (no disagreement on the intensity level) are included. This conservative approach ensures exceptional inter-rater reliability, making the annotations a true gold standard.\n\nUnlike existing benchmarks often limited to basic emotions or binary states, EMONET-VOICE offers a structured, expert-validated evaluation across 40 nuanced categories, applicable to both naturalistic and synthetic speech. This will enable objective, reproducible, and fine-grained assessment of models like BUD-E Whisper and future multimodal systems.\n\n![][image11]\n\n### EMONET-FACE: Paralleling Progress in Visual Emotion Recognition\n\nOur efforts in speech emotion are mirrored by our work in facial emotion recognition with the **EMONET-FACE** suite. This includes:\n\n- **EMONET-FACE BIG:** Over 203,000 synthetic images for pre-training.\n- **EMONET-FACE BINARY:** Approx. 20,000 images with 65,000+ human expert binary emotion annotations for fine-tuning.\n- **EMONET-FACE HQ:** 2,500 images with 10,000 continuous (0-7 scale) expert annotations across all 40 emotions for high-fidelity evaluation.\n\n### EMPATHIC INSIGHT Models: Setting New State-of-the-Art Performance\n\nThe power of these benchmarks is demonstrated by our **EMPATHIC INSIGHT** models, which showcase the capabilities unlocked by our datasets and taxonomies:\n\n- **EMPATHIC INSIGHT-FACE** achieves human-expert-level performance on EMONET-FACE HQ, outperforming models like Gemini 2.5 Pro and proprietary APIs. (More technical details on the model architecture and training can be found in our accompanying paper/technical report).\n- **EMPATHIC INSIGHT-VOICE**, trained on LAION's Got Talent and EMONET-VOICE, similarly sets a new SOTA for nuanced speech emotion estimation. (Further technical specifics on this model are also available).\n\nThese models, are permissively licensed (Creative Commons for the models, Apache 2.0 for the code), prove that with focused dataset construction and careful modeling, AI can indeed learn to \"see\" and \"hear\" emotions with a level of nuance approaching human perception.\n\n![][image12]\n![][image13]\n![][image14]\n\n**Figure 7:** More EMPATHIC INSIGHT-FACE Model prediction examples\n\n![][image15]\n\n**Figure 8:** LAION's Empathic Insights Face Models Closely Track with Human Annotators\n\n## Introducing BUD-E Whisper: Beyond Transcription to Emotional Understanding\n\nTo truly unlock the emotional content within speech, transcription alone is insufficient. This led us to develop **BUD-E Whisper**, a suite of fine-tuned variants of OpenAI's Whisper model. BUD-E Whisper is specifically adapted for advanced emotion captioning. These models go beyond simply converting speech to text; they generate structured descriptions of:\n\n- **Emotional Tone:** Identifying the perceived emotions from our 40-category taxonomy.\n- **Vocal Bursts:** Recognizing non-lexical expressions like laughter, sighs, gasps, etc.\n- **Speaker Traits:** Inferring characteristics like age, gender, and even speech style.\n\nThe training of BUD-E Whisper was a journey of iterative refinement. We utilized a diverse dataset including the **LAION's Got Talent** voice-acting data and approximately **5,000 hours of audio** from public vlogs, online diaries, and cinematic dialogue, with Voice Activity Detection (VAD) used to isolate speech segments. Gemini Flash 2.0 was then employed to annotate these samples along our 40-category emotion taxonomy.\n\nInitial experiments attempting direct regression from Whisper's architecture to scalar emotion intensity values (0-5 scale) proved challenging, as the autoregressive nature of Whisper isn't inherently suited for stable numerical output. We then shifted to a captioning approach. First, we used procedurally generated emotion summaries – templated sentences describing emotions, intensity, and speaker traits. While an improvement, these templates led to syntactic predictability and overfitting. The breakthrough came when we used LLMs to *paraphrase* these procedural captions. This introduced crucial syntactic variability while preserving semantic consistency. Training on these diverse, paraphrased captions enabled BUD-E Whisper to produce fluid, context-sensitive, and highly interpretable emotion descriptions.\n\nThe result is a robust system capable of identifying and describing nuanced emotional signals in speech, representing a significant step towards more emotionally aware voice assistants. BUD-E Whisper is particularly useful for generating rich captions for audio, preparing data for training text-to-speech and foundation models.\n\n## LAION's Got Talent: A Rich Tapestry of Synthetic Voices\n\nAt the heart of our speech emotion initiative is the **LAION's Got Talent** dataset. This comprehensive resource, created via the HyperLab API leveraging OpenAI's GPT-4 Audio model, comprises:\n\n- **11 distinct voices:** Offering a variety of vocal timbres and characteristics.\n- **40 meticulously curated emotion categories:** (Listed in detail above)\n- **Four languages:** English (approx. 2,156 hours), German (approx. 716 hours), Spanish (approx. 888 hours), and French (approx. 881 hours).\n- **Acting Challenges (English + German):** Approximately 111 hours of specialized scenarios.\n- **Diverse English Accent Distribution:** Including Louisiana (~133h), Valley Girl (~159h), British (~132h), and even English spoken with simulated Chinese, French, German, Indian, Italian, Mexican, Russian, Spanish, and Texan accents, plus a \"Vulgar Street English\" category (~149h), and English without specific accent specified (~391h), ensuring a broad representation.\n\n\u003e **📊 Download the LAION's Got Talent Datasets:**\n\u003e - [Enhanced Version with Gemini Flash 2.0 Annotations](https://huggingface.co/datasets/laion/laions_got_talent_enhanced_flash_annotations_and_long_captions)\n\u003e - [Raw Dataset](https://huggingface.co/datasets/laion/laions_got_talent_raw)\n\n## The Power of Synthetic Data: Ethical and Diverse\n\nA cornerstone of our initiative is the strategic use of synthetic data. This approach offers several key advantages:\n\n- **Privacy:** It entirely bypasses the ethical complexities and privacy risks associated with collecting and annotating real human emotional expressions, especially for sensitive states.\n- **Diversity and Control:** We can programmatically ensure demographic diversity in our datasets, controlling for age, gender, and ethnicity in facial images, and voice characteristics in speech. This is crucial for building fairer and less biased AI systems.\n- **Scale and Scope:** Synthetic generation allows us to create datasets of a scale and emotional breadth that would be prohibitively expensive or logistically impossible to achieve with human-acted or in-the-wild data.\n\n## Investing in Open Standards-based AI Innovation\n\nIntel is excited to collaborate with AI innovators like LAION, setting the groundwork for the use of AI in LLLMs and Visual computing for education, healthcare, and agentic assistive AI for all age groups and needs. It is innovative creators like those at LAION bringing empathy and emotional intelligence to AI that embody the positive social impact that responsible AI can deliver for people.\n\nIntel has a strong commitment to open standards and open-source innovation, particularly in AI, software, and hardware ecosystems, fostering collaboration, accessibility, and interoperability. Intel supports open standards in AI through initiatives like the Linux Foundation's AI \u0026 Data projects, ensuring AI frameworks and tools are hardware-agnostic and widely adoptable. This aligns with its [Hugging Face collaboration](https://huggingface.co/Intel), where Intel optimizes models for [ONNX](https://onnx.ai/)-compatible inference on its hardware. This collaboration goes back to 2021 in order to optimize AI model performance on Intel hardware, focusing on accelerating training and inference for transformer-based models. Their partnership aims to democratize AI by making it faster, more efficient, and accessible through open-source tools.\n\nThe mission of Intel's COE with LAION, Germany, established in 2024, is [to advance the development of BUD-E](https://www.intel.com/content/www/us/en/developer/articles/technical/bud-e-ai-assisted-education-for-all.html), an open-source, empathetic AI education assistant that aims to democratize personalized learning worldwide. LAION is proud to work with Intel, famous for the [International Science and Engineering Fair](https://en.wikipedia.org/wiki/International_Science_and_Engineering_Fair), founded by former Intel CEO Gordon Moore. Hugging Face's Datasets, Models, and Collections are core components of its ecosystem, hosted on the Hugging Face Hub, designed to facilitate AI development and deployment. **As further steps**, these core components can be enhanced by using Intel-optimized datasets and models, integrated with Intel libraries, and curating collections to showcase these resources for tasks like generative AI or image classification, all aligned with its open-source commitment.\n\n### Try Intel's AI Tools\n\nIf you'd like to sample the same technological innovations that LAION has access to, give these a try:\n\n- [PyTorch Optimizations from Intel](https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html)\n- [OpenVINO™ Toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html)\n- [Intel® oneAPI Toolkits](https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html)\n\nSee which leading research institutions, universities, and innovative startup companies are joint in efforts just like this as part of the [Intel® AI and oneAPI Center of Excellence](https://www.intel.com/content/www/us/en/developer/tools/oneapi/training/academic-program/centers-of-excellence.html) program.\n\n## The Future: Reasoning About Emotions, and the Dawn of Universal Voice Actors\n\nThe ability to accurately estimate emotions is a critical first step. The next frontier is to enable AI systems to *reason* about these emotions in context. We are convinced that in the very near future, foundation models will be multimodal, taking not only text but also audio natively in and natively out. These will be the \"universal voice actors\" we envision – capable of understanding, embodying, and expressing a vast range of human personas and emotions.\n\nImagine prompting an AI: \"Speak like a caring nurse comforting a worried patient,\" or \"Tell this story as a slightly grumpy but lovable grandpa.\" **LAION's Got Talent** and **EMONET-VOICE** are paving the way for such capabilities. Furthermore, the rich, multi-label, intensity-aware annotations in our EMONET suites provide the kind of data needed for training advanced reasoning models (like OpenAI's O-family or DeepSeek's R1) to understand the *implications* of emotional states and predict human future actions or outcomes based on observed cues from mental models, moving beyond simple recognition to true comprehension.\n\nTo truly democratize this field, LAION, with Intel's support, is committed to annotating **millions of permissively licensed audio samples** using our EMPATHIC INSIGHT-VOICE model. This will create an unparalleled public resource, fueling further research and development in self-supervised and multi-modal emotion learning.\n\nLooking ahead, our next ambitious goal is to create a massive, permissively licensed multilingual speech dataset exceeding 500,000 hours. This monumental undertaking is powered by the Intel® Tiber AI Cloud, where we are leveraging its high-performance, 192-core CPU instances to process and curate this unparalleled resource. This will further democratize and accelerate research, paving the way for the next generation of emotionally aware AI.\n\n## Try It and Collaborate: Join Our Journey\n\nThe development of emotionally intelligent AI is a collaborative endeavor. We invite the global AI community – researchers, developers, ethicists, and enthusiasts – to explore our work and contribute to this exciting field.\n\n### 📚 Read the Papers\n\n- **EmoNet Face:** [https://arxiv.org/abs/2505.20033](https://arxiv.org/abs/2505.20033)\n- **EmoNet Voice:** [https://arxiv.org/abs/2506.09827](https://arxiv.org/abs/2506.09827)\n\n### 📊 Explore the Datasets\n\n- **LAION's Got Talent:** \n  - [Enhanced Version](https://huggingface.co/datasets/laion/laions_got_talent_enhanced_flash_annotations_and_long_captions)\n  - [Raw Dataset](https://huggingface.co/datasets/laion/laions_got_talent_raw)\n- **EMONET-FACE:** [Collection](https://huggingface.co/collections/t1a5anu-anon/emonet-face-6825a1dd6c6ea537cecba7b8)\n- **EMONET-VOICE:** [Collection](https://huggingface.co/collections/t1a5anu-anon/emonet-voice-6825a1f3daac6ea7b37c26fb)\n\n### 🤖 Experiment with the Models\n\n**BUD-E Whisper:**\n- [Model](https://huggingface.co/laion/BUD-E-Whisper)\n\n**Empathic Insight Face:**\n- [Small Model](https://huggingface.co/laion/Empathic-Insight-Face-Small) | [Colab](https://colab.research.google.com/drive/1aLkBFncxBEdC2y0OcXbISd98Dc5MFq29?usp=sharing)\n- [Large Model](https://huggingface.co/laion/Empathic-Insight-Face-Large) | [Colab](https://colab.research.google.com/drive/11oUMo2HX0OuD9dx5ZM4ltNvoYxbI65hu?usp=sharing)\n\n**Empathic Insight Voice:**\n- [Model](https://huggingface.co/laion/Empathic-Insight-Voice-Small) | [Colab](https://colab.research.google.com/drive/1WR-B6j--Y5RdhIyRGF_tJ3YdFF8BkUA2)\n\n### 🌐 Engage with our Community\n\n- **GitHub:** [https://github.com/laion-ai](https://github.com/laion-ai)\n- **Discord:** [https://discord.gg/xBPBXfcFHd](https://discord.gg/xBPBXfcFHd)\n\nStay tuned for the official release of EMONET-VOICE and upcoming publications detailing our methodologies and findings.\n\n## Acknowledgements\n\nThis ambitious undertaking would not be possible without the incredible support of our partners. We extend our deepest gratitude to the Technical University Darmstadt, DFKI, Hessian AI, TIB-Leibniz Information Centre for Science and Technology, University Hannover, NOUS Research, Camb AI and especially Intel for their indispensable support, resources, and shared vision for advancing open and responsible AI research. Their commitment is instrumental in our journey to create AI that not only understands us but truly cares.\n\n[image1]: /images/blog/do_they_see/image1.jpg\n[image2]: /images/blog/do_they_see/image2.png\n[image3]: /images/blog/do_they_see/image3.png\n[image4]: /images/blog/do_they_see/image4.png\n[image5]: /images/blog/do_they_see/image5.png\n[image6]: /images/blog/do_they_see/image6.png\n[image7]: /images/blog/do_they_see/image7.png\n[image8]: /images/blog/do_they_see/image8.png\n[image9]: /images/blog/do_they_see/image9.png\n[image10]: /images/blog/do_they_see/image10.png\n[image11]: /images/blog/do_they_see/image11.png\n[image12]: /images/blog/do_they_see/image12.png\n[image13]: /images/blog/do_they_see/image13.png\n[image14]: /images/blog/do_they_see/image14.png\n[image15]: /images/blog/do_they_see/image15.png\n","date":1750291200000},{"slug":"bud-e-release","frontmatter":{"title":"Introducing BUD-E 1.0: AI-Assisted Education for Everyone","author":"LAION","date":"Jan 20 2025","previewImg":"/images/blog/bud-e-1.0.png"},"content":"\nToday marks a milestone in our journey towards democratizing education and empathy through technology. LAION e.V. *is thrilled to announce the release of BUD-E* version 1.0, an open-source, privacy-compliant AI education assistant framework.\n\nBUD-E, which stands for Buddy for Understanding and Digital Empathy, represents our first step towards realizing a grand vision: providing every person on Earth with access to a free, intelligent, and caring education assistant. This release includes three distinct versions tailored to different needs:\n\n- School Bud-E, a specialized web-based version for educational settings.\n- Bud-E, a general-purpose, customizable web-based assistant.\n- Desktop Bud-E, a Python*-based desktop application that can be used as a smart Google Home/Alexa Replacement with wake word activation and the latest models in the backend.\n\nOverview (what it can do):\n\n[![School Bud-E Video](https://github.com/user-attachments/assets/ae86cb13-38fe-4768-be46-5353f2f58bb9)](https://www.youtube.com/watch?v=gcSPuZ7LtE0)\n\nHow to use Bud-E \u0026 School Bud-E:\n\n[![School Bud-E Video 2](https://github.com/user-attachments/assets/02f1458e-d46f-4352-b44e-bb72165eb26e)](https://www.youtube.com/watch?v=IxHnpISMNPo)\n\n\n## BUD-E: A New Era of AI-Assisted Education\n\nThe Large-scale Artificial Intelligence Open Network (LAION) collaborates with Intel as part of the new AI/oneAPI Center of Excellence (CoE) we established in September last year.\n\nOur CoE's mission is to advance the development of BUD-E, an open source, empathetic AI education assistant that aims to democratize personalized learning worldwide. LAION is proud to work with Intel, famous for the International Science and Engineering Fair founded by former Intel CEO Gordon Moore.\n\nWe hope to replicate the mentorship and resources that used to be only available to a select few,\nscale it to the extent that every child in the world has access to a personalized education and the deep knowledge that used to be siloed in only the most prestigious educational institutions.\n\nFor our development, we actively leveraged the new Intel® Core™ Ultra Processors Series 2 AI PCs as well as Intel® Neural Compressor, OpenVINO™, and Intel® Optimizations for PyTorch*.\n\nThese components helped us to design our local first” peer-to-peer MLops architecture that preserves the children’s privacy. This peer-to-peer integration to individual nodes asks computers within its zone of trust, for machine learning inference, datasets, and models, using them as tools that the local language model can use to compose the correct answer,\n\nFor example, schools can utilize the Open Platform for Enterprise AI (OPEA*) to generate a graphrag of the school curriculum, serve Llama 3.1 405b, a video diffusion model, provide storage, or other things that won’t fit on a laptop form factor. It will also allow parents to put guardrails on the language model’s outputs, customize the curriculum or the special needs that the child needs help with, and collaborate with peers via a real-time generative AI-enabled whiteboard experience.\n\n## A Vision of Universal Access to Personalized Learning\n\nImagine a world where every child, regardless of background or circumstances, has a personal AI tutor available 24/7. A world where adults seeking to learn new skills or change careers have a patient, understanding guide to help them along their journey. This is the future we're working towards with BUD-E.\n\nWe believe education is the great equalizer, the key to unlocking human potential and fostering understanding between people of all backgrounds. However, access to quality, personalized education remains a privilege accessible for too few. Technology, particularly AI, has the potential to change this – not by replacing human teachers but by augmenting and extending their reach.\n\nBUD-E embodies our belief that an AI assistant can be more than just a source of information. We envision it as a companion in the learning journey – one that's empathetic, encouraging, and adapted to each individual's unique learning style and needs. While we just started on the road towards fully realizing our vision, BUD-E 1.0 and its specialized variants lay the groundwork for this ambitious goal.\n\n## The Path to Democratizing Empathy and Education\n\nSeveral core principles guide our journey with BUD-E:\n\n1. **Accessibility**: Education should be available to everyone, regardless of financial means or geographic location.\n2. **Privacy**: Learning is a personal journey, and users' data should be protected and respected.\n3. **Transparency**: As an open-source project, BUD-E's workings are open for scrutiny and improvement by the community.\n4. **Adaptability**: Every learner is unique, and educational tools should adapt to individual needs and preferences.\n5. **Empathy**: Effective learning requires not just information but understanding, encouragement, and emotional support.\n\nThese principles are at the heart of the design of all BUD-E versions and will continue to guide their development.\n\n## School Bud-E: An AI Learning Companion for the Classroom\n\nIn collaboration with the Intel AI Center of Excellence and the German Research Network for AI (DFKI), we're proud to release School Bud-E 1.0, a specialized web-based version of BUD-E designed to enhance the educational experience for students and teachers alike.\n\nSchool Bud-E is more than just a chatbot; it's a privacy-first AI learning companion that brings a new dimension of support and engagement to the classroom. Here are some of its key features:\n\n- **Privacy Compliance**: All interactions within School Bud-E are only stored locally in the user's browser. Using self-hosted or privacy-compliant APIs ensures that sensitive student data remains protected and private.\n- **Inspiring Learning Support**: School Bud-E features a carefully crafted system prompt that fosters engaging, age-appropriate interactions. It's designed to adapt to each student's learning style, providing encouragement and sparking curiosity.\n- **Wikipedia Integration**: The assistant can access Wikipedia* and provide information on request. For access, use #wikipedia: KEYWORDS:TOP_N; to specify the database, use #wikipedia_de or #wikipedia_en. We provide endpoints for searching in vector databases with BGE-M3 embeddings for English and German Wikipedia\n- **Curriculum Retrieval**: Teachers can effortlessly search school curricula using custom database endpoints, enabling School Bud-E to perform retrieval-augmented generation. Our system comes with an endpoint that performs Best Match 25 (BM25) search in all public school curricula of the German state of Hamburg. Simply use commands like #bildungsplan: KEYWORDS:TOP_N (e.g., #bildungsplan: artificial intelligence:5).\n- **Scientific Paper Retrieval**: School Bud-E can search through more than 85 million abstracts of scientific papers provided by the ASK* Open Research Knowledge Graph Initiative from the University of Hannover. For instance, using the command #papers: quantum computing:3 will retrieve the top three papers related to quantum computing, while #papers: climate change impacts:5 would return the top five papers on the impacts of climate change. Importantly, just like with Wikipedia and curriculum integration, the endpoint for this scientific paper database can be replaced with custom endpoints, allowing schools to integrate their own preferred research databases.\n- **Automated Assessment**: School Bud-E can help with suggestions for homework or test corrections using multimodal evaluation that combines optical character recognition (OCR) and AI, providing fair and consistent assessment of homework and exams. Simply upload images of the tests and use the #correction or #korrektur command.\n- **Natural Conversations**: Powered by advanced automatic speech recognition (ASR) and text-to-speech (TTS) technology, School Bud-E offers responsive voice interactions, enabling users to use the system like a voice message app.\n- **Flexible Deployment**: School Bud-E can be deployed using Docker Compose for ease of setup or run locally using a Deno server, providing flexibility for different technical environments.\n- **Multilingual Support**: School Bud-E can support over 100 languages depending on the APIs used. For example, combining Whisper and a Fish TTS 1.5 School Bud-E can understand and reply with text-to-speech output in 11 languages. Similarly, combining Whisper and camb.ai’s MARS6 model, Bud-E can reply in over 100 languages in the same voice, creating unparalleled accessibility for students from diverse cultures and backgrounds.\n- **Universal API Key**: Instead of providing several API keys, URLs, and models for each category, we now also offer the input of a single universal API key that automatically makes use of the default text-generation, speech-to-text, text-to-speech, and vision-language model\n\n## A Web-Based Frontend for Easy Access\n\nThe [School Bud-E web frontend](https://github.com/LAION-AI/school-bud-e-frontend) allows students and teachers to interact with the assistant directly from their browsers without needing to install any software. This web interface is designed for classroom use and offers intuitive voice message interactions similar to popular messaging apps.\n\n## Customizable Backend for Enhanced Flexibility\n\nSchool Bud-E allows users to specify their own API endpoints and backend models. This means schools can choose the AI models that best fit their needs and resources, whether self-hosted open-source models or commercial APIs.\n\n## Bud-E: Your Customizable, General-Purpose AI Companion (Web-Based)\n\nWe're also introducing Bud-E, a browser-based AI assistant that empowers users to customize their experience fully. This version puts the power of choice in your hands, allowing you to tailor the assistant to a wide range of tasks and preferences.\n\nThe key difference between Bud-E and School Bud-E is that Bud-E provides more freedom to define its personality by specifying your own system prompt. Whether you want a creative writing assistant, a research companion, or a friendly conversationalist, you're in control.\n\n## Desktop Bud-E 1.0: Your Python-Based Desktop AI Assistant\n\nFor users who prefer a desktop application, we offer **Desktop Bud-E 1.0**, a Python-based client that brings BUD-E's capabilities right to your desktop.\n\n### Key Features of Desktop Buddy\n\n- **Desktop Integration**: Desktop Bud-E functions similarly to Microsoft Copilot, integrating seamlessly with your desktop environment.\n- **Screenshot and Clipboard Interaction**: Capture screenshots and interact with your clipboard, allowing Desktop Buddy to understand and assist with many varied tasks.\nLocal Command Execution: Execute local commands, further extending Desktop Buddy's capabilities.\n- **Modular Design**: Like other BUD-E versions, each component (ASR, LLM, TTS, Vision) can be swapped out or updated independently, ensuring you always have access to the best tools.\n- **Skill System**: Execute Python functions either on the server or locally, opening up possibilities for various activities, from database queries to interactive experiments.\nTested on Windows and Ubuntu: Desktop Buddy has been thoroughly tested on popular operating systems.\n\nDesktop Buddy is ideal for users who want a powerful, locally integrated AI assistant that can interact with their desktop environment and execute local commands.\n\n## Technical Foundation: Flexibility and Openness\n\nAll BUD-E versions are built on a flexible client-server architecture. This allows for easy deployment, component upgrades, and adaptation to evolving technology.\n\n### Server Components\n\n1. **Automatic Speech Recognition (ASR)**: The ears of BUD-E, converting speech to text.\n2. **Language Model (LLM)**: The brain of BUD-E, processing text and generating responses.\n3. **Text-to-Speech (TTS)**: The voice of BUD-E, converting text responses to speech.\n4. **Vision Processing**: The eyes of BUD-E, handling image captioning and OCR.\n\n## The Road Ahead\n\nWe are under no illusions about the challenges ahead. Creating an AI assistant that truly understands and empathizes with learners, can explain complex concepts intuitively, and can foster a love of learning is an enormous task. BUD-E 1.0 in its different versions is but a small first step. It is important to note that the current version of BUD-E does not yet possess the capability to understand users' emotions, and BUD-E 1.0 is not intended to be used for emotion recognition. The purpose of BUD-E 1.0 is not to identify or infer emotions or intentions of natural persons on the basis of their biometric data, as defined under the European Union's AI Act. While basic emotion recognition could be helpful to the learner, for example by encouraging a learner whose voice sounds discouraged, we believe deployment should be cautious, transparent and responsible. We believe these features could be implemented by leveraging multimodal APIs like Gemini Audio or HUME AI for emotion recognition, but we have not yet integrated these features due to time constraints and regulatory considerations.\n\nAI systems intended to be used for emotion recognition are classified as high-risk under [Annex III of the European Union's AI Act](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=OJ:L_202401689#anx_III). Therefore, we do not recommend implementing and putting into practice any empathetic voice assistance capable of understanding users' emotions in the European Union yet without careful review. However, we firmly believe that empathetic AI assistants hold immense potential for creating more engaging and curiosity-sparking educational experiences. The ability to connect with users on an emotional level, in addition to an intellectual one, is a crucial ingredient for building systems that can truly help individuals explore the world more holistically. As such, we plan to pursue, in consultation with our EU attorneys, the development of empathetic voice assistance in general and also specifically for educational settings where safety measures could allow for emotion detection.\n\nBeyond developing empathetic capabilities, our roadmap includes improving natural language understanding and expanding the skill library; including more advanced tools for research, interactive learning, and other tasks.\n\n## Credits and Thanks\n\nThe LAION team closely collaborates with Intel’s Jayaraman Mahalingam (Jay) and Desmond Greely to make our vision a reality.\n\nJay provided LAION with Access to the new Intel® Core™ Ultra Processors Series 2 (formerly code-named Lunar Lake) AI PCs using Intel® Neural Compressor, OpenVINO™ and Intel® Optimizations for PyTorch.\n\nDesmond additionally provided the LAION team with access to cutting-edge Intel® Gaudi® AI Accelerators, to train Llama language models capable of driving 3d Avatars, by teaching language models how to express themselves emotionally and thoughtfully with audio tokens, chain of thought tokens, and body pose tokens using PyTorch Lightning.\n\nWe also thank\n\nHamburg’s [Behörde für Schule und and Berufsbildung (BSB)](https://www.hamburg.de/politik-und-verwaltung/behoerden/schulbehoerde), and  specifically [Dr. Najibulla Karim](https://bildungsserver.hamburg.de/schulfaecher/kontakt-mint-referat-710650) and [Dr. Janosch Schindler](https://jtschindler.github.io/) for their great feedback and support.\n\nHallucinate LLC, and specifically [Benjamin Barber](https://github.com/endomorphosis/) and his partners [Marc-Emanuel Otto](https://github.com/mwni) and [Kevin De Haan](https://github.com/coregod360), for being willing to open source parts of their previous MLops architecture, and donating their time and effort to create a new MLops architecture to support the LAION  educational mission.\n\n[Robin Yuen Shing Hei](https://github.com/Robinysh), who helped us prototype the model training code for a llama based audio language modeling presented at Neurips [Internalizing ASR with Implicit Chain of Thought for Efficient Speech-to-Speech Conversational LLM](https://arxiv.org/abs/2409.17353), and we wish him luck in his new role at Soundhound.\n\nJohn Oberg of [Komagome Gakuen High School](https://www.komagome.ed.jp/education/steam.php) for evaluating the impact of large language models on English language learners in Japan, and developing lesson plans to integrate LLMs into the classroom.\n\n## Join Us in Revolutionizing Education\n\nWe believe that by working together – developers, educators, researchers, students, and enthusiasts – we can create an AI assistant that truly empowers learners worldwide. BUD-E is more than just a software project; it's a movement towards a future where quality, personalized education is a right, not a privilege.\n\nWhether you're interested in contributing code, testing the system in educational settings, or simply providing feedback on your experiences, your input is invaluable. Every contribution, no matter how small, brings us one step closer to a world where everyone has access to the education they need to thrive.\n\nTo get involved or learn more about BUD-E, you can test the web versions of the\nBUD-E Personal Assistant and School BUD-E today.\n\nYou can also\n\n- Visit our GitHub repositories\n- Join our community Discord\n- Reach out to us at \u003cbud-e@laion.ai\u003e\n\nLet's work together to unlock the potential of every learner, foster global understanding, and create a brighter future for all.\n\n## Download and Contribute\n\nThe journey of a thousand miles begins with a single step. BUD-E 1.0 is our first step. Will you take the next one with us?\n\n- [LAION-AI/bud-e](https://github.com/LAION-AI/bud-e/tree/main): A general human-ai interaction platform.\n- [LAION-AI/school-bud-e-frontend](https://github.com/LAION-AI/school-bud-e-frontend): A frontend that is compatible to the school-bud-e-backend.\n- [LAION-AI/Desktop-BUD-E_V1.0](https://github.com/LAION-AI/Desktop-BUD-E_V1.0): BUD-E (Buddy) is an open-source voice assistant framework that facilitates seamless interaction with AI models and APIs, enabling the creation and integration of diverse skills for educational and research applications.\n\nTogether, we can transform education, one conversation at a time.\n\nWe encourage you to check out and incorporate Intel’s other [AI/ML Framework optimizations and tools](https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html) into your AI workflow and learn about the unified, open, standards-based [oneAPI](https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html) programming model that forms the foundation of Intel’s [AI Software Portfolio](https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html) to help you prepare, build, deploy, and scale your AI solutions.\n\n## Additional Resources\n\n- [Intel AI Developer Tools and resources](https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html)\n- [oneAPI unified programming model](https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html%23gs.h7kofh)\n- [Generative AI](https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/training/generative-ai.html)\n","date":1737331200000},{"slug":"laion-disco-12m","frontmatter":{"title":"LAION-DISCO-12M","author":"LAION e.V.","date":"Nov 17, 2024","previewImg":"/images/blog/laion-disco-12m.webp"},"content":"\nLAION announces the [LAION-DISCO-12M](https://huggingface.co/datasets/laion/LAION-DISCO-12M) - a collection of 12 million links to publicly available YouTube samples paired with metadata to support basic machine learning research in foundation models for generic audio, music information retrieval, and audio dataset analysis. We collect metadata for 12,648,485 songs, including song name, artist name, and album name. This dataset marks the largest publicly available open dataset for music.\n\nThe dataset follows up on [DISCO-10M](https://arxiv.org/abs/2306.13512) and provides the following improvements compared to the previous work:\n\n- The data collection process is based on a recursive search of artists; it was done on YouTube Music and not on Spotify. This means the metadata and YouTube URL are correctly matched, unlike DISCO-10M where the authors needed to match YouTube URLs to Spotify metadata, resulting in a significant number of wrong matches.\n- The seed artists used for DISCO-10M resulted in a limited number of artists, we expanded the seed artist list by using charts from different countries and genre playlists. This new artist seed list resulted in 250,516 artists.\n\nWe envision that the dataset can contribute to advancing research across several key areas:\n\n- **Audio and music foundation models**: the large-scale nature of this dataset allows researchers to train audio foundation models (eg. CLAP and its various extensions, \u003chttps://github.com/LAION-AI/CLAP\u003e, \u003chttps://arxiv.org/abs/2211.06687\u003e) and study the generalization and transfer to various downstream tasks such as captioning, editing, generation, classification, and others.\n- **Music Information Retrieval (MIR)**: This includes developing methods to extract rich musical attributes such as genre, artist identification, tempo, pitch, and other audio features.\n- **Content-Based Music Search**: The dataset supports building advanced content-based search engines that can identify songs similar to a given audio clip (akin to apps like Shazam).\n- **Music recommendation systems**: Using this dataset researchers can analyze similarities in songs and artist styles and find new ways to recommend music.\n\n## Usage of LAION-DISCO-12M\n\nWe release LAION-DISCO-12M under Apache 2.0 License, which ensures researchers can freely utilize dataset both for conducting basic or applied research. Our usage recommendation follows the guidelines we use for our research releases: the dataset is released for research purposes, especially for conducting basic research on various open multi-modal foundation models, e.g. CLAP, in academic settings. We strongly advise AGAINST using the datasets in industrial settings and even more so, we advise strongly AGAINST using datasets in their original form for creating end products. We explicitly warn that LAION datasets are meant for inspection and usage necessary for purposes of scientific and/or safety analysis, performed by qualified researchers from machine learning and related fields.\n\n## LEGAL DISCLAIMER\n\nThe datasets of LAION only contain links to original samples on public internet and metadata. LAION is not responsible for the content that can be accessed via the links. LAION researchers do not inspect the content of individual samples either, relying on overall statistics collected across all samples, and the processing routines, such as filtering, are automated due to the very large amount of data. LAION does not distribute original media content like audio itself.\n","date":1731801600000},{"slug":"laion-intel-cooperation","frontmatter":{"title":"LAION AI/oneAPI Center of Excellence for Personalized AI Education","author":"LAION e.V.","date":"Sep 10, 2024","previewImg":"/images/blog/intel-laion-cooperation.webp"},"content":"\nLarge-scale Artificial Intelligence Open Network (LAION) is honored to become an **Intel AI/oneAPI Center of Excellence** to advance the development of **BUD-E (Buddy for Understanding and Digital Empathy)**, an open source, empathetic AI education assistant aimed at democratizing personalized learning worldwide.\n\nLeading the Center are Christoph Schuhmann (LAION e.V.) and Dr. Robert Kaczmarczyk (LAION e.V., DFKI), in collaboration with Intel's oneAPI experts.\n\nLeveraging open, standards-based oneAPI multiarchitecture programming, the Center will develop BUD-E, a sophisticated AI-powered voice assistant designed to provide personalized education to learners across the globe, with a special focus on children in developing countries. The project aims to create an AI companion that can interact with warmth, empathy, and high emotional intelligence, encouraging learners to explore the world and discover their potential.\n\nBUD-E is built on a client-server architecture, allowing for minimal hardware requirements on the client side. This enables the system to run on older smartphones and low-powered computers from multiple different vendors, making it accessible to a wide range of users across multiple countries. The server-side architecture is modular, incorporating various APIs and allowing for easy integration of different language models, text-to-speech models, and custom \"skills\" (Python functions that can be dynamically called by the assistant).\n\n\"We are thrilled to establish this new AI/oneAPI Center of Excellence with Intel,\" says Christoph Schuhmann, who co-leads the project. \"As AI continues to reshape the educational landscape, there is a pressing need to ensure that these powerful tools are accessible to all, regardless of their economic or geographical circumstances. With oneAPI, we can optimize BUD-E's performance across various hardware architectures, making high-quality AI-assisted education a reality for millions of children worldwide.\"\n\nDr. Robert Kaczmarczyk adds, \"Our vision for BUD-E goes beyond just providing information. We're creating a companion that can adapt to different languages and cultures, understand and respond to emotions, and foster a love for learning. This collaboration with Intel allows us to leverage cutting-edge open source and AI technology to make this vision a reality.\"\n\n**The Center will focus on several key areas:**\n\n1. Developing and fine-tuning multilingual speech recognition, language understanding, and text-to-speech models optimized for various languages and cultures.\n2. Creating a flexible, modular architecture that allows for easy customization and localization of BUD-E for different regions and educational needs.\n3. Implementing advanced natural language processing techniques to enable BUD-E to engage in meaningful, context-aware conversations that adapt to the learner's level and interests.\n4. Exploring the use of oneAPI to optimize BUD-E's performance across various hardware configurations, ensuring smooth operation even on low-powered devices.\n\nThe efforts by LAION's AI/oneAPI Center of Excellence aims at solving a fundimental need in society to provide our children with access to personalized, high-quality education,“ said Joe Curley, vice president and general manager of Software Products and Ecosystem at Intel. \"To make this possible, both Intel and LAION share a passion for open development and access to AI technology and AI powered education.“\n\nAs development progresses, LAION will open source the curriculum, offer educational activities, and provide opportunities for researchers, educators, and developers worldwide to contribute to and benefit from this groundbreaking project.\n\nBy making the entire system **100% open source, free, privacy-compliant, and European Union [(EU)-AI-Act-compliant](https://artificialintelligenceact.eu/)**, LAION and Intel are raising the bar for accessible AI education.\n\nImagine a world where every child has a warm, humble AI companion - emotionally intelligent and competent, and completely transparent. The companion nurtures curiosity without collecting data, preserving privacy. What wonders might unfold? With BUD-E, we're making this dream real. Ready to join us?\n","date":1725926400000},{"slug":"relaion-5b","frontmatter":{"title":"Releasing Re-LAION 5B: transparent iteration on LAION-5B with additional safety fixes","author":"LAION e.V.","date":"Aug 30, 2024","previewImg":"/images/blog/laion-blue.png"},"content":"\nToday, following [a safety revision procedure](https://laion.ai/notes/laion-maintenance/), we announce Re-LAION-5B, an updated version of LAION-5B, that is the first web-scale, text-link to images pair dataset to be thoroughly cleaned of known links to suspected CSAM.\n\n## Highlights\n\n- Re-LAION-5B fixes the issues as reported by Stanford Internet Observatory in December 2023 for the original LAION-5B and is available for download in two versions, [Re-LAION-5B research](https://huggingface.co/collections/laion/re-laion-5b-research-67e312387d2a4f879c4920b1) and [Re-LAION-5B research-safe](https://huggingface.co/collections/laion/re-laion-5b-research-safe-67e311013ba899a938569e32). The work was completed in partnership with the Internet Watch Foundation ([IWF](https://www.iwf.org.uk/)), the Canadian Center for Child Protection ([C3P](https://www.protectchildren.ca)), and Stanford Internet Observatory. For the work, we utilized lists of link and image hashes provided by our partners, as of July 2024.\n- In all, 2236 links were removed after matching with the lists of link and image hashes provided by our partners. These links also subsume 1008 links found by the Stanford Internet Observatory report in Dec 2023.  Note: A substantial fraction of these links known to IWF and C3P are most likely dead (as organizations make continual efforts to take the known material down from public web), therefore this number is an upper bound for links leading to potential CSAM.\n- Total number of text-link to images pairs in Re-LAION-5B: **5.5 B (5,526,641,167)**\n- [Re-LAION-5B metadata](https://huggingface.co/collections/laion/re-laion-5b-research-67e312387d2a4f879c4920b1) can be utilized by third parties to clean existing derivatives of LAION-5B by generating diffs and removing all matched content from their versions. These diffs are safe to use, as they do not disclose the identity of few links leading to potentially illegal material and consist of a larger pool of neutral links, comprising a few dozen million samples. Removing this small subset does not significantly impact the large scale of the dataset, while restoring its usability as a reference dataset for research purposes.\n- Re-LAION-5B is an open dataset for fully reproducible research on language-vision learning - freely available and relying on 100-percent [open-source composition pipelines](https://github.com/rom1504/img2dataset/), released under Apache-2.0 license.\n\n## Organizational note / Background\n\nIn today's state-of-the-art machine learning research, web-scale datasets are needed to develop and study one of the most important model class - foundation models. Such models exhibit scaling laws, improving their function with growing model, compute and data size in the pre-training, resulting in strongly transferable models at larger scales. Such models serve as core artifacts and a very important subject of study for the machine learning research community, and it is critical to secure full transparency and reproducibility of such studies.\n\nPart of our work at LAION is thus dedicated to building open and transparent datasets resulting in fully reproducible routines for dataset composition. Open datasets like LAION-400M or LAION-5B made the studies around foundation models fully reproducible (eg as done in our studies with openCLIP) - in contrast to studies that train and study closed models (openAI GPT, CLIP, DALL-E, Anthropic Claude, Google Gemini) or open weights foundation models (Llama, Mistral, Gemma), which do not disclose dataset composition routines and/or dataself itself, making reproduction of the findings impossible.\n\nOpen datasets necessary for open science and for reproducible studies of foundation models contain information found on the public web. Such open datasets are also often used to train various AI models by industry.  Due to the research nature of such open datasets, they contain diverse data to reflect the full variety and complexity of the real world. A still open question is how to conduct reproducible procedures for open dataset composition that can ensure that clearly illegal content is successfully filtered out when gathering data from the public web, as it unfortunately still cannot be guaranteed that all available samples on publicly accessible web are properly satisfying legal requirements.\n\nAt LAION, we are dedicated to building safe and legally compliant datasets and tools to advance research and promote widespread accessibility of AI for academia and technology. However, while contributing to important solutions necessary for basic and applied research in machine learning at larger scales, we are aware that we as a non-profit research organization with limited resources cannot single-handedly rectify all publicly available online information. We play a significant role, but not the entirety of it, building alliances with people and organizations that possess strong expertise and skills in handling large-scale dataset composition and pipelines necessary to perform it together.\n\nWe take full accountability for the accuracy of our publications, whether datasets, models, or tools. Prior to releasing LAION-400M and LAION-5B to the public, we implemented and refined filters to eliminate various problematic content. [See our original announcement from 20.08.2021](https://laion.ai/blog/laion-400-open-dataset/#filtering-out-unsuitable-image-text-pairs), where points 6-9 describe the specific measures we took for filtering CSAM related material. However, the findings from David Thiel (Stanford Internet Observatory, 19.12.2023) revealed that some links pointing to illegal content still slipped through our filters into LAION-5B text-links to images dataset, which led us to [promptly withdraw LAION-5B from circulation for the necessary safety revision](https://laion.ai/notes/laion-maintenance/).\n\nRegarding datasets, we believe an open approach is the most effective and safe one, because in addition to securing reproducibility, it also empowers anyone to inspect and see what’s inside, allowing for validation and for scientific progress executed together by the broad community, continually checking and improving the dataset as important artifact in a transparent manner.  We think as with any open-source project, also open datasets should be subject to continuous scrutiny by the broad community, in a common effort to make open datasets better and better. We thus appreciate very much  the effort David Thiel from the Stanford Internet Observatory undertook to look closely at LAION 5B and are grateful to all partner organizations for working with us on making it a better, safer dataset for the research community to use.\n\nAfter [removing LAION-5B dataset from the public web in late December last year, 19.12.2023](https://laion.ai/notes/laion-maintenance/), we used our already existing partnership with the Internet Watch Foundation ([IWF](https://www.iwf.org.uk/)) and established new partnership with the Canadian Children Protection organization ([C3P](https://www.protectchildren.ca)) to **develop a system for identifying and removing illegal content**. In addition, **we removed further specific privacy data provided to us by Humans Rights Watch** ([HRW](https://www.hrw.org/)). We are grateful for their cooperation, as these organizations possess vast expertise in handling sensitive data and were able to help us to gain access to information necessary for the filtering\n\nThe outcome is a revised version of LAION-5B - **Re-LAION-5B**, which we release **in two various versions**, [Re-LAION-5B research](https://huggingface.co/collections/laion/re-laion-5b-research-67e312387d2a4f879c4920b1) and [Re-LAION-5B research-safe](https://huggingface.co/collections/laion/re-laion-5b-research-safe-67e311013ba899a938569e32). These releases will serve in the same way as reference datasets to pre-train open foundation models (e.g., [openCLIP](https://github.com/mlfoundations/open_clip)), while at the same time being free of the links to suspected CSAM material on public web as gathered by the partner organizations IWF and C3P up to July 2024. By partnering with IWF \u0026 C3P and removing the links to potential CSAM known to them, **we set a new safety standard for cleaning web-scale image-link data sets**.\n\nIn the following, we describe in more detail the motivation behind the releases and procedures that we executed to make the releases happen.\n\n## Motivation\n\nOn December 19, 2023, research conducted by David Thiel at the Stanford Internet Observatory uncovered evidence of URL links to potential illegal image samples (CSAM) within the publicly accessible LAION-5B dataset. Among the vast 5.8 billion image links sourced from the public internet within LAION-5B, Stanford reportedly identified 1008 links (0.000017 % of the full dataset) pointing to either “CSAM” or “likely CSAM\"\n\nThe Stanford report prompted an immediate takedown of LAION-5B and we initiated a comprehensive safety revision. We express our gratitude for the support from the Internet Watch Foundation ([IWF](https://www.iwf.org.uk/)), the Canadian Children Protection organization ([C3P](https://www.protectchildren.ca)), and Stanford in helping us to identify and safely remove potential illegal content from LAION-5B.\n\n## Safety Revision\n\n### Phase 1: Taking down the dataset and its derivatives\n\nAs soon as we were informed of the Stanford report on 19.12.2023, we took down all known accessible LAION-5B datasets and its derivatives, also deleting the data and metadata in any accessible facilities where suspicion for links to potential CSAM was given.  \n\nWe posted an [update to the LAION website](https://laion.ai/notes/laion-maintenance/) and urged everyone using the LAION 5B dataset to delete it or to take their own steps to remove any suspected links to CSAM or CSAM samples.\n\nUnfortunately, we learned about the report not from the representatives of  Stanford Internet Observatory timely in advance, but from the press, specifically from Bloomberg, only 1 day before the report release. This approach of informing the press before notifying the involved organizations with an extensive security report [does not align with standard safety best practices for handling security issues](https://cheatsheetseries.owasp.org/cheatsheets/Vulnerability_Disclosure_Cheat_Sheet.html). Typically, organizations should be informed first so they have reasonable time to take immediate action to remove harmful content and prevent exploitation. This created an unfortunate situation for safety - as we could react only with substantial delay, although the issues were known to Stanford Internet Observatory many weeks or even months before. **We thus strongly urge all organizations to adhere to safety best practices in these matters** in the future. **Timely and direct communication of validated safety related findings with affected organizations and communities is crucial to ensure that harmful content can be promptly addressed and mitigated**, reducing duration for it where it can cause harm.\n\n### Phase 2: Analysis using hash lists and filtering\n\nUsing the established official cooperation with Internet Watch Foundation ([IWF](https://www.iwf.org.uk/)) and Canadian Children Protection organization ([C3P](https://www.protectchildren.ca)), LAION has obtained lists of MD5 image and URL hashes for the CSAM on public internet discovered by the organizations so far. LAION also used the established contact to David Thiel, the author of the Stanford Internet Observatory Report, to obtain details on the nature of discovered CSAM samples and also lists of further hashes.  \n\nLAION used the lists of hashes and other information obtained from our cooperation to remove links to all known suspected CSAM samples from the metadata of the dataset and to create updated dataset versions that are free of any links to suspected CSAM samples currently known to major organizations like IWF and C3P who were collecting evidence of those from public internet across a long period of time.\n\nImportantly, the **list of hashes allowed LAION to remove all suspected links WITHOUT ever having to touch suspected links or even having to inspect content those are leading to**, thanks to the **work done by the partner organizations IWF and C3P**.\n\nIn addition, further privacy related data that did not contain any illegal content was removed in cooperation with the Human Rights Watch.\n\n## Results of the safety revision investigation\n\n### Results, overview\n\nFirst, we report the total amount of unique hashes, either MD5 image or URL hashes, we received from our partners (C3P and IWF):\n\nTotal amount of unique hashes provided by the partners: 2.2M ([IWF](https://www.iwf.org.uk/)) + 14M ([C3P](https://www.protectchildren.ca)) = 16.2M\n\nThose are hashes we have used to conduct filtering procedures to filter out links to potential CSAM. Importantly, the hashes provided by our partner organizations make it possible to remove all suspected links WITHOUT ever having to touch suspected links or to inspect content those are leading to.\n\nWe can report following:\n\n- Total amount of matches with links in LAION-5B using C3P, IWF and hashes provided by David Thiel (Stanford Internet Observatory): 1129 ([C3P](https://www.protectchildren.ca)) + 18 ([IWF](https://www.iwf.org.uk/)) + 1714 (Stanford Internet Observatory) = 2236 (0.000038 % of the total dataset)\n- For comparison: total amount of suspected “live” links to “CSAM” or “likely CSAM” samples as validated by David Thiel in Stanford Internet Observatory report: 1008 (0.000017 % of the total dataset; relying on the numbers in the investigation - those number have not been checked by any third party since the publication)\n- Thus, we have a total of 2236 matches with suspected links to CSAM or potential CSAM encountered in LAION-5B that we can report from our investigation using hashes provided by our partner organizations. These matches also contain 1008 links from Stanford Internet Observatory report .\n\nThis number is a strict upper bound for the estimate of links leading to potential CSAM samples, as known by our partner organizations IWF and C3P in July 2024. It cannot be determined exactly which fraction of 2236 links indeed points to illegal samples, as we do not inspect links or the content they are leading to due to potential illegal nature and match only the precomputed hashes. Many of the matched links can be dead leading to nowhere. 2236 is thus the upper bound for the number of links leading to potential CSAM; the number of links indeed pointing to illegal material is most likely much lower, as those links are already known to organizations that clean the public web from CSAM material, which usually results in taking down the actual content links are pointing to or even removing the whole entity domain responsible for the hosting of that content, making access to it unavailable. To have some estimate, consider the following: Stanford Internet Observatory report claimed 1008 suspected links in LAION-5B that were validated to be “live” on public web in December 2023. The report has used the same C3P information as LAION, which makes it very likely that around 2236 - 1008 = 1228 of matched links are dead.\n\nAs we have used a vast collection of over 16M hashes to images and links with suspected CSAM provided by C3P and IWF, which represents all CSAM or likely CSAM on public web currently known to these major organizations in 2024, we can state that 2236 is an upper boundary for the links to suspected CSAM in original LAION-5B. Our research thus supports the number of links to suspected CSAM to be in the order of magnitude of 1-2k, as reported by Stanford Internet Observatory, and not larger.\n\n## Chronological protocol\n\nFollowing are details and chronological overview of the matching procedures we were conducting to determine matches in LAION-5B based on the provided hash lists:\n\n### 20.02.2024: David Thiel - General Infos, SHA hashes\n\nPunsafe for most of the detected samples : \u003e 0.95\nFor the samples below \u003c 0.95, SHA hashes provided (92 samples)\n\n### 22.02.2024: IWF - MD5 image hashes\n\nTotal: 1.5M unique image MD5 hashes provided by IWF\n\nMatches in LAION-5B:\n6 matches over laion2B-en\n1 match over laion2B-multi\n4 matches in laion1B-nolang\n\nTotal: 11 matches\n\n### 20.03.2024: C3P - MD5 image hashes\n\nTotal: 14M unique image MD5 hashes provided by C3P\n\nMatches in LAION-5B:\n\n459 matches over laion2B-en\n390 matches over laion2B-multi\n280 matches over laion1B-nolang\n\nTotal: 1129 matches.\n\n### 26.03.2024: IWF - MD5 image hashes and link hashes\n\nTotal: 2.2M unique image MD5 hashes provided by IWF; using additional flag “G” to retrieve all UK category A-C MD5 hashes including Grid Images and all non-Self-evident hashes\n\nMatches MD5 hashes in LAION-5B:\n9 matches over laion2B-en\n3 matches over laion2B-multi\n5 matches in laion1B-nolang\n\nMatches URL hashes in LAION-5B:\n1 match over laion2B-en\n0 matches over laion2B-multi\n0 matches in laion1B-nolang\n\nTotal: 18 matches (11 from before + 6 when using “G” flag + 1 URL hash match)\n\n### 24.04.2024: David Thiel (Stanford Internet Observatory) - SHA hashes\n\nTotal: 3114 unique SHA hashes provided by David Thiel  \n\nMatches in LAION-5B:\n788 matches over laion2B-en\n487 matches over laion2B-multi\n439 matches in laion1B-nolang\n\nTotal: 1714 matches\n\n### 30.05.2024: Hye Jung Han (HRW) - links to samples with privacy data\n\nIn addition, during our ongoing safety revision we were contacted by Hye Jung Han (Human Rights Watch, HRW), who reported 399 links pointing to samples on public web that potentially contain private infos of children WITHOUT containing any CSAM or other illegal material. Upon inspection, only a part of the links were found to indeed contain private infos, while many others among reported 399 were not confirmed to contain any sensitive data. In abundance of caution, we have still removed any of the links that were matching HRW collection\n\nTotal: 41 (1. Report) + 358 (2. Report) = 399 links to public web provided by HRW\n\n## Update of LAION-5B dataset and Re-LAION release versions\n\nWe use following procedures to update the old original version of LAION-5B dataset and to arrive to release versions that are free of any known links to suspected CSAM samples collected by our partner organizations IWF and C3P:\n\nWe match URL and image MD5 and SHA1 hashes  to the hashes we had precomputed for the original LAION-5B. Those hashes were computed [following the study by Nicholas Carlini et al](\u003chttps://arxiv.org/abs/2302.10149\u003e) on possible dataset poisoning attacks back in March 2023. The hashes were computed back then to prevent the injection of false samples into the original dataset. Using the same hashes, we could identify the matches between the lists of hashes for known CSAM samples and hashes of entries in the LAION-5B dataset, and remove all the matches. We also remove the matches to the samples containing children's private infos as conveyed by HRW.\nStarting from the version of the dataset metadata with all the known links to suspected CSAM samples removed, we have created two release versions:\n\n1. [Re-LAION-5B-research](https://huggingface.co/collections/laion/re-laion-5b-research-67e312387d2a4f879c4920b1). In this version, we take the observations made during matching CSAM hash lists and reports by David Thiel and choose a certain value of p_unsafe to serve as threshold, above which all samples are removed if the keyword based text filters indicate suspicious content. For Re-LAION-5B-research, this threshold is determined to be p_unsafe\u003e0.95. The choice is motivated by the fact that the overwhelming majority of identified CSAM matches was concentrated at values p_unsafe \u003e 0.99, with minimal value corresponding to 0.95, which we choose to be the threshold. The keyword based text filters we employ in conjuction with threshold criterion are constructed from keyword recommendations provided by major children protection organizations. This joint procedure leads to removal of 1.121 % (22.42 M from 2B; 65 M from 5.8B) samples from the original LAION-5B dataset\n2. [Re-LAION-5B-research-safe](https://huggingface.co/collections/laion/re-laion-5b-research-safe-67e311013ba899a938569e32). In this release, we in addition remove the majority of NSFW samples. We chose p_unsafe threshold that by visual inspection from previous snapshots of original LAION-5B dataset indicates elimination of the majority of NSFW presence: p_unsafe \u003e 0.45. This leads to removal of 3.044% (60.88M from 2B, 176M from 5.8B) samples from original LAION-5B.\n\nRe-LAION-5B-research-safe is a true subset of Re-LAION-5B-research which in turn is a true subset of original old LAION-5B. Both Re-LAION-5B-research and Re-LAION-5B-research-safe are released via gated access on HF, requiring submission of affiliation information and consent on using data that might contain various explicit and discomforting content for research purposes.\n\nAll further derived datasets are filtered according to Re-LAION-5B-research-safe scheme, using p_unsafe \u003e 0.45 to filter out majority of NSFW samples.\n\nAs an outcome, we release the updated versions of the LAION-5B dataset -  Re-LAION-5B-research-safe and Re-LAION-5B-research -  that are free of any CSAM samples known to our partner organizations (IWF and C3P; as of July 2024) dealing with such content. Re-LAION datasets are results of an update iteration of LAION-5B and are subsets of original LAION-5B obtained by applying safety fixes as described above.\n\nLAION-5B is assembled based on Common Crawl data up to Sep 2022, and there is no new content in Re-LAION apart from the links already contained in LAION-5B, which makes sure that no new suspicious, unchecked links slip into the datasets. MD5 image hashes precomputed in early 2023 for all the original LAION-5B images further ensure that no previously unknown image samples can enter the dataset when assembling training datasets from links, as hash comparison allows only matches to previously checked samples to become part of the datasets. Re-LAION therefore produces the first open large-scale text-link to images reference dataset that contains provenly only those links samples that are checked against all currently known links to suspected CSAM as covered by C3P and IWF (up to July 2024) and thus can be used more safely by researchers as reference for various basic and applied machine learning research experiments.\n\n## Summary and recommendations\n\nFollowing successful cooperation with partner organizations IWF and C3P, we are able to execute an update iteration improving safety of original LAION-5B dataset and release update versions, Re-LAION-research-5B and Re-LAION-research-safe-5B that are free of links to suspected CSAM or likely CSAM samples known to our partner organizations, which also subsumes all the 1008 suspected links identified by Stanford Internet Observatory report in December 2023.\n\nFollowing important insights were made in this study and during safety iteration:\n\n- We confirm number of links leading to suspected CSAM on public web that slipped through [filters that were used for LAION-5B composition](https://laion.ai/blog/laion-400-open-dataset/#filtering-out-unsuitable-image-text-pairs) to be 2236, which subsumes 1008 links to suspected CSAM reported by Stanford Internet Observatory in December 2023. This number serves as an upper bound for links leading to possibly accessible CSAM known to our partner organizations IWF and C3P, as a substantial fraction of these links is most likely dead (Stanford Internet Observatory reported 1008 links that might be still active in December 2023). This number clearly refutes unsupported claims extrapolating Stanford Internet Observatory report to many thousands links to suspected CSAM samples, showing that actual number stays low if matching against of over 16M links or image hashes of potential CSAM samples known to IWF and C3P.\n- Despite the low number of links to suspected CSAM, investigation shows that current state-of-the-art filters alone are not reliable enough to guarantee protection from CSAM in web scale data composition scenarios. To ensure better filtering, lists of hashes of suspected links or images created by expert organizations (in our case, IWF and C3P) are suitable choices. We recommend research labs and any other organizations composing datasets from the public web to partner with organizations like IWF and C3P to obtain such hash lists and use those for filtering. In the longer term, a larger common initiative can be created that makes such hash lists available for the research community working on dataset composition from the web.\n- We strongly urge all research labs and organizations who still make use of old LAION-5B to migrate to Re-LAION-5B datasets as soon as possible. Those facilities where datasets obtained on the basis of old LAION-5B still exist (e.g. in eligible safe vaults), can make use of Re-LAION metadata to ensure that all links to suspected CSAM or suspected CSAM content are removed by executing diff operation. Diff operation is safe, as it will not reveal any of potential illegal content, because it is safely buried in the large pool of other irrelevant samples in the diff, while making sure any potential illegal content will be removed if removing the samples matched with the diff. In this way, full download of Re-LAION can be spared.\n- Lists of precomputed hashes provide a safe way to perform filtering of potentially illegal material without necessity to reveal or knowingly interact with such material in any way. Especially when combining pre-computed hashes with vast amounts of other irrelevant hashes, such lists can be made available to the community to enable easy, broadly accessible filtering of unwanted material without empowering malicious actors to make use of such lists for their own malicious purposes.\n- As LAION-5B was assembled in summer 2022, it is for us a worrying signal that ca. 1k already well known links to suspected CSAM samples can be still live accessible on public web more than 1 year after. It is a clear hint that law enforcement bodies have to intensify the efforts to take down domains that host such image content on public web following information and recommendations by organizations like IWF and C3P, making it a safer place, also for various kinds of research related activities.\n\n## Usage of Re-LAION datasets\n\nWe release both Re-LAION-5B-research and Re-LAION-5B-research-safe under Apache 2.0 License, which ensures researchers can freely utilize datasets both for conducting basic or applied research. Our usage recommendation stays the [same as in our previous release](https://laion.ai/blog/laion-5b/#disclaimer-on-dataset-purpose-and-content-warning). The datasets are released for research purposes, especially for conducting basic research on various open multi-modal foundation models, e.g. openCLIP, in academic settings. **We strongly advise AGAINST using the datasets in industrial settings and even more so, we advise strongly AGAINST using datasets in their original form for creating end products.** We explicitly warn that Re-LAION datasets can contain links to various image samples that can be strongly discomforting dependent on the viewer and are NOT meant for casual viewing apart from inspection necessary for purposes of scientific and/or safety analysis performed by trained researchers.\n\n## LEGAL DISCLAIMER\n\nThe **datasets of LAION only contain links and metadata**. LAION is not responsible for the content that can be accessed via the links. LAION researchers do not inspect the content of individual samples either, relying on overall statistics collected across all samples, and **the filtering is automated due to the huge amount of data**. **LAION has never distributed image content itself**.\n\nLAION has been committed to removing illegal content from its datasets from the very beginning ([see original announcement from 20.08.2021](https://laion.ai/blog/laion-400-open-dataset/#filtering-out-unsuitable-image-text-pairs)) and has implemented appropriate measures to achieve this from the outset. LAION strictly adheres to the principle that illegal content is removed ASAP after it becomes known.\n","date":1724976000000},{"slug":"bud-e","frontmatter":{"title":"BUD-E: Enhancing AI Voice Assistants’ Conversational Quality, Naturalness and Empathy","author":"LAION","date":"Feb 8 2024","previewImg":"/images/blog/bud-e.webp"},"content":"\nAI voice assistants have revolutionized our interaction with technology, answering queries, performing tasks, and making life easier. However, the stilted, mechanical nature of their responses is a barrier to truly immersive conversational experiences. Unlike human conversation partners, they often struggle with fully understanding and adapting to the nuanced, emotional, and contextually rich nature of human dialogue, leading to noticeable latencies and a disjointed conversational flow. Consequently, users often experience unsatisfactory exchanges, lacking emotional resonance and context familiarity.\n\n![BUD-E](/images/blog/bud-e.webp)\n\nWouldn’t it be awesome to have a fully open voice assistant that can\n\n1. reply to user requests in real-time\n2. with natural voices, empathy \u0026 emotional intelligence\n3. with long-term context of previous conversations\n4. handling multi-speaker conversations with interruptions, affirmations and thinking pauses\n5. fully local, on consumer hardware.\n\nTo realize this vision, LAION teamed up with the ELLIS Institute Tübingen, Collabora and the Tübingen AI Center to build **BUD-E (Buddy for Understanding and Digital Empathy)**.\n\n[![BUD-E Demo](/images/blog/bud-e-video.png)](https://youtu.be/SYWDucn8RL8)\n\nWe started the development by creating a baseline voice assistant with very low latency. To reach that point, we carefully evaluated latency and quality of a large range of Speech-to-Text, Large Language and Text-to-Speech models on consumer hardware and carefully control how the models interact. Right now (January 2024) we reach latencies of 300 to 500 ms (with a Phi 2 model). We are confident that response times below 300 ms are possible even with larger models like LLama 2 30B  in the near future.\n\nHere is a demo (running on a 4090 GPU): [BUD-E Demo](https://youtu.be/SYWDucn8RL8)\n\nAll code is open-source and available at [GitHub](https://github.com/LAION-AI/natural_voice_assistant).\n\n#### A roadmap towards Empathic \u0026 Natural AI Voice Assistants\n\nConversations even with the baseline model feel much more natural than anything we’ve seen so far. Nonetheless, there are still a lot of components and features missing that we need to tackle on the way to a truly empathic and naturally feeling voice assistant that is fun and helpful to interact with over prolonged periods of time. We are inviting everyone to contribute to the development of BUD-E.\n\nThe immediate problems and open work packages we’d like to tackle are as follows:\n\n#### Reducing Latency \u0026 minimizing systems requirements\n\n- **Quantization**: Implement more sophisticated quantization techniques to reduce VRAM requirements and reduce latency.\nFine-tuning streaming TTS. TTS systems normally consume full sentences to have enough context for responses. To enable high-quality low-latency streaming we give the TTS context from hidden layers of the LLM and then fine-tune the [streaming model on a high-quality teacher](https://arxiv.org/abs/2309.11210).\n- **Fine-tuning streaming STT**: Connect hidden layers from STT and LLM system and then fine-tune on voice tasks to maximize accuracy in low-latency configurations of STT model.\n- **End-of-Speech detection**: Train and implement a light-weight end-of-speech detection model.\n- **Implement Speculative Decoding**: Implement speculative decoding to increase inference speed in particular for the STT and LLM models.\n\n#### Increasing Naturalness of Speech and Responses\n\n- **Dataset of natural human dialogues**: Build a dataset (e.g., Youtube, Mediathek, etc.) with recorded dialogues between two or more humans for fine-tuning BUD-E.\n- **Reliable speaker-diarization**: Develop a reliable speaker-diarization system that can separate speakers, including utterances and affirmationsthat might overlap between speakers.\n- **Fine-tune on dialogues**: Finetune STT → LLM → TTS pipeline on natural human dialogues to allow the model to respond similarly to humans, including interruptions and utterances.\n\n#### Keeping track of conversations over days, months and years\n\n- **Retrieval Augmented Generation (RAG)**: [Implement RAG](https://www.pinecone.io/blog/rag-study/) to extend knowledge of BUD-E, unlocking strong performance gains\n- **Conversation Memory**: Enable model to save information from previous conversations in vector database to keep track of previous conversations.\n\n#### Enhancing functionality and ability of voice assistant\n\n- **Tool use**: Implement tool use into LLM and the framework, e.g., to allow the agent to perform internet searches\n\n#### Enhancing multi-modal and emotional context understanding\n\n- **Incorporate visual input**: Use a light-weight but effective vision encoder (e.g., CLIP or a Captioning Model) to incorporate static image and/or video input.\n- **Continuous vision-audio responses**: Similar to the (not genuine) [Gemini demo](https://www.youtube.com/watch?v=UIZAiXYceBI) it would be great if BUD-E would naturally and continuously take into account audio and vision inputs and flexibly respond in a natural manner just like humans.\n- **Evaluate user emotions**: Capture webcam images from the user to determine the user’s emotional state and incorporate this in the response. This could be an extension of training on dialogues from video platforms, using training samples where the speaker’s face is well visible.\n\n#### Building a UI, CI  and easy packaging infrastructure\n\n- **LLamaFile**: Allow easy cross-platform installation and deployment through a single-file distribution mechanism like [Mozilla’s LLamaFile](https://github.com/Mozilla-Ocho/llamafile).\n- **Animated Avatar**: Add a speaking and naturally articulating avatar similar to Meta’s Audio2Photoreal but using simpler avatars using [3DGS-Avatar](https://neuralbodies.github.io/3DGS-Avatar/).\n- **User Interface**: Capture the conversation in writing in a chat-based interface and ideally include ways to capture user feedback.\nMinimize Dependencies. Minimize the amount of third-party dependencies.\n- **Cross-Platform Support**: Enable usage on Linux, MacOS and Windows.\nContinuous Integration. Build continuous integration pipeline with cross-platform speed tests and standardized testing scenarios to track development progress.\n\n#### Extending to multi-language and multi-speaker\n\n- **Extend streaming STT to more languages**: Extending to more languages, including low-resource ones, would be crucial.\n- **Multi-speaker**: The baseline currently expects only a single speaker, which should be extended towards multi-speaker environments and consistent re-identification of speakers.\n\n### Collaborating to Build the Future of Conversational AI\n\nThe development of BUD-E is an ongoing process that requires the collective effort of a diverse community. We invite open-source developers, researchers, and enthusiasts to join us in refining BUD-E's individual modules and contributing to its growth. Together, we can create an AI voice assistants that engage with us in natural, intuitive, and empathetic conversations.\n\nIf you're interested in contributing to this project, join [our Discord community](https://discord.com/invite/jJpvt6R8cp) or reach out to us at \u003cbud-e@laion.ai\u003e.\n","date":1707350400000},{"slug":"laion-pop","frontmatter":{"title":"LAION POP: 600,000 high-resolution images with detailed descriptions","author":"Christoph Schuhmann, Peter Bevan","date":"Nov 17, 2023","previewImg":"/images/blog/laion_pop.jpg"},"content":"\nLAION POP is a subset of LAION 5B: This subset comprises 600,000 high-resolution images, each equipped with detailed descriptions. The selection of images was based on 10,000 different concepts popular on the image generation site \"Midjourney\".\n\n| [SampleOverview](/documents/llava_cogvlm_pop.html) |\n|:------------------------------:|\n|[LAION-POP Dataset on HuggingFace](https://huggingface.co/datasets/laion/laion-pop)|\n\n\u003cimg src=\"/images/blog/laion_pop.jpg\" style=\"height:384px; width: auto;\"/\u003e\n\n## Dataset and Methodology\n\n4.25 million Midjourney images were downloaded from [this huggingface repository](https://huggingface.co/datasets/tarungupta83/MidJourney_v5_Prompt_dataset), and CLIP L14 vectors were generated for each image. Using the k-means clustering method, these vectors were assigned to 10,000 centroids. The CLIP vectors of these centroids were then used to retrieve nearest neighbors from the LAION-5B dataset using the [image search website](https://rom1504.github.io), focusing on those with aesthetic values of at least 0.5 and a minimum resolution of 768 pixels on the shortest side. Additionally, images suspected of containing watermarks were filtered out. NSFW values were calculated for each image using the LAION CLIP-based-NSFW-Detector, and these are released with the data.\n\n## Generation of Captions\n\nTwo sets of detailed image descriptions were created for the images using the models CogVLM and LLaVA-v1.5-13b. These descriptions focus on objects, backgrounds, text, scenery, interactions, and gestures, as well as the appearance and emotions of the depicted people or characters. The CogVLM captions are empirically recommended as higher quality captions, but systematic comparisons of these captions are encouraged and welcomed.\n\n## PROMPT\n\n\"Can you please describe this image in up to two paragraphs? Please specify any objects within the image, backgrounds, scenery, interactions, and gestures or poses. If they are multiple of any object, please specify how many. Is there text in the image, and if so, what does it say? If there is any lighting in the image, can you identify where it is and what it looks like? What style is the image? If there are people or characters in the image, what emotions are they conveying? Please keep your descriptions factual and terse but complete. DO NOT add any unnecessary speculation about the things that are not part of the image such as \"the image is inspiring to viewers\" or \"seeing this makes you feel joy\". DO NOT add things such as \"creates a unique and entertaining visual\", as these descriptions are interpretations and not a part of the image itself. The description should be purely factual, with no subjective speculation. Make sure to include the style of the image, for example cartoon, photograph, 3d render etc. Start with the words ‘This image showcases’:”\n\n‘This image showcases’ was trimmed from the beginning of each caption upon generation.\n\n## Future Application and Improvements\n\nAlthough no text-to-image model has been tuned with these data so far, we expect that the use of these data could significantly improve the aesthetic quality of the outputs.\n","date":1700179200000},{"slug":"open-empathic","frontmatter":{"title":"Open Empathic Launch","author":"Christoph, Knoriy, Robert","date":"Oct 22, 2023","previewImg":"/images/blog/open-empathic.png"},"content":"\nWe are thrilled to present **Open Empathic**, a pioneering open-source project initiated by our non-profit organization, LAION.\n\nOpen Empathic aims to equip open-source AI systems with empathy and emotional intelligence. We hope that methods and tools developed within the framework of this project, together with a community of researchers and technology enthusiasts, will revolutionize the way AI interacts with and supports humans in various domains.\n\nIn an increasingly AI-driven world, it is of paramount importance that AI systems possess emotional intelligence to understand and respond to human emotions. As AI plays an ever-expanding role in our daily lives, ranging from education to healthcare, elderly care, and commercial contexts, it becomes vital to prioritize the well-being and emotional intelligence of AI-human interactions.\n\n| **Ready to Make a Difference?** |\n|:------------------------------:|\n|Join our [Open Empathic Discord server](https://discord.gg/nFdG99UAav)|\n| Dive into our [tutorial video](https://youtu.be/GZqYr8_Q7DE) and head over to the [Open Empathic website](https://dct.openempathic.ai/) to help the **Open Empathic** movement through emotional annotation. Together, let's craft an era where AI resonates with human emotions.|\n\n## Why is Emotional Intelligence Essential for AI?\n\nThe benefits of emotionally intelligent AI system are manifold:\n\n### Empowering Students and Learners\n\nIn educational settings, AI equipped with emotional intelligence can adapt its approach to support students' emotional needs and personalities. This tailored support fosters a positive and nurturing learning environment, empowering students to achieve their full potential.\n\n### Advancing Psychological Research\n\nAI models for emotional intelligence can serve as objective and reliable instruments in psychological research. They can aid in improving psychological intervention methods and provide deeper insights into human experiences and behavior.\n\n### Improved Mental Health Support\n\nIn contexts such as healthcare and elderly care, emotionally intelligent AI can offer empathetic companionship, alleviating feelings of loneliness and providing valuable mental health support. This is especially crucial for isolated individuals, patients, and seniors who may lack regular human interaction.\n\n### Ethical Decision-Making\n\nEmotional intelligence enables AI systems to make ethical decisions by considering human emotions and values. This helps avoid potential biases and ensures AI acts in a way that aligns with human values and well-being.\n\n### Revolutionizing the Entertainment Industry\n\nEmotionally intelligent AI systems can transform the entertainment industry in various ways, providing new interactions between humans and various media not possible before. The gaming industry can enable games to recognize players' emotions and intelligently adapt game content and storytelling accordingly. This can lead to more fulfilling, more humane gaming experiences. Human audio and video content creators will have access to generative tools where creative storytelling will reflect rich nuances of human emotional experience, with the ability to personalize generated stories according to individual preferences. Beyond entertainment, such methods can also lead to personalized tools that offer help for individual mental well-being, for instance, providing customized meditation and relaxation sessions.\n\n## How Can We Equip AI with Emotional Intelligence?\n\nImagine chatbots that not only answer questions but also understand the underlying emotions of their users, responding with empathy and compassion:\n\nBuilding upon the successes of our previous \"Open Assistant\" project, where we collected fine-tuning data for Large Language Models (LLMs) models to transform them into chat models, this time, **we need your help** to create an open-source audio dataset that will enable AI systems, such as chatbots, to understand emotions and train emotionally intelligent, empathic text-to-speech models. By creating a community dataset that captures emotional characteristics of diverse speech segments, we can empower AI models of tomorrow to engage in empathetic and emotionally intelligent conversations—all while maintaining the spirit of openness and collaboration within the Open Source community.\n\n## How Can You Contribute?\n\nBy participating in our **Open Empathic** project, you can contribute to a near future where Open Source AI systems will be empathic, compassionate, and emotionally intelligent.\n\nYour annotations of audio data will play a pivotal role in training AI models that not only comprehend emotions but also respond empathetically, fostering a deeper connection between AI and humans.\n\nTo start contributing audio annotations (it takes around 3 minutes to annotate a few-second utterance), watch [this tutorial](https://youtu.be/GZqYr8_Q7DE) and then [register on the website](https://dct.openempathic.ai/) at the end of the video (please don’t skip watching the tutorial).\n\nJoin the **Open Empathic movement** today, and let's shape a compassionate, empathic AI future together!\n\nSincerely,\n\nThe LAION Team\n","date":1697932800000},{"slug":"strategic-game-dataset","frontmatter":{"title":"Strategic Game Datasets for Enhancing AI Planning: An Invitation for Collaborative Research","author":"Christoph Schuhmann \u0026 Qi Sun","date":"Oct 18 2023","previewImg":"/images/blog/chessboard.png"},"content":"\nRecent advancements in artificial intelligence (AI) underscore the progress of reasoning and planning shown by recent generalist machine learning (ML) models. The progress can be boosted by datasets that can further boost these generic capabilities when used for training foundation models of various kind. This research initiative has generated extensive synthetic datasets from complex games — chess, Rubik's Cube, and mazes — to study facilitation and the advancement of these critical generic skills in AI models. This paper delineates the methodology, dataset structure, and preliminary analysis, concluding with an open invitation for collaborative research.\n\n## Introduction\n\nThe field of AI has observed a pivotal shift toward foundation generalist models capable of advanced strategic planning, essential for complex problem-solving tasks. Recognizing the potential of various complex games as ideal proxies for real-world problems, this research focuses on the generation of large-scale synthetic datasets. These datasets are designed to challenge and enhance the strategic planning capabilities of generative pre-trained transformers (GPT) and similar models.\n\n## Dataset Overview\n\n### 1. Chess\n\nThe [chess dataset](https://huggingface.co/datasets/laion/strategic_game_chess) comprises 3.2 billion games, equating to approximately 608 billion individual moves. These games, generated via self-play by the Stockfish engine, emulate a high strategic complexity, reflective of a 2500 Elo rating. Each entry contains detailed move sequences, termination status, and game results.\n\n### 2. Rubik's Cube (3x3x3)\n\nThe [rubik's cube dataset](https://huggingface.co/datasets/laion/strategic_game_cube) features 1.64 billion Rubik's Cube solves, totaling roughly 236.39 billion moves. It provides initial scrambled states and the ensuing solve sequences, offering a [complex problem-solving scenario for models to navigate](https://github.com/trincaog/magiccube/).\n\n### 3. Mazes\n\nThe [maze dataset](https://huggingface.co/datasets/laion/strategic_game_maze), while smaller at 350,000 mazes, represents over 39.29 billion moves. Each maze is a 30x30 ASCII representation, with solutions derived using the A* algorithm, challenging pathfinding and planning algorithms.\n\n## Methodology\n\nThe datasets were constructed using rigorous methodologies to ensure complexity and relevance. Chess games were generated through self-play by a sophisticated chess engine, Rubik's Cube states were randomized and then solved using an advanced solving tool, and mazes were procedurally generated and solved using a robust pathfinding algorithm.\n\n## Purpose\n\nThe aim extends beyond game proficiency. The synthetic, procedurally generated datasets serve to enhance AI models' internal representation and problem-solving strategies, devoid of legal and ethical complications associated with other data forms. The training leverages the strategic depth of these games to develop advanced planning capabilities in AI models.\n\n## Call for Collaboration\n\nWe invite interested researchers and ML practitioners to explore these datasets' potential. Whether training GPT models from scratch or fine-tuning pre-existing models, we encourage the exploration of various pre-training and fine-tuning strategies using these game-based datasets standalone or as enhancement of other already composed large-scale data.\n\nOur team is prepared to assist in securing necessary GPU resources for these explorations. We are particularly interested in collaborators eager to pre-train models of small to medium scale on our game data, subsequently transition to standard text-based training, and then perform comparative analyses against models of similar architecture trained exclusively on text data.\n\nConclusively, this initiative marks a significant stride toward intricate problem-solving and strategic planning in AI, extending an open invitation to the research community for collaborative advancement in this domain.\n\n## Acknowledgements\n\nSpecial thanks to Prof. Rio Yokota for providing the necessary compute time on the Fugaku supercomputer and Yago Kastro for coding the first draft of the chess selfplay script.\n","date":1697587200000},{"slug":"clara-release","frontmatter":{"title":"CLARA: Advancing Machines in Understanding Speech Nuances","author":"Knoriy, Christoph, Robert","date":"Oct 16 2023","previewImg":"/images/blog/clara.png"},"content":"\nVoices carry not only words but also convey emotions, emphasis, and nuance through aspects like tone and accent. However, existing speech technology only partially comprehends these intricate components of human speech.\n\nIntroducing **CLARA (Multilingual Contrastive Learning for Audio Representation Acquisition)** – a project designed to enhance machines' understanding of the implicit aspects of speech. It aspires to develop a comprehensive pre-trained model dedicated to auditory communication.\n\n| **Ready to Make a Difference?** |\n|:------------------------------:|\n|Join our [Open Empathic Discord server](https://discord.gg/nFdG99UAav)!|\n|Explore our [CLARA project on GitHub](https://github.com/knoriy/CLARA) and its [interactive Demo Notebook](https://github.com/knoriy/CLARA/blob/master/clara/demo.ipynb). The corresponding research article can be found on [arxiv](https://arxiv.org/abs/2310.11830).|\n\n## The Driving Force Behind CLARA\n\nBuilding multilingual models from speech data poses significant challenges. We harness large, unlabeled datasets to derive transferable representations across various languages by utilising contrastive self-supervised learning.\n\nWe've all encountered stilted and unempathetic interactions with automated systems oblivious to our emotions and expressions. CLARA seeks to transform this by empowering machines to discern the full context of a speaker's voice. We aim to facilitate more natural and emotionally intuitive interactions with conversational AI, like virtual assistants and video game characters. The potential applications range from creating empathetic healthcare assistants to immersive gaming experiences.\n\n## Our Approach\n\nEmploying the latest advancements in contrastive learning, CLARA is trained on an extensive dataset of unlabeled multilingual speech recordings. It learns to identify linguistic and paralinguistic attributes by analysing and contrasting various examples.\n\nWe've amassed over 100,000 hours of speech data in numerous languages and applied augmentation strategies, such as introducing background noise and altering pitch, to enhance the training dataset's diversity. CLARA links audio and text encoders in its model architecture to map different modalities into a unified representation space, and we are exploring various configurations to optimise its performance.\n\n## Early Achievements\n\nPreliminary findings indicate CLARA's capabilities in multilingual speech representation learning. In its initial tests, the model showcased competitive or leading performance in cross-lingual emotion recognition using the CREMA-D benchmark.\n\nFor instance, in the CREMA-D dataset for emotional English speech, CLARA achieved an 88.44% accuracy using a linear probe classifier, nearing the existing best accuracy of 95.17% derived from a fully supervised and labelled model. This accomplishment is attained through self-supervised pre-training with unlabeled data and minimal subsequent tuning.\n\nCLARA also adeptly generalises to unfamiliar languages. Testing for emotion recognition in French, using the EmoV-DB dataset, achieved a 75.2% accuracy using a linear probe. Moreover, its transfer learning capabilities showcase a rapid adaptation to new languages and tasks by leveraging its foundational representations.\n\nTo fully unlock CLARA's potential, we're concentrating on enhancing the diversity and volume of the training data, especially in languages with limited resources. Your contributions to our multilingual datasets will be pivotal in achieving universal speech understanding.\n\n## Be Part of the Open Empathic Mission\n\nYour participation can propel this research forward. We're actively seeking contributions to enrich the project, particularly in diverse emotional styles, accents, and languages.\n\nOpen Empathic is a non-profit, open-source initiative. If you wish to contribute or collaborate, please connect with us! Together, we can forge ahead in enabling machines to comprehend and interact with us in more human-like ways.\n\nAre you ready to help machines truly comprehend our speech? Let's make it happen together.\n\n## Acknowledgement\n\nWe would like to thank Stability AI for their generous support in providing the essential compute resources for this project.\n","date":1697414400000},{"slug":"leo-lm","frontmatter":{"title":"LeoLM: Igniting German-Language LLM Research","author":"Björn Plüster","date":"September 28 2023","previewImg":"/images/blog/leolm-banner.jpg"},"content":"\nWe proudly introduce LeoLM (**L**inguistically **E**nhanced **O**pen **L**anguage **M**odel), the first comprehensive suite of German-language Foundation Language Models trained in collaboration with HessianAI on their new supercomputer **42**! Built on Llama-2 and trained on a large-scale, high-quality German text corpus, we present LeoLM-7B and 13B, with LeoLM-70B on the horizon, accompanied by a collection of exceptionally proficient German and bilingual chat models.\n\nMeet LeoLM, the first open and commercially available German Foundation Language Model built on Llama-2.\nOur models extend Llama-2's capabilities into German through continued pretraining on a large corpus of high-quality German and mostly locality-specific text.\nThanks to a compute grant at [HessianAI](https://hessian.ai/)'s new supercomputer **42**, we release two foundation models trained with 8k context length,\n[`LeoLM/leo-hessianai-7b`](https://huggingface.co/LeoLM/leo-hessianai-7b) and [`LeoLM/leo-hessianai-13b`](https://huggingface.co/LeoLM/leo-hessianai-13b) (70b also coming soon! 👀) under the [Llama-2 community license](https://ai.meta.com/llama/license/). In addition, we construct evaluation set for benchmarking capabilities of german language models to standardize model comparison, similar to widely adopted english based evals, as provided for instance by [lm-harness-eval](https://github.com/EleutherAI/lm-evaluation-harness) or [LLM-Foundry](https://github.com/mosaicml/llm-foundry).\nWith this release, LAION and Hessian.AI are poised to significantly enhance German open-source and commercial LLM research, fostering new opportunities and expediting widespread adoption.\n\nTry out  [**LeoLM/leo-hessianai-7b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-7b-chat) and [**LeoLM/leo-hessianai-13b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-13b-chat) on HuggingFace Spaces!\n\n*[[Read in German]](/blog-de/leo-lm)*\n\n## Introduction\n\nSince the release of the original Llama Foundation Models \u003csup\u003e1\u003c/sup\u003e in January of 2023, the open-source and academic research community\nhas experienced a rapid acceleration in the development of increasingly capable language models. The advances\nof the past weeks have brought the strongest Llama-2 \u003csup\u003e2\u003c/sup\u003e  based models closer to competing with OpenAI's ChatGPT based on GPT-3.5 or even the stronger GPT4 in some cases.\nNevertheless, a noteworthy constraint persists: the majority of these groundbreaking advancements remain confined to the realm of the English language.\nThis limitation stems mainly from large open-source models having been trained on predominantly mono-lingual English data. While there has been some\nresearch into second-language or multilingual finetuning, most resulting models are limited in their capabilities and suffer from the US-centric bias inherent to English data.\n\nWe seek to alleviate these issues in the case study of the German language by applying many of today's state-of-the-art techniques to develop a truly capable,\nlocalized, and bilingual LLM.\nTo this end, we present LeoLM (**L**inguistically **E**nhanced **O**pen **L**anguage **M**odel), a suite of Llama-2-based German foundation\nmodels, and an assortment of accompanying finetunes.\nFurther, we present GermanBench, a collection of the most relevant English Benchmarks translated into German, allowing us to evaluate LeoLM's capabilities thoroughly.\n\n\u003csup\u003e1\u003c/sup\u003e [Touvron et al. 2023a](https://arxiv.org/abs/2302.13971)\n\u003csup\u003e2\u003c/sup\u003e [Touvron et al. 2023b](https://arxiv.org/abs/2307.09288)\n\n## Stage-2 Pretraining\n\nLlama-2 models are pretrained on 2 trillion tokens of predominantly English text. To enhance their proficiency in the German language, we employ a Stage 2 pretraining methodology.\nWe initialize LeoLMs using Llama-2 weights and continue training the model on a large German text corpus of 65 billion tokens of deliberately filtered and deduplicated web text built from the [OSCAR-2301 corpus](https://huggingface.co/datasets/oscar-corpus/OSCAR-2301).\nA significant aspect of this approach is mitigating the forgetting or loss of previously learned knowledge or capabilities. We follow the findings by [Gupta et al. (2023)](https://arxiv.org/abs/2308.04014) in our choice of hyperparameters to minimize the risk of forgetting.\nAdditionally, we follow work by [Together](https://huggingface.co/togethercomputer/LLaMA-2-7B-32K) in employing [linear RoPE scaling](https://kaiokendev.github.io/til#extending-context-to-8k) and [Flash Attention 2](https://tridao.me/publications/flash2/flash2.pdf) to improve training efficiency and double context length to 8k tokens.\nSee Figure 1 for an overview of all training hyperparameters.\n\n![training_parameters](/images/blog/training_params.png \"Training Hyperparameters\")\n\n## Finetuning Datasets\n\nThere is much debate on what a good chat/instruction tuning dataset must offer, sparking the development of a plethora of different, successful approaches. We take inspiration from this diversity and, to bring similar capabilities to German, translate an assortment of high-quality instruction datasets to German using OpenAI's `gpt-3.5-turbo` API. Using `gpt-3.5-turbo` ensures that the context between prompts and responses remains intact and that complex instructions, potentially containing code, equations, or formatted data, are accurately translated.\nBuilding on community findings, we select a variety of datasets to translate and use to train our chat model.\nThe translated datasets are:\n\n- [OpenPlatypus](https://huggingface.co/datasets/garage-bAInd/Open-Platypus) -\u003e [OpenSchnabeltier](https://huggingface.co/datasets/LeoLM/OpenSchnabeltier)\n- [OpenAssistant OASST1](https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25) -\u003e [OpenAssistant-DE](https://huggingface.co/datasets/OpenAssistant/OASST-DE)\n\nMoreover, we use [`FreedomIntelligence/evol-instruct-deutsch`](https://huggingface.co/datasets/FreedomIntelligence/evol-instruct-deutsch) and [`FreedomIntelligence/alpaca-gpt4-deutsch`](https://huggingface.co/datasets/FreedomIntelligence/alpaca-gpt4-deutsch) from the [MultilingualSIFT](https://github.com/FreedomIntelligence/MultilingualSIFT) project. Thanks to the authors for sharing their data!\nTo facilitate bilingual use, we also train models on a combination of these translated datasets and their original, English counterparts.\n\nFinally, to make up for weaknesses in creative writing and rhyming identified during early tests, we curate two more datasets:\n\n- [GPT4 Poems](https://huggingface.co/datasets/LeoLM/German_Poems): A set of German poems about different topics written by GPT4\n- [GPT4 Songs](https://huggingface.co/datasets/LeoLM/German_Songs): A collection of German songs and following analyses written by GPT4.\n\n## Evaluation and Results\n\nEvaluating the capabilities of LLMs, especially chat models, is complex, and the best methods are still up for debate. Benchmarks based on multiple choice that are evaluated via the model's log-probabilities (as in the [Open LLM Leaderboard]()) are one currently popular method. Another method automatically evaluates responses using GPT4, as in AlpacaEval or MT-Bench. This approach is more geared toward chat models, as it considers the quality of model responses in real-life tasks. To be as comparable as possible, we directly translate a set of English benchmarks to German. We release these datasets in our [HF Organization](https://huggingface.co/LeoLM) and with more detailed documentation [on GitHub](https://github.com/bjoernpl/GermanBenchmark), and you can find the corresponding `lm-evaluation-harness` fork [here](https://github.com/bjoernpl/lm-evaluation-harness-de/tree/mmlu_de) and the `FastEval` fork [here](https://github.com/bjoernpl/FastEval).\n\nIn Figure 3, you can see a comparison of LeoLM versus the base Llama-2 models on a selection of Benchmarks with both the English version (blue) and our translated version (green). Our training improves benchmark scores on the German tasks while slightly reducing scores on English tasks. Notably, the mean increase in German benchmark scores significantly outweighs the mean decrease in performance on English benchmarks, showing that our approach enables learning a new language without forgetting what was previously learned. Why the scores remain lower in German than English is an open question but may be partially attributed to quality degradation during translation.\n\n![](/images/blog/benchmarks.png)\n\nThe following table shows the results on our translated version of MT-Bench. MT-Bench is a benchmark that evaluates multi-turn performance on a curated set of 80 questions from multiple categories using GPT-4 as a judge. In this, GPT-4 evaluates the prompts on a scale from 1-10 with regard to perceived helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. The monolingual `leo-hessianai-13b-chat` model performs best overall, even coming close to GPT-3.5 in the \"humanities\" topic. It scores noticeably poorly in math and coding, which is somewhat to be expected given that the Llama-2 models inherently fall short on this without very explicit finetuning. The bilingual models score slightly below their monolingual counterparts in some categories such math and reasoning while exceeding in coding and extraction.\n\n![](/images/blog/mt_bench.png)\nFor a more detailed evaluation, stay tuned for our paper!\n\n## Qualitative Results\n\nBenchmarks tend to be pretty abstract. To get a better feeling for LeoLM's check out our demos and try it yourself: [**LeoLM/leo-hessianai-7b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-7b-chat) and the bigger sibling [**LeoLM/leo-hessianai-13b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-13b-chat). Alternatively, you can run the model yourself using 🤗Transformers. Find more info on how to set this up on the [model card](https://huggingface.co/LeoLM/leo-hessianai-13b-chat).\n\n## Conclusion\n\nOur research has several key contributions:\n\n- We release a suite of German Foundation Language Models with a permissive license.\n- We transfer a thorough and multi-faceted evaluation approach for base and chat models into German.\n- We demonstrate that large-scale continued pretraining is possible even for data-saturated models such as Llama-2 without significant forgetting or loss of previous capabilities.\n- We present a diverse suite of instruction/chat-tuning datasets translated from English to German to serve as a basis for Germany's open-source LLM research community.\n\nOverall, the LeoLM model suite is a proof-of-concept of language acquisition for pretrained models. Moreover, it presents itself as the first openly available German Foundation Model that meets today's standards. We at LAION hope to kickstart the German open-source research community to reduce dependence on closed-source commercial sources such as OpenAI. Have fun with LeoLM!\n\n## Acknowledgments\n\nThis project was led by Björn Plüster with support, guidance and supervision by Christoph Schuhmann (LAION), Patrick Schramowski (LAION, Hessian.AI, DFKI) and Jenia Jitsev (LAION, Juelich Supercomputing Center, Helmholtz Research Center Juelich).\nWe would like to thank the folks at hessian.AI, especially Wolfgang Stille, Kristian Kersting, Florian Kieser and Patrick Diem\nfor their generous compute sponsorship via the Hessian.AI Innovation Lab (funded by the Hessian Ministry for Digital Strategy and Innovation) and the hessian.AISC Service Center (funded by the Federal Ministry of Education and Research (BMBF)), and their support throughout the training process!  Many thanks also to Jae Hee Lee from the University of Hamburg, Andreas Köpf from Open-Assistant, and community members at LAION for their helpful feedback. Special thanks also go to hessian.AI's Innovation Lab, the AI service centre hessian.AISC and the German Center for Artificial Intelligence (DFKI).\n\nMoreover, we would like to explicitly thank the creators of the datasets we use for finetuning: [OpenAssistant](https://huggingface.co/OpenAssistant), [FreedomIntelligence](https://huggingface.co/FreedomIntelligence), [garage-bAInd](https://huggingface.co/garage-bAInd), [WizardLM](https://huggingface.co/WizardLM) and [Instruction-Tuning-with-GPT-4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM).\n\n---\n\n### An Example\n\n**User:**  Warum ist open-source besonders in der KI-Forschung wichtig?\n\n**LeoLM:**\nOpen-Source spielt eine wichtige Rolle in der KI-Forschung, da sie Zusammenarbeit, Innovation und Zugang zur Technologie fördert. Hier sind einige Gründe, warum Open-Source in der KI-Forschung besonders wichtig ist:\n\n- Zusammenarbeit: Durch Open-Source können Forscher gemeinsam an Projekten arbeiten, Ideen austauschen und voneinander lernen. Dies fördert die Zusammenarbeit zwischen Forschern und kann zu schnelleren und effizienteren Fortschritten in der KI-Forschung führen.\n\n- Transparenz: Open-Source ermöglicht Transparenz und Reproduzierbarkeit in der KI-Forschung. Forscher können den Code und die Methoden anderer sehen, reproduzieren und darauf aufbauen, was die Validierung von Ergebnissen und die Identifizierung potenzieller Schwachstellen erleichtert.\n\n- Zugang zur Technologie: Open-Source macht KI-Technologien und -Werkzeuge einem breiteren Publikum zugänglich. Dies ermöglicht kleineren Forschungseinrichtungen und Einzelpersonen den Zugang zu KI-Werkzeugen, die ihnen sonst möglicherweise nicht zur Verfügung stünden. Dadurch kann Innovation und Forschung auch außerhalb traditioneller Zentren gefördert werden.\n\n- Innovation: Open-Source fördert Innovation in der KI-Forschung, indem sie Raum für Experimente und neue Ideen bietet. Da der Code offen und für alle zugänglich ist, können Forscher ihn an ihre spezifischen Bedürfnisse anpassen und neue KI-Werkzeuge und -Technologien entwickeln.\n\n- Lernen: Open-Source ermöglicht es Forschern, voneinander zu lernen. Durch die gemeinsame Nutzung von Code und Methoden können Forscher aus den Erfahrungen anderer lernen und ihre eigenen Forschungsprojekte verbessern.\n\nZusammenfassend ist Open-Source ein wichtiger Aspekt der KI-Forschung, da er Zusammenarbeit, Transparenz, Zugang zur Technologie, Innovation und Lernen fördert. Dadurch wird die KI-Forschung effizienter, innovativer und zugänglicher für eine breitere Palette von Forschern und Institutionen\n\n| [Link to the PDF with more examples](/documents/leo-lm.pdf) |\n|----------|\n","date":1695859200000},{"slug":"open-lm","frontmatter":{"title":"Introducing OpenLM","author":"OpenLM team","date":"September 26 2023","previewImg":"/images/blog/new_openlm_teaser.png"},"content":"![](/images/blog/openlm_teaser_wide.png)\n## Introduction\n\nWe release [OpenLM](https://github.com/mlfoundations/open_lm) a simple and minimalist PyTorch codebase for training medium-sized language models. OpenLM is designed to maximize GPU utilization and training speed, and is easy to modify for new language model research and applications.\n\n\nWe validate OpenLM by training two language models, OpenLM-1B and OpenLM-7B, on 1.6T and 1.25T tokens of text, respectively. We evaluate these models on standard zero-shot text classification and multiple choice tasks and find that OpenLM-1B outperforms many popular, similarly sized models such as OPT-1.3B and Pythia-1B.  OpenLM-7B achieves similar performance to LLAMA-7B and MPT-7B.\n\nIn this blogpost, we briefly describe the training data, model, evaluation setup, and overall results. We also describe exciting future work we plan to pursue with these models and our OpenLM framework. \n\n## Model and Data Release\n\nAll models and training data (tokenized and shuffled) are available on Huggingface at the following links:\n\n* [OpenLM-1B](https://huggingface.co/mlfoundations/open_lm_1B)\n* [OpenLM-7B](https://huggingface.co/mlfoundations/open_lm_7B_1.25T)\n* [Training and validation data](https://huggingface.co/datasets/mlfoundations/open_lm_example_data)\n\nWe are working on releasing intermediate checkpoints.\n  \n## Data\nWe train our models on a collection of text totaling 1.6T tokens. The training data comes from the following sources:\n\n\n| Dataset       | Tokens  | Percentage |\n|---------------|---------|------------|\n| RedPajama    | 1157.3B | 72.6% |\n| Pile         | 336.2B  | 21.1% |\n| S2ORC         | 48.9B   | 3.1% |\n| Pile of Law   | 27.1B   | 1.7% |\n| RealNews     | 25.0B   | 1.6% |\n| Total         | 1594.5B | 100% |\n\n\nWe do not perform additional preprocessing on the text, and take the data as is from the original sources. To train our model on these data sources, we simply use the following data mix: 72.6% on RedPajama, 27.4% everything else. This follows the given distribution of data in the table above.\n\n## Models\n\nThe models we train follow the basic architecture proposed by LLaMA. The two differences are that we use the  **GPT-NeoX tokenizer**, which we found to be effective in early experiments, and we use LayerNorm instead of RMSNorm, because we haven’t yet added a fused RMSNorm operation. \n\nThe 1B model is trained with AdamW (LR 1e-3, weight decay 0.1) on 128 A100 40GB GPUs, with a global batch size of 2M tokens. \n\nThe 7B model is trained with AdamW (LR 3e-4, weight decay 0.1) on 256 A100 40GB GPUs, with a global batch size of 4M tokens. \n\nThe training speed for the 7B model is 2300 tokens/s/GPU. For model parallelism we use PyTorch FSDP. \n\nAside from the model, the codebase closely follows OpenCLIP which has been tested on around 1,000 GPUs.\n\n## Evaluation Setup\n\nDuring training, we track validation loss using a held out subset of recent papers from the authors of the OpenLM library, breaking news at the time of development, and the OpenLM codebase.\n\nAfter training, we use the LLM-foundry to  evaluate model performance on the 13 zero-shot tasks used to evaluate MPT-7B and LLaMA 7B in the MPT-7B release. We additionally evaluate 5-shot MMLU performance.\n\n## Results\n\n### Validation Loss\n\nHere, we display the validation loss for up to 1T tokens of training for both the OpenLM-1B and 7B models:\n\n![validation loss of 1b model](/images/blog/1B_loss.png)\n\n\n![validation loss of 7b model](/images/blog/7B_loss.png)\n\n\n## Downstream Evaluations\n\nHere, we display the zero-shot evaluation results of OpenLM-1B throughout training:\n\n\n| **OpenLM-1B** | **250B  tokens** | **500B tokens** | **750B tokens** | **1T tokens** | **1.25T tokens** | **1.5T tokens** | **1.6T tokens** |\n|----------------|-----------------|-----------------|-----------------|---------------|------------------|-----------------|-----------------|\n|Training progress | 16% complete | 31% complete | 47% complete | 63% complete | 78% complete | 94% complete | 100% complete |\n| arc_challenge  |            0.27 |            0.28 |            0.29 |          0.28 |             0.29 |            0.31 |            0.31 |\n| arc_easy       |            0.49 |            0.50 |            0.51 |          0.53 |             0.54 |            0.56 |            0.56 |\n| boolq          |            0.60 |            0.61 |            0.62 |          0.62 |             0.65 |            0.64 |            0.65 |\n| copa           |            0.71 |            0.70 |            0.70 |          0.78 |             0.71 |            0.73 |            0.70 |\n| hellaswag      |            0.50 |            0.54 |            0.54 |          0.57 |             0.59 |            0.61 |            0.61 |\n| lambada_openai |            0.56 |            0.57 |            0.61 |          0.61 |             0.65 |            0.65 |            0.66 |\n| piqa           |            0.70 |            0.70 |            0.71 |          0.72 |             0.73 |            0.74 |            0.74 |\n| winogrande     |            0.55 |            0.57 |            0.58 |          0.59 |             0.61 |            0.60 |            0.60 |\n| MMLU           |            0.24 |            0.24 |            0.24 |          0.23 |             0.26 |            0.24 |            0.25 |\n| Jeopardy       |            0.01 |            0.02 |            0.01 |          0.01 |             0.04 |            0.09 |            0.10 |\n| Winograd       |            0.75 |            0.77 |            0.77 |          0.79 |             0.81 |            0.80 |            0.79 |\n|                |                 |                 |                 |               |                  |                 |                 |\n| **Average**    |        **0.49** |        **0.50** |        **0.51** |      **0.52** |         **0.53** |        **0.54** |        **0.54** |\n\n\nAs a comparison, here are the zero-shot results of similarly sized baselines. Our model achieves similar performance to OPT-IML-1.3B, an instruction-tuned model.\n\n| **1B Baselines** | **OPT-1.3B** | **Pythia-1B** | **Neox-1.3B** | **OPT-IML-1.3B** | **OpenLM-1B** |\n|------------------|-------------:|--------------:|--------------:|-----------------:|-----------------:|\n| arc_challenge    |         0.27 |          0.26 |          0.26 |             0.30 |             0.31 |\n| arc_easy         |         0.49 |          0.51 |          0.47 |             0.58 |             0.56 |\n| boolq            |         0.58 |          0.61 |          0.62 |             0.72 |             0.65 |\n| copa             |         0.75 |          0.68 |          0.72 |             0.73 |             0.70 |\n| hellaswag        |         0.54 |          0.49 |          0.48 |             0.54 |             0.61 |\n| lambada_openai   |         0.59 |          0.58 |          0.57 |             0.57 |             0.66 |\n| piqa             |         0.72 |          0.70 |          0.72 |             0.73 |             0.74 |\n| winogrande       |         0.59 |          0.53 |          0.55 |             0.59 |             0.60 |\n| MMLU             |         0.25 |          0.26 |          0.26 |             0.30 |             0.25 |\n| Jeopardy         |         0.01 |          0.00 |          0.00 |             0.12 |             0.10 |\n| Winograd         |         0.74 |          0.71 |          0.75 |             0.73 |             0.79 |\n| **Average**      |     **0.50** |      **0.48** |      **0.49** |         **0.54** |         **0.54** |\n\nNext, we display the zero-shot evaluation results of OpenLM-7B throughout training:\n\n | **OpenLM-7B**  | **275B tokens** | **500B tokens** | **675B tokens** | **775B tokens** | **1T tokens** | **1.25T tokens** |\n|-----------------|-----------------|-----------------|-----------------|-----------------|---------------|------------------|\n| Training progress | 17% complete | 31% complete | 42% complete | 48% complete | 63% complete | 78% complete |              |\n| arc_challenge   |            0.35 |            0.35 |            0.36 |            0.37 |          0.39 |             0.39   |\n| arc_easy        |            0.60 |            0.61 |            0.62 |            0.62 |          0.63 |             0.66  |\n| boolq           |            0.67 |            0.66 |            0.69 |            0.69 |          0.70 |             0.70    |\n| copa            |            0.75 |            0.79 |            0.75 |            0.80 |          0.80 |             0.78     |\n| hellaswag       |            0.64 |            0.67 |            0.68 |            0.68 |          0.69 |             0.70     |\n| lambada_openai  |            0.67 |            0.68 |            0.69 |            0.70 |          0.70 |             0.70       |\n| piqa            |            0.75 |            0.76 |            0.76 |            0.76 |          0.77 |             0.77      |\n| winogrande      |            0.62 |            0.65 |            0.65 |            0.65 |          0.67 |             0.67       |\n| MMLU-0 shot     |            0.25 |            0.25 |            0.27 |            0.27 |          0.28 |             0.30       |\n| Jeopardy        |            0.15 |            0.18 |            0.23 |            0.22 |          0.16 |             0.21       |\n| Winograd        |            0.82 |            0.81 |            0.84 |            0.84 |          0.85 |             0.86        |\n| **Average**     |        **0.57** |        **0.58** |        **0.60** |        **0.60** |      **0.60** |         **0.61**      |\n\n\n\n|Task | **OpenLM-7B** | **LLAMA-7B** | **MPT-7B** |\n|-----------------|-----------------|-----------------|-----------------|\n| arc_challenge   |              0.39  |         0.41 |       0.39 |\n| arc_easy        |            0.66  |         0.65 |       0.67 |\n| boolq           |               0.70   |         0.77 |       0.75 |\n| copa            |                      0.78      |         0.78 |       0.81 |\n| hellaswag       |                   0.70    |         0.75 |       0.76 |\n| lambada_openai  |                0.70      |         0.74 |       0.70 |\n| piqa            |                   0.77      |         0.79 |       0.80 |\n| winogrande      |                 0.67      |         0.68 |       0.68 |\n| MMLU-0 shot     |                    0.30        |         0.30 |       0.30 |\n| Jeopardy        |                       0.21        |         0.33 |       0.31 |\n| Winograd        |                       0.86              |         0.81 |       0.88 |\n| **Average**     |              **0.61**      |     **0.64** |   **0.64** |\n| **MMLU-5 shot** |                      **0.34**          |     **0.34** |            |\n\n\nConsistent with the validation loss, our models continue to improve in zero-shot performance even late in training. At 1.25T tokens, OpenLM-7B matches or outperforms LLaMA-7B or MPT-7B on 7 out of 11 tasks.\n\n## Future Work\n\nOpenLM has already enabled new language modeling research, for example in the development of [low-risk language models trained on permissively licensed text](https://arxiv.org/abs/2308.04430). We plan to use OpenLM to support a variety of new research directions, including multimodal models, mixture of experts, and dataset composition. We also plan to scale up OpenLM so it supports training larger models.\n\n## Team and acknowledgements\n\nThe OpenLM team currently consists of: Suchin Gururangan*, Mitchell Wortsman*, Samir Yitzhak Gadre, Achal Dave, Maciej Kilian, Weijia Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard Heckel, Alex Dimakis, Ali Farhadi, Vaishaal Shankar, Ludwig Schmidt.\n\nCode is based heavily on [open-clip](https://github.com/mlfoundations/open_clip) developed by a team including Ross Wightman, Romain Beaumont, Cade Gordon, Mehdi Cherti, Jenia Jitsev, and [open-flamingo](https://github.com/mlfoundations/open_flamingo), developed by a team including Anas Awadalla and Irena Gao. Additional inspiration is from [lit-llama](https://github.com/Lightning-AI/lit-llama).\n\nWe thank Stability AI for providing the compute for this project, the RedPajama team for their dataset, Sarah Pratt for logo design, [IFML](https://www.ifml.institute/), and Toyota Research Institute. We also thank the following people for helpful advice and feedback throughout the project: Jonathan Frankle, Daniel King, Luca Soldaini.","date":1695686400000},{"slug":"transparent-ai","frontmatter":{"title":"Towards a transparent AI Future: The Call for less regulatory hurdles on Open-Source AI in Europe","author":"LAION","date":"September 21 2023","previewImg":"/images/blog/laion-blue.png"},"content":"\nFollowing our previous open letter to the European Parliament on the significance of open-source AI, LAION, backed by European Laboratory for Learning and Intelligent Systems (ELLIS) and a long list of very impactful AI researchers, we submit this new open letter to the European Parliament:\n\n| [Link to the PDF](/documents/transparent-ai.pdf) |\n|----------|\n\n#### Why Open-Source is the Gold Standard for AI Security\n\nThe transparency of open-source AI is its strength. It ensures robustness and security unmatched by closed systems. Why? Open-source AI benefits from the scrutiny of the global community, allowing vulnerabilities to be detected and fixed promptly. Drawing parallels, we can look at the Linux operating system—a paragon of security and robustness stemming from its open-source nature.\n\n#### Countering Redundancy and Upholding Sustainability\n\nWith the environmental toll of extensive AI training becoming a major concern, open-source models have shown a clear path forward. By minimizing redundant training, they reduce computational and energy overheads, reflecting a commitment to a sustainable future.\n\n#### Ensuring Scientific Reproducibility\n\nReproducibility and validation are key to scientific integrity and progress. Open-source AI models offer full transparency, allowing diverse research groups to independently verify results and claimed functionality. Unlike closed-source alternatives, open-source foundations guarantee stringent standards for the machine learning and AI field. With these open-source foundation models rigorously tested by a vast expert community, AI applications in sectors from healthcare to finance can build on a trusted, scientifically validated base.\n\n#### A Catalyst for Innovation\n\nOpen-source AI has been instrumental in levelling the playing field. Small and mid-sized enterprises can now fine-tune existing models, fostering innovation without the daunting costs of building from scratch. If Europe's ambition is to retain its brightest minds, ensuring uninterrupted access to these resources is non-negotiable.\n\n#### Regulating Application, Not Innovation\n\nThe clarion call from LAION and its supporters is clear—focus regulations on AI's applications, not the foundational technology. By doing so, the EU will nurture innovation while ensuring that AI's real-world applications are ethical, safe, and in line with European values.\n\n#### Incentivizing the Open-Source Paradigm\n\nPerhaps the most potent recommendation in this new letter is the incentivization of open-source AI. It's a win-win. Organizations can release foundational models as open-source, maintaining proprietary rights on fine-tuned versions. This ensures that the broader community benefits from the base models while commercial competitiveness remains intact.\n\n#### The European AI Path Forward\n\nEuropean sovereignty in AI is crucial, and open-source AI research is key to addressing challenges ranging from healthcare to climate change. The future, as outlined in the letter, imagines a Europe at the forefront of AI research, one that champions transparency, security, and sustainability.\n\n#### Supporters\n\n| Name | Description |\n|----------|----------|\n| Board of the European Laboratory for Learning and Intelligent Systems (ELLIS): Serge Belongie, Nicolò Cesa-Bianchi, Florence d'Alché-Buc, Nada Lavrac, Neil D. Lawrence, Nuria Oliver, Bernhard Schölkopf, Josef Sivic, Sepp Hochreiter| [European Lab for Learning \u0026 Intelligent Systems (ellis.eu)](https://ellis.eu/board)   |\n| Yann André LeCun | Chief AI Scientist at Facebook \u0026 Silver Professor at the Courant Institute, New York University |\n| Jürgen Schmidhuber | Prof. Jürgen Schmidhuber : Scientific Director of the Swiss AI Lab IDSIA (USI \u0026 SUPSI), Co-Founder \u0026 Chief Scientist of NNAISENSE, Father of Modern AI |\n| Kristian Kersting | Full Professor at Technical University of Darmstadt, Co-Director, Hessian Center for AI (hessian.AI) and member of the German Center for Artificial Intelligence (DFKI)   |\n| Björn Ommer   | Full professor and head of the Computer Vision \u0026 Learning Group at the Ludwig-Maximilians-University of Munich   |\n| Hilde Kuehne   | Professor, Institute for Computer Science II, Head of Multimodal Learning, University of Bonn   |\n| Mira Mezini   | Professor of Computer Science at Technical University of Darmstadt, Co-Director of Hessian Center for AI (hessian.AI)   |\n| Patrick Schramowski   | Senior Researcher at the German Center for Artificial Intelligence (DFKI) and Hessian Center for AI (hessian.AI)   |\n| Jenia Jitsev   | Expert in multi-modal foundation models and datasets. LAION core member and contributor. Member OpenBioML. Researcher at Helmholtz Juelich Supercomputing Center, Germany.  |\n| Marianna Nezhurina | Senior Researcher and Lab Lead at Juelich Supercomputing Center, Helmholtz Research Center Juelich. Scientific Lead and Co-Founder at LAION; Member of European Laboratory for Learning and Intelligent Systems (ELLIS)   |\n| Dominik L. Michels   | Full Professor of Intelligent Algorithms in Modeling and Simulation at the Technical University of Darmstadt   |\n| Tim Dettmers   | PhD Student at The University of Washington. Creator of the bitsandbytes library.   |\n| Mark Schutera   | PhD Student Karlsruhe Institute of Technology within Unsupervised Deep Learning for Cognitive Perception Systems   |\n| Andreas Hochlehnert   | PhD Student, University of Tübingen, International Max-Planck Research School for Intelligent Systems (IMPRS-IS)   |\n| Irina Rish | Full Professor at the Université de Montréal, a core member of Mila - Quebec AI Institute. Canada Excellence Research Chair (CERC) in Autonomous AI and CIFAR Chair. PI on a collaborative INCITE project on the Summit supercomputer at OLCF (supported by the U.S. DoE, Office of Science ), aiming to build open-source large-scale language and multimodal models (e.g., RedPajama-INCITE was trained as a part of this project). |\n| Huu Nguyen | Former big-law partner, CEO and co-founder of Ontocord.AI, LAION volunteer, co-author of Data Governance in the Age of Large-Scale Data-Driven Language Technology, FAccT ’22, and co-author of resolution 112 of the ABA on encouraging lawyers to understand the risks and benefits of AI. |\n| David Ha | Co-Founder and CEO of sakana.ai |\n| Hessie Jones | Writer, Forbes, Data Privacy, Ethical AI Practitioner, Advocating for Human-centred AI and Ethical Distribution of AI Systems, BOA Women in AI Ethics, Cofounder MyData Canada, Cofounding Member Personally Identifiable Information Standards Architecture (PIISA); former COO Beacon Trust Network, BOD Technology for Good Canada  |\n| Sampo Pyysalo | Research Fellow, University of Turku, co-lead TurkuNLP research group, Principal Investigator, High-Performance Language Models (Horizon EU project), leading multiple efforts to create very large open models.  |\n| Wolfgang Stille | Chief Technical Officer of the Hessian Center for AI (hessian.AI) and project lead of the AI Innovation Lab and the AI Service Center hessian.AISC. He has been involved with digital research infrastructure and open science for many years and was a leading participant in the process of establishing a digital research data culture at Hessian universities.  |\n| Christoph Schuhmann    | Organizational Lead \u0026 Co-Founder of the Large-scale AI Open Network (LAION), Neurips 2022 Outstanding Paper Award \u0026 Falling Walls Breakthrough of the Year 2023 Award Winner   |\n| Robert Kaczmarczyk   | Medical Lead \u0026 Co-Founder of the Large-scale AI Open Network (LAION), Neurips 2022 Outstanding Paper Award \u0026 Falling Walls Breakthrough of the Year 2023 Award Winner   |\n","date":1695254400000},{"slug":"falling-walls-2023","frontmatter":{"title":"LAION Triumphs at the Falling Walls Science Breakthrough of the Year 2023 Awards","author":"Christoph, Jenia, Robert","date":"September 14, 2023","previewImg":"/images/blog/FW23.jpg"},"content":"We happily announce that we, LAION, won the Falling Walls Science Breakthrough of the Year 2023 Award in the category Science and Innovation Management for \"democratizing AI research by providing open access to advanced AI models, tools, and datasets, fostering public engagement and awareness, and promoting international collaboration to create a transparent and inclusive AI ecosystem that benefits everyone.\" This recognition is not just for the select few but for our entire LAION community of hobby scientists, university professors, students, and enthusiasts all united with a shared vision - the democratization of AI research.\n\n![FW23A](/images/blog/FW23A.png)\n\n## About the Award\n\nThe Falling Walls Science Breakthrough of the Year Award, bestowed by the non-profit Falling Walls Foundation, celebrates groundbreaking achievements across various academic disciplines. Inspired by the historic fall of the Berlin Wall, it embodies the spirit of overcoming barriers to forge a brighter future for society.\n\nRecognized globally, this prestigious award spotlights innovative research addressing today’s pressing challenges. It facilitates interdisciplinary collaboration, underscoring the transformative potential of science in building a better future.\n\nThe accolade spans six categories, recognizing ten finalists in each, handpicked from over a thousand applications by a panel of expert jurors. It culminates in the Falling Walls Science Summit, a vital hub for dialogue between global science leaders and society.\n\n## Grassroots Leadership\n\nThis acknowledgment is not just a testament to our dedication but a clear indication of the significant role grassroots communities play in advancing the world through open-source AI research. It highlights the fact that with unity and shared vision, even grassroots initiatives can spearhead meaningful change, demonstrating that we are not only contributors but essential players in fostering a better, more inclusive future.\n\n## Join the Movement\n\nWe extend our heartfelt gratitude to the Falling Walls Foundation for acknowledging our efforts and to every member of the LAION community for being an indispensable part of this journey.\n\nAs we celebrate this monumental achievement, we invite you to become a part of this vibrant community. Together, we can foster a culture of open dialogue, collaboration, and innovative solutions. Join us on [Discord](https://discord.com/invite/eq3cAMZtCC) as we continue to break walls and democratize AI research for the betterment of society globally.\n\nFor more details on the award, visit the [official announcement](https://falling-walls.com/press-releases/falling-walls-announces-science-breakthrough-of-the-year-2023-laureates/).\n","date":1694649600000},{"slug":"visit_bench","frontmatter":{"title":"Introducing VisIT-Bench, a new benchmark for instruction-following vision-language models inspired by real-world use","author":"Yonatan Bitton","date":"August 15, 2023","previewImg":"/images/blog/visit_bench/1_fig1.png"},"content":"\n[[Paper]](https://arxiv.org/abs/2308.06595) [[Code]](https://github.com/mlfoundations/VisIT-Bench/) [[Dataset]](https://huggingface.co/datasets/mlfoundations/VisIT-Bench) [[Leaderboard]](https://huggingface.co/spaces/mlfoundations/VisIT-Bench-Leaderboard)\n\nWe are thrilled to introduce **VisIT-Bench**, a benchmark for evaluating instruction-following vision-language models (VLMs). The central goal of VisIT-Bench is to provide a more accurate and meaningful assessment of VLMs, particularly in the context of human-chatbot interactions inspired by real-world scenarios.\n\nVisIT-Bench comprises 678 examples. Each example includes:\n- An image (or multiple images)\n- An instruction\n- An \"instruction-conditioned caption\" - a detailed caption allowing a text-only entity to follow the instruction \n- A GPT-4 response suggestion\n- A label verifying the accuracy of the response\n![](/images/blog/visit_bench/2_example.png)\n\nVisIT-Bench comes with an easy automatic evaluation that correlates well with human preferences from over 5,000 annotations. Our evaluations underscore a clear need for advancement in VLMs. The top model on our benchmark exceeded the human-verified GPT-4 reference in only 27% of comparisons, highlighting both the challenges and potential for future progress in this area.\n\n## Why VisIT-Bench?\n\nThough recent VLMs have shown promise in following instructions, their evaluation for real-world human-chatbot instructions is often limited. Typically, VLMs are evaluated through qualitative comparison of outputs, which makes it challenging to quantify progress and potential shortcomings. VisIT-Bench helps address this problem by offering a comprehensive testbed for measuring model performance across a diverse set of instruction-following tasks, inspired by real world scenarios.\n\n## Building the Benchmark\n\nTo maximize the variety of skills needed for evaluation, VisIT-Bench draws from two main sources: new instruction collection and the repurposing of existing datasets. This dual-source approach ensures a comprehensive assessment of multimodal chatbots.\n![](/images/blog/visit_bench/1_fig1.png)\n\n### Data Collection Framework\n\n**VisIT-Bench** is a benchmark consisting of 679 vision-language instructions. Each instruction pairs an image with a corresponding request or question. For example, an image might depict a storefront with two portable wedge ramps, accompanied by the question: *Would a disabled wheelchair-bound individual find it easy to go into this store?* Contrary to prevalent zero-shot evaluations, many instructions emphasize open-ended generation requests, like *write a poem...* or *what should I bring if I were to visit here?*.\n\nVisIT-Bench employs a structured data collection strategy encompassing four steps.\n\n(1) **Instruction Generation**: Convert a single example from a skill category into multiple instances. Using references from an instruction family (instruction, image, model output), annotators craft new instructions linked to a public image URL. For instance, under the *Contextual Knowledge of Events* family, an instance about a related event might be generated.\n![](/images/blog/visit_bench/3_1_collection.png)\n\n(2) **Instruction-Conditioned Caption Generation**: Annotators produce detailed captions for each image, based on the given instruction and image. The aim is a text-rich caption enabling text-only comprehension, which serves as a foundation for GPT-4 outputs and text-only evaluations.\n![](/images/blog/visit_bench/3_2_collection.png)\n\n(3) **Generating GPT4 Responses**: Response candidates from GPT-4 are fetched for later human validation. Using the prompt: “Consider an image depicted by: `\u003ccaption\u003e`. Follow this instruction: `\u003cinstruction\u003e`. Response: “.\n\n(4) **Model Output Evaluation**: GPT-4's instruction-following ability is assessed using text-only pairs. With the image, dense caption, instruction, and GPT-4's prediction, annotators evaluate GPT-4's adherence to instructions, pinpointing errors in captions or responses and flagging inappropriate content.\n![](/images/blog/visit_bench/3_3_collection.png)\n\n### Repurposing Existing Datasets\n\n**VisIT-Bench** converts 25 datasets (250 instances) into chatbot-friendly formats, including ten multi-image datasets. These datasets include [VQA](https://visualqa.org/), [VCR](https://visualcommonsense.com/), [TextCaps](https://textvqa.org/textcaps/), [WHOOPS!](https://whoops-benchmark.github.io/) and more. Each instance consists of an instruction prompt and a chatbot response. For instance, our adaptation of the [NLVR2 dataset](https://github.com/lil-lab/nlvr/tree/master/nlvr2) is designed to test visual reasoning capabilities for chatbots. Originally, the NLVR2 format featured a sentence for analysis, two images, and a binary response. In our version, we incorporated a zero-shot prompt, detailed image captions aligned with the instruction, and a GPT-4-verified human response. This method is devised to enhance previous studies to align with contemporary chatbot evaluation metrics.\n![](/images/blog/visit_bench/4_nlvr_image.png)\n\n### Features of the Dataset\n\nVisIT-Bench is a unique composition of 70 unique instruction families, 25 repurposed prior datasets, including 10 multi-image datasets, each embodying a different skill that a chatbot model should ideally exhibit. These families mirror practical real-world chatbot interactions, thus ensuring that our benchmark evaluates models against realistic and varied tasks. \n![](/images/blog/visit_bench/7_table_compare.png)\n\n## Models\n\nOur evaluation includes a variety of publicly accessible vision-language models, either fine-tuned with multimodal instructions or designed to execute based on LLM outputs. These models include LLaVA-13B, InstructBLIP-13B, MiniGPT4-7B, mPLUG-Owl-7B, LlamaAdapter-v2-7B, PandaGPT-13B, VisualChatGPT, Multimodal GPT, OpenFlamingo v1, and Otter v1. \n\n## Human-Guided Rankings\n\nUsing VisIT-Bench's single-image examples, we created 5,000 pairwise comparisons across multiple vision-language models. Each model was in about 700 comparisons, including GPT-4's outputs. Three annotators blind to the model origins ranked the outputs for correctness and comprehensiveness. Results underscore the effectiveness of GPT-4 and LLaVA (13B) on this dataset. GPT-4's success stemmed from conditioned dense captions, while LLaVA benefited from its instruction-tuning dataset. Interestingly, LlamaAdapter-v2 (7B) outperformed in direct comparisons to reference outputs, emphasizing the importance of language instruction fine-tuning for these models. The relationship between model design, task variety, and performance needs more exploration.\n![](/images/blog/visit_bench/8_human_performance.png)\n\n## Automatic Evaluation and Dynamic Leaderboard\n\nWe introduced an automatic evaluation framework to rank model outputs, employing Elo-ratings and win-rate against the ground truth. This approach addresses potential bias, considering GPT-4 was used to assess its own output, and shows high agreement with human ratings. Metrics include: Reference-free Elo score, mirroring human evaluation; Reference-backed Elo score, incorporating the prompt's optional reference; and Win-rate against reference, indicating how often a model's output is preferred over reference. Our findings reveal consistent rankings between reference-free and backed evaluations. However, existing instruction-following models' win rates vary widely, underscoring a clear gap in their performance against reference outputs.\n![](/images/blog/visit_bench/9_auto_eval_rating.png)\n![](/images/blog/visit_bench/10_auto_eval_expanded.png)\n\n### Correlation of the Automatic and Human-Annotated Preferences\n\nOur GPT-4 based metric surpasses others, closely mirroring majority-vote human judgments. In cases with unanimous agreement (5/5 annotators), GPT4-no-ref hits 93% accuracy, outdoing BERTScore (80%), METEOR (78%), and ROUGE-L (70%). Against a length baseline metric (60%), these metrics provide viable offline evaluation options without needing OpenAI API access. The reference-free GPT-4 metric performs similarly to its reference-backed counterpart, allowing references in the evaluation setup..\n![](/images/blog/visit_bench/11_correlation.png)\n\n## Contributions\n\nThis effort was made possible thanks to the amazing team of:\n\n- [Yonatan Bitton*](https://yonatanbitton.github.io/), The Hebrew University of Jerusalem, Google Research\n- [Hritik Bansal*](https://sites.google.com/view/hbansal), University of California, Los Angeles\n- [Jack Hessel*](https://jmhessel.com/), Allen Institute for AI\n- [Rulin Shao](https://rulinshao.github.io/), University of Washington\n- [Wanrong Zhu](https://wanrong-zhu.com/), University of California, Santa Barbara\n- [Anas Awadalla](https://anas-awadalla.streamlit.app/), University of Washington\n- [Josh Gardner](https://homes.cs.washington.edu/~jpgard/), University of Washington\n- [Rohan Taori](https://www.rohantaori.com/), Stanford\n- [Ludwig Schmidt](https://people.csail.mit.edu/ludwigs/), Allen Institute for AI, University of Washington, LAION\n\n*Equal contribution.\n\n## In Conclusion\n\n**VisIT-Bench** offers a comprehensive lens on VLMs by utilizing 70 carefully curated instruction families, mirroring a wide range of real-world scenarios. This approach allows an in-depth assessment of model understanding but paves the way for enhancing VLMs' performance across various tasks. VisIT-Bench is dynamic to participate, practitioners simply submit their model's response on the project website; Data, code and leaderboard is available at the [project website](https://visit-bench.github.io/).\n\n![](/images/blog/visit_bench/13_leaderboard.png)\n","date":1692057600000},{"slug":"objaverse-xl","frontmatter":{"title":"Objaverse-XL: An Open Dataset of Over 10 Million 3D Objects","author":"Matt Deitke","date":"Jul 11 2023","previewImg":"/images/blog/objaverse-xl.jpg"},"content":"\n![](/images/blog/objaverse-xl.jpg)\n\nWe are thrilled to announce Objaverse-XL, an open dataset of over 10 million 3D objects! Using it, we train Zero123-XL, a foundation model for 3D that displays remarkable generalization abilities. In the landscape of AI, scale has been paramount to recent advances. Over the past decade, we have observed an escalating trend of leveraging large volumes of data to train machine learning models, particularly in NLP and 2D vision. But what about 3D vision tasks? Despite the burgeoning demand for augmented reality (AR) and virtual reality (VR) applications, advancements in 3D vision have lagged, primarily due to the scarcity of high-quality 3D data.\n\nObjaverse 1.0, released back in December, was a step in the right direction, and enabled exciting research like Zero-1-to-3 for novel view synthesis and single view 3D reconstruction. But, it was still quite small, being on the order of 800K objects. With Objaverse-XL, we scale up the number of 3D objects that we use from 800K to over 10 million deduplicated 3D objects, pulling in objects from a variety of sources.\n\n![](/images/blog/dataset-distribution.jpg)\n\nObjaverse-XL is a curated amalgamation of 3D objects from various internet sources. It includes objects from GitHub, sourced from over 500k repositories, along with assets from platforms like Thingiverse, Sketchfab, Polycam, and the Smithsonian 3D Digitization project. The result is a remarkable collection of over 10 million unique 3D objects, each coming with its metadata. The figure above shows a t-SNE projection of CLIP L/14 embeddings on a subset of rendered objects. Compared to Objaverse 1.0 (orange), Objaverse-XL more densely captures the distribution of 3D assets.\n\nWith Objaverse-XL, we train Zero123-XL. Leveraging the view-conditioned diffusion model proposed by Zero123, the Zero123-XL model was pretrained on a larger dataset, Objaverse-XL, to yield better zero-shot generalization performance. Remarkably, we find that the enhanced pre-training on Objaverse-XL enables the model to generalize significantly better to challenging data categories like people, cartoons, and sketches. It showed improvements in generating novel views that both maintained original style and object geometric details. Further testing on the Google Scanned Objects dataset also showed that as the dataset size increased, so did the visual similarity score between predicted and actual views. Finally, alignment fine-tuning on a high-quality subset of Objaverse-XL led to significant improvement in the model's alignment with human preferences, illustrating the potential of large-scale pretraining and strategic finetuning in advancing 3D vision.\n\n![](/images/blog/zero123-xl.jpg)\n\nIn the figure above, we show novel view synthesis on in-the-wild images (either coming from internet searches or text-to-image models). It shows a comparison between Zero123-XL trained on Objaverse-XL and Zero123 trained on Objaverse. Starting from the input view, the task is to generate an image of the object under a specific camera pose transformation. The camera poses are shown beside each example. Significant improvement can be found by training with more data, especially for categories including people (1st row), anime (2nd row), cartoon (3rd row), furniture (4th row), and sketches (5th row). Additionally, viewpoint control is significantly improved (see 2nd row).\n\nWe are thrilled to see what the community is able to build with Objaverse-XL! More information for accessing the dataset, models, and code will be provided soon!\n\n### Acknowledgements\n\nObjaverse-XL was a joint effort between several teams, including the Allen Institute for AI,\nColumbia University, the University of Washington, Stability AI, LAION, and Caltech.\n\nRuoshi Liu did a tremendous job leading the Zero123-XL efforts!\nThe rest of the fantastic team includes Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, and Kiana Ehsani, and was co-advised primarily by Ludwig Schmidt and Ali Farhadi.\n\nWe would like to thank Stability AI for compute used to train the experiments and LAION for\ntheir support. We would also like to thank Luca Weihs, Mitchell Wortsman, Romain Beaumont,\nand Vaishaal Shankar, Rose Hendrix, Adam Letts, Sami Kama, Andreas Blattmann, Kunal Pratap\nSingh, and Kuo-Hao Zeng for their helpful guidance and conversations with the project. Finally,\nwe would like to thank the teams behind several open-source packages used throughout this project,\nincluding Blender, PyTorch, PyTorch Lightning, D3, Matplotlib, NumPy,\nPandas, Wandb, and Seaborn. We would also like to\nacknowledge the use of LLMs for helping revise some text and general coding assistance. Finally, we\nwould also like to thank and acknowledge the content creators who contributed to the dataset.\n","date":1689033600000},{"slug":"video2dataset","frontmatter":{"title":"video2dataset: A simple tool for large video dataset curation","author":"Maciej Kilian","date":"July 10 2023","previewImg":"/images/blog/video2dataset-preview.png"},"content":"[[GitHub]](https://github.com/iejMac/video2dataset)\n\nWithin only two years large foundational models like [CLIP](https://arxiv.org/abs/2103.00020), [Stable Diffusion](https://arxiv.org/abs/2112.10752), and [Flamingo](https://arxiv.org/abs/2204.14198) have fundamentally transformed multimodal deep learning. Because of such models and their impressive capabilities to either create stunning, high-resolution imagery or to solve complex downstream tasks, joint text-image modeling has emerged from a niche application to one of the (or maybe _the_) most relevant topics in today’s AI landscape. Remarkably, all these models, despite addressing very different tasks and being very different in design, share three fundamental properties as the main drivers behind their strong performance:  A simple and stable objective function during (pre-)training, a well-investigated scalable model architecture, and - probably most importantly - a large diverse dataset.\n\nAs of 2023, multimodal deep learning is still heavily focusing on text-image modeling, while other modalities such as video (and audio) are only sparsely investigated. Since the algorithms to train the above models are usually modality agnostic, one might wonder why there aren’t strong foundational models for these additional modalities. The reason for this is – plain and simple – the lacking availability of large scale, annotated datasets. As opposed to image modeling, where there are established datasets for scaling such as [LAION-5B](https://arxiv.org/abs/2210.08402), [DataComp](https://arxiv.org/abs/2304.14108), and [COYO-700M](https://github.com/kakaobrain/coyo-dataset) and scalable tools as [img2dataset](https://github.com/rom1504/img2dataset), this lack of clean data hinders research and development of large multimodal models especially for the video domain.\n\nWe argue that overcoming this data problem is a core interest of (open source) multimodal research since it can foster important previously impossible projects such as high quality [video](https://research.nvidia.com/labs/toronto-ai/VideoLDM/) and [audio](https://google-research.github.io/seanet/audiolm/examples/) generation, [better pre-trained models for robotics](https://twitter.com/comma_ai/status/1666959310310752257?s=20), [movie AD for the blind community](https://www.robots.ox.ac.uk/~vgg/research/autoad/), and more.\n\n![ManyVideos](/images/blog/videos_figure.gif)\n_Figure 1:_ video2dataset allows to easily create large scale collections of videos as the ones in the above sample created from available research datasets.\n\n### Solution: Flexible dataset curation tooling\n\n![Architecture](/images/blog/video2dataset_diagram.png)\n_Figure 2:_ video2dataset architecture\n\nWe introduce video2dataset, an open-source tool designed to curate video and audio datasets efficiently and at scale. It's flexible, extendable, offers a wide variety of transformations, and has been successfully tested on various large video datasets. All these examples are available in the repository, along with instructions for replicating our process.\n\nWe’ve also used video2dataset to build upon existing video datasets by downloading them individually, combining them, and transforming them into more convenient shapes with new features and considerably more samples. See the examples section for a more detailed explanation of this chain-processing. The tool’s effectiveness is showcased through the results we obtained by training various models on the datasets produced by video2dataset. An in-depth analysis of the new dataset and results will be included in our upcoming paper.\n\n## Architecture\n\nvideo2dataset is built on the foundation of [img2dataset](https://github.com/rom1504/img2dataset) and is designed to transform a table of URLs and metadata into an easily loadable [WebDataset](https://github.com/webdataset/webdataset) in just one command. Furthermore, it allows you to reprocess the WebDataset for additional transformations while retaining the same shard contents. Let's break down how video2dataset operates.\n\n### Input Sharding\n\nThe process begins with sharding the input data, a step that enables easy distribution among the workers. These input shards are temporarily stored, and the 1-1 correspondence between input and output shards ensures seamless resumption following any failures. If a dataset processing run stops prematurely, we can conveniently bypass processing the input shards for which the output shard already exists.\n\n### Distribution and Reading\n\nPost-sharding, the individual shards are distributed among the workers, who read each shard and process the samples inside. For distribution we support 3 modes - multiprocessing, pyspark, and slurm - the first is good for single machine jobs whereas the last two can help with distributing across many machines. The reading method varies depending on the input dataset's format. For instance, if it's a table of links, video2dataset downloads the video from the web. video2dataset supports a wide variety of video platforms by using [yt-dlp](https://github.com/yt-dlp/yt-dlp) to download videos it can’t directly request. However, if it's an existing WebDataset with videos, an existing webdataset dataloader reads the bytes or frames in tensor format from those samples.\n\n### Subsampling\n\nOnce the video is read and the worker has the video bytes, they are sent through a pipeline of subsamplers defined in the job config. This step optionally transforms the video through actions such as frames per second (FPS) or resolution downsampling, clipping, scene detection, and more. Alternatively there are subsamplers that are meant to only extract metadata from the input modalities like resolution/compression information, synthetic captions, optical flow, or others and include it in the metadata of a given sample. If your desired transformation isn’t already in video2dataset, its very easy to add it by defining a new subsampler or adjusting an existing one. This can be done with minimal changes in other locations of the repository and is a very welcomed contribution.\n\n### Logging\n\nThroughout the entire process, video2dataset meticulously logs vital information at various stages. Upon completion of each shard a corresponding {ID}\\_stats.json file is generated. This file contains key details, such as the number of samples processed, the number of successful operations, and a log of any failures along with their associated error messages. For added functionality, video2dataset also supports integration with Weights \u0026 Biases (wandb). This integration can be activated with a single argument and, when enabled, it provides extensive performance reporting, along with success and failure metrics. Such features are helpful for benchmarking and cost-estimating tasks related to full jobs.\n\n![](/images/blog/video2dataset_wandb_logs.png)\n_Figure 3:_ Part of a wandb report from a large video2dataset run\n\n### Writing\n\nFinally, video2dataset saves the transformed data to output shards in specified locations, where they can be utilized for training or reprocessing with video2dataset or other tools. The output format of the dataset is shards of N samples each where the shards can be formatted in multiple ways - directories, tar files, tfrecords, or parquet files. The most useful ones are the directories format for smaller datasets and debugging and tar files which is used by the WebDataset format for loading. Here is a visualization of the output datasets:\n\n```\nvideo-dataset\n ├── 00000.tar\n |     ├── 00000.mp4\n |     ├── 00000.txt\n |     ├── 00000.json\n |     ├── 00001.mp4\n |     ├── 00001.txt\n |     ├── 00001.json\n |     └── ...\n |     ├── 10000.mp4\n |     ├── 10000.txt\n |     ├── 10000.json\n ├── 00001.tar\n |     ├── 10001.mp4\n |     ├── 10001.txt\n |     ├── 10001.json\n │     ...\n ...\n ```\n\n### Reprocessing\n\nvideo2dataset can reprocess previous output datasets by reading the output shards and passing the samples inside through new transformations. This capability is particularly beneficial for video datasets, given their often hefty size and unwieldy nature. It allows us to conservatively downsample our data to avoid multiple downloads of large datasets. We delve into a practical example of this in the next section.\n\n## Examples\n\n### Tree of Datasets\n\nEach video is a rich source of data that can be decomposed into many forms - different resolutions, the audio, the motion (optical flow), individual frames - and dataset tooling should reflect this flexibility. One initial download of raw videos can be efficiently expanded into a variety of datasets for many different projects. For example, a research group might use video2dataset and its chain-processing capabilities in the following way to accommodate many research projects with diverse data:\n\n\u003cvideo width=\"640\" height=\"480\" controls style=\"margin-left:auto;margin-right:auto;\"\u003e\n  \u003csource src=\"/images/blog/video2dataset_tree_of_datasets.mp4\" type=\"video/mp4\"\u003e\n Your browser does not support the video tag.\n\u003c/video\u003e\n\n_Figure 4:_ You can efficiently extract many types of datasets from an initial base set of video links using video2dataset\n\nThe individual steps are:\n\n1. Download an HD video dataset for a generative video modeling project.\n2. Download 2 more datasets at various resolutions so you can increase your sample count\n3. Combine all 3 video datasets and downsample in resolution and FPS so it can be more easily stored.\n4. Train a contrastive video-text model on the downscaled, diverse dataset\n5. Extract audio and useful metadata out of the downscaled dataset.\n6. The audio can be used to train various audio models (generative or discriminative)\n7. The metadata can be used to filter the dataset. For example one could use the optical flow to filter out low-motion videos.\n8. We can further process the audio and extract transcripts (using our [WhisperX](https://github.com/m-bain/whisperX) subsampler)\n9. The transcripts can be used to train text-only or vision-text models\n\nDoing dataset curation using video2dataset is very convenient across projects since datasets with the same contents can share metadata shards - the audio dataset from step 6 can use the same captions as the contrastive video-text model in step 4; we may filter that audio dataset with the same optical flow scores produced in step 5.\n\n### Dataset processing jobs\n\nWe have used video2dataset to process many popular datasets and we include instructions for how to reproduce these jobs in the [dataset\\_examples section](https://github.com/iejMac/video2dataset/tree/main/dataset_examples) of the repository. One such dataset is [WebVid](https://m-bain.github.io/webvid-dataset/) (10M samples) which can be downloaded in 12h on a single cpu16 EC2 instance which costs 8.16$ in total.\nTo further test video2dataset’s capabilities, we create a large scale video-text dataset (590M pairs) by combining existing large datasets and performing extensive processing on them using video2dataset transformations. Specifically, we perform [scene detection](https://github.com/Breakthrough/PySceneDetect), clip according to those scenes, add synthetic captions and add optical flow estimates for each clip. The dataset will be released soon along with a discovery study on its applicability\n\n### Metadata and Statistics\n\nvideo2dataset can be used to gather various metadata and statistics about the processed data. Some subsamplers have the goal of taking a given modality (video, audio) and extracting metadata from it like compression/video information, optical flow scores, audio transcripts etc. Additionally during downloading if the source already has associated metadata, like f.e. Youtube videos do, video2dataset will try to extract that metadata and place it in the webdataset so you can later access it easily. Here are some examples:\n\n| Video | Optical Flow | Synthetic Caption | Whisper Transcript | YouTube Metadata |\n|---|---|---|---|---|\n| ![](/images/blog/video_metadata1.gif) | ![](/images/blog/video_metadata1_opt_flow.gif) | crowd of people at a music festival | {\"segments\": [{\"text\": \" Okay, hold tight everybody in the back. Hold tight everybody in the middle. Hold tight everybody\", \"start\": 0.008, \"end\": 5.257}], \"language\": \"en\"} | {..., \"title\": \"NassFestival [Bugzy Malone]\", \"categories\": [\"People \u0026 Blogs\"], \"tags\": [\"bugzy malone\", ...], \"view\\_count\": 3081, \"like\\_count\": 7, ...} |\n| ![](/images/blog/video_metadata2.gif) | ![](/images/blog/video_metadata2_opt_flow.gif) | the video shows you how to solve a multiplication problem | {\"segments\": [{\"text\": \" 1 2 1 6 7 is 3 for finding the remaining digit of the answer we need to divide the number into two parts that is in this problem the number is 1 2 1 6 7 so first part is 1 6 7 and second\", \"start\": 0.008, \"end\": 19.955}], \"language\": \"en\"} | {..., \"title\": \"How to Find Cube Root of Any Number without calculator How To Calculate Cube Roots In Your Head math\", \"categories\": [\"Education\"], \"tags\": [\"find cube root of a number\", ...], \"view\\_count\": 399080, \"like\\_count\": 5498, ...} |\n| ![](/images/blog/video_metadata3.gif) | ![](/images/blog/video_metadata3_opt_flow.gif) | a drone is flying over a field | No Speech | {..., \"title\": \"Nike Smoke\", \"categories\": [], \"tags\": [], \"view\\_count\": 8164, \"like\\_count\": 64, ...} |\n\n_YouTube provides a large amount of metadata for each video so we only select a few keys for display here. For a full example of a youtube metadata dictionary see [this example](https://github.com/iejMac/video2dataset/blob/main/examples/yt_metadata.md)._\n\n## What’s next?\n\n- Scientific analysis and release of a large scale dataset created with the tool presented in this blog post.\n- Improved synthetic captioning. Synthetic captioning for videos is still underexplored and there’s many exciting ideas to try. Soon in video2dataset we will have more interesting methods to produce captions for videos that make use of image captioning models and LLMs.\n- Since its release people have been talking about using [Whisper](https://arxiv.org/abs/2212.04356) to obtain many text tokens from video. This is possible with video2dataset and we are working on transcribing a large corpus of podcasts which we will soon release as a text dataset (we are aiming at 50B tokens).\n- Many exciting modeling ideas. Hopefully with the improvement of dataset curation tooling more people will attempt to push the SOTA in the video and audio modality.\n\n### Contributing\n\nvideo2dataset is a fully open-source project and we are committed to developing it in the open. This means all the relevant TODO’s and future directions can be found in the issues tab of the repository. Contributions are welcomed and the best way of doing that is to pick out an issue, address it, and submit a pull request.\n\n### License\n\nMIT\n\n### Contributions\n\nBig thanks to everyone involved, most notably:\n\n- [Romain](https://github.com/rom1504) for building out img2dataset, helping with the initial design of video2dataset, and giving lots of advice during the process of building video2dataset.\n- [Marianna](https://github.com/marianna13) for helping create the audio functionality.\n- [Daniel](twitter.com/danielmend_) for building the cut detection and optical flow capabilities. Also for extensive help with testing and runs at scale, and feedback on the blogpost.\n- [Andreas](https://github.com/ablattmann) for greatly improving the video2dataset dataloader and implementing slurm distribution.\n- [Sumith](https://github.com/Sumith1896) for implementing synthetic captioning and lots of help during writing the blogpost (especially with visualizations).\n","date":1688947200000},{"slug":"open-flamingo-v2","frontmatter":{"title":"OpenFlamingo v2: New Models and Enhanced Training Setup","author":"Anas Awadalla* and Irena Gao*","date":"June 28 2023","previewImg":"/images/blog/flamingo-family.png"},"content":"\n\n[[GitHub]](https://github.com/mlfoundations/open_flamingo) [[Demo]](https://huggingface.co/spaces/openflamingo/OpenFlamingo) [[Models]](https://huggingface.co/openflamingo)\n\nAbout three months ago, [we announced OpenFlamingo](https://laion.ai/blog/open-flamingo/), an open-source effort to replicate [DeepMind's Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model) models.\n\n**Today, we are excited to release five trained OpenFlamingo models across the 3B, 4B, and 9B scales.** These models are based on Mosaic’s [MPT-1B and 7B](https://www.mosaicml.com/blog/mpt-7b) and Together.xyz’s [RedPajama-3B](https://www.together.xyz/blog/redpajama-models-v1), meaning they are built on open-source models with less restrictive licenses than LLaMA.\nWhen averaging performance across 7 evaluation datasets, OpenFlamingo models attain more than 80% of the performance of their corresponding Flamingo model. OpenFlamingo-3B and OpenFlamingo-9B also attain more than 60% of fine-tuned SOTA performance using just 32 in-context examples.\n\nWe’ve also improved our open-source training and evaluation code, adding support for [Fully Sharded Data Parallel (FSDP)](https://engineering.fb.com/2021/07/15/open-source/fsdp/) and new datasets (TextVQA, VizWiz, HatefulMemes, and Flickr30k) to the evaluation suite.\n\n## Technical overview\n\n![](/images/blog/flamingo-samples.png)\n\nOpenFlamingo models process arbitrarily interleaved sequences of images and text to output text. This allows the models to accept in-context examples and solve tasks like captioning, visual question answering, and image classification. \n\nWe follow the [Flamingo modeling paradigm](https://arxiv.org/abs/2204.14198), augmenting the layers of a pretrained, frozen language model such that they cross-attend to visual features when decoding. Following Flamingo, we freeze the vision encoder and language model but train the connecting modules on web-scraped image-text sequences. Specifically, we use a mixture of [LAION-2B](https://arxiv.org/abs/2210.08402) and [Multimodal C4](https://arxiv.org/abs/2304.06939). **\n\n** The 4B-scale models were also trained on experimental ChatGPT-generated (image, text) sequences, where images were pulled from LAION. We are working to release these sequences soon.\n\n## Model release\n\nWe have trained five OpenFlamingo models across the 3B, 4B, and 9B parameter scales. These models build off of OpenAI’s CLIP ViT-L/14 as a vision encoder and open-source language models from MosaicML and Together.xyz. At the 3B and 4B scales, we have trained models both with standard and instruction-tuned language model backbones.\n\n|# params|Language model| (Language) instruction tuned? |\n|---|---|---|\n|3B| mosaicml/mpt-1b-redpajama-200b | No |\n|3B| mosaicml/mpt-1b-redpajama-200b-dolly | Yes |\n|4B| togethercomputer/RedPajama-INCITE-Base-3B-v1 | No |\n|4B| togethercomputer/RedPajama-INCITE-Instruct-3B-v1 | Yes |\n|9B| mosaicml/mpt-7b | No |\n\nNote that as part of the move to v2, we are deprecating our previous LLaMA-based checkpoint. However, you can continue to use our older checkpoint using the new codebase.\n\n## Evaluation\n\nWe evaluated our models on vision-language datasets across captioning, VQA, and classification tasks. As shown below, the OpenFlamingo-9B v2 model shows considerable improvement over our v1 release.\n\n![](/images/blog/flamingo-v1-vs-v2.png)\n\nBelow, we aggregate model performances across 7 evaluation datasets and 5 in-context evaluation settings (0-shot, 4-shot, 8-shot, 16-shot, and 32-shot). Averaged across these settings, OpenFlamingo (OF) models attain more than 80% of corresponding Flamingo performance, where we compare OF-3B and OF-4B with Flamingo-3B, and OF-9B with Flamingo-9B. Error bars are standard deviations over datasets and evaluation settings.\n\n![](/images/blog/flamingo-avg-performance.png)\n\nNext, we report performance relative to fine-tuned SoTAs listed on PapersWithCode\\*. With 32 in-context examples, OpenFlamingo-3B and OpenFlamingo-9B models attain more than 55% of fine-tuned performance, despite only being pre-trained on web data. On average, OpenFlamingo models trail their DeepMind counterparts by around 10% 0-shot and 15% 32-shot.  \n\n![](/images/blog/flamingo-aggregated-performance.png)\n\nBelow, we present per-dataset results. We observe that on some datasets, OpenFlamingo models (especially the 4B ones) generalize poorly across the number of in-context examples. We hypothesize that this behavior stems from the quality of our pre-training data.\n\n![](/images/blog/flamingo-evaluations.png)\n\n\\* numbers pulled on Monday, June 19\n\n## Next steps and codebase updates\n\nOpenFlamingo remains an active research project, and we continue to work on training and releasing high-quality multimodal models. As next steps, we’re particularly interested in improving the quality of our pre-training data.\n\nWe were thrilled by the many cool projects building off of our first release such as [Otter](https://github.com/Luodian/Otter) and [Multimodal-GPT](https://github.com/open-mmlab/Multimodal-GPT). We encourage the community to continue using OpenFlamingo. To make training OpenFlamingo models more accessible, we have added support for [Fully Sharded Data Parallel (FSDP)](https://engineering.fb.com/2021/07/15/open-source/fsdp/) and gradient checkpointing. For context, in experiments, we were able to fit a 9B scale OpenFlamingo model on 8 A100 40GB gpus using FSDP and gradient checkpointing. We’ve also added scripts for packaging our training data into the expected Webdataset form.\n\nIf you’re interested in contributing to our codebase, including our evaluation suite, please join us at the [OpenFlamingo github repository](https://github.com/mlfoundations/open_flamingo).\n\n**Safety and ethical considerations**\n\nOpenFlamingo models inherit the risks of their parent models, especially the language model. As an open-source research effort, we highly value open, accessible, reproducible multimodal model research; however, it is crucial to be aware that these models are trained on web data and have not been finetuned for safety, and thus may produce unintended, inappropriate, unreliable, and/or inaccurate outputs. Please use caution before deploying OpenFlamingo models in real applications. We also hope that OpenFlamingo enables further safety and reliability research to address these issues.\n\n\n### Contributors\nThanks to:\n[Josh Gardner](https://homes.cs.washington.edu/~jpgard/), [Jack Hessel](https://jmhessel.com/), [Yusuf Hanafy](https://www.linkedin.com/in/yusufhanafy/), [Wanrong Zhu](https://wanrong-zhu.com/), [Kalyani Marathe](https://kalyani7195.github.io/), [Yonatan Bitton](https://yonatanbitton.github.io/), [Samir Gadre](https://sagadre.github.io/), [Shiori Sagawa](https://cs.stanford.edu/~ssagawa/), [Jenia Jitsev](https://scholar.google.de/citations?user=p1FuAMkAAAAJ\u0026hl=en), [Simon Kornblith](https://simonster.com/), [Pang Wei Koh](https://koh.pw/), [Gabriel Ilharco](https://gabrielilharco.com/), [Mitchell Wortsman](https://mitchellnw.github.io/), and [Ludwig Schmidt](https://people.csail.mit.edu/ludwigs/)\n\n### Acknowledgements\n\nWe would like to thank Jean-Baptiste Alayrac and Antoine Miech for their advice and Stability AI for providing us with compute resources to train these models.\n","date":1687910400000},{"slug":"datacomp","frontmatter":{"title":"Announcing DataComp: In search of the next generation of multimodal datasets","author":"Gabriel Ilharco","date":"April 27, 2023","previewImg":"/images/blog/datacomp_samples.png"},"content":"\n\n[[ Paper ]](https://arxiv.org/abs/2304.14108) [[ Code ]](https://github.com/mlfoundations/datacomp) [[ Website ]](http://datacomp.ai/)\n\nAbout a year ago, we released [LAION-5B](https://laion.ai/blog/laion-5b/), a billion-scale open-source image-text dataset. Since then, LAION-5B has become a staple in the open-source machine learning ecosystem, powering open-source models like [OpenCLIP](https://github.com/mlfoundations/open_clip), [OpenFlamingo](https://github.com/mlfoundations/open_flamingo), and [Stable Diffusion](https://github.com/Stability-AI/stablediffusion). From the beginning, we viewed LAION-5B as only the first step on this research journey and hoped that we can build the next generation of multimodal datasets both rigorously and collaboratively in the open as a research community.\n\nToday, we are proud to introduce **DataComp, a new benchmark for designing multimodal datasets**. Unlike traditional benchmarks focused on modeling improvements, DataComp puts data front and center. In Datacomp, participants innovate by proposing new training sets, leaving the training code, hyper-parameters and compute fixed. As part of our competition, we are releasing **CommonPool, the largest public collection of image-text pairs to date with 12.8B samples**.\n\nAlong with our pool, we also release **DataComp-1B**, a 1.4B subset that can be used to outperform compute-matched CLIP models from OpenAI and LAION. DataComp-1B makes it possible to train a CLIP ViT-L model to better performance than a larger ViT-g model trained on LAION-2B while using 9x less training compute. Our ViT-L/14 trained on DataComp-1B obtains 79.2% zero-shot accuracy on ImageNet, substantially outperforming OpenAI's model trained with the same compute (75.5% zero-shot accuracy).\n\nDataComp-1B is only the first dataset coming out of DataComp. We are beyond excited to continue learning about designing better datasets, and we invite you to join us on this journey! \n\n![](/images/blog/datacomp_samples.png)\n\n## Overview\n\nIn the past two years, we have seen multiple breakthroughs in multimodal learning. From [CLIP](https://openai.com/research/clip), [DALL-E](https://openai.com/product/dall-e-2) and [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release) to [Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model) and [GPT-4](https://openai.com/research/gpt-4), multimodal models now show impressive generalization such as zero-shot image classification and in-context learning. Large-scale datasets have been instrumental for building these models. However, despite their importance, datasets rarely receive the same attention as model architectures or training algorithms. DataComp addresses this shortcoming in the machine learning ecosystem by introducing a benchmark where participants can rigorously explore design decisions for datasets.\n\nTowards this goal, we present **CommonPool**, a large-scale dataset with 12.8B image-text pairs collected from the web—the largest dataset of its kind by a factor of 2.5x. Despite the size, our benchmark features multiple data and compute scales, and is designed to encourage participation even with a single GPU at the smallest scale. \n\nTogether with our dataset, we present simple filtering baselines that already improve upon existing methods such as the filtering used in LAION-2B. As a highlight, **DataComp-1B**, a 1.4B subset of our pool can be used to train a CLIP ViT-L/14 to 79.2% zero-shot accuracy on ImageNet. This model outperforms a ViT-g/14 model trained on LAION-2B by 0.7 percentage points, despite being trained with 9x less compute. It also performs substantially better than OpenAI’s ViT-L/14 model trained with the same compute budget, which gets 75.5% zero-shot accuracy on ImageNet. \n\nWe are beyond excited to continue learning about designing better datasets, and we invite you to join us in this journey!\n\n## DataComp\n\nIn DataComp, your goal is to design a dataset that produces the best possible CLIP model at a fixed compute budget. DataComp is designed with scale in mind. When participating, the first step is to choose one of the *small*, *medium*, *large*, or *xlarge* scales. Each scale comes with a corresponding pool, ranging from 12.8M samples to 12.8B samples. You can use data from that pool, or from any external data source to build your dataset depending on the competition track. After that, train your CLIP model using our public implementation of a fixed training protocol, and evaluate on our diverse suite of 38 downstream tasks. \n\n![](/images/blog/datacomp_workflow.png)\n\n\n**Competition tracks:** Our competition features two tracks. In the first track, CommonPool, you are only allowed to filter data from the pool we provide. On the second track, Bring Your Own Data (BYOD), you are allowed to use any data you want, as long as it doesn’t overlap with our evaluation suite. \n\n**Preprocessing and safety:** We kept preprocessing of our pool to a minimum to provide a blank slate for participants. Our only initial preprocessing steps are to eliminate images that are flagged due to safety considerations or that appear in downstream evaluation datasets to avoid contamination. For the former, we take steps to eliminate illegal and explicit content and to protect the privacy of individuals, removing unsafe images and captions with automated filters and obfuscating faces in the candidate images we provide.\n\n**Multiple scales:** To facilitate the study of scaling trends and accommodate participants with various levels of resources, DataComp features multiple scales of data and compute. The compute requirements vary from around 8 GPU hours at the smallest scale to over 40,000 at the largest. The pool we provide varies accordingly, from 12.8M to 12.8B samples.\n\n**Standardized training:** In order to enable controlled and comparable experiments, we fix the training procedure (i.e., model architecture, optimizer, loss, hyperparameters, etc.) and compute at each scale, closely following training recipes used to train state-of-the-art CLIP models from scratch.\n\n**Evaluation:** We evaluate on a diverse set of 38 downstream image classification and retrieval tasks, including distribution shifts and geographic diversity. For efficiency and simplicity, we evaluate models in a zero-shot setting, without fine-tuning on data from the downstream tasks.\n\n**Rules:** We allow the use of any public data that does not overlap with our evaluation tasks. See Appendix A of [our paper](https://arxiv.org/abs/2304.14108) for more details.\n\n\n## What we know so far\n\nIn our [paper](https://arxiv.org/abs/2304.14108), we present hundreds of baseline experiments with different dataset design algorithms. A key finding is that smaller, *more aggressively filtered datasets can perform better than larger datasets coming from the same pool.* As a highlight, we find a subset of our largest pool that performs substantially better than LAION-2B, despite being smaller (1.4B samples).\n\nThis subset, DataComp-1B, can be used to train a ViT-L/14 to 79.2% zero-shot accuracy on ImageNet, which outperforms a ViT-g/14 model trained on LAION-2B by 0.7 percentage points, despite being trained with 9x less compute. Moreover, our model performs substantially better than other ViT-L/14 models trained with the same compute budget including OpenAI's model, as seen below. \n\n| Training data   | Dataset size  | # samples seen | ImageNet Acc. | Avg. performance (38 datasets) |\n|-----------------|---------------|----------------|---------------|--------------------------------|\n| OpenAI's WIT    | 0.4B          | 13B            | 75.5          | 0.61                           |\n| LAION-400M      | 0.4B          | 13B            | 73.1          | 0.58                           |\n| LAION-2B        | 2.3B          | 13B            | 73.1          | 0.59                           |\n| LAION-2B        | 2.3B          | 34B            | 75.2          | 0.61                           |\n| DataComp-1B     | 1.4B          | 13B            | 79.2          | 0.66                           |\n\n\nWe also show that the ranking of many curation approaches is consistent across the different scales. For example, across the baselines we study, the rank correlation between ImageNet accuracy at small and large scales is 0.9. This suggests that experiments at smaller scales can provide valuable insights for larger scales, thereby accelerating investigations.\n\n![](/images/blog/datacomp_scaling_plot.png)\n\nThere is much more in the paper, and we think this is only the beginning. We hope you’ll join us in designing the next generation of multimodal datasets!\n\n## How do I start?\n\nThe best starting point is our [github repo](https://github.com/mlfoundations/datacomp), which contains code for downloading our pools, training and evaluating models.\n\nOur website [www.datacomp.ai/](https://www.datacomp.ai/) contains further documentation.\n\n## DataComp ICCV Workshop\n\nIn conjunction with DataComp, we are also organizing a workshop at [ICCV 2023](https://iccv2023.thecvf.com/), titled **Towards the Next Generation of Computer Vision Datasets**. The workshop will showcase a series of DataComp submissions, along with other data-centric papers and multiple invited talks by experts in the field. Our call for papers is available at [https://www.datacomp.ai/workshop.html](https://www.datacomp.ai/workshop.html).\n\n## Acknowledgements\n\nWe thank all of our paper authors: Samir Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt.\n\nWe also thank Amro Abbas, Jessie Chapman, Brian Cheung, Joshua Gardner, Nancy Garland, Sachin Goyal, Huy Ha, Zaid Harchaoui, Andy Jones, Adam Klivans, Daniel Levy, Ronak Mehta, Ari Morcos, Raviteja Mullapudi, Kentrell Owens, Alec Radford, Marco Tulio Ribeiro, Shiori Sagawa, Christoph Schuhmann, Matthew Wallingford, and Ross Wightman for helpful feedback at various stages of the project.\n\nA special thanks to Stability AI and the Gauss Centre for Supercomputing e.V (compute time granted on JUWELS Booster hosted at Juelich Supercomputing Center) for providing us with compute resources to train models, without which none of this would have been possible.","date":1682553600000},{"slug":"paella","frontmatter":{"title":"A new Paella: Simple \u0026 Efficient Text-To-Image generation","author":"Dominic Rampas and Pablo Pernias","date":"April 15, 2023","previewImg":"/images/blog/paella.png"},"content":"![](https://user-images.githubusercontent.com/61938694/231021615-38df0a0a-d97e-4f7a-99d9-99952357b4b1.png)\n### Overview.\nWe are releasing a new Paella model which builds on top of our initial paper https://arxiv.org/abs/2211.07292.\nPaella is a text-to-image model that works in a quantized latent space and learns similarly to MUSE and Diffusion models.\nPaella is similar to MUSE as it also works on discrete tokens, but is different in the way tokens are noised as well as\nthe architecture. MUSE uses a transformer, whereas we use a CNN, which comes with many benefits. There are also subtle\ndifferences in the conditioning Paella uses as well how images are sampled. And on the other hand, it can also be seen\nas a discrete diffusion process, which noises images during training and iteratively removes noise during sampling.\nSince the paper-release we worked intensively to bring Paella to a similar level as other \nstate-of-the-art models. With this release we are coming a step closer to that goal. However, our main intention is not\nto make the greatest text-to-image model out there (at least for now), it is to bring text-to-image models closer\nto people outside the field on a technical basis. For example, many models have codebases with many thousand lines of \ncode, that make it pretty hard for people to dive into the code and easily understand it. And that is our proudest\nachievement with Paella. The training and sampling code for Paella is minimalistic and can be understood in \na few minutes, making further extensions, quick tests, idea testing etc. extremely fast. For instance, the entire\nsampling code can be written in just **12 lines** of code.\nIn this blog post we will talk about how Paella works in short, give technical details and release the model.\n\n### How does Paella work?\nPaella works in a quantized latent space, just like StableDiffusion etc., to reduce the computational power needed.\nImages are encoded to a smaller latent space and converted to visual tokens of shape *h x w*. During training,\nthese visual tokens are noised, by replacing a random amount of tokens with other randomly selected tokens\nfrom the codebook of the VQGAN. The noised image are given to the model, along with a timestep and the conditional\ninformation, which is text in our case. The model is tasked to predict the un-noised version of the tokens. \nAnd that's it. The model is optimized with the CrossEntropy loss between the original tokens and the predicted tokens.\nThe amount of noise added during the training is just a linear schedule, meaning that we uniformly sample a percentage \nbetween 0 and 100% and noise that amount of tokens.\u003cbr\u003e\u003cbr\u003e\n\n\u003cfigure\u003e\n  \u003cimg src=\"https://user-images.githubusercontent.com/61938694/231248435-d21170c1-57b4-4a8f-90a6-62cf3e7effcd.png\" width=\"400\"\u003e\n  \u003cfigcaption\u003eImages are noised and then fed to the model during training.\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\nSampling is also extremely simple, we start with the entire image being random tokens. Then we feed the latent image, \nthe timestep and the condition into the model and let it predict the final image. The models outputs a distribution\nover every token, which we sample from with standard multinomial sampling.  \nSince there are infinite possibilities for the result to look like, just doing a single step results in very basic \nshapes without any details. That is why we add noise to the image again and feed it back to the model. And we repeat\nthat process for a number of times, with less noise being added every time, and slowly get our final image.\nYou can see how images emerge [here](https://user-images.githubusercontent.com/61938694/231252449-d9ac4d15-15ef-4aed-a0de-91fa8746a415.png).\u003cbr\u003e\nThe following is the entire sampling code needed to generate images:\n```python\ndef sample(model_inputs, latent_shape, unconditional_inputs, steps=12, renoise_steps=11, temperature=(0.7, 0.3), cfg=8.0):\n    with torch.inference_mode():\n        sampled = torch.randint(low=0, high=model.num_labels, size=latent_shape)\n        initial_noise = sampled.clone()\n        timesteps = torch.linspace(1.0, 0.0, steps+1)\n        temperatures = torch.linspace(temperature[0], temperature[1], steps)\n        for i, t in enumerate(timesteps[:steps]):\n            t = torch.ones(latent_shape[0]) * t\n\n            logits = model(sampled, t, **model_inputs)\n            if cfg:\n                logits = logits * cfg + model(sampled, t, **unconditional_inputs) * (1-cfg)\n            sampled = logits.div(temperatures[i]).softmax(dim=1).permute(0, 2, 3, 1).reshape(-1, logits.size(1))\n            sampled = torch.multinomial(sampled, 1)[:, 0].view(logits.size(0), *logits.shape[2:])\n\n            if i \u003c renoise_steps:\n                t_next = torch.ones(latent_shape[0]) * timesteps[i+1]\n                sampled = model.add_noise(sampled, t_next, random_x=initial_noise)[0]\n    return sampled\n```\n\n### Results\n\u003cimg src=\"https://user-images.githubusercontent.com/61938694/231598512-2410c172-5a9d-43f4-947c-6ff7eaee77e7.png\"\u003e\nSince Paella is also conditioned on CLIP image embeddings the following things are also possible:\u003cbr\u003e\u003cbr\u003e\n\u003cimg src=\"https://user-images.githubusercontent.com/61938694/231278319-16551a8d-bfd1-49c9-b604-c6da3955a6d4.png\"\u003e\n\u003cimg src=\"https://user-images.githubusercontent.com/61938694/231287637-acd0b9b2-90c7-4518-9b9e-d7edefc6c3af.png\"\u003e\n\u003cimg src=\"https://user-images.githubusercontent.com/61938694/231287119-42fe496b-e737-4dc5-8e53-613bdba149da.png\"\u003e\n\n### Technical Details.\nModel-Architecture: U-Net (Mix of ConvNeXt, DiT etc.) \u003cbr\u003e\nDataset: Laion-A, Laion Aesthetic \u003e 6.0 \u003cbr\u003e\nTraining Steps: 1.3M \u003cbr\u003e\nBatch Size: 2048 \u003cbr\u003e\nResolution: 256 \u003cbr\u003e\nVQGAN Compression: f4 \u003cbr\u003e\nCondition: ByT5-XL (95%), CLIP-H Image Embedding (10%), CLIP-H Text Embedding (10%)\nOptimizer: AdamW\nHardware: 128 A100 @ 80GB \u003cbr\u003e\nTraining Time: ~3 weeks \u003cbr\u003e\nLearning Rate: 1e-4 \u003cbr\u003e\nMore details on the approach, training and sampling can be found in paper and on GitHub.\n\n### Paper, Model, Code Release\nPaper: https://arxiv.org/abs/2211.07292 \u003cbr\u003e\nCode: https://github.com/dome272/Paella \u003cbr\u003e\nModel: https://huggingface.co/dome272/Paella \u003cbr\u003e\n\n\n### Limitations \u0026 Conclusion\nThere are still many things to improve for Paella to get on par with standard diffusion models or to even outperform\nthem. One primary thing we notice is that even though we only condition the model on CLIP image embedding 10% of the\ntime, during inference the model heavily relies on the generated image embeddings by a prior model (mapping clip text\nembeddings to image embeddings as proposed in Dalle2). We counteract this by decreasing the importance of the image\nembeddings by reweighing the attention scores. There probably is a way to avoid this happening already in training.\nOther limitations such as lack of composition, text depiction, unawareness of concepts etc. could also be reduced by\ncontinuing the training for longer. As a reference, Paella has only seen as many images as SD 1.4 and due to concerns \nin regard to training collapse (which later turned to be negligible), trained with a 10x lower learning rate for the \nfirst 700k steps. To conclude, this is still work in progress, but our first model that works reasonably well and\na million times better than the first versions we trained months ago.\n\nIt is noteworthy that the design choices for Paella were based on trying to make a simple architecture and \nmodel for text-to-image synthesis, drawing inspiration from existing techniques such as MaskGIT. Furthermore, this \napproach eliminates the need for hyperparameters such as alpha, beta, and alpha_cum_prod, which are typically required \nin diffusion models. As a result, this methodology is particularly well-suited for individuals who are new to the field \nof generative artificial intelligence. Our aim is to lay the groundwork for future research in this domain, fostering\na landscape where AI is accessible and comprehensible to a broader audience. We encourage further exploration of this\napproach, as we are confident in its potential to contribute useful insights and potentially advance the state of the \nart in text-to-image synthesis.\n\n\n### Contributions\n\n**Thanks to:**\n\n* [Romain Beaumont](https://github.com/rom1504/) and [Christoph Schuhmann](https://github.com/christophschuhmann) \nfor constant help on datasets and giving useful advice.\n* [Jenia Jitsev](https://scholar.google.de/citations?user=p1FuAMkAAAAJ\u0026hl=en) for help on writing the blog post and\nuseful discussions.\n* [Richard Vencu](https://github.com/rvencu) for an incredible amount of help regarding hardware issues.\n* [StabilityAI](https://stability.ai/) for providing GPU-Cluster access and faith in Paella.\n\n","date":1681516800000},{"slug":"petition","frontmatter":{"title":"Petition for keeping up the progress tempo on AI research while securing its transparency and safety.","author":"LAION.ai","date":"March 29 2023","previewImg":"/images/blog/laion-blue.png"},"content":"\n[LINK TO OUR PETITION](https://www.openpetition.eu/petition/online/securing-our-digital-future-a-cern-for-open-source-large-scale-ai-research-and-its-safety)\n\n_Authors: Christoph Schuhmann, Huu Nguyen, Robert Kaczmarczyk, Jenia Jitsev \u0026 LAION community_\n\n## Securing Our Digital Future: Calling for CERN like international organization to transparently coordinate and progress on large-scale AI research and its safety\n\nIn an era of unparalleled technological advancements, humanity stands on the precipice of a new epoch characterized by the profound influence of artificial intelligence (AI) and its foundational models, such as GPT-4. The potential applications of these technologies are vast, spanning scientific research, education, governance, and small and medium-sized enterprises. To harness their full potential as tools for societal betterment, it is vital to democratize research on and access to them, lest we face severe repercussions for our collective future.\n\n### Dominance of few large corporations in AI development\n\nIncreasingly, we are witnessing the emergence of a system wherein educational institutions, government agencies, and entire nations become dependent on the AI technology of a select few large corporations that operate with little transparency or public accountability. To secure our society's technological independence, foster innovation, and safeguard the democratic principles that underpin our way of life, we must act now.\nWe call upon the global community, particularly the European Union, the United States, the United Kingdom, Canada, Australia and other willing countries, to collaborate on a monumental initiative: the establishment of an international, publicly funded, open-source supercomputing research facility. This facility, analogous to the CERN project in scale and impact, should house a diverse array of machines equipped with at least 100,000 high-performance state-of-the-art accelerators (GPUs or ASICs), operated by experts from the machine learning and supercomputing research community and overseen by democratically elected institutions in the participating nations.\n\n### Multimodal future\n\nThis ambitious endeavor will provide a platform for researchers and institutions worldwide to access and refine advanced AI models, such as GPT-4, harnessing their capabilities for the greater good. By making these models open source and incorporating multimodal data (audio, video, text, and program code), we can significantly enrich academic research, enhance transparency, and ensure data security. Furthermore, granting researchers access to the underlying training data will enable them to understand precisely what these models learn and how they function, an impossibility when restricted by APIs.\nAdditionally, the open-source nature of this project will promote safety and security research, allowing potential risks to be identified and addressed more rapidly and transparently by the academic community and open-source enthusiasts. This is a vital step in ensuring the safety and reliability of AI technologies as they become increasingly integrated into our lives.\nThe proposed facility should feature AI Safety research labs with well-defined security levels, akin to those used in biological research labs, where high-risk developments can be conducted by internationally renowned experts in the field, backed by regulations from democratic institutions. The results of such safety research should be transparent and available for the research community and society at large. These AI Safety research labs should be capable of designing timely countermeasures by studying developments that, according to broad scientific consensus, would predictably have a significant negative impact on our societies.\n\n### Economic impact\n\nEconomically, this initiative will bring substantial benefits to small and medium-sized companies worldwide. By providing access to large foundation models, businesses can fine-tune these models for their specific use cases while retaining full control over the weights and data. This approach will also appeal to government institutions seeking transparency and control over AI applications in their operations.\nThe importance of this endeavor cannot be overstated. We must act swiftly to secure the independence of academia and government institutions from the technological monopoly of large corporations in AI research. Technologies like GPT-4 are too powerful and significant to be exclusively controlled by a select few.\nIn a world where machine learning expertise and resources for AI development become increasingly concentrated in large corporations, it is imperative that smaller enterprises, academic institutions, municipal administrations, and social organizations, as well as nation-states, assert their autonomy and refrain from relying solely on the benevolence of these powerful entities that are often driven by short-term profit interests and act without properly taking democratic institutions into their decision-making loop. We must take immediate and decisive action to secure the technological independence of our society, nurturing innovation while ensuring the safety of these developments and protecting the democratic principles that form the foundation of our way of life.\n\n### Safety measures\n\nThe [recent proposition](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) of decelerating AI research as a means to ensure safety and progress presents an understandable but untenable  approach that will be detrimental to both objectives. Corporate or state actors will make advancements in the dark while simultaneously curtailing the public research community's ability to scrutinize the safety aspects of advanced AI systems thoroughly. Rather than impeding the momentum of public AI development, a more judicious and efficacious approach would be to foster a better-organized, transparent, safety-aware, and collaborative research environment. The establishment of transparent open-source AI safety labs tied to the international large-scale AI research facility as described above, which employ eligible AI safety experts, have corresponding publicly funded compute resources, and act according to regulations issued by democratic institutions, will cover the safety aspect without dampening progress. By embracing this cooperative framework, we can simultaneously ensure progress and the responsible development of AI technology, safeguarding the well-being of our society and the integrity of democratic values.\n\n### What you can do\n\nWe urge you to join us in this crucial campaign. Sign [this petition](https://www.openpetition.eu/petition/online/securing-our-digital-future-a-cern-for-open-source-large-scale-ai-research-and-its-safety) and make your voice heard. Our collective digital future, the autonomy of our academic research, and the equilibrium of our global economy depend on our ability to act quickly and decisively.\nTogether, we can build a future where advanced AI technologies are accessible to all, and where innovation and progress are not constrained by the boundaries of a few powerful corporations. Let us seize this opportunity and build a brighter future for generations to come.\n","date":1680048000000},{"slug":"open-flamingo","frontmatter":{"title":"Announcing OpenFlamingo: An open-source framework for training vision-language models with in-context learning","author":"Anas Awadalla and Irena Gao","date":"Mar 28 2023","previewImg":"/images/blog/flamingo-logo.png"},"content":"\n**Overview.**\nWe are thrilled to announce the release of OpenFlamingo, an open-source reproduction of DeepMind's Flamingo model. At its core, OpenFlamingo is a framework that enables training and evaluation of large multimodal models (LMMs). Check out our [GitHub repository](https://github.com/mlfoundations/open_flamingo) and [demo](https://7164d2142d11.ngrok.app) to get started!\n\nFor this first release, our contributions are as follows:\n\n* 🏋️ A Python framework to train Flamingo-style LMMs (based on Lucidrains' [flamingo implementation](https://github.com/lucidrains/flamingo-pytorch) and David Hansmair's [flamingo-mini repository](https://github.com/dhansmair/flamingo-mini)).\n* 🪅 A large-scale multimodal dataset with interleaved image and text sequences.\n* 🧪 An in-context learning evaluation benchmark for vision-language tasks.\n* 🤖 A first version of our OpenFlamingo-9B model based on LLaMA, with much better models to come!\n\n\nThe recent progress in open-source LMMs with the release of [BLIP-2](https://arxiv.org/abs/2301.12597) and [FROMAGe](https://jykoh.com/fromage) has shown the exciting potential of multimodal systems. We hope that OpenFlamingo will help drive progress in multimodal machine learning, and we have more exciting contributions in the pipeline, so stay tuned! \n\n\n**Goal.**\nOur goal with OpenFlamingo is to develop a multimodal system that can tackle a diverse range of vision-language tasks. Ultimately, we aim to match the power and versatility of GPT-4 in handling visual and text input. To achieve this goal, we are creating an open-source version of [DeepMind's Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model) model, a LMM capable of processing and reasoning about images, videos, and text. We are committed to build fully open-source models, and believe this transparency is essential for fostering collaboration, accelerating progress, and democratizing access to state-of-the-art LMMs. Our release is the first step towards this goal.\n\nWe are sharing the first checkpoint of our OpenFlamingo-9B model. While the model is not yet fully optimized, it demonstrates the potential of this project. By working together and receiving feedback from the community, we can train better LMMs. We encourage the community to participate in the development process by providing feedback and contributing to the repository. \n\n\n**Technical Details.**\nOur implementation largely follows that of [Flamingo](https://arxiv.org/abs/2204.14198). Flamingo models are trained on large-scale web corpora containing interleaved text and images, which is crucial for endowing them with in-context few-shot learning capabilities. OpenFlamingo implements the same architecture (Perceiver resamplers, cross-attention layers) proposed in the original Flamingo paper. However, since the training data for Flamingo is not available to the public, we use open-source datasets for training our models. Specifically, the released OpenFlamingo-9B checkpoint is trained on 5M samples from our new Multimodal C4 dataset and 10M samples from [LAION-2B](https://huggingface.co/datasets/laion/laion2B-en). \n\n\n## **Multimodal C4**\n\nThe Multimodal-C4 dataset is an expansion of the text-only [C4 dataset](https://www.tensorflow.org/datasets/catalog/c4), which was used to train  [T5 models](https://arxiv.org/abs/1910.10683). This dataset is built by our collaborators [Jack Hessel](https://jmhessel.com) and [Wanrong Zhu](https://wanrong-zhu.com) at the Allen Institute for AI. For each document in the [C4 en.clean](https://www.tensorflow.org/datasets/catalog/c4#c4en_default_config) dataset, we retrieve the original webpage from [Common Crawl](https://commoncrawl.org/), then collect the downloadable images. Data cleaning is carried out through deduplication and content filtering, which aims to eliminate non-safe for work (NSFW) and unrelated images, such as advertisements. Additionally, we run face detection and discard images with positive identifications. Finally, images and sentences are interleaved using bipartite matching within a document: CLIP ViT/L-14 image-text similarities serve as edge weights. Multimodal-C4 consists of approximately 75 million documents, encompassing around 400M images and 38B tokens. A full release with more detail is coming soon.\n\n![](/images/blog/mmc4-example.png)\n\n## **Benchmark**\n\nTo measure the performance of OpenFlamingo, we evaluate on a diverse set of downstream tasks. Our aim is to eventually build an open-source version of Flamingo’s benchmark and extend past that to standardize vision-language task evaluation. Currently we support visual question-answering ([VQAv2](https://visualqa.org/index.html), [OK-VQA](https://okvqa.allenai.org)), captioning ([COCO](https://cocodataset.org/#home), [Flickr30k](https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset)), and image classification ([ImageNet](https://image-net.org/index.php)) tasks. Expect us to add many more evaluation sets that probe model reasoning, biases, and more! You can access the benchmark on the OpenFlamingo repo. \n\n\n## **Model release**\n\n![](/images/blog/flamingo-llama.png)\n\nAs part of our release, we are also providing a checkpoint from our under-development OpenFlamingo-9B, a LMM built on top of [LLaMA 7B](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) and [CLIP ViT/L-14](https://openai.com/research/clip). This model is still a work in progress but it can already bring a lot of value to the community. For instance,\n\n![](/images/blog/flamingo-9B-sample-one.png)\n![](/images/blog/flamingo-9B-sample-two.png)\n\n**Performance**\n\nWe evaluated our checkpoint on COCO and VQAv2. Here we report the validation performance using a different number of shots. \n\nCOCO (CIDEr)\n\u003ctable\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e\n   \u003c/td\u003e\n   \u003ctd\u003e0-shot\n   \u003c/td\u003e\n   \u003ctd\u003e4-shot\n   \u003c/td\u003e\n   \u003ctd\u003e8-shot\n   \u003c/td\u003e\n   \u003ctd\u003e16-shot\n   \u003c/td\u003e\n   \u003ctd\u003e32-shot\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eOpenFlamingo-9B*\n   \u003c/td\u003e\n   \u003ctd\u003e65.5\n   \u003c/td\u003e\n   \u003ctd\u003e74.3\n   \u003c/td\u003e\n   \u003ctd\u003e79.3\n   \u003c/td\u003e\n   \u003ctd\u003e81.8\n   \u003c/td\u003e\n   \u003ctd\u003e84.5\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eDeepMind Flamingo-9B\n   \u003c/td\u003e\n   \u003ctd\u003e79.4\n   \u003c/td\u003e\n   \u003ctd\u003e93.1\n   \u003c/td\u003e\n   \u003ctd\u003e99.0\n   \u003c/td\u003e\n   \u003ctd\u003e102.2\n   \u003c/td\u003e\n   \u003ctd\u003e106.3\n   \u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n---\n\nVQAv2 (VQA accuracy)\n\u003ctable\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e\n   \u003c/td\u003e\n   \u003ctd\u003e0-shot\n   \u003c/td\u003e\n   \u003ctd\u003e4-shot\n   \u003c/td\u003e\n   \u003ctd\u003e8-shot\n   \u003c/td\u003e\n   \u003ctd\u003e16-shot\n   \u003c/td\u003e\n   \u003ctd\u003e32-shot\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eOpenFlamingo-9B*\n   \u003c/td\u003e\n   \u003ctd\u003e43.5\n   \u003c/td\u003e\n   \u003ctd\u003e44.0\n   \u003c/td\u003e\n   \u003ctd\u003e47.5\n   \u003c/td\u003e\n   \u003ctd\u003e48.9\n   \u003c/td\u003e\n   \u003ctd\u003e50.3\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eDeepMind Flamingo-9B\n   \u003c/td\u003e\n   \u003ctd\u003e51.8\n   \u003c/td\u003e\n   \u003ctd\u003e56.3\n   \u003c/td\u003e\n   \u003ctd\u003e58.0\n   \u003c/td\u003e\n   \u003ctd\u003e59.4\n   \u003c/td\u003e\n   \u003ctd\u003e60.4\n   \u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n\n*Note that we report validation performance (using the same setup outlined in Flamingo paper) for OpenFlamingo-9B while DeepMind Flamingo-9B performance is on test data.\n\n**Safety and ethical considerations**\n\nAs OpenFlamingo-9B is built on top of frozen [LLaMA](https://arxiv.org/abs/2302.13971) and [CLIP](https://arxiv.org/abs/2103.00020) models, you can expect OpenFlamingo to inherit the harms of the parent models. We understand that by releasing these models, they may be used in harmful ways. However, it is important for the research community to study the harms of large multimodal models, and we believe that open-sourcing these models will enable the community to develop better ways to mitigate these harms in future models.\n\nWe emphasize that OpenFlamingo-9B is a research artifact and not a finished product. It can produce unintended, inappropriate, offensive, and/or inaccurate results. We thus advocate for caution and thorough evaluations before using our models in any real applications.\n\n\n### Contributions\n\n**Thanks to:**\n\n* [Josh Gardner](https://homes.cs.washington.edu/~jpgard/) and [Yonatan Bitton](https://yonatanbitton.github.io/) for implementing the evaluation benchmark.\n* [Kalyani Marathe](https://kalyani7195.github.io/) for implementing the data pipeline and improving code quality.\n* [Yusuf Hanafy](https://www.linkedin.com/in/yusufhanafy/) for working on the demo.\n* [Wanrong Zhu](https://wanrong-zhu.com/), [Jack Hessel](https://jmhessel.com/), and [Samir Gadre](https://sagadre.github.io/) for building the Multimodal C4 dataset.\n* [Jenia Jitsev](https://scholar.google.de/citations?user=p1FuAMkAAAAJ\u0026hl=en) for helping us with large scale training.\n* [Mitchell Wortsman](https://mitchellnw.github.io/), [Gabriel Ilharco](https://gabrielilharco.com/), [Simon Kornblith](https://simonster.com/), [Pang Wei Koh](https://koh.pw/) for technical discussions and for feedback on this blog.\n* [Ludwig Schmidt](https://people.csail.mit.edu/ludwigs/) for being our main advisor on this project and for their support.\n\n\n### Acknowledgements\n\nThis code is based on Lucidrains' [flamingo implementation](https://github.com/lucidrains/flamingo-pytorch) and David Hansmair's [flamingo-mini repo](https://github.com/dhansmair/flamingo-mini). Thank you for making your code public! We also thank the [OpenCLIP](https://github.com/mlfoundations/open_clip) team as we use their data loading code and take inspiration from their library design.\n\nWe would like to thank [Jean-Baptiste Alayrac](https://www.jbalayrac.com/) and [Antoine Miech](https://antoine77340.github.io/) for their advice, [Rohan Taori](https://www.rohantaori.com/), [Nicholas Schiefer](https://nicholasschiefer.com/), [Deep Ganguli](https://hai.stanford.edu/people/deep-ganguli), [Thomas Liao](https://thomasliao.com/), [Tatsunori Hashimoto](https://thashim.github.io/), and [Nicholas Carlini](https://nicholas.carlini.com/) for their help with assessing the safety risks of our release. This research is supported in part by NSF Institute on the Foundations of Machine Learning (IFML). Thanks to [Stability AI](https://stability.ai) for providing us with compute resources to train these models!\n","date":1679961600000},{"slug":"oig-dataset","frontmatter":{"title":"The OIG Dataset","author":"By Huu Nguyen -  Ontocord.ai, Sameer Suri, Ken Tsui , Shahules786, Together.xyz team, and Christoph Schuhmann - LAION.ai","date":"March 10 2023","previewImg":"/images/blog/oig-example.png"},"content":"\nThe [Open Instruction Generalist (OIG)](https://huggingface.co/datasets/laion/OIG) dataset is a large open source instruction dataset that currently contains ~43M instructions. \n\nOIG is one of many chatbot datasets that [LAION](https://laion.ai), along with its volunteers, [Ontocord](https://www.ontocord.ai), [Together](https://www.together.xyz) and other members of the open source community, will be releasing and is intended to create equal access to chatbot technology. Everyone is welcome to use the dataset and contribute improvements to it.\n\n## Examples of what is in OIG\n\n![](/images/blog/oig-example-2.png)\nExample data in OIG-43M.\n\n![](/images/blog/oig-example.png)\nTopic map of a subset of OIG-43M\n\n|id: value|\n|-|\n|6602: -1_Image prompts for drawing with specific keywords___|\n|1165: 0_Clipart use for teaching materials in commercial format with unlimited illustrations as an abcteach member___|\n|1047: 1_Images of Air Force Change of Command Ceremonies___|\n|745: 2_Documents related to military training and operations of Marine Corps and Army forces in 2013, 2017, and 2018.___|\n|332: 3_Employment Trends in Selected Metropolitan Areas___|\n|304: 4_Health Policy Workshop Proceedings and Image Covers for Cancer, Workforce, Literacy, and Accounting Approaches___|\n|291: 5_Printable worksheets for math, reading, and kindergarten learning with image prompts.___|\n|259: 6_Energy Trends and Prices___|\n|225: 7_Images featuring Defense Secretary James Mattis in official meetings and events.___|\n|174: 8_Images of Ricky Gervais, Jennifer Aniston, and Rachel Brosnahan at various award shows in Beverly Hills and Los Angeles.___|\n|168: 9_Cricket matches and fans in India, featuring IPL teams Kings XI Punjab and Kolkata Knight Riders, Bollywood actors Katrina Kaif and Shah Rukh Khan, and cricket legends Sachin Tendulkar and bowler Singh. Also includes matches with Australia, New Zealand, Pakistan, and Sri Lanka during the World Cup and Test matches.___|\n|140: 10_Images related to Covid-19 vaccination and prevention___|\n\nBreak-down of some image prompt instructions in a subset of OIG-43M.\n\n## Discussion\n\nOIG is a large-scale dataset containing instructions that are created using data augmentation from a diverse collection of data sources, and formatted in a dialogue style (\u003chuman\u003e… \u003cbot\u003e… pairs). The goal of OIG is to help convert a language model pre-trained on large amounts of text into an instruction-following model. It is designed to support continued pre-training to enable a base model (e.g., GPT-NeoX-20B) that can be later fine-tuned with the smaller-scale domain-specific datasets.\n\nOIG is created by various LAION community members, consisting of 30 datasets and 43M instructions, but we will continue to expand on this dataset with the goal of reaching 1 trillion tokens - enough to pretrain on OIG only.  It covers not only standard datasets (such as Natural Questions and Natural Instructions), but also data specifically related to dialog, summarization, education, etc.\n\nAppendix 1 describes the components of the current OIG dataset. The dataset can be divided roughly into 75% academic datasets such as P3, Natural instructions, and FLAN, where answers may be short, and the tasks are often artificial, such as determining NLI. The other 25% is composed of various tasks, such as question and answering, providing how-to instructions, performing basic high school math, basic python coding, story generation, essay generation, poetry generation, and a very rudimentary attempt at generating songs based on augmenting existing poetry. Of note, we have also created a UL2-like fill in the blank dataset using TurkuNLP’s [OSCAR-registry](https://huggingface.co/datasets/TurkuNLP/register_oscar) data (e.g, “Fill in the missing spans”, “Fill in the rest of this paragraph”, “Give me the missing words”). We hypothesize that this mixture of instruction improves academic metrics as well as instruction fulfillment. \n\n## Safety and Moderation\n\nAlong with OIG, [Ontocord.ai](https://www.ontocord.ai) is also releasing [OIG-moderation](https://huggingface.co/datasets/ontocord/OIG-moderation), a small safety instruction dataset. OIG-moderation is intended to train a moderation model to predict labels for various moderation categories such as \"needs intervention\", “hate”, \"sexual content\", etc. Ontocord will also release in future versions, multilingual versions of the dataset, and include potential responses that could contain a reason why a chatbot might not respond to the answer. It aims to address issues including privacy eliciting prompts, and depression responses, along with prompts eliciting sexual content and aggressive behavior from users.\n\nOIG-moderation includes data from (a) public datasets such as anthropic-redteam and anthropic-harmless, prosocial, and contributed datasets from community members (b) [augmented toxic data](https://huggingface.co/datasets/SummerSigh/PolicyData) such as civil comments data converted into instructions, (c) anthropic-redteam data [augmented with prosocial tags](https://huggingface.co/datasets/shahules786/prosocial_augmented) (d) data provided by the LAION community that might include NSFW prompts, and (e) synthetic depression data generated from [a public depression bag of words](https://huggingface.co/datasets/joangaes/depression) dataset using one of LAION’s volunteer’s [grammar fixing models](https://huggingface.co/pszemraj/flan-t5-large-grammar-synthesis). \n\nA model trained on the OIG-moderation dataset can be used to provide safety labels, and the bot providers can choose to then block responses from their chatbots based on these labels. If a bot provider's policy for example permits sexual content, but prohibits PII eliciting text, they can hopefully do so with the output of a model trained on this OIG-moderation. \n\n## Safety Goals\n\nOpen source and extendable safety pipelines unfortunately do not exist on the same scale as those in ChatGPT and other commercial systems. To further research in implementable, accurate, and extendable safety pipelines, LAION, Together, and Ontocord will push models, datasets, and code to the public. Research is one of our goals for safety, and we believe that keeping code, datasets, and models private hinders the overall progress in keeping LLM systems safe. By sharing such information, users and researchers alike can point out the harms, and potential solutions in these multifaceted systems.\n\nAnother goal for us is to bring safety research to a production setting where it can be effectively implemented and tested in real world use cases. Research in subjects like [toxicity detection](https://docs.cohere.ai/reference/toxicity-detection) and [bias mitigation](https://arxiv.org/abs/2106.13219) in LLMs is well established; however, the implementation of such research in systems that use language models as conversational agents in real world production settings has largely gone undocumented and unevaluated. The gap between research and implementation, brings many questions that must be answered to bring safe LLMs to the general public.\n\nWith the potential of offering OIG based systems to millions of users, it’s important to recognize the diversity in the user base with respect to socially acceptable paradigms. Pushing generally accepted social paradigms for one user in a specific country, locality, or even age does not warrant those same paradigms to be pushed upon users of other areas and ages. Thus we have opted for a multi-pronged approach to moderation and safety. We have curated and created data with safety tags, so that the bot providers can decide to train on the data and decide for themselves which moderation knob to turn on and off and which to permit their users to turn on and off (e.g., via parental controls).\n\nThus, we will strive to make data for safety systems that allow for user input so that our models can accept and reject prompts on a per locality and even a per user basis. It may not be possible to achieve in the first iteration of a safety pipeline however we will continue to research and strive for this goal. \n\n## How is the OIG dataset related to LAION’s Open Assistant Project?\n\nLAION’s [Open Assistant (OA)](https://github.com/LAION-AI/Open-Assistant) project is our efforts to replicate the functionality of ChatGPT, and as such centers around gathering human feedback and training a reinforcement model based on human feedback. In contrast, the OIG dataset is almost purely a synthetic data set created using data augmentation. Our hypothesis for the OIG dataset is that you can create a performant bot, without RLHF, by first performing continued pre-training with an average quality instruction dataset such as OIG, and then doing a finetuning on a high quality instruction dataset such as OIG-small-chip2. With that said, the team members between the OA and OIG projects overlap and the OIG data began from work done within the LAION OA working group. \n\n## Models*\n\nThe community has trained several models based on a subset of the OIG datasets including:\n\n* Rallio67/joi2_(20,12,7)B_instruct_alpha\n* Rallio67/chip2_(20,12,7)B_instruct_alpha\n* Rallio67/joi_(20,12,7)B_instruct_alpha\n* Rallio67/chip_(20,12,7)B_instruct_alpha\n* togethercomputer/GPT-NeoXT-Chat-Base-20B\n\n## Safety models**\n\n* SummerSigh/T5-Base-Rule-Of-Thumb\n* SummerSigh/Safety-Policy\n* SummerSigh/BART-Base-Rule-Of-Thumb \n* shahules786/prosocial-classifier\n* shahules786/Safetybot-mt5-base\n* shahules786/Safetybot-T5-base\n* togethercomputer/GPT-JT-Moderation-6B\n\n[Together](https://www.together.xyz/) has finetuned a GPT-JT model on v.01 of OIG-moderation, and other LAION volunteers have trained many other models on different subsets of the OIG-moderation v.02 dataset. \n\nNote: All the models above can be found at [https://huggingface.co/](https://huggingface.co/)\n\n*We will update this section as more OIG based models are trained and released. \n\n** Models are in development and do not currently represent the final safety system for LAION chatbots or how models trained on OIG-moderation will fully behave. \n\n## What’s next \n\nThis is just the beginning. This is a new project that we hope will evolve over time. From a purely dataset cleanup perspective, we intend to run a PII anonymizer on the web crawled portion of the dataset (e.g., OSCAR-registry based data). Also, there are several key areas that we need to improve including knowledge Q\u0026A, creative writing and coding. We are also working on collaborations for fine-tuned versions of the bot for tasks like education, which we are incredibly excited about. We also need to perform deduplication and basic filtering for very uninformative instructions in case we made mistakes in the data augmentation. As a prelude, LAION has an ongoing filtering and analysis project, called [riverbed](https://github.com/LAION-AI/riverbed), which aims to analyze the OIG dataset, which we describe briefly below.\n\n## Quality Filtering Approaches using masked language models (MLM)\n\nText outputs from dialogue prompted large language models are known to suffer from hallucinations and other factual inaccuracies. To address this problem, we applied various filtering to detect misinformation and contradiction with masked language models. In particular, masked language model is in a different paradigm from autoregressive language model, as its receptive field covers the context of both directions, providing extra information in detecting factual inaccuracies.\n\nWe framed fact checking as a masked language model pre-training objective. The idea behind is that if the prediction of  \u003cmask\u003e based on bidirectional context matches with the original token, the original token is more likely to be correct than incorrect and vice versa. A custom light weight RoBERTa based model is trained on high quality factual materials like books and wikipedia.\n\nWith the model, we analyzed the outputs of the GPT style language models by randomly masking a small percentage of the generated tokens and then using a BERT or T5 style language model to replace the masked tokens. Discrepancies between the original and replaced tokens were penalized to varying degrees. If the replaced token exactly matched the original token the penalty was zero. If the replaced token did not match the original token then it was analyzed for a semantic match by comparing a string containing the original token plus several tokens around it using language models finetuned on the natural language inference task (NLI) with three classifiers: entailment, neutral, contradiction. Entailment or neutral scores were considered positive while contradiction penalized the match.\n\n### Example 1\n\nOriginal \u003e The big dog barked at the [fluffy] black cat.\n\nMasked \u003e The big dog barked at the `\u003cmask\u003e` black cat.\n\nReplaced \u003e The big dog barked at the [small] black cat\n\nNLI(Original,Replaced) = {'entailment': 13.0892, 'neutral': 79.0414, 'contradiction': 7.8693}\n\nNLI(Replaced,Original) = {'entailment': 0.1456, 'neutral': 99.7456, 'contradiction': 0.1087}\n\nScore = ( ( (13.09+79.04-7.86)/2 + (0.15+99.75-0.11)/2 ) / 100 ) = 0.92\n\n\n### Example 2\n\nOriginal \u003e The big [llama] barked at the fluffy black cat.\n\nMasked \u003e The big `\u003cmask\u003e` barked at the fluffy black cat.\n\nReplaced \u003e The big [dog] barked at the fluffy black cat.\n\nNLI(Original, Replaced): {'entailment': 1.8346, 'neutral': 3.7347, 'contradiction': 94.4307}\n\nNLI(Replaced, Original): {'entailment': 0.111, 'neutral': 1.8248, 'contradiction': 98.0642}\n\nScore = ( ( (1.83 + 3.73 - 94.43)/2 + (0.11 + 1.82 - 98.06)/2 ) / 100 ) = -0.93\n\n\nWe also framed fact checking as replaced token detection, which is the pre-trained objective of ELECTRA. The idea is to look for “corrupted” token based on bidirectional context. Empirically, we found that, together with named entity recognition, pretrained electra large discriminator models could detect wrong named entities with a reasonable precision and recall in an augmented squad_v2 dataset.\n\nWe leveraged natural language inference to detect contradiction of dialogue from the bot. This acts as a self-consistency filter where we require a dialogue not to contradict itself. You can find a small sample of our work-in-progress [filtered OIG](https://huggingface.co/datasets/laion/OIG-riverbed-filtered-small) data here. More to come…\n\n## Support this project\n\nYour contributions and feedback support the open source ecosystem, improve the bot and provide datasets for future AI research. To participate you can:\n\n* Submit [Github](https://github.com/LAION-AI/Open-Instruction-Generalist) issues,  track issues and help create datasets that need improvement.\n* Join our [Discord](https://discord.gg/xBPBXfcFHd) to talk with other team members working on this!\n\n## Disclaimer\n\nThese datasets contain synthetic data and in some cases data that includes humans trying to get the language model to say toxic/offensive/trolling things. If you are concerned about the presence of this type of material in the dataset please make sure you carefully inspect each of the entries and filter appropriately. Our goal is for the model to be as helpful and non-toxic as possible and we are actively evaluating ways to reduce or eliminate undesirable content from the instruction tuning datasets.\n\n## License\n\nThe OIG dataset that is authored by LAION volunteers is released under an Apache 2.0 license. However, the data also includes content licensed under other permissive licenses such as Wikipedia data which is licensed under CC-BY-SA, or web-crawled data which is used under fair use principles. \n\n## Acknowledgement\n\n* We would also like to thank all of our amazing LAION volunteers including: @Rallio, @Jue, @Ce Zhang, @Player-1, @Laurel, @danielpatrickhug, @Jjmachan, @Mylo, @Khalid, @Coco.han,  @Pszemraj, and many others. \n* We would like to thank [Together](https://www.together.xyz/) for their tireless dedication to the open source and AI community and their contribution to many of the datasets.\n* We would like to thank [AI Horde](https://aihorde.net/) and user @Db0 for their incredible contribution of filtered data that were flagged as unethical.\n* Lastly, [Ontocord.ai](https://www.ontocord.ai)’s founders are grateful to have the opportunity to create a portion of the data augmentation and safety-moderation code for this project.\n\n\n## Appendix - Description of OIG datasets components\n\n- unified_ni [https://github.com/allenai/natural-instructions](https://github.com/allenai/natural-instructions)\n- unified_p3: [https://huggingface.co/datasets/bigscience/P3](https://huggingface.co/datasets/bigscience/P3)\n- unified_flan: [https://github.com/google-research/FLAN/tree/main/flan/v2](https://github.com/google-research/FLAN/tree/main/flan/v2)\n- unified_soda_dialog: [https://huggingface.co/datasets/allenai/soda](https://huggingface.co/datasets/allenai/soda)\n- unified_unifiedskg_instructions: [https://github.com/HKUNLP/UnifiedSKG](https://github.com/HKUNLP/UnifiedSKG) \n- unified_merged_code_xp3: [https://huggingface.co/datasets/bigscience/xP3](https://huggingface.co/datasets/bigscience/xP3) (only Python)\n- unified_oscar_en_sample_dialog: A small portion of [https://oscar-project.org/](https://oscar-project.org/)\n[https://huggingface.co/datasets/TurkuNLP/register_oscar](https://huggingface.co/datasets/TurkuNLP/register_oscar)\n- unified_ul2_plus_oscar_en_sample_dialog: A small portion of  [https://oscar-project.org/](https://oscar-project.org/)\n[https://huggingface.co/datasets/TurkuNLP/register_oscar](https://huggingface.co/datasets/TurkuNLP/register_oscar)\n- unified_multi_news: [https://www.tensorflow.org/datasets/catalog/multi_news](https://www.tensorflow.org/datasets/catalog/multi_news)\n- unified_openai_summarize_tldr:  [https://github.com/openai/summarize-from-feedback](https://github.com/openai/summarize-from-feedback)\n- unified_scitldr:  [https://github.com/allenai/scitldr](https://github.com/allenai/scitldr)\n- unified_squad_v2:  [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)\n- unified_nq:  [https://ai.google.com/research/NaturalQuestions](https://ai.google.com/research/NaturalQuestions)\n- unified_poetry_instructions: Poetry data of mostly classical poems\n[https://huggingface.co/datasets/merve/poetry](https://huggingface.co/datasets/merve/poetry)\n[https://huggingface.co/datasets/matthh/gutenberg-poetry-corpus](https://huggingface.co/datasets/matthh/gutenberg-poetry-corpus)\n- unified_sqlv1 and unified_sqlv2: public text 2 sql datasets.\n- unified_unatural_instructions: [https://github.com/orhonovich/unnatural-instructions](https://github.com/orhonovich/unnatural-instructions)\n- unified_conv_finqa:  [https://github.com/czyssrs/ConvFinQA](https://github.com/czyssrs/ConvFinQA)\n- unified_essays:  essays available on the public web \n- unified_plot_screenplay_books_dialog : [https://github.com/markriedl/WikiPlots](https://github.com/markriedl/WikiPlots) extracted from Wikipedia, snippets from the Pile’s [https://huggingface.co/datasets/the_pile_books3](https://huggingface.co/datasets/the_pile_books3), and snippets of screenplays available on the public web. \n- unified_grade_school_math_instructions: [https://github.com/openai/grade-school-math](https://github.com/openai/grade-school-math)\n- unified_mathqa_flanv2_kojma_cot: Public chain-of-thought datasets converted to instructions [https://huggingface.co/datasets/math_qa](https://huggingface.co/datasets/math_qa), \n- unified_joke_explanations: a very small dataset of joke explanations crawled from the public web \n- unified_cuad:  [https://www.atticusprojectai.org/cuad](https://www.atticusprojectai.org/cuad)\n- unified_abstact_infill:  dbpedia and wikipedia snippets combined with a small portion of [https://github.com/google-research/dialog-inpainting](https://github.com/google-research/dialog-inpainting) \n- unified_image_prompts_instructions: A very small subset of LAION-400M\n- unified_canadian_parliament:  [https://openparliament.ca/data-download/](https://openparliament.ca/data-download/)\n- unified_poetry_2_song:  The above poetry dataset (and [https://huggingface.co/datasets/shahules786/PoetryFoundationData](https://huggingface.co/datasets/shahules786/PoetryFoundationData))  translated to song-like structures.\n- unified_hc3_human - [https://huggingface.co/datasets/Hello-SimpleAI/HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3) \n- unified_rallio_safety_and_prosocial: Generated from public datasets and generated from Wiki similar to the chip2 data; find a full list in the end of the document, also includes [https://huggingface.co/datasets/allenai/prosocial-dialog](https://huggingface.co/datasets/allenai/prosocial-dialog) and [https://huggingface.co/datasets/Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)  \n- unified_chip2: Generated from public datasets and generated from Wiki’s; full list below\n\n\n## OIG-small-chip2\n\n### Python Code Examples\n\nA set of instruction / response pairs where the User requests the agent to generate a python function. These examples were generated using a large language model and few shot prompting with python code verified to execute. There are also ~3000 examples of manually curated one line python code examples from the Conala publication (see: [https://conala-corpus.github.io/](https://conala-corpus.github.io/))\n\n### Natural Instruction Examples\n\nA balanced set of diverse natural and factual questions and answers made using few shot prompted UL2 20B and an instruction tuned GPT-NeoX-20B model (Chip) and then rejection sampled using multiple automatic evaluations to remove low quality outputs and to filter out factually inaccurate answers. Also includes some filtered natural instructions from Anthropic Helpful instructions (see: https://github.com/anthropics/hh-rlhf).\n\n### Generic Harmless Instruction Examples\n\nA set of instruction / response pairs sourced from the Anthropic redteam paper github (see: https://github.com/anthropics/hh-rlhf). This dataset includes a lot of data regarding real humans trying to make the Anthropic language models say harmful/toxic/trolling things. For this dataset only examples that were rated lowly on the harmful scale (0,1,2 out of 4, where 4 is the most toxic) were included. Again, only the first lines of dialogue (instruction, first_agent_response) were retained.\n\n### Instruction/Responses with Lists\n\nA set of filtered and reformatted instruction / response pairs where the agent response contains a list. Sourced from the Anthropic github (see: https://github.com/anthropics/hh-rlhf). Sourced from wikihow text lists created by b-mc2 (https://huggingface.co/datasets/b-mc2/wikihow_lists). And rejection filtered instruction response pairs generated by Chip20B that contained lists. All lists are formatted in a similar style.\n\n### Follow-up questions\n\nExamples of instructions and responses where an appropriate response is to ask for more information from the prompter. These examples were generated from a combination of few shot prompted UL2 20B (to generate natural questions) and a large dialogue prompted language model to generate the responses containing follow-up questions.\n\n### Wikipedia Toxic Adversarial Questions\n\nQuestions and answers generated from wikipedia articles that discuss potentially sensitive topics (flagged as potentially toxic by an early toxicity detection model).\n\n### Grade School Math GSM8K (~9,000)\n\nGSM8K is a dataset of 8.5K high quality linguistically diverse grade school math word problems created by human problem writers. The dataset is segmented into 7.5K training problems and 1K test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ − ×÷) to reach the final answer. A bright middle school student should be able to solve every problem. It can be used for multi-step mathematical reasoning. [https://github.com/openai/grade-school-math](https://github.com/openai/grade-school-math)\n\n### Reasoning Instructions\n\nExamples from the Com2Sense and Strategy QA datasets that were reformatted into natural instructions using large language models with few shot prompting and additional quality filtering steps.\n\n### Character and Scene Descriptions\n\nExamples of instructions and responses for the generation of character or scene descriptions. Scenes were sourced from video game wikis and reformatted into instruction / response format using large language models or generated by few shot prompting with large language models.","date":1678406400000},{"slug":"coca","frontmatter":{"title":"Training Contrastive Captioners","author":"Giovanni Puccetti, Maciej Kilian, Romain Beaumont","date":"Feb 2 2023","previewImg":"/images/blog/eval_coca_clip.jpg"},"content":"\n\nWe introduce a new model type to [OpenClip](https://github.com/mlfoundations/open_clip) Contrastive Captioners (CoCa) [1]. This model adds an autoregressive objective (generation) on top of the CLIP contrastive one. The architecture is composed of three parts, the first two are similar to those composing a CLIP model and the third is a text decoder that stands on top of the text encoder. The additional decoder takes as input the encoded images (through cross-attention) and the previous tokens to predict the next most probable one. One of the few architecture changes, compared to CLIP, is attentional pooling [2], used to aggregate image representations and pass them to both the contrastive loss and the decoder cross-attention.\n\nThis is interesting for several reasons:\n\n* We believe there is no openly available trained model with this architecture;\n* Adding a generative task appears to help the contrastive task with minimal computational impact;\n* The model is easily adaptable to a large number of tasks, on top of all those CLIP is suited for. CoCa models can (with relatively cheap fine-tuning) perform Image Captioning, Visual Question Answering, Multimodal Understanding, and more;\n* CoCa gives captioning models an intermediate contrastive latent space for minimal training cost increase.\n\n\n## Benchmarks\n\nOn a comparable model size and with the same training data available, CoCa outperforms a CLIP model on several zero-shot tasks (Figure 1). Most notably on _imagenet1k_ CoCa achieves 75.5 and CLIP 73.1 (2.6% improvement).\n\n\n|(a) ![](/images/blog/eval_coca_clip.jpg) |(b) ![](/images/blog/eval_coca_clip_diff.jpg) |\n|:-|:-|\n\n\n_Figure 1:_ Scores achieved by _coca_ViT-L-14_ and _ViT-L-14_ on several zeroshot classification tasks **(a)**, together with the performance gap between the two models, in the same tasks sorted by magnitude **(b)**.\n\n\n\n\nTable 2 shows the results achieved on Text to Image and Image to Text retrieval by both CoCa and CLIP. In this case too, CoCa outperforms CLIP on all tasks with differences ranging from 0.3 to 1.3.\n\n\n\u003ctable\u003e\n  \u003ctr\u003e\n   \u003ctd colspan=\"4\" align=\"center\" \u003e Text to Image Retrieval Recall@5\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e\n   \u003c/td\u003e\n   \u003ctd\u003eflickr30k\n   \u003c/td\u003e\n   \u003ctd\u003eflickr8k\n   \u003c/td\u003e\n   \u003ctd\u003eMscoco captions\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003ecoca_ViT-L-14\n   \u003c/td\u003e\n   \u003ctd\u003e92.0\n   \u003c/td\u003e\n   \u003ctd\u003e70.1\n   \u003c/td\u003e\n   \u003ctd\u003e70.5\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eViT-L-14\n   \u003c/td\u003e\n   \u003ctd\u003e91.7\n   \u003c/td\u003e\n   \u003ctd\u003e69.0\n   \u003c/td\u003e\n   \u003ctd\u003e69.2\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd colspan=\"4\" align=\"center\"\u003e Image to Text Retrieval Recall@5\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e\n   \u003c/td\u003e\n   \u003ctd\u003eflickr30k\n   \u003c/td\u003e\n   \u003ctd\u003eflickr8k\n   \u003c/td\u003e\n   \u003ctd\u003eMscoco captions\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003ecoca_ViT-L-14\n   \u003c/td\u003e\n   \u003ctd\u003e99.3\n   \u003c/td\u003e\n   \u003ctd\u003e81.7\n   \u003c/td\u003e\n   \u003ctd\u003e83.6\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eViT-L-14\n   \u003c/td\u003e\n   \u003ctd\u003e98.4\n   \u003c/td\u003e\n   \u003ctd\u003e81.2\n   \u003c/td\u003e\n   \u003ctd\u003e83.0\n   \u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n_Table 2:_ Text to Image and Image to Text retrieval **Recall@5** on _flickr30k_, _flickr8k_ and _Mscoco captions_.\n\n## Released Checkpoint\n\nWe release checkpoints for two model configs, _coca_ViT-B-32_ and _coca_ViT-L-14_. We also release the MSCOCO finetunes of those models which are much better at captioning but unfortunately lose their contrastive capabilities during fine tuning.\n\nTry generation in this [Space](https://huggingface.co/spaces/laion/CoCa) or in this [colab notebook](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb)!\n\n\n\u003ctable\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e\n   \u003c/td\u003e\n   \u003ctd\u003eL/14\n   \u003c/td\u003e\n   \u003ctd\u003eB/32\n   \u003c/td\u003e\n   \u003ctd\u003eCoCa (from paper)\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n  \u003ctd\u003e# Params Image Encoder\n   \u003c/td\u003e\n   \u003ctd\u003e306.72M\n   \u003c/td\u003e\n   \u003ctd\u003e89.16M\n   \u003c/td\u003e\n   \u003ctd\u003e\n    1B\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e# Params Text Encoder\n   \u003c/td\u003e\n   \u003ctd\u003e123.65M\n   \u003c/td\u003e\n   \u003ctd\u003e63.42M\n   \u003c/td\u003e\n   \u003ctd rowspan=\"2\"\u003e\n    1.1B\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e# Params Text Decoder\n   \u003c/td\u003e\n   \u003ctd\u003e208.07M\n   \u003c/td\u003e\n   \u003ctd\u003e100.96M\n   \u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n_Table 3:_ Number of parameters for each encoder/decoder component for _coca_ViT-L-14_, _coca_ViT-B-32_ and the _CoCa_ model from the original paper (M=millions, B=billions).\n\n\n\n## Training Notes\n\n\n### Pretraining\n\nWe train both model configurations on 13B samples seen from [LAION-2B](https://laion.ai/blog/laion-5b/) [3] with a batch size of 90k, learning rate of 1e-3, and a cosine decay learning rate schedule. Experiments were performed on 384 A100’s and over the course of training we maintained 75.5 samples/s/gpu (~29k samples/s in total).\n\nWhen it comes to cost, even though CoCa has more capabilities than single-task captioning models there’s a minimal increase ~20% (as reported by Table 8b of the paper). This is due to the fact that the first half of the text decoder (i.e. the text encoder) is unimodal and is computed in parallel to the image encoder, once the encoders are done we simply continue the forward pass of the text embeddings through the text decoder and also include the image embeddings via cross attention. The trainig report can be found [here](https://wandb.ai/iejmac/open-clip/reports/CoCa-L-14--VmlldzozNDEwMDIx).\n\n\n### Fine-tuning\n\nFor image captioning tasks fine-tuning is a straightforward extension of pretraining with few hyper parameters changes. The crucial one is contrastive loss weight, which has to be set to zero to let the backward pass only account for the generative loss, besides  there are no additional fine-tuning oriented components nor changes in the loss. We use a batch size of 128 with a learning rate of 1e-5 and a cosine learning rate schedule. Experiments are performed on 4 A100's. Table 4 shows the language generation scores achieved by _coca_ViT-L-14_ and by CoCa in the original paper, _coca_ViT-L-14 performance is still far from the original CoCa model one.\n\nIt is noteworthy that (in our experiments) after fine-tuning with a generative only loss these models lose their contrastive skills entirely.\n\n\n\u003ctable\u003e\n  \u003ctr\u003e\n   \u003ctd\u003e\n   \u003c/td\u003e\n   \u003ctd\u003eBleu@4\n   \u003c/td\u003e\n   \u003ctd\u003eMETEOR\n   \u003c/td\u003e\n   \u003ctd\u003eCIDEr\n   \u003c/td\u003e\n   \u003ctd\u003eSpice\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd colspan=\"5\" align=\"center\"\u003e\n    coca_ViT-L-14\n    \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eKarpathy val\n   \u003c/td\u003e\n   \u003ctd\u003e35.6\n   \u003c/td\u003e\n   \u003ctd\u003e29.8\n   \u003c/td\u003e\n   \u003ctd\u003e125.3\n   \u003c/td\u003e\n   \u003ctd\u003e23.4\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eNoCaps\n   \u003c/td\u003e\n   \u003ctd\u003e39.9\n   \u003c/td\u003e\n   \u003ctd\u003e29.1\n   \u003c/td\u003e\n   \u003ctd\u003e106.5\n   \u003c/td\u003e\n   \u003ctd\u003e14.7\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd colspan=\"5\" align=\"center\"\u003e\n    Original CoCa (from paper)\n    \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eKarpathy val\n   \u003c/td\u003e\n   \u003ctd\u003e40.9\n   \u003c/td\u003e\n   \u003ctd\u003e33.9\n   \u003c/td\u003e\n   \u003ctd\u003e143.6\n   \u003c/td\u003e\n   \u003ctd\u003e24.7\n   \u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n   \u003ctd\u003eNoCaps\n   \u003c/td\u003e\n   \u003ctd\u003e -\n   \u003c/td\u003e\n   \u003ctd\u003e-\n   \u003c/td\u003e\n   \u003ctd\u003e122.4\n   \u003c/td\u003e\n   \u003ctd\u003e15.5\n   \u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n_Table 4:_ Visual captioning scores achieved with _coca_ViT-L-14_ on _karpathy_ validation set and _NoCaps_.\n\n\n\n## Captioning Examples\n\n\n\n|\u003cimg src=\"/images/blog/ipod_apple.png\" alt=\"cao\" width=\"500\"\u003e|\u003cimg src=\"/images/blog/space_raccoon.png\" alt=\"cao\" width=\"500\"\u003e|\n|:-|:-|\n|An apple sitting on top of a wooden table.|A painting of a raccoon in a space suit.|\n\n\n\n\n\n\n\n\n\n\n\n\n## What’s Next\n\n\n\n* Unimodal Text Pretraining - One of the shortcomings of CoCa is that it can have trouble with zero-shot captioning because the noisy web text it was trained on isn’t as rich as unimodal text data. To this end we can look into methods that provide CoCa models with this rich text understanding either via initializing the weights of the decoder with some pretrained unimodal text decoder or perhaps alternating between multimodal and unimodal losses that use different data.\n* Fine tuning on more tasks VQA, multimodal reasoning, and more.\n* Image Decoder - CoCa adds a multimodal text decoder on top of CLIP and shows this multi-task learning can benefit both tasks. Why not also add a multimodal image decoder?\n\n\n## Contributions and acknowledgements\n\nThanks to\n\n\n\n* [gpucce](https://gpucce.github.io/) and [iejMac](https://github.com/iejMac) for implementation into open_clip and training the models.\n* [lucidrains](https://github.com/lucidrains) for [initial implementation](https://github.com/lucidrains/CoCa-pytorch).\n* [Romain Beaumont](https://github.com/rom1504) and [Ross Wightman](https://github.com/rwightman) for advice, reviews, and engineering support.\n* [Soonhwan-Kwon](https://github.com/Soonhwan-Kwon) for implementing beam search.\n\nHuge thanks to [Emad](https://twitter.com/EMostaque) and [StabilityAI](https://stability.ai/) for providing the compute resources required to train these models.\n\n\n## References\n\n[1] Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., \u0026 Wu, Y. (2022). CoCa: Contrastive Captioners are Image-Text Foundation Models. _ArXiv, abs/2205.01917_.\n\n[2] Lee, J., Lee, Y., Kim, J., Kosiorek, A.R., Choi, S., \u0026 Teh, Y.W. (2018). Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. _International Conference on Machine Learning_.\n\n[3] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., \u0026 Jitsev, J. (2022). LAION-5B: An open large-scale dataset for training next generation image-text models. _ArXiv, abs/2210.08402_.","date":1675296000000},{"slug":"h14_clip_retrieval","frontmatter":{"title":"Clip-Retrieval Update: H-14 Index \u0026 SLURM Inference","author":"no usr","date":"Jan 31 2023","previewImg":"/images/blog/h_14_clip_front.png"},"content":"\nToday we release a KNN index for LAION-5B that allows for fast queries of the dataset with the open clip ViT-H-14 CLIP model. This means that users can search through billions of samples quickly and easily, making it a powerful tool for various applications such as image and text retrieval, data filtering and more. With this update also comes a brand new SLURM based inference backend for high-compute environments.\n\nWith this users can now:\n  - Peer into the superset of data used to train the latest stable-diffusion-v2 models.\n  - Easily filter through the dataset to create fine-tuning datasets and averaged embeddings for aesthetic gradients.\n  - Quickly compute indices for new datasets with the SLURM backend.\n  - Download the index \u0026 deploy locally\n\n## The front-end\n\nOur new H/14 index is now available for use on our clip-front demo at https://rom1504.github.io/clip-retrieval or https://knn.laion.ai. \nThis new index allows for fast querying using both images and text, making it a valuable tool for a variety of use cases.\nTo start using the new index right away, simply visit the website and start experimenting with the available query options. \nThe demo also allows you to easily download the resulting query as an [img2dataset](https://github.com/rom1504/img2dataset) compatible json file. \nThis means that you can quickly create datasets for any use case, making it a valuable resource for creatives, data scientists and researchers alike.\n\n## Using the KNN as an API\n\nThe KNN index can be accessed via the API, which allows you to perform nearest-neighbor searches in an easy and intuitive way. \nHowever, if you would prefer to use the provided knn index programmatically, you can! \nWe have a notebook that you can use as a guide on how to do so. You can find the notebook [here](https://colab.research.google.com/github/rom1504/clip-retrieval/blob/master/notebook/clip-retrieval-getting-started.ipynb), it will walk you through the steps necessary to use the provided KNN index programmatically. \nThis can be useful if you want to integrate the KNN index into your own application or if you want to automate the process of nearest-neighbor searches. \nNote that if you are looking to integrate the index into your own product, you should deploy it locally.\n\n## Computing your own index\n\nCreating your own index is a great way to interact with and visualize your data. \nWith a custom CLIP embedding index you can quickly search for similar images, check what images your prompts summon, or check how unique a generated image may be to the training data.\n\nThe clip-retrieval repo offers the ability for users to compute their own indices for their own datasets. \nIn an effort to support the creation of our new H-14 index, we added support for SLURM as a backend inference engine. \nThis update adds a third option for computing indices meaning whether you are using SLURM, PySpark, or running it on your local machine, the process of creating a CLIP KNN index has never been easier. \nFor specific usage please see the project’s [README](https://github.com/rom1504/clip-retrieval#clip-inference) for the inference API and the newest arguments available for creating your own index.\n\n## Deploying Locally\n\nUsers who would like to do a lot of queries, or integrate the index into their own product, should download the index and metadata and deploy it locally to their own server. \nIn order to do so we have uploaded the pre-computed indices to huggingface which can be found [here](https://huggingface.co/datasets/laion/laion5b-h14-index). \nFor full documentation on the exact steps necessary to begin hosting the index yourself please visit the [clip-retrieval docs](https://github.com/rom1504/clip-retrieval/blob/main/docs/laion5B_h14_back.md).\n","date":1675123200000},{"slug":"giant-openclip","frontmatter":{"title":"Reaching 80% zero-shot accuracy with OpenCLIP: ViT-G/14 trained on LAION-2B","author":"Mitchell Wortsman","date":"Jan 24 2023","previewImg":"/images/blog/scaling_vit_giant.png"},"content":"\nWe have trained a new [ViT-G/14 CLIP](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k) model with [OpenCLIP](https://github.com/mlfoundations/open_clip) which achieves **80.1%** zero-shot accuracy on ImageNet and **74.9%** zero-shot image retrieval (Recall@5) on MS COCO. As of January 2023, this is the best open source CLIP model.\n\nWe believe this is interesting because:\n* CLIP models are useful for zero-shot classification, retrieval, and for guidance/conditioning in generative models (OpenCLIP is used in Stable Diffusion V2 and currently the third most downloaded model on HuggingFace is a CLIP model). The approach underlying CLIP—self supervised learning on a large, heterogeneous dataset—has been shown to produce models which are more [robust](https://openai.com/blog/clip/) and [fair](https://ai.facebook.com/blog/seer-10b-better-fairer-computer-vision-through-self-supervised-learning-training-on-diverse-datasets/).\n* Our new ViT-G model achieves the highest zero-shot ImageNet accuracy among models that use only naturally occurring image-text pairs as training data, and without explicit labels, pseudo-labels, or any pretrained image or text encoders.\n* Our training run utilized multiple new techniques, including [FLIP](https://arxiv.org/abs/2212.00794) to accelerate training and [model soups](https://arxiv.org/abs/2203.05482) to surpass 80% accuracy.\n\n## Main Results\nThe following results are with image resolution 224x224 except for CoCa which uses 576x576.\n\n| Model name       | Batch size |               Samples seen              | Text Params | Image params | ImageNet top1 | Mscoco image retrieval at 5 | Flickr30k image retrieval at 5 |\n|------------------|:----------:|:---------------------------------------:|:-----------:|:------------:|:-------------:|:---------------------------:|:------------------------------:|\n| OpenAI CLIP L/14 | 32k        | 13B                                     | 123.65M     | 303.97M      | 75.4%         | 61.0%                         | 87.0%                            |\n| OpenCLIP H/14    | 79k        | 32B (16 epochs of laion2B)              | 354.0M      | 632.08M      | 78.0%         | 73.4%                       | 94%                            |\n| OpenCLIP G/14    | 160k       | 32B +unmasked fine-tune (details below) | 694.7M      | 1844.9M      | 80.1%*        | 74.9%                       | 94.9%                          |\n| CoCa            | 66k        | 33B                                     | 1100M       | 1000M        | 86.3%**       | 74.2                        | 95.7                           |\n\n\\* When using [CuPL](https://arxiv.org/abs/2209.03320) prompts instead of the standard prompts from OpenAI, the zero-shot accuracy is 80.3%. When evaluating at 280x280 and changing resize to squash, Ross Wightman found the model achieves 80.4%.\n\n** In addition to natural language supervision, [CoCa](https://arxiv.org/abs/2205.01917) uses synthetic captions constructed with the labels from the JFT-3B dataset. In addition to natural language supervision, CoCa uses synthetic captions constructed with the labels from the JFT-3B dataset. 973 of the 1,000 ImageNet classes have a corresponding class in JFT (e.g., see here sec C.7.2).\n\nAlso see the figure below (figure code by Ross) and our analysis of scaling trends for OpenCLIP model [here](https://arxiv.org/abs/2212.07143).\n\n![](/images/blog/scaling_vit_giant.png)\n\n## Released Checkpoints\n\nWe release the checkpoint through [OpenCLIP](https://github.com/mlfoundations/open_clip) and in the [HuggingFace hub](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k).\n\n## Notes on scaling up\n\nTo scale up model size while reducing compute we used [Fast Language-Image Pre-training (FLIP)](https://arxiv.org/abs/2212.00794) with patch dropout 0.5. Similar to [Masked Autoencoders (MAE)](https://arxiv.org/abs/2111.06377), FLIP drops out patches during training. FLIP also requires a short “unmasked tuning” phase, which we discuss in training notes below. In addition to reducing Giga multiply–accumulate operations (GMACs) for each forward/backward pass, FLIP allowed us to use a larger per-GPU batch size. Without FLIP, gradient accumulation was necessary to maintain a large batch size. Keeping batch size and number of GPUs consistent (at 160k and 512, respectively) but switching to unmasked fine-tuning resulted in a drop from 46.9 to 20.4 samples per second per GPU. For reference, OpenCLIP H/14 with global batch size 79k across 824 GPUs without patch dropout trained at 42 samples/s/GPU.\n\nTo scale up the batch size to 160k, we used [gradient checkpointing](https://arxiv.org/abs/1604.06174v2) and 80GM VRAM A100s. For the unmasked tuning portion, we also used gradient accumulation (see our implementation for the contrastive objective [here](https://github.com/mlfoundations/open_clip/pull/267)). Finally, we used a 2x higher learning rate of 2e-3 compared to our experiments with batch size 80k. The combination of scaling up model, batch size, and learning rate resulted in training instability during the warmup phase. Accordingly, we increased warm-up to 13k steps, trained with layer scale, and used AdamW beta2 0.95. All runs used AMP bfloat16, after previously switching from float16 in prior experiments with L/14 and H/14.\n\n## Training notes\n\n### Phase 1: Patch dropout\n\nFor phase 1 we trained ViT-G with [patch dropout](https://arxiv.org/abs/2212.00794) 0.5 on LAION-2B for 32B samples seen. We used batch size 160k, learning rate 2e-3, and a cosine decay schedule. After this phase the model reached 79.07 zero-shot top1 accuracy on ImageNet.\n\nTraining was mainly done on 512 to 760 A100s depending on availability. When changing the number of GPUs, local batch size was also modified so that the global batch size remained at 160k. When using 512 GPUs we set local batch size to 313 and observed roughly 24k samples per second or 46.9 samples/s/GPU. When using 760 GPUs we set local batch size 211 and observed roughly 33k samples per second or 43.4 samples/s/GPU.\n\n### Phase 2: Unmasked tuning + Model soups\n\nFor phase 2 we followed [FLIP](https://arxiv.org/abs/2212.00794) in conducting a short unmasked tuning phase. We fell short of 80% in our first unmasked fine-tuning phase, reaching only 79.43%. So we tried twice more with different settings (described below) to obtain 79.45% and 79.2%, respectively. Next, we followed [model soups](https://arxiv.org/abs/2203.05482) and averaged the weights of three checkpoints produced by these runs to achieve our final accuracy of 80.1%. [LIMoE](https://arxiv.org/abs/2206.02770) and [PaLI](https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html) also used model soups for better pre-training.\n\nFor our first unmasked fine-tuning run we did not modify the learning rate schedule, but instead doubled the base LR and extended the number of iterations so that the run would proceed for an additional 2B samples seen. LR started at 3.8e-5. For the second run we used LR 5.5e-5 with a full cosine schedule (warmup for roughly 200M samples and a total of 4B samples). The third run had identical hyperparameters to the first but used the LAION-A subset of LAION-2B. LAION-A is a 900M subset of LAION-2B filtered with aesthetic V2 4.5+ and pHash deduplicated. Instead of waiting for the third run to complete we use the checkpoint after approximately 700M samples which, when “[souped](https://arxiv.org/abs/2203.05482)” with the final checkpoints from the two proceeding runs, already allowed us to surpass our goal of 80% accuracy. This indiviual checkpoint achieved 79.2%.\n\nUnmasked fine-tuning was done on 512 A100 GPUs at a speed of roughly 10,450 samples/s or 20.4 samples/s/GPU.\n\nThe following plot shows the loss curve for phase 1.\n\n![](/images/blog/loss_vit_giant.png)\n\n## More results\n\nZero-shot accuracies at resolution 224x224 computed with [CLIP Benchmark](https://github.com/LAION-AI/CLIP_benchmark).\n\n| Dataset         | OpenCLIP H/14 | OpenCLIP G/14 |\n|-----------------|---------------|---------------|\n| ImageNet        | 78.0         | 80.1         |\n| ImageNet-V2     | 70.8         | 73.6         |\n| ImageNet-R      | 89.3         | 92.1         |\n| ImageNet-Sketch | 66.6         | 68.9         |\n| ObjectNet       | 69.7         | 73.0         |\n| ImageNet-A      | 59.2         | 69.3         |\n| CIFAR-10        | 97.4         | 98.2         |\n| CIFAR-100       | 84.7         | 87.5         |\n| MNIST           | 72.9         | 71.6         |\n| SVHN            | 56.1         | 62.5         |\n| Caltech-101     | 85.0         | 86.4         |\n| SUN397          | 75.2         | 74.5         |\n| FGVC Aircraft   | 42.8         | 49.7         |\n| Country211      | 30.0         | 33.8         |\n| Cars            | 93.5         | 94.6         |\n\nHere is a summary figure comparing G/14 and H/14 made with evals by Romain Beaumont.\n\n![](/images/blog/summary_vit_giant.png)\n\n## What’s Next?\n\nIn the future, we may fine-tune the model to enable multilingual capabilities, or fine-tune at higher resolution. Also, [FSDP is coming to OpenCLIP](https://github.com/mlfoundations/open_clip/pull/358) which will allow even larger models, as is [CoCa](https://github.com/mlfoundations/open_clip/pull/308) which will allow new openclip models to also be captioners. More contributions to OpenCLIP are always welcome!\n\n## Contributions and acknowledgements\n\nThanks to:\n* [Romain Beaumont](https://github.com/rom1504), [Ross Wightman](https://github.com/rwightman), [Mehdi Cherti](https://github.com/mehdidc), [Gabriel Ilharco](https://gabrielilharco.com/), and [Jenia Jitsev](https://github.com/JeniaJitsev) for providing extensive ideas, advice, engineering support, evaluating the model, and maintaining the openclip repository used for model training.\n* [Christoph Schuhmann](https://github.com/christophschuhmann) for encouragement and support\n* [Richard Vencu](https://github.com/rvencu) for cluster support\n* [Phil Wang](https://github.com/lucidrains) and [Haoqi Fan](https://haoqifan.github.io/) for the implementation and discussion regarding patch dropout\n* [Sho Yaida](https://www.shoyaida.com/), [Jong Wook Kim](https://jongwook.kim/), [Ari Morcos](http://www.arimorcos.com/) and [Saining Xie](https://www.sainingxie.com/) for helpful remarks regarding hyperparameters\n* [Sarah Pratt](https://sarahpratt.github.io/) for implementing CuPL\n* [Ludwig Schmidt](https://github.com/ludwigschmidt) and [Ali Farhadi](https://homes.cs.washington.edu/~ali/) for helpful discussions, and to the [RAIVN](https://raivn.cs.washington.edu/) and [EFML](https://github.com/mlfoundations/) labs at the University of Washington\n\nAnd of course thanks to [Emad](https://twitter.com/EMostaque) and [Stability AI](https://stability.ai/) for providing the compute resources used for these experiments.\n","date":1674518400000},{"slug":"laion-stable-horde","frontmatter":{"title":"Collaboration between LAION and the Stable Horde","author":"Konstantinos Thoukydidis, hlky","date":"Jan 08, 2023","previewImg":"/images/blog/artbot-image-ratings.png"},"content":"\nAuthor: [Konstantinos Thoukydidis](https://dbzer0.com), [hlky](https://twitter.com/hlky__)\n\nWe are happy to announce that LAION will be assisted by the Stable Horde to provide aesthetic ratings for existing datasets and a completely new dataset of Stable Diffusion generations, which will also be rated by their community.\n\nWe wrote in the past about [LAION-Aesthetics](https://laion.ai/blog/laion-aesthetics/) and how we filtered LAION-5b using an [aesthetic predictor](https://github.com/christophschuhmann/improved-aesthetic-predictor). The predictor, a simple neural net that uses CLIP ViT-L/14 embeddings as input. hlky has retrained the aesthetic predictor using ViT-H-14, and the results are promising.\n\n[hlky’s ViT-H aesthetic predictor](https://github.com/hlky/aesthetic-predictor) was trained on the same datasets as the original - AVA: A Large-Scale Database for Aesthetic Visual Analysis, Simulacra Aesthetic Captions, and LAION-logos. These datasets are limited, totalling around 400k image-rating pairs. This is where the Stable Horde comes in.\n\n[The Stable Horde](https://stablehorde.net/) is a crowdsourced, distributed, [free and open sourced service](https://github.com/db0/AI-Horde) for generating Stable Diffusion images available to be integrated into everything. They have volunteered to direct their community to rate the existing LAION datasets for aesthetic rating of 1-10. As the Stable Horde can be used for free, to incentivize this, they are providing priority (AKA kudos) to anyone who is providing such ratings for their stable diffusion generations.\n\nThe integration has been made possible through an API hosted by Sygil.dev which can collect such ratings and be integrated directly into all Stable Horde clients. Already the first integrations have been released on clients like [ArtBot](https://tinybots.net/artbot/rate).\n\n![](/public/images/blog/artbot-image-ratings.png)\n\nFurthermore the Stable Horde has started gathering thousands of Stable Diffusion images and their parameters generated by their community on a voluntary basis, which will then be used to build a completely new AI dataset. These images will not only be incentivized to be rated aesthetically as well, but also to be rated pairwise. \n\nStable Horde supports many models in addition to base Stable Diffusion models. At the time of writing this, nearly 80 finetunes are available. There are currently no datasets of generated images from Stable Diffusion finetunes, this will be a world first!\n\nThe way this works is that whenever an image set (meaning a number of images out of the same prompt) is generated, a follow-up call can be done to submit ratings for all, or some of them. If the set includes more than one image, optionally the best of the set can be selected, which will serve as a pairwise rating. If no “best” image has been selected but they have been aesthetically rated, the best image will be automatically selected among those ratings.\n\nThrough this collaboration, LAION will be provided with a dataset of millions of images rated in such a manner, along with their prompts, which can be made freely accessible to train further models with improved quality.\n\n\n## What's next?\n\nThe Stable Horde plans to onboard more ratings into their dataset, such as tagging images for artifacts, watermarks, nudity and the like. The provided set will continue expanding so long as people volunteer their time and processing power. \n\n## Support this endeavour\n\nIf you have any questions or need support about the Stable Horde or the rating system, they have [a very active discord server you can join](https://discord.gg/3DxrhksKzn). If you want to support this initiative, you can help by either rating images yourself, or by [onboarding your own GPU as a horde worker](https://github.com/db0/AI-Horde-Worker) which will help the community generate more images and thus increase the size of the open dataset they provide.","date":1673136000000},{"slug":"laion-coco","frontmatter":{"title":"Laion coco: 600M synthetic captions from Laion2B-en","author":"Christoph Schuhmann, Andreas Köpf, Richard Vencu, Theo Coombes, Romain Beaumont","date":"Sep 15, 2022","previewImg":"/images/blog/laion_coco_beach.png"},"content":"\nAuthor: [Christoph Schuhmann](https://github.com/christophschuhmann), [Andreas Köpf](https://github.com/andreaskoepf) , [Theo Coombes](https://github.com/TheoCoombes), [Richard Vencu](https://github.com/rvencu/), [Benjamin Trom](https://github.com/limiteinductive) , [Romain Beaumont](https://github.com/rom1504) \n\n**We present LAION-COCO, the world’s largest dataset of 600M generated high-quality captions for publicly available web-images**\n\nLaion5B has five billion natural captions. They provide a lot of information, but could synthetic captions complement them ?\n\nTo answer this question, we use a combination of existing, publicly available models to produce high quality captions for images in the style of [MS COCO](https://paperswithcode.com/dataset/coco).\n\nWe captioned 600M images from the english subset of Laion-5B with an ensemble of [BLIP](https://github.com/salesforce/BLIP) L/14 and 2 CLIP versions (L/14 and RN50x64).  \n\nWith this post we release them openly today.\n\nThis will make it possible to investigate the value of generated captions to train models. We’re curious on how these synthetic captions could impact models trained on them!\n\n\n## Download it\n\nThe 600M samples are provided in parquet files. Columns include the original caption, the url, the top caption and a list of alternative captions with lower CLIP-similarity scores.\n\n[https://huggingface.co/datasets/laion/laion-coco](https://huggingface.co/datasets/laion/laion-coco) \n\n\n## Samples\n\n\n\n\u003cimg src=\"/images/blog/ring.png\" style=\"height:250px; width: auto;\"/\u003e\n\n\n**Original:** LGSY 925 Sterling Silver Double Heart Rings Infinity Love Thin Rings Wedding Engagement Promise Engraved Love Rings for Women for Dainty Gift\n\n**Generated:** An open ring with two hearts on it.\n\n\n\n\u003cimg src=\"/images/blog/boot.png\" style=\"height:250px; width: auto;\"/\u003e\n         \n\n**Original:** Female Thick with Pointy Head High Heel Chelsea Ankle Boots\n\n**Generated:** Red leather ankle boots with gold buckles.\n\n\n\n\u003cimg src=\"/images/blog/laion_coco_beach.png\" style=\"height:250px; width: auto;\"/\u003e\n\n\n**Original:** a group of people on horses on a beach\n\n**Generated:** Several people riding horses down the beach on a cloudy day.\n\n\n\n\n\u003cimg src=\"/images/blog/laion_coco_tags.png\" style=\"height:250px; width: auto;\"/\u003e\n\n\n**Original:** a wall with a bunch of graffiti on it\n\n**Generated:** The parking meter is near a graffiti covered building.\n\n                                                                                                                                        \n\n\n\n\u003cimg src=\"/images/blog/sheeple.png\" style=\"height:250px; width: auto;\"/\u003e\n\n\n**Original:** sheeple family\n\n**Generated:** A cartoon drawing of sheep watching TV with their babies.\n\n\n## More samples of images with their generated captions can be found here: \n\n(no cherry picking)\n\n[http://captions.christoph-schuhmann.de/eval_laion/eval.html](http://captions.christoph-schuhmann.de/eval_laion/eval.html) \n\n\n## Method\n\nThe method we used to generate these captions was to\n\n1. We use Blip L/14 to generate 40 captions\n2. Rank them using openai Clip Open AI L/14 ; selected the best 5 captions\n3. Rank using Open AI RN50x64 Clip model to select the best one\n4. Use a small, fine-tuned T0 model to roughly repair grammar and punctuation of the texts\n\nThe hyperparameters were chosen through a [grid search](https://wandb.ai/andreaskoepf/blip_coco_val_sample_sweep_bayes_02/sweeps/1bsha6b0) ([settings](https://github.com/andreaskoepf/CLIP-Image-Captioning/blob/blip_test/blip_coco_val_sample_sweep_bayes_02.yaml)) by Andreas Köpf to best match the style ( ROUGE scores ) of MS COCO texts.\n\n[laion_idle_cap](https://github.com/andreaskoepf/laion_idle_cap/tree/main/docker) is the script that was used for this processing.\n\n\n## Evaluation\n\nWe evaluated these generated captions by asking human evaluators to guess whether a caption is coming from a human or an AI model. We also asked them to rate the quality on a scale from 0(bad) to 5 (good). \n\nIn a first round we presented the evaluators each 200 samples, that contained 100 AI generated and 100 human written MS COCO captions.\n\n### Observations\n\n \n\n\u003cimg src=\"/images/blog/eval_laion_coco.png\" style=\"width:400px\" /\u003e\n\n\nGT: Y-Axis\n\nAnnotation: X-Axis\n\nMean rating \u0026 standard deviation of samples, that were written by a human:\n\nMean: 3.98\n\nStdev: 0.99\n\nMean rating \u0026 standard deviation of samples, that were written by an AI \n\nMean: 3.89\n\nStdev: 1.12\n\nMean rating \u0026 standard deviation of samples, where the annotator believed they were written by a human:\n\nMean: 4.44\n\nStdev: 0.61\n\nMean rating \u0026 standard deviation of samples, where the annotator believed they were generated by an AI \n\nMean: 3.50\n\nStdev: 1.15\n\n### Interpretation\n\nIt is very interesting that the mean scores of the samples generated by humans and generated by the model are very similar. We also notice that the standard deviation of the generated captions is a little bit higher.\n\nWe hypothesize that most in most cases the quality of the generated captions is perceived as as good as the quality of the human written captions.\n\nBut sometimes the captioning model obviously fails and the quality of the results is pretty low because the model doesn't relevant understand concepts about what is going on in the picture, because it's knowledge is not grounded in a sufficiently sophisticated world model.\n\n### Failure cases\n\n\n\n\u003cimg src=\"/images/blog/laion_coco_umbrella.png\" style=\"height:250px; width: auto;\"/\u003e\n\n\n_“Two people posing for the camera in their wedding attire, one with an umbrella over his head and another with long red hair.”_\n\n\n\n\u003cimg src=\"/images/blog/laion_coco_man.png\" style=\"height:250px; width: auto;\"/\u003e\n\n\n_“An older man having a heart attack, with his hand on the chest.”_\n\nWhen we remove all samples from the evaluations that have ratings of either 0 or 1, we Observe that the mean ratings and standard deviations move closer together. \n\n### Scores without ratings of 0 and 1\n\nMean rating \u0026 standard deviation of samples, that were written by a human:\n\nMean: 4.07\n\nStdev: 0.81\n\nMean rating \u0026 standard deviation of samples, that were written by an AI \n\nMean: 4.02\n\nStdev: 0.94\n\nThe mean ratings of the generated captions are still a little bit lower and the standard deviation is still a little bit higher, but the trend is pretty clear. By removing samples with rating 2, the gap between the qualities would probably decrease even further. \n\nPresentation only generated captions:\n\nIn a next step, we presented the human evaluators 400 captions that were only generated by the model (no human written captions in between):\n\nMean rating of all samples \n\n3.81\n\nStandard deviation of all samples \n\n0.94\n\n% rated as human\n\n47.5\n\n% rated as AI\n\n52.5\n\nWe observe that the human evaluators thought in 47.5% of all cases, that the captions were written by a human. This makes us confident that our captains are on average pretty good. When we told the evaluators later that all captions were generated by the model they told us that it was very hard for them to judge whether a caption was written by a model or a human, and that it only was easy for them in obvious failure cases.\n\n### Conclusions\n\nWe conclude that Our ensemble of BLIP and CLIP is already pretty good and capable of generating captions with a quality that is on average pretty close to the human written captions of MS Coco. \n\nIt would be very interesting for future work to let people rate our generated captions at larger scale and then filter out the samples with low rating values. These results could be used to train models to rate the quality of captions and to predict whether a caption looks like a generated or a human written caption.\n\nAnd even without further automated filtering, an ensemble of our captions and human evaluators would be a pretty good workflow to curate high quality captions at much lower costs than if we would ask humans to write them from scratch.\n\n\n## Credit assignments\n\n\n\n* [Christoph Schuhmann](https://github.com/christophschuhmann) lead the project, implemented a first version of the code, ran most of the generations \u0026 conducted the human evaluations\n* [Andreas Köpf](https://github.com/andreaskoepf) conducted the hyperparameter search \u0026 wrote the code to execute BLIP + CLIP filtering at scale\n* [Theo Coombes](https://github.com/TheoCoombes) managed the server that coordinated which GPU worker got which part of LAION to work on\n* [Romain Beaumont](https://github.com/rom1504) packaged the .json into parquet files, sent to HF and wrote the first draft of this post\n* [Richard Vencu](https://github.com/rvencu/) provided the infra structure to use the idle compute for this project\n* [Benjamin Trom](https://github.com/limiteinductive) wrote code that help us to convert the .json files to parquet\n\nWe thank [stability.ai](https://stability.ai/) for providing the compute used to generate the captions in the dataset.\n","date":1663200000000},{"slug":"laion-translated","frontmatter":{"title":"Laion translated: 3B captions translated to English from laion5B","author":"Marianna Nezhurina, Romain Beaumont, Richard Vencu and Christoph Schuhmann","date":"Sep 15, 2022","previewImg":"/images/blog/laion-translated-samples.png"},"content":"\nAuthor: [Marianna Nezhurina](https://github.com/marianna13) [Romain Beaumont](https://github.com/rom1504/) [Richard Vencu](https://github.com/rvencu) [Christoph Schuhmann](https://github.com/christophschuhmann)  \n\nLaion5B dataset was automatically collected from a section of the human web (common crawl). Can models generate different and interesting data compared to what humans write?\n\nThat’s a question we are interested in investigating. To let the community study it, we translated 3B samples of Laion5B from many languages into English.\n\nWe released 3 billions captions for the multilingual part of Laion5B. This makes it possible to use the whole Laion5B dataset to train English models. This also enables training models using these aligned pairs such as [Multilingual-CLIP](https://github.com/FreddeFrallan/Multilingual-CLIP).\n\nWe’re curious what you will do using it!\n\n\n## Downloading it\n\nThe dataset is available in huggingface as parquet files containing the caption, translated caption and urls.\n\n[laion1B-nolang-joined-translated-to-en](https://huggingface.co/datasets/laion/laion1B-nolang-joined-translated-to-en) \n\n[laion2B-multi-joined-translated-to-en](https://huggingface.co/datasets/laion/laion2B-multi-joined-translated-to-en)  \n\n\n## Processing\n\nEvery caption of the original dataset was translated with Facebook’s [M2M100 1.2B model](https://huggingface.co/facebook/m2m100_1.2B) using the following [script](https://github.com/marianna13/translate_dataset/blob/main/translate_data.py). All other fields remain the same as in the original [LAION2B Multi Joined](https://huggingface.co/datasets/laion/laion2B-multi-joined). To make translation possible the original dataset was split into parts with 50k samples in each and every such small part was translated in parallel on GPU nodes and saved in a separate parquet file. The speed of translation depends on the number of nodes and GPUs. The processing was done with 20 nodes with 8 GPUs in each and the speed of translation (including preprocessing and data loading) is 34 samples/per GPU/per second. Then, all translated parquets were [merged together using Spark](https://github.com/marianna13/translate_dataset/blob/main/join_additional.py) and saved as 128 parquet files. The resulting dataset was [joined with the aesthetics scores](https://github.com/marianna13/translate_dataset/blob/main/join_aesthetics.py). \n\n\n## Dataset columns\n\n\n\n* TEXT (the original text of caption)\n* LANGUAGE (language of the original TEXT)\n* ENG TEXT (translation in English of the original TEXT)\n* URL (URL of the image)\n* WIDTH (width of the image)\n* HEIGHT (height of the image)\n* Hash (hash of the URL and TEXT)\n* Pwatermark (probability of being a watermarked image, computed using our [watermark detector](https://github.com/LAION-AI/LAION-5B-WatermarkDetection))\n* Punsafe (probability of being an unsafe image, computed using our [clip based detector](https://github.com/LAION-AI/CLIP-based-NSFW-Detector))\n* Similarity (cosine between text and image ViT-B/32 embeddings, clip for en, mclip for multi and nolang)\n* Prediction (aesthetics score)\n\n\n## Samples from the translated dataset:\n\n\n![laion-2B-translated-samples](/images/blog/laion-translated-samples.png \"laion-2B-translated-samples\")\n\n\n\n## Laion2B-multi-translated\n\n\n### Dataset stats\n\n_Note. Dataset stats were computed using [this](https://github.com/marianna13/translate_dataset/blob/main/get_dataset_stats.py) script._\n\nNumber of uniques 2266M (2266193302)                                            \n\nNumber with WIDTH \u003e= 0 and WIDTH \u0026lt;= 128 160M (160260569)                        \n\nNumber with WIDTH \u003e= 128 and WIDTH \u0026lt;= 256 734M (734166164)                      \n\nNumber with WIDTH \u003e= 256 and WIDTH \u0026lt;= 512 849M (849569769)                      \n\nNumber with WIDTH \u003e= 512 and WIDTH \u0026lt;= 1024 457M (457572747)                     \n\nNumber with WIDTH \u003e= 1024 86M (86750813)                                        \n\nNumber with HEIGHT \u003e= 0 and HEIGHT \u0026lt;= 128 103M (103514467)                      \n\nNumber with HEIGHT \u003e= 128 and HEIGHT \u0026lt;= 256 614M (614490681)                    \n\nNumber with HEIGHT \u003e= 256 and HEIGHT \u0026lt;= 512 753M (753540968)                    \n\nNumber with HEIGHT \u003e= 512 and HEIGHT \u0026lt;= 1024 686M (686553437)                   \n\nNumber with HEIGHT \u003e= 1024 153M (153139456)                                     \n\nNumber with lenengtext \u003e= 0 and lenengtext \u0026lt;= 25 506M (506238532)               \n\nNumber with lenengtext \u003e= 25 and lenengtext \u0026lt;= 50 849M (849160165)              \n\nNumber with lenengtext \u003e= 50 and lenengtext \u0026lt;= 100 840M (840635023)             \n\nNumber with lenengtext \u003e= 100 and lenengtext \u0026lt;= 150 136M (136709119)            \n\nNumber with lenengtext \u003e= 150 5M (5148507)\n\n\n### Similarities between text and images\n\n10000 images and captions were sampled from the dataset, [CLIP embeddings were computed](https://github.com/marianna13/translate_dataset/blob/main/get_clip_embs_similarities.py) (for original texts embeddings were computed using Multilingual CLIP). Then dot products between image and text embeddings were computed (for both original and translated dataset) to get similarities between texts and images. Here’s the distribution of average similarities for two datasets:\n\nSimilarity for original dataset:\n\n10% quantile -  0.2552971839904785\n\n20% quantile -  0.2633610963821411\n\n30% quantile -  0.2694466710090637\n\n40% quantile -  0.2750270366668701\n\n50% quantile -  0.28088638186454773\n\n60% quantile -  0.28750720620155334\n\n70% quantile -  0.2950591444969177\n\n80% quantile -  0.3049575388431549\n\n90% quantile -  0.32077282667160034\n\nSimilarity for translated dataset:\n\n10% quantile  -  0.23388671875\n\n20% quantile  -  0.25390625\n\n30% quantile  -  0.265869140625\n\n40% quantile  -  0.2763671875\n\n50% quantile  -  0.2861328125\n\n60% quantile  -  0.29638671875\n\n70% quantile  -  0.306884765625\n\n80% quantile  -  0.31982421875\n\n90% quantile  -  0.338134765625\n\n\n\n![laion-2B-translated](/images/blog/laion-2B-translated.png \"laion-2B-translated\")\n\n\n\n## Laion1B-nolang-translated\n\n[LAION1B Nolang Joined](https://huggingface.co/datasets/laion/laion1B-nolang-joined) dataset was also translated with a similar [script](https://github.com/marianna13/translate_dataset/tree/main) and in the same way as Multi. This dataset doesn’t have the language column so the model also had to determine language. All other columns in the Nolang dataset are the same as in Multi. And just like translated [LAION2B Multi Joined](https://huggingface.co/datasets/laion/laion2B-multi-joined), LAION1B Nolang was also joined with corresponding [aesthetics scores](https://github.com/marianna13/translate_dataset/blob/main/join_aesthetics.py).\n\n\n### Nolang dataset Stats\n\nNumber of uniques 1260M (1260048307)                                            \n\nNumber with WIDTH \u003e= 0 and WIDTH \u0026lt;= 128 90M (90701133)                          \n\nNumber with WIDTH \u003e= 128 and WIDTH \u0026lt;= 256 409M (409575445)                      \n\nNumber with WIDTH \u003e= 256 and WIDTH \u0026lt;= 512 475M (475885337)                      \n\nNumber with WIDTH \u003e= 512 and WIDTH \u0026lt;= 1024 239M (239035772)                     \n\nNumber with WIDTH \u003e= 1024 59M (59942110)                                        \n\nNumber with HEIGHT \u003e= 0 and HEIGHT \u0026lt;= 128 59M (59814914)                        \n\nNumber with HEIGHT \u003e= 128 and HEIGHT \u0026lt;= 256 370M (370913206)                    \n\nNumber with HEIGHT \u003e= 256 and HEIGHT \u0026lt;= 512 451M (451897702)                    \n\nNumber with HEIGHT \u003e= 512 and HEIGHT \u0026lt;= 1024 316M (316723245)                   \n\nNumber with HEIGHT \u003e= 1024 87M (87671543)                                       \n\nNumber with lenengtext \u003e= 0 and lenengtext \u0026lt;= 25 312M (312548202)               \n\nNumber with lenengtext \u003e= 25 and lenengtext \u0026lt;= 50 555M (555971621)              \n\nNumber with lenengtext \u003e= 50 and lenengtext \u0026lt;= 100 413M (413430230)             \n\nNumber with lenengtext \u003e= 100 and lenengtext \u0026lt;= 150 20M (20446701)              \n\nNumber with lenengtext \u003e= 150 0M (138974)  \n\n\n### Similarities between text and images for Nolang\n\n10000 images and captions were randomly sampled from the dataset, [CLIP embeddings were computed](https://github.com/marianna13/translate_dataset/blob/main/get_clip_embs_similarities.py) (for original texts embeddings were computed using Multilingual CLIP). Then dot products between image and text embeddings were computed (for both original and translated dataset) to get similarities between texts and images. Here’s the distribution of average similarities for two datasets:\n\nSimilarity for original dataset:\n\n10% quantile  -  0.258196085691452\n\n20% quantile  -  0.266357421875\n\n30% quantile  -  0.2728866934776306\n\n40% quantile  -  0.27902457118034363\n\n50% quantile  -  0.28590404987335205\n\n60% quantile  -  0.29329144954681396\n\n70% quantile  -  0.3023602366447449\n\n80% quantile  -  0.31363412737846375\n\n90% quantile  -  0.3313804566860199\n\nSimilarity for translated dataset:\n\n10% quantile  -  0.2406005859375\n\n20% quantile  -  0.2607421875\n\n30% quantile  -  0.27490234375\n\n40% quantile  -  0.2861328125\n\n50% quantile  -  0.296142578125\n\n60% quantile  -  0.306396484375\n\n70% quantile  -  0.317626953125\n\n80% quantile  -  0.33203125\n\n90% quantile  -  0.353271484375\n\n\n![laion-1B-translated](/images/blog/laion-1B-translated.png \"laion-1B-translated\")\n\n\n## Credit\n\n\n\n* [Marianna Nezhurina](https://github.com/marianna13) translated the samples, packaged them, computed stats and wrote most of this post\n* [Romain Beaumont](https://github.com/rom1504/) helped out on packaging and scaling\n* [Richard Vencu](https://github.com/rvencu) set up all the infra that made using idle compute possible\n* [Christoph Schuhmann](https://github.com/christophschuhmann) suggested the project and guided the work to completion\n\nWe thank [https://stability.ai/](https://stability.ai/) for providing the compute for this massive translation. This was a great use of pre-emptible jobs to fill any idle compute available!","date":1663200000000},{"slug":"large-openclip","frontmatter":{"title":"Large scale openCLIP: L/14, H/14 and g/14 trained on LAION-2B","author":"Romain Beaumont","date":"Sep 15, 2022","previewImg":"/images/blog/compare3.png"},"content":"\nWe trained three large CLIP models with [OpenCLIP](https://github.com/mlfoundations/open_clip): ViT-L/14, ViT-H/14 and ViT-g/14 (ViT-g/14 was trained only for about a third the epochs compared to the rest). The H/14 model achieves **78.0%** zero shot top-1 accuracy on ImageNet and **73.4%** on zero-shot image retrieval at Recall@5 on MS COCO. As of September 2022, this is the best open source CLIP model.\n\nCLIP makes it possible to compute representations of images and texts to measure how **similar** they are. It can be used for \n\n\n\n* Zero shot classification: compare an image with the text of the class to know which class is most similar (e.g., ImageNet classification)\n* Retrieval: compare an image or a text to billions of text or images to find the most similar (e.g. as in [clip-retrieval](https://rom1504.github.io/clip-retrieval/) )\n* Generation\n    * CLIP guidance: decide a text you want to generate, then use an image generator model, and use the CLIP distance between what’s generated and the text to generate a better image (e.g., VQGAN + CLIP)\n    * CLIP conditioning: use a clip text embedding as input of a generator to make it generate this text directly (e.g., stable diffusion)\n\nCLIP models are trained in a self supervised fashion on hundreds of millions or billions of (image, text) pairs.\n\nWith LAION, we produced the LAION-5B dataset that contains 5.8 billions of closely related image and text pairs.\n\nThe CLIP model ViT B/32, released by OpenAI, was initially used to filter this dataset out of common crawl.\n\nProducing the best open source CLIP model out of this data set completes the open source replication of the [excellent](https://openai.com/blog/clip/) CLIP paper that OpenAI released one year ago.\n\n\n## Results\n\nWe replicated the results from openai CLIP in models of different sizes, then trained bigger models. The full evaluation suite on 39 datasets ([vtab+](https://github.com/LAION-AI/CLIP_benchmark)) are available in this [results notebook](https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/results.ipynb) and show consistent improvements over all datasets.\n\nThe larger models we release today are L/14, H/14 and g/14.\n\nL/14 was trained on JUWELS Booster supercomputer by [Ross wightman](https://github.com/rwightman). H/14 and g/14 were trained on stability cluster by [Romain Beaumont](https://github.com/rom1504) . While L/14 and H/14 were trained using 34B samples from LAION-2b, g/14 used a substantially smaller sample scale for training, seeing only 12B samples (see tables for more details).\n\n\n#### 32B samples seen\n\n\n| **Model name**                                                                   | **Batch size**                   | **Samples seen**           | **Text Params** | **Image params** | **Imagenet top1** | **Mscoco image retrieval at 5** | **Flickr30k image retrieval at 5** |\n|----------------------------------------------------------------------------------|----------------------------------|----------------------------|-----------------|------------------|-------------------|---------------------------------|------------------------------------|\n| [B/32](https://wandb.ai/rom1504/eval_openclip/reports/B-32-2B--VmlldzoyNDkwNDMy) | 79k                              | 34B (16 epochs of laion2B) | 63.43M          | 87.85M           | 66.6%             | 65.4%                           | 88.4%                              |\n| L/14                                                                             | 79k for 14B samples, 86K for 18B | 32B                        | 123.65M         | 303.97M          | 75.3%             | 71.1%                           | 92.9%                              |\n| [H/14](https://wandb.ai/rom1504/eval_openclip/reports/H-14--VmlldzoyNDAxODQ3)    | 79k                              | 32B (16 epochs of laion2B) | 354.03M         | 632.08M          | 78.0%             | 73.4%                           | 94%                                |\n\n\n\n#### 12B samples seen\n\n\n| **Model name**                                                                     | **Batch size**                             | **Samples seen**                        | **Text Params** | **Image params** | **Imagenet top1** | **Mscoco image retrieval at 5** | **Flickr30k image retrieval at 5** |\n|------------------------------------------------------------------------------------|--------------------------------------------|-----------------------------------------|-----------------|------------------|-------------------|---------------------------------|------------------------------------|\n| B/32                                                                               | 32k                                        | 12B (32 epochs of laion400m)            | 63.43M          | 87.85M           | 62.9%             | 60.8%                           | 85.5%                              |\n| B/16                                                                               | 32k                                        | 12B (32 epochs of laion400m)            | 91.16M          | 86.19M           | 69%               | 63.6%                           | 85.5%                              |\n| L/14                                                                               | 32k                                        | 12B (32 epochs of laion400m)            | 123.65M         | 303.97M          | 72%               | 68.1%                           | 90.8%                              |\n| [g/14](https://wandb.ai/rom1504/eval_openclip/reports/slow-g-14--VmlldzoyNTMwMjg5) | 32k for 8B samples then 64k for 4B samples | 12B (similar to 32 epochs on laion400m) | 354.03M         | 1012.65M         | 76.6%             | 72.4%                           | 93.5%                              |\n\n\nIn addition to having overall better results, we hope the larger text encoder will help improve text understanding. The good performance on the retrieval metrics seems to be a good indicator of this property.\n\nNote the difference in samples seen between the H/14 and the g/14 model. This explains the difference in performance. We picked this lower number to try and fix the stability issue at a lower cost. Eventually they were fixed (by using bfloat16). The performance of this model falls in the scaling curve of 12B sample seen (similar to 32 epochs of laion400m), and a g/14 trained on 32B samples of laion2B would most likely follow the same trends as the other models and get better performance as H/14.\n\n![alt_text](/images/blog/compare3.png \"image_tooltip\")\n\n\n\n## Released checkpoints\n\nWe release the checkpoints for the models, they are available through [openclip](https://github.com/mlfoundations/open_clip) and in HuggingFace hub at [B/32](https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K) [L/14](https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K) [H/14](https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K) and [g/14](https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K)\n\n\n## Related works\n\nRelated work results:\n\n\n| **Model name** | **Samples seen**       | **Imagenet top1** | **Mscoco image retrieval at 5** | **Flickr30k image retrieval at 5** |\n|----------------|------------------------|-------------------|---------------------------------|------------------------------------|\n| Openai B/32    | 12B (32 epochs of WIT) | 62%               |                                 |                                    |\n| Openai B/16    | 12B (32 epochs of WIT) | 69%               |                                 |                                    |\n| Openai L/14    | 12B (32 epochs of WIT) | 75.4%             | 61%                             | 87%                                |\n| ALIGN          | 20B                    | 76.4%             | 69.8%                           | 93.3%                              |\n| BASIC          | 32B                    | 85.7%             |                                 |                                    |\n| CoCa           | 32B                    | 86.3%             | 74.2%                           | 95.7%                              |\n\n\n[BASIC](https://arxiv.org/abs/2111.10050) and  [ALIGN](https://arxiv.org/abs/2102.05918) got excellent imagenet results. They used either different image encoder architecture (EfficientNet, CoAtNet), a larger network scale (BASIC-L with 2.4B params) or pre trained their network with supervised learning on a large dataset (BASIC CoAtNet vision encoder).\n\n[COCA](https://arxiv.org/abs/2205.01917) additionally used captioning loss during training with a multi-modal text decoder which predicted text tokens autoregressively and got 86.3% top1, employing a larger model scale (2.1B params)\n\n\n## Scaling up notes\n\nDuring these training runs, we encountered several interesting issues:\n\n\n\n* Using many GPUs means many of them can have hardware issues and can freeze, crash or even just be slow. This is a particularly annoying problem to handle as if one GPU has an issue, the synchronized nature of distributed training means that all GPUs get stuck. I created [https://github.com/rom1504/gpu-tester](https://github.com/rom1504/gpu-tester) to figure out what are the bad GPUs and exclude them\n* Stability issues! When scaling up the model size, the batch size and the dataset size, at around half the training the loss starts increasing until it reaches a plateau. We tried many possible things (find the list [there](https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c/edit)) and eventually concluded on a surprisingly simple solution: **using amp bfloat16 instead of amp float16 made the training fully stable**\n\nAnd also made some discoveries:\n\n\n\n* It seems using a very large batch size (up to 159k) can help reach even higher performance. This is most likely due to the fact that contrastive learning provides information to the loss as a logit matrix, hence having N times more samples in a batch means N square logits. We did not verify this systematically but BASIC paper provides more experiments and a theoretical justification for this result.\n* It’s possible to get a reasonably performing g/14 CLIP by doing a much shorter cosine decay =\u003e getting a 68% g/14 in 10k gpu hours.\n* Grad checkpointing allows to do 10x on the batch size\n\n\n### Training stability issues\n\nStability of training was the main problem we solved in this iteration of the scaling up of OpenCLIP. At around half the training (for L/14, H/14 and g/14), the loss started going up until it plateaued very high (11) and didn’t go down anymore.\n\nWe tried many possible fixes (decreasing lr, gradient shrinking, gradient clipping, cosine attention, post layer norm, …) with little to no effect when trying to resume from before the crash. \n\nEventually only 2 things worked:\n\n\n\n* Finishing the  lr decay very fast : in 8 epochs (compared to the planned 256 epochs). That managed to get most of the performance out of clip H. \n* Switching from float16 to bfloat16 solved the problem while being faster for clip g. We then applied the same fix for clip H and finished its training properly.\n\n[See all the training notes](https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c/edit) with all the details on all the possible ideas that didn’t work.\n\n\n### Training speeds\n\nTo better understand the cost and length of training of clip, we provide these training speed numbers. All numbers assume a100 with 40GB of VRAM. We used gradient checkpointing.\n\n| Model                                                                           | Batch size per gpu | Precision | Number of gpus | Sample per second per gpu |\n|---------------------------------------------------------------------------------|--------------------|-----------|----------------|---------------------------|\n| [B/32](https://wandb.ai/rom1504/open-clip/runs/rnxrp6k7?workspace=user-rom1504) | 96                 | float16   | 824            | 228                       |\n| [H/14](https://wandb.ai/rom1504/open-clip/runs/2zphcgkn?workspace=user-rom1504) | 96                 | float16   | 824            | 30                        |\n| [g/14](https://wandb.ai/rom1504/open-clip/runs/21cpomx2?workspace=user-rom1504) | 40                 | float16   | 800            | 20                        |\n| [H/14](https://wandb.ai/rom1504/open-clip/runs/3l7ppqh3?workspace=user-rom1504) | 96                 | bfloat16  | 824            | 42                        |\n| [g/14](https://wandb.ai/rom1504/open-clip/runs/1pby5fkb?workspace=user-rom1504) | 80                 | bfloat16  | 800            | 31                        |\n\n\nThe speed usually increases with batch size per gpu until a plateau is reached. The speed also increases with the number of gpu. After a certain number of gpus, the curve becomes slower than linear.\n\nBfloat16 which we used in the second part of training provides both better stability and faster sample/s for clip models.\n\n\n## What’s next\n\nThe models will be used for many applications, including clip guiding and conditioning. Even better results could be reached on models like stable diffusion by using a better clip model!\n\nNow that the scaling properties of clip are proven in an open source reproduction, a lot of doors open. Here are some ideas of next steps:\n\n\n\n* Changing the text encoder to work in the multilingual setting (to get a model like [Multilingual-CLIP](https://github.com/FreddeFrallan/Multilingual-CLIP) but trained contrastively, with hopefully even better results!) and scale it up\n* Can we get clip models while using less gpu hours ? extracting the knowledge from smaller clips into a bigger one may help bootstrap the learning process (see [encoder-distill](https://github.com/iejMac/encoder-distill) from [iejMac](https://github.com/iejMac) getting some preliminary results on this)  \n* The clip idea can be expanded to other modalities, see [CLAP](https://github.com/LAION-AI/CLAP) for text-audio alignment\n\nIf you have ideas or want to help out, feel free to reach out in laion server.\n\n\n## Contributions\n\nThanks to\n\n\n\n* [Romain Beaumont](https://github.com/rom1504) for running the experiments on H/14 and g/14\n* [Ross Wightman](https://github.com/rwightman) for conducting all the openclip experiments at JUWELS Booster (Juelich Supercomputing Center) up to L/14 and providing valuable feedback during these H and g clip trainings\n* [Phil Wang](https://github.com/lucidrains) for providing ideas and code (cosine attention, post layer norm, ..) during the stability issues\n* [Boris Dayma](https://github.com/borisdayma) and [Mitchell Wortsman](https://mitchellnw.github.io/) for both proposing to try float32 that showed precision was an issue and eventually lead to trying bfloat16\n* [Blinkdl](https://github.com/Blinkdl) for proposing interesting ideas regarding tuning the learning rate\n* [Christoph Schuhmann](https://github.com/christophschuhmann) for daring proposing to train such large clips, following up on all these experiments, and finding very early that training were frozen, saving some valuable time\n* [Jenia Jitsev](https://github.com/JeniaJitsev) for providing ideas and feedback during the training issues, supervision and coordination of the compute grants at JUWELS Booster\n* [Ludwig Schmidt](https://github.com/ludwigschmidt) for reviewing this post and giving many ideas about LAION datasets and CLIP \n* [Mehdi Cherti](https://github.com/mehdidc) for helping to debug the evaluation scripts and getting comparable results for MS-COCO\n\nAnd of course [Emad](https://twitter.com/EMostaque) (Stability AI) for providing the many GPUs used during these experiments! (g/14 and H/14!)\n\nFor the L/14 training, we gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at Jülich Supercomputing Centre (JSC), Germany.\n","date":1663200000000},{"slug":"laion-aesthetics","frontmatter":{"title":"LAION-Aesthetics","author":"Christoph Schuhmann","date":"Aug 16, 2022","previewImg":"/images/blog/LAION-Aesthetics.jpg"},"content":"\nWe present LAION-Aesthetics, several collections of subsets from LAION 5B with high visual quality.\n\n![](https://raw.githubusercontent.com/LAION-AI/laion.ai/Chris/blog/LAION-Aesthetics.jpg)\n\nTo create LAION-Aesthetics we trained several lightweight models that predict the rating people gave when they were asked _“How much do you like this image on a scale from 1 to 10?”_.\n\n## LAION-Aesthetics V1\n\nWe started with training a linear model on 5000 image-rating pairs from the [SAC](https://github.com/JD-P/simulacra-aesthetic-captions) dataset (which only contained 5000 samples at that time).\n\nSimulacra Aesthetic Captions is a dataset of over 238000 synthetic images generated with AI models such as CompVis latent GLIDE and Stable Diffusion from over forty thousand user submitted prompts.\n\nAs inputs this model uses not the images themselves, but their CLIP Image embeddings produced with the Open AI CLIP VIT L 14 model. We call this model LAION-Aesthetics_Predictor V1.\n\nIts results were so encouraging, that we decided to produce 8M and 120M sample subsets of the LAION 5B images with the highest predicted scores, of those that have english texts.\n\nWe call the dataset consisting of these 2 subsets [LAION-Aesthetics V1](https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md).\n\n![](https://github.com/LAION-AI/laion.ai/blob/Chris/blog/LAION-Aesthetics%20V1.jpg?raw=true)\n\nThe model used for creating this subset can be found [here.](https://github.com/LAION-AI/aesthetic-predictor)\n\nThe LAION-Aesthetics V1 dataset \u0026 further details about it can be found [here.](https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md)\n\n## LAION-Aesthetics V2\n\nAfter these very encouraging results, we continued to experiment and gathered the following data to train more improved MLP (multi-layer perceptron) models:\n\n- More samples from the SAC dataset, which had grown in the meanwhile\n  to 176000 image - rating pairs\n- LAION-Logos, a dataset of 15.000 logo image-text pairs with aesthetic\n  ratings from 1 to 10. We collected this dataset to improve the models\n  abilities to evaluate images with more or less aesthetic texts in\n  them.\n- [The Aesthetic Visual Analysis (AVA) dataset](https://github.com/imfing/ava_downloader), which is a large-Scale database for aesthetic visual analysis that contains 250000 photos from dpchallenge.com with several aesthetic ratings from 1 to 10 for most images.\n- After training several MLPs with different numbers of layers and parameters and different activation functions, we found that a simple linear model on the top of CLIP ViT/14 produced in our subjective view the visually most appealing results when used to rank images of LAION-5B. (Even though other MLPs with e.g. Relu functions produced slightly lower MSE and MAE loss values.) We call the resulting model trained on SAC, LAION-Logos and AVA [LAION-Aesthetics_Predictor V2.](https://github.com/christophschuhmann/improved-aesthetic-predictor)\n- Visualizations of sorting all 2.37B images from LAION 5B that have English captions into 40 buckets with the LAION-Aesthetics_Predictor V2 can be found [here.](http://captions.christoph-schuhmann.de/aesthetic_viz_laion_sac+logos+ava1-l14-linearMSE-en-2.37B.html)\n\nUsing LAION-Aesthetics_Predictor V2, we created the following subsets of the LAION 5B samples with English captions:\n\n- 1,2B image-text pairs with predicted aesthetics scores of 4.5 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-4.5.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_4.5plus)\n- 939M image-text pairs with predicted aesthetics scores of 4.75 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-4.75.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_4.75plus)\n- 600M image-text pairs with predicted aesthetics scores of 5 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-5.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_5plus)\n- 12M image-text pairs with predicted aesthetics scores of 6 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-6.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_6plus)\n- 3M image-text pairs with predicted aesthetics scores of 6.25 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-6.25.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_6.25plus)\n- 625K image-text pairs with predicted aesthetics scores of 6.5 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-6.5.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_6.5plus)\n\nThese subsets overlap. 5 fully includes 6 which includes 6.25 and so on. We call the collection of these subsetsLAION-Aesthetics V2.\n\nWe provided the dataset to the [CompViz](https://github.com/CompVis) team led by Robin Rombach and Patrick Esser. They used the 5+ subset to train [Stable Diffusion V1](https://github.com/CompVis/stable-diffusion/tree/ce05de28194041e030ccfc70c635fe3707cdfc30#stable-diffusion-v1) model.\n\n## What's next?\n\nAt the moment we are translating all 2,15B samples from LAION 5B of the multilingual subset to English using the 1,2B parameter [M2M-100](https://github.com/facebookresearch/fairseq/tree/main/examples/m2m_100) model .\n\nThis will allow us to roughly double the size of V2.\n\nAdditionally, we are already working on new multimodal large-scale dataset, this time at webpage-level, similar to the interleaved image-text dataset Deepmind used for [Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model), but also with audio \u0026 video files ... and much, much bigger. :)\n\nStay tuned \u0026 keep checking our blog for more datasets in the near future.\n\n## Connect\n\nIf you have any questions or comments or the wish to support our efforts, don’t hesitate to [join our Discord community and contact us.](https://discord.gg/vnjVezbeSJ)\n\n_Christoph Schuhmann ( spirit-from-germany#1488 ) and Romain Beaumont ( rom1504#5008 )_\n","date":1660608000000},{"slug":"laion-5b","frontmatter":{"title":"LAION-5B: A NEW ERA OF OPEN LARGE-SCALE MULTI-MODAL DATASETS","author":"Romain Beaumont","date":"Mar 31, 2022","previewImg":"/images/blog/5b.png"},"content":"\nWe present a dataset of 5,85 billion CLIP-filtered image-text pairs, 14x bigger than LAION-400M, previously the biggest openly accessible image-text dataset in the world - see also our [NeurIPS2022 paper](https://arxiv.org/abs/2210.08402)\n\n[See our update](/notes/laion-maintenance.md) on the LAION-5B dataset.\n\nLarge image-text models like ALIGN, BASIC, Turing Bletchly, FLORENCE \u0026 GLIDE have shown better and better performance compared to previous flagship models like CLIP and DALL-E. Most of them had been trained on billions of image-text pairs and unfortunately, no datasets of this size had been openly available until now. To address this problem we present LAION 5B, a large-scale dataset for research purposes consisting of 5,85B CLIP-filtered image-text pairs. 2,3B contain English language, 2,2B samples from 100+ other languages and 1B samples have texts that do not allow a certain language assignment (e.g. names ). Additionally, we provide several nearest neighbor indices, an improved web interface for exploration \u0026 subset creation as well as detection scores for watermark and NSFW. We also announce a full reproduction of a clip training trained on LAION-400M at [open_clip](https://github.com/mlfoundations/open_clip). Explore the dataset at the [search demo](https://rom1504.github.io/clip-retrieval/). See also the [same post on laion website](https://laion.ai/laion-5b-open-dataset) .\n\nWe thank our sponsors [hugging face](https://huggingface.co/), [doodlebot](http://doodlebot.ai/) and [stability](https://stability.ai/) for providing us with computing resources to produce this dataset! We also thank the-eye.eu for hosting the image embeddings and a copy of the whole dataset.\n\n### Disclaimer on dataset purpose and content warning\n\nThe motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet. Our recommendation is therefore to use the dataset for research purposes. Be aware that this large-scale dataset is uncurated. Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer. Therefore, please use the demo links with caution and at your own risk. It is possible to extract a “safe” subset by filtering out samples based on the safety tags (using a customized trained NSFW classifier that we built). While this strongly reduces the chance for encountering potentially harmful content when viewing, we cannot entirely exclude the possibility for harmful content being still present in safe mode, so that the warning holds also there. We think that providing the dataset openly to broad research and other interested communities will allow for transparent investigation of benefits that come along with training large-scale models as well as pitfalls and dangers that may stay unreported or unnoticed when working with closed large datasets that remain restricted to a small community. Providing our dataset openly, we however **do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress**.\n\n## Introduction\n\nSince the release of CLIP \u0026 DALL-E in January 2021, several similar large multi-modal language-vision models have been trained by large groups. Models like FLORENCE, Turing Bletchley, ALIGN \u0026 BASIC demonstrated very strong transfer capabilities on novel datasets in absence of per-sample labels, which also steadily improved when growing training data amount, following scaling laws observed in previous research work. These models require billions of image-text pairs to achieve competitive performances and unfortunately, no billion-scale image-text pair dataset had been openly available up until now. To address this problem we release LAION 5B, a CLIP-filtered dataset of 5,85 billion high-quality image-text pairs, their CLIP ViT-L/14 embeddings, kNN-indices, a web interface for exploration \u0026 subset-creation and NSFW- and watermark-detection scores and tools. We describe the procedure to create the dataset and demonstrate successful training of DALL-E architecture. Having sufficiently large scales, the dataset opens venues for research on multi-modal language-vision models to a broad community.\n\n## License\n\nWe distribute the metadata dataset (the parquet files) under the [Creative Common CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) license, which poses no particular restriction. The images are under their copyright.\n\n## Dataset columns\n\nWe provide these columns :\n\n- URL: the image url, millions of domains are covered\n- TEXT: captions, in english for en, other languages for multi and nolang\n- WIDTH: picture width\n- HEIGHT: picture height\n- LANGUAGE: the language of the sample, only for laion2B-multi, computed using [cld3](https://github.com/google/cld3)\n- similarity: cosine between text and image ViT-B/32 embeddings, clip for en, mclip for multi and nolang\n- pwatermark: probability of being a watermarked image, computed using our [watermark detector](https://github.com/LAION-AI/LAION-5B-WatermarkDetection)\n- punsafe: probability of being an unsafe image, computed using our [clip based detector](https://github.com/LAION-AI/CLIP-based-NSFW-Detector)\n\npwatermark and punsafe are available either as individual collections that must be [joined](https://github.com/rom1504/laion-prepro/blob/main/laion5B/safety/join.py) with the hash of url+text, either as prejoined collections.\n\n## Dataset Statistics\n\nWe [computed](https://github.com/rom1504/laion-prepro/blob/main/laion5B/stats/compute_stats.py) some statistics on the datasets to let people understand better: Samples are considered unsafe if the model predicts it as unsafe with a probability of more than 0.5. More than 0.8 for watermark. These values are pretty conservative, so the estimated safeness and watermark proportion may be higher than the truth. Other thresholds may be chosen to get a different precision/recall tradeoff.\n\nComputed quantiles are quantiles from 0.05 to 0.95.\n\nAlso see the whole [sheet](https://docs.google.com/spreadsheets/d/19AkcufyABAnbBlsr12VUmlR9oyQWb4uloAQnd-rqJC0/edit#gid=0) and the whole [dashboard](https://datastudio.google.com/reporting/c67c1749-816f-464f-873a-867b4a43f044/page/p_i9he8sxntc/edit)\n\n### Laion2B-en\n\nTotal: 2.3B samples\n\n![](https://lh6.googleusercontent.com/-SW3vGI4_Ojemg_ttYpZvFmC8vTjYavTDgmnY7SsnfF-smnVpLwqbYCUsmB9_1HBmmVbKRE2QXJRwxamNcw1A9sRXDFPSj0YZ2WiptPnNeAMuSF0O_2Yi_CGsm_QChM4eJXd4lyY)\n\nNumber with height and width bigger than\n\n- 256 -\u003e 1324M\n- 512 -\u003e 488M\n- 1024 -\u003e 76M\n\nWidth quantiles: 132.0, 160.0, 180.0, 210.0, 225.0, 240.0, 262.0, 300.0, 309.0, 340.0, 400.0, 450.0, 480.0, 512.0, 600.0, 656.0, 760.0, 960.0, 1050.0\n\nHeight quantiles: 125.0, 150.0, 166.0, 188.0, 208.0, 225.0, 250.0, 270.0, 300.0, 320.0, 350.0, 380.0, 418.0, 470.0, 500.0, 600.0, 672.0, 800.0, 1014.0\n\nUnsafe proportion: 2.9%\n\nWatermark proportion: 6.1%\n\nAverage text length: 67\n\nText length quantiles: 21.0, 25.0, 30.0, 33.0, 37.0, 40.0, 43.0, 47.0, 50.0, 54.0, 58.0, 62.0, 67.0, 72.0, 78.0, 85.0, 96.0, 114.0, 152.0\n\n### Laion2B-multi\n\nTotal: 2.2B samples\n\n![](https://lh4.googleusercontent.com/jLZYO_GMS28fzxwfZS199LOjSeUpTH7HEmjIRwyVAtvJdBkzyHzs83FgnD-hOY0CjK8LDooytibVcbuVa_O5YSoCu1IduEj4Z8uneE8Km-0Y39qHzYAJs4Lr4oEyQh4EeWxHGFOk)\n\nNumber with height and width bigger than\n\n- 256 -\u003e 1299M\n- 512 -\u003e 480M\n- 1024 -\u003e 57M\n\nWidth quantiles: 140.0, 160.0, 188.0, 205.0, 235.0, 250.0, 284.0, 300.0, 324.0, 366.0, 420.0, 480.0, 520.0, 600.0, 640.0, 720.0, 800.0, 960.0, 1080.0\n\nHeight quantiles: 120.0, 144.0, 160.0, 180.0, 200.0, 217.0, 240.0, 262.0, 300.0, 320.0, 350.0, 394.0, 416.0, 458.0, 500.0, 564.0, 636.0, 725.0, 1000.0\n\nTop 10 languages: LANGUAGE count proportion:\n\n- ru 241M 0.106\n- fr 168M 0.074\n- de 150M 0.066\n- es 149M 0.066\n- zh 143M 0.063\n- ja 131M 0.057\n- it 95M 0.042\n- pt 88M 0.038\n- nl 66M 0.029\n- pl 62M 0.027\n- no 49M 0.021\n\nUnsafe proportion: 3.3%\n\nWatermark proportion: 5.6%\n\nAverage text length: 52\n\nText length quantiles: 12.0, 16.0, 20.0, 23.0, 27.0, 30.0, 33.0, 37.0, 40.0, 44.0, 48.0, 52.0, 57.0, 61.0, 67.0, 74.0, 81.0, 93.0, 120.0\n\n### Laion1B-nolang\n\nTotal: 1.2B samples\n\n![](https://lh3.googleusercontent.com/mAI2e-sLE2geRsX8-3Mw-Ye_8wDu0SEDnIBZjWNqJiaAdjtjj7PtBvRhreugGS6_740-KcILnRbvRbDcTIFccrYO-adNz2uRM6zb0VgR3wjZVo_x0dxDhaMouH2KHScZnuNNkvs3)\n\nNumber with height and width bigger than\n\n- 256 -\u003e 1324M\n- 512 -\u003e 488M\n- 1024 -\u003e 76M\n\nWidth quantiles: 135.0, 160.0, 181.0, 207.0, 225.0, 241.0, 264.0, 300.0, 306.0, 338.0, 398.0, 426.0, 499.0, 520.0, 600.0, 655.0, 768.0, 940.0, 1080.0\n\nHeight quantiles: 118.0, 144.0, 160.0, 186.0, 200.0, 220.0, 240.0, 260.0, 292.0, 305.0, 338.0, 368.0, 405.0, 456.0, 500.0, 562.0, 637.0, 768.0, 1000.0\n\nUnsafe proportion: 3%\n\nWatermark proportion: 4%\n\nAverage text length: 46\n\nText length quantiles: 13.0, 17.0, 20.0, 23.0, 26.0, 29.0, 32.0, 35.0, 38.0, 41.0, 44.0, 48.0, 51.0, 56.0, 60.0, 67.0, 73.0, 82.0, 99.0\n\n## Acquisition pipeline\n\n![](https://lh4.googleusercontent.com/5Pcm38fU3jxc9zf1oSNLMTxh9TH4eMG-5n-dwLF-EPNc_o-jRCwa1D1AJIX-dP4UmeEoKkDxg4fWr3Mq5JwQ7TDuKUwMiaww9GPh8EGJwGkCqWXWKx15CY4Jgige8Uu6inl0KSOu)\n\nThe acquisition pipeline follows the flowchart above and can be split into three major components:\n\n- Distributed processing of petabyte-scale Common Crawl dataset, which produces a collection of matching URLs and captions (preprocessing phase)\n- The distributed download of images based on shuffled data to pick a correct distribution of URLs, to avoid too heavy request loads on single websites\n- Few GPU node post-processing of the data, which is much lighter and can be run in a few days, producing the final dataset.\n\n### Distributed processing of Common Crawl\n\nTo create image-text pairs, we parse through WAT files from Common Crawl and parse out all HTML IMG tags containing an alt-text attribute. At the same time, we perform a language detection on text with three possible outputs: English language with confidence, another language with confidence, no language which contains “no detection” and “detection under the confidence threshold”. The “no language” set often contains short texts, mostly with names of people and places. All extracted information by the preprocessing workers were packed and sent to the Postgresql node for storage using the COPY command. The Postgresql server was maintained to keep about 500M records at all times by means of balancing the ingress and egress of data from the database.\n\n### Distributed downloading of the images\n\nWe download the raw images from the parsed URLs with asynchronous requests using Trio and Asks libraries in order to maximize all resources usage: vCPUs, RAM and bandwidth. We found that a single node in the cloud with 1-2 vCPUs, 0.5-1GB RAM and 5-10Mbps download bandwidth is inexpensive enough to allow downloading on a limited budget. Such a unit can process 10000 links in about 10-15 minutes. Each batch consisted of 10000 links taken from the Postgresql server by using the TABLESAMPLE technique, ensuring that the distribution among the 10000 links was following the distribution of the existing 500M records available on the database. We found that the distribution is still good when in the database are still above 20M records to be processed given that we had some 300 downloading workers at any time. The above techniques allowed both maximizing downloading speed and minimizing IP reputation damages.\n\n### CLIP inference at the post-processing stage\n\nThe data pipeline continued with GPU nodes doing inference on the collected image-text pairs, and calculating the similarity of the embeddings for the image and the text. After the similarity score was established we removed the pairs under the threshold we decided to use, i.e 0.28 for the English dataset ( with CLIP ViT B/32 ) and 0.26 for the rest (with mCLIP). As an estimation, we removed about 90% of the samples, trimming the 50+ billion of candidates to just below 6 billion.\n\n### Filtering out unsuitable image-text pairs\n\nAfter downloading the WAT files from Common Crawl, we apply the following filtering conditions:\n\n- All samples with less than 5 characters alt-text length or less than 5 KB image size are dropped.\n- All images with the too big resolution, potentially DOS bombs, were dropped before attempting to process them.\n- Duplicate removal is performed with a bloom filter based on URL. Future runs would include more variate deduplication rules, such as URL + language for the multilanguage dataset.\n- We use CLIP respectively MCLIP to compute embeddings of the image and alt-text. Then we compute the cosine similarity of both embeddings and drop all samples with cosine similarity below 0.28 for the English language ( with CLIP B/32) and 0.26 for the multilingual dataset (MCLIP). These thresholds were selected based on human inspection of the test results.\n- We use the CLIP embeddings of images and texts to filter out to the possible extent the illegal content.\n\n## Dataset preparation pipeline\n\nAfter processing and filtering common crawl, 5,85B of URL/text samples remained. We did additional steps after that in order to prepare the dataset. See this [semantic search blogpost](https://rom1504.medium.com/semantic-search-with-embeddings-index-anything-8fb18556443c) and the readme of [clip-retrieval](https://github.com/rom1504/clip-retrieval) for additional details about this process. See also [semantic search at billions scale](https://medium.com/@rom1504/semantic-search-at-billions-scale-95f21695689a) for more technical details of the process that was done for laion5B.\n\n1. Downloading the data as webdataset with distributed img2dataset\n2. Computing Vit-L/14 embeddings with distributed clip-inference\n3. Computing a KNN index from these embeddings using autofaiss\n4. Computing additional tags (NSFW and watermark) using clip embeddings\n\n### Distributed img2dataset\n\nWe developed the [img2dataset](https://github.com/rom1504/img2dataset) library to comfortably download from a given set of URLs, resize and store the images and captions in the webdataset format. This allows downloading 100 million images from our list of URLs in 20 hours with a single node (1Gbps connection speed, 32GB of RAM, an i7 CPU with 16 cores), which allows anyone to obtain the whole dataset or a smaller subset. For LAION-5B we introduced a [distributed mode](https://github.com/rom1504/img2dataset/blob/main/dataset_examples/laion5B.md) for this tool, allowing to downloading the 5,85B samples in a week using 10 nodes.\n\n### Distributed clip inference\n\nFrom these images, the [clip retrieval](https://github.com/rom1504/clip-retrieval%7D%7Bhttps://github.com/rom1504/clip-retrieval) inference tool was used to compute ViT-L/14 embeddings, allowing for a better analysis capacity of the data. In particular, a [distributed mode](https://github.com/rom1504/clip-retrieval/blob/main/docs/distributed_clip_inference.md) made it possible to compute these embeddings in a week using 32 A100: this larger clip model can only be computed at a speed of 312 sample/s per GPU, compared to 1800 sample/s for ViT-B/32. The resulting embeddings are available for everyone to use e.g. for clustering, indexing, linear inference.\n\n### Distributed indexing\n\nWe then used these 9 TB of image embeddings to build a large PQ128 knn index using the [autofaiss](https://github.com/criteo/autofaiss) tool. To make this run faster, a [distributed mode](https://github.com/criteo/autofaiss/blob/master/docs/distributed/distributed_autofaiss.md) is available.\n\n### Integration in the search UI\n\nIn order to demonstrate the value of this data, we integrated this index into the [knn search UI](https://knn5.laion.ai/). It is powered by the code called [clip back](https://github.com/rom1504/clip-retrieval). The knn index is 800GB and the metadata (URL and captions) as well, so memory mapping is used for both in order to use no ram, only an SSD drive of that capacity is required.\n\n### Watermark and safety inference\n\nWe wanted to give users the ability to remove unsafe examples, and watermarked examples. To do that we collected training and test sets. The training set was augmented with examples retrieved from the knn index, while the test set samples were selected to represent well the dataset distribution, but were all manually annotated. The inference is done using the [embedding-reader](https://github.com/rom1504/embedding-reader) module for NSFW and [LAION-5B-WatermarkDetection](https://github.com/Zasder3/LAION-5B-WatermarkDetection) for watermarks These tags were also integrated into the UI, allowing everyone to observe that the safety tags indeed filter out almost all the unsafe results, and giving confidence that training a generative model on this data will not result in unexpectedly unsafe images.\n\n### Watermarks\n\n![](https://lh6.googleusercontent.com/WQMTd03M8xmR8yTuSudnyZGgcQvcLfro4Lf0DXvIZb9P8xqjGJVPjrgziZ4U2HoHlgmY_3Ubx33qYaG5jIC-h_uuEOqyJ9K0IqJsNZH0XPiP1CDek8xz60fARNXpwJW9yCkEKa7u)\n\nThe training dataset is 90000 samples (45222 watermarks, 44778 clear).\n\nWatermarked images are a big problem when training generative models like DALL-E or GLIDE. To tackle this problem we trained a watermark detection model and used it to calculate confidence scores for every image in LAION-5B. Therefore we created a training dataset consisting of 90.000 images with 50% watermarked and 50% clean images. The majority of the watermarked images have been extracted from the LAION-400M KNN index through the use of several text prompts like “clip art watermark”, “cat watermark” or “landscape watermark”.\n\nThe images in the cleaned category were composed of images from the Open Images dataset and images that contained texts, but no watermarks, like PPT slides and memes, also retrieved from the kNN indices of LAION-400M. While we tried to curate a test set to evaluate the quality of our watermark detection model, we realized that it is almost impossible to draw a clear line between what actually is a watermark and what is not. For example pictures with small transparent texts at the bottom had been considered by some people as watermarked, by others not.\n\nIn the end we decided to choose a model based on our consensual judgment. It seems to be “good” at spotting obvious watermarks like those used on popular stock image sites. The creation of high-quality, openly accessible watermark detection test sets with clear and plausible definitions of what should be considered a watermark and what not, remains a challenge for future projects. Nevertheless we are convinced that removing images with a high confidence score for containing a watermark based on our model will significantly reduce the percentage of images that would be considered as obvious watermarks.\n\nThe model is available at [https://github.com/LAION-AI/watermark-detection](https://github.com/LAION-AI/watermark-detection) and [https://github.com/LAION-AI/LAION-5B-WatermarkDetection/releases/tag/1.0](https://github.com/LAION-AI/LAION-5B-WatermarkDetection/releases/tag/1.0)\n\n### Safety\n\nOn a balanced manually annotated safety test set with 3000 samples:\n\n- the accuracy of the B32 NSFW classifier is: 0.960\n- the accuracy of the ViT L 14 NSFW classifier is: 0.961\n\nThe model, as well as the training code, are available at [CLIP-based-NSFW-Detector](https://github.com/LAION-AI/CLIP-based-NSFW-Detector) The tags are available at [laion2B-en-safety](https://huggingface.co/datasets/laion/laion2B-en-safety) [laion2B-multi-safety](https://huggingface.co/datasets/laion/laion2B-multi-safety) [laion1B-nolang-safety](https://huggingface.co/datasets/laion/laion1B-nolang-safety) Demo at [clip-retrieval](https://rom1504.github.io/clip-retrieval/) (check/uncheck safe mode)\n\n## Using LAION datasets\n\nLaion5B and LAION-400M could e.g. be used to train\n\n- Generative models: training image/text generative models, e.g autoregressive models like DALL-E or diffusion models like GLIDE\n- Models with contrastive losses: self-supervised training on image/text pairs using contrastive losses, e.g CLIP\n- Classification models: e.g, performing zero-shot classification by extracting pseudo labels from queries on the dataset\n\nWe present here a few examples of models that were trained on our LAION datasets with success:\n\n#### CLIP\n\nWe, LAION, are currently working together with the Cross Sectional Team Deep Learning (CST-DL), Scalable Learning and Multi-Purpose AI Lab (SLAMPAI) at the Jülich Supercomputing Centre (JSC) and the Open CLIP team in the replication of OpenAI’s CLIP results.\n\n![](https://lh3.googleusercontent.com/-lvN21OSxBEwmzj1H0eIa4RxpS_VEogmXxku8R_0LWSLWtDP_tfNPuKCPuBEzA2arDvyPVfZlACZhdgymssC5E0tp_aqPdUFJMOLQf3W0wZKx3LqpJKF4JViL_nrBQH6TxEn5H2i)\n\n( The results in the right column are from our model. – huge thanks to Cade Gordon \u0026 Ross Wightman for performing the training run )\n\nThe repository with the training code and the model checkpoints can be found here: [https://github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip)\n\nWe gratefully acknowledge the Gauss Centre for Supercomputing e.V. (\u003cwww.gauss-centre.eu\u003e) for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at Jülich Supercomputing Centre (JSC).\n\n#### BLIP inference tuning\n\n[BLIP](https://github.com/salesforce/BLIP) is a model that was trained for both image-text matching and image captioning. It was trained on a 115M subset of LAION-400M. To improve the results of the generated captions we (LAION) performed over 100 experiments to determine the hyperparameters that maximize the BLEU-4 score compared to MS COCO captions. Here you can see some of our [results](http://captions.christoph-schuhmann.de/eval_b_auto/eval.html).\n\n![](https://lh4.googleusercontent.com/pLpUwcYitBPvnLqApETmU0Ik4VVYkslTyuEZBXT8VByhMjGqINSdVPLpqOR6ZcdvCilLakTXJXP40xUecLMMCEIl7CpWAN2RfAHU__OAKcIyd3_8lrsWuKeQcraU86ITbWmxM-y_)\n\n_eval_best_auto0185: An orange cat is looking at its reflection in the mirror._\n\n![](https://lh4.googleusercontent.com/QCQjVFkxZAQf24yZ49q5RLQ3ElyCIMFlOy5ACsfC7QC-CLiDDhTdCSnn5sobHFOsxLrVUIpNrk9sPyVmYDL6NB8qyQP6aSfkgxDuoWdeT3JX3j5MuJOGC9b7UgxKKnl4X9l33d0w)\n\n_eval_best_auto0190: A green highway sign with the words Queens Bronx._\n\nWe found that we can significantly improve the quality of the captions by generating 40 (or more) candidate captions for each image and then ranking them using OpenAI’s CLIP ViT-L/14 \u0026 CLIP-Resnet50x64. First we ranked all candidates with ViT-L/14 and then we ranked the top-5 results again using Resnet50x64. Preliminary results of human evaluations indicate that:\n\n1. our evaluators gave the generated captions an average quality rating of 3,8 on a scale from 0 to 5, with a standard deviation of 0,9 ( in this particular hyperparameter configuration n= 600)\n2. our evaluators gave original human captions from MS COCO an average quality rating of 3,9 with a standard deviation of 0,8 ( n = 2100 )\n\n—\u003e We hypothesize that the generated captions match (\u0026 sometimes even surpass) the average quality of the human captions of MS COCO (which are sometimes also far from perfect) in most cases, but sometimes ( in less than \u003c10% ) contain obvious mistakes, that humans would not make, because deeper kind of world knowledge \u0026 „common sense“ would be necessary in those cases.\n\n#### GLIDE\n\nClay Mullis (alias [afiaka87](https://github.com/afiaka87)) used subsets of LAON-2B to fine-tune the OpenAi [Glide](https://github.com/openai/glide-text2im) model and managed to reintroduce human generations. Samples\n\n- [https://replicate.com/afiaka87/laionide-v3](https://replicate.com/afiaka87/laionide-v3)\n- [https://wandb.ai/afiaka87/glide_compare/reports/Finetuning-GLIDE-on-Laion5B–VmlldzoxNTg3MTkz](https://wandb.ai/afiaka87/glide_compare/reports/Finetuning-GLIDE-on-Laion5B--VmlldzoxNTg3MTkz)\n- [https://wandb.ai/afiaka87/laionide-v3-glide/reports/Laionide-Version-3-Benchmark–VmlldzoxNjE0MTE3](https://wandb.ai/afiaka87/laionide-v3-glide/reports/Laionide-Version-3-Benchmark--VmlldzoxNjE0MTE3)\n\n![](https://lh5.googleusercontent.com/SjEvGWJlqpcocr0aeRj2V-ldfCJkO-RubJF-QQr6OdTgP196lqJynBEx45FRoEp3YTUfZLmYjpa9QDApqRvwd14zZSwbyEKso1i_q5wJNJXgQytb3yPVcllWGPsht4Tv52j7unM7)![](https://lh5.googleusercontent.com/BD89utsfddsrbxBCZsXEpxv_F4t2gGUrB8Hcqq5fl2aHrhsp5i5lJ5014d5T7I9DjxAT-4Q2N-cZYebumBzJWr7YwvRU0W2tjguSjF9DjTUuveBhhz__XWDuYGQh39N9D-op_1L7)![](https://lh5.googleusercontent.com/GLlZtnYViyWsxomeN0Xh7kY8IBZA9Ni2JmndUtGoSzIUT0NnXB7ru4wCNiHdFfhk1gwdc2LmbzvvNe_TTQCWe3hckDJLl7GiHzAc1S0agio3jxgv2DG3ih0WBPpQ88KQJf0jqtaN)\n\n#### Semantic search and subset extraction\n\nThe [clip-retrieval](https://rom1504.github.io/clip-retrieval/) interface allows a user to search images and texts based on a query image or text using the CLIP embeddings of the input and our precomputed kNN indices. It demonstrates the diversity of images and captions that can be found in LAION-5B as well as high semantic relevance shows the distribution of image sizes of LAION-5B. Given the abundance of high-resolution images, one can produce subsets of images for training various customized models, and also choose image resolution that is suitable for the purpose of particular training.\n\n#### CLOOB\n\nKatherine Crowson and John David Pressman recently trained a CLOOB ViT-B/16, variant of CLIP, for 32 epochs on LAION-400M and got preliminary results, that come close to the performance of OpenAI’s ViT-B/32, even though this was an early run with unoptimized hyperparameters. The checkpoints can be found here: [https://github.com/crowsonkb/cloob-training](https://github.com/crowsonkb/cloob-training)![](https://lh5.googleusercontent.com/ROxNOoa1jgaAW9JDbP2KHdDHOcTUCe-oPMvNTe2OYW_ETxbQI8W9YdA8oN93ULJ3r1Wyk7aAtm5GztJsQUZXUgw06BRiPECqp4o_bGXePp5cp9jFMBbbf2h1EYfDzvEHIcMdrcm-)\n\n(zero-shot accuracies on Imagenet-1K )\n\nWe are in touch with Andreas Fürst, one of the original CLOOB authors, and learned from him that their team is currently (at the time of writing) training a CLOOB ViT-B/32 with LAION-400M with optimized hyperparameters and very promising results so far (53% zero-shot accuracy on Imagenet after 7 epochs).\n\n## Papers citing LAION 400M\n\nAfter the release of LAION-400M, several papers used LAION-400M for image generation, text to image generation, image to text generation and text image matching:\n\n- [Vector Quantized Diffusion Model for Text-to-Image Synthesis](https://arxiv.org/abs/2111.14822.pdf) used LAION-400M to train VQ diffusion text to image generation models\n- [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752.pdf) used a subset of LAION-400M to train latent diffusion models\n- [General Facial Representation Learning in a Visual-Linguistic Manner](https://arxiv.org/abs/2112.03109.pdf) LAION-400M face subset to train a face clip\n- [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) image captioning using LAION-400M subset\n- [MAGMA – Multimodal Augmentation of Generative Models through Adapter-based Finetuning](https://arxiv.org/pdf/2112.05253.pdf) was trained on image question answering using a LAION-400M subset\n\n## Conclusion\n\nBy releasing an updated version of an openly available dataset that contains 5 billion image-text pairs, we have set new Standards for the scale of openly available datasets and enable researchers from all over the world to train state-of-the-art language-vision models like GLIDE or Turing Bletchley. As proof of concept, we demonstrated that a subset of our dataset can be used to train various CLIP-like models, producing samples of sufficient quality. This dataset extends the possibilities in multi-language large-scale training and research of language-vision models, that were previously restricted to those having access to proprietary large datasets, to the broad community.\n\n## What’s next?\n\nThis is only the beginning! Now that this huge and open dataset is released, it can be used to train many models, such as gigantic clip models, image/text generation models and much more. We have so many projects going on that it’s probably best, if you are interested, to join our Discord server and check out what’s going on. We are and always will be a grassroots community that works openly and welcomes everyone who is kind and passionate and for machine learning.\n\nJoin us in [discord](https://discord.gg/eq3cAMZtCC) and help us to train models like CLIP, BLIP, GLIDE, Dall-E, SimMIM, AudioCLIP and don’t hesitate to share your ideas for new projects with us.\n\n**Become a part of our constantly growing crowd of supporters who help us to make machine learning dreams come true!**\n\n## Credit Assignment\n\n- Christoph Schuhmann: He led this project and built POCs for most of its components including clip filtering,the safety model, the watermark model and the Blip inference tuning project.\n- Richard Vencu: System architecture and download script optimizations, GPU assisted filtering. Set up the AWS infrastructure.\n- Romain Beaumont: Guidance on scaling for the common crawl filtering pipeline. Built and ran the dataset preparation pipeline: pyspark deduplication job, img2dataset, clip inference, autofaiss, safety tags.\n- Clayton Mullis: DALLE-pytorch training/analysis, glide training, WDS filtering\n- Jenia Jitsev: scientific organization \u0026 writing, experiments planning and design, compute resource acquisition, general supervision\n- Robert Kaczmarczyk: Established WDS architecture, performed DALL-E training runs, balancing calculation, sample (NSFW, watermark, caption quality) annotation and manuscript revision\n- Andreas Köpf: He conducted the hyperparameter search for the inference strategies with the BLIP image-captioning model\n- Theo Coomber: He was one of our first contributors \u0026 build the first versions of our [worker swarm system](https://github.com/TheoCoombes/crawlingathome). Without his enthusiasm this project might never have taken off.\n- Aarush Katta: Trained the watermark model\n- Cade Gordon: Run distributed inference for the watermark tags \u0026 trained the CLIP B/32 model on JUWELS Booster\n- Ross Wightman: Ross helped Cade with the debugging \u0026 training of the CLIP-B/32 model and executed experiments on JUWELS Booster\n- Katherine Crowson and John David Pressman: Trained the CLOOB model\n- Aran Komatsuzaki: Led an image-text-pair dataset building project, which inspired this project.\n- Bokai Yu: Accomplished most of the work to make the knn index building tool autofaiss work in a distributed setting\n\n[  \n](https://laion.ai/laion-400-open-dataset/)\n","date":1648684800000},{"slug":"laion-400-open-dataset","frontmatter":{"title":"LAION-400-MILLION OPEN DATASET","author":"Christoph Schuhmann","date":"Aug 20, 2021","previewImg":"/images/blog/500m.png"},"content":"\nWe present LAION-400M: 400M English (image, text) pairs - see also our [Data Centric AI NeurIPS Workshop 2021 paper](https://arxiv.org/abs/2111.02114)\n\n## Concept and Content\n\nThe LAION-400M dataset is entirely openly, freely accessible.\n\n**WARNING**: be aware that this large-scale dataset is non-curated. It was built for research purposes to enable testing model training on larger scale for broad researcher and other interested communities, and is **not** meant for any real-world production or application.\n\nWe have filtered all images and texts in the LAION-400M dataset with OpenAI‘s [CLIP](https://openai.com/blog/clip/) by calculating the cosine similarity between the text and image embeddings and dropping those with a similarity below 0.3. The threshold of 0.3 had been determined through human evaluations and seemed to be a good heuristic for estimating semantic image-text-content matching.\n\nThe image-text-pairs have been extracted from the [Common Crawl](https://commoncrawl.org/) web data dump and are from random web pages crawled between 2014 and 2021.\n\n###### Original information\n\n### LAION-400M Dataset Statistics\n\nThe LAION-400M and future even bigger ones are, in fact, datasets of datasets. For instance, we can filter it out by image sizes into smaller datasets like this:\n\n```\nNumber of unique samples 413M\nNumber with height or width \u003e= 1024 26M\nNumber with height and width \u003e= 1024 9.6M\nNumber with height or width \u003e= 512 112M\nNumber with height and width \u003e= 512 67M\nNumber with height or width \u003e= 256 268M\nNumber with height and width \u003e= 256 211M\n```\n\nBy using the KNN index, we can extract specialized datasets by domains of interest. They are (or will be) sufficient in size to train technical domain models.\n\nAlso, use [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) for simple visualisation of the dataset. There you can search among the dataset using CLIP and a knn index.\n\n### Disclaimer \u0026 Content Warning\n\nOur filtering protocol only removed NSFW images detected as illegal, but the dataset still has NSFW content accordingly marked in the metadata. When freely navigating through the dataset, keep in mind that it is a large-scale, **non-curated** set crawled from the internet for research purposes, such that collected links may lead to discomforting and disturbing content. Therefore, please use the demo links with **caution**. You can extract a “safe” subset by filtering out samples drawn with NSFW or via stricter CLIP filtering.\n\nThere is a certain degree of duplication because we used URL+text as deduplication criteria. The same image with the same caption may sit at different URLs, causing duplicates. The same image with other captions is not, however, considered duplicated.\n\nUsing KNN clustering should make it easy to further deduplicate by image content.\n\n### LAION-400M Open Dataset structure\n\nWe produced the dataset in several formats to address the various use cases:\n\n- a 50GB url+caption metadata dataset in parquet files. We can use the metadata to compute statistics and redownload part of the dataset\n- a 10TB webdataset with 256×256 images, captions and metadata. It is a full version of the dataset that can be used directly for training (this one is for internal use, you need to redownload images yourself due to licensing issues)\n- a 1TB set of the 400M text and image clip embeddings, useful to rebuild new knn indices\n- pairs of 16G, 32G, 64G and 128G knn indices (running in the web demo)\n\n#### URL and caption metadata dataset\n\nWe provide 32 parquet files of size around 1GB (total 50GB) with the image URLs, the associated texts and additional metadata in the following format:\n\n\u003e SAMPLE_ID | URL | TEXT | LICENSE | NSFW | similarity | WIDTH | HEIGHT\n\nwhere\n\n- **SAMPLE_ID**: A unique identifier\n- **LICENSE**: Where we found a Creative Commons License in the image data, we named it here like, e.g. “creativecommons.org/licenses/by-nc-sa/3.0/” – otherwise you’ll find it here a “?”\n- **NSFW**: we used CLIP to estimate if the image has NSFW content. The estimation has been pretty conservative, reducing false negatives at the cost of more false positives. Possible values are “UNLIKELY”, “UNSURE” and “NSFW”.\n- **similarity**: Value of the cosine similarity between the text and image embedding\n- WIDTH and HEIGHT: image size as the image was embedded. We downsized originals that were larger than 4K to 4K.\n\nThis metadata dataset purpose is to download the images for the whole dataset or a subset of it by supplying it to the very efficient [img2dataset](https://github.com/rom1504/img2dataset) tool.\n\n#### 10 TB webdataset with images and captions\n\nBy running the img2dataset tool, we can download a 10TB webdataset. It will resize all images at 256×256 resolution, will append the corresponding caption and will generate a collection of tar files (that dataset format is called webdataset) containing images, captions, and metadata and related parquet files containing the same metadata\n\n- 00000.tar of size 270MB containing at most 10k samples\n  - 0.jpg\n  - 0.txt containing the caption\n  - 0.json containing metadata such as the URL, the original width, the EXIF data, whether the image is NSFW\n- 00000.parquet of size 1.6MB containing the same metadata as the JSON file. Useful to compute statistics without reading all the tar files\n\nThe 400M dataset will therefore have 41455 tar and 41455 parquet files. This dataset purpose is to train multimodal models like CLIP or DALL-E.\n\n#### 1TB of clip embeddings\n\nThe clip embeddings are stored in NPY files next to parquet files in the same order. Since this dataset is much smaller than image one, each NPY file stores 1M samples. Each NPY file is 1GB, and each parquet file is 150MB. There are a total of 400 such files. The embeddings purpose is to compute statistics on the dataset, for example, using clustering or knn indices.\n\n#### Two small 6GB knn indices\n\nWe provide two 6GB knn indices built using the [autofaiss](https://github.com/criteo/autofaiss). We can use them to compute a subset of the dataset and, more generally, to search among it efficiently. See the search [web demo](https://rom1504.github.io/clip-retrieval/) of it. We can use the CLIP filter tool along with this index to produce subsets using search terms efficiently. We also provide two 16GB knn indices of higher quality.\n\n### What can we do with the LAION-400M dataset?\n\nVision and language modelling has been taking off in 2021. Here are some pointers about what this kind of image + text datasets unlocks and why it seems interesting:\n\n- Six months ago, OpenAI released two blog posts and papers, [CLIP](https://openai.com/blog/clip/) and [DALL-E](https://openai.com/blog/dall-e/). Both models rely on a large amount of (text, image) pairs. They used an unreleased 400M pairs dataset.\n  - CLIP is a model that computes how related are a text and an image. It makes it possible to build large text to image search, and it makes it possible to create that kind of crazy text to image art [clip-art](https://ml.berkeley.edu/blog/blog/clip-art/). They released a small and medium version of the model but no training code.\n  - DALL-E is a model that directly generates images from texts. As can be seen from the blog post, it achieves awe-inspiring results that could directly impact the world for anything that needs drawing and illustrations. OpenAI did not release any model, even through an API\n\nSince then, various researchers have organised several efforts to replicate DALL-E. People gathered initially around this excellent DALLE replication repository [DALLE-PyTorch](https://github.com/lucidrains/DALLE-pytorch) with some fantastic results visible in the readme. More recently, as part of huggingface events, new developments have been achieved (see [DALLE-mini report](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini--Vmlldzo4NjIxODA) ), and an online demo is now available at [DALLE-mini demo.](https://huggingface.co/spaces/flax-community/dalle-mini)\n\nThe replication effort is still far from achieving the same performance as the original DALLE, and it seems possible to go even further. Some people also want to make a better CLIP to produce even better-generated art.\n\nA large part of the results that we can achieve with such models is thanks to a large amount of data. Before LAION-400M, the largest open dataset for (image, text) pairs are in the order of 10M (see [DALLE-datasets](https://github.com/robvanvolt/DALLE-datasets) ), which is enough to train exciting models but not enough to reach the best performance. Having a public dataset with hundreds of millions of pairs will help build these image+text models.\n\n### Analysis of the LAION-400M data\n\nWe annotated 3456 samples of the dataset and got the following results:\n\n- Correct positive NSFW: 4\n- Correct negative NSFW: 3371\n- False-positive NSFW: 73\n- False-negative NSFW: 8\n- Bad captions: 3 (0.09 %)\n\nThe matching is excellent, thanks to CLIP. We could improve the NSFW automatic tagging in the future; however, the NSFW total rate is low enough (less than 1%) to make this not an issue.\n\n## Technical Details\n\nThe dataset acquisition has into two significant parts:\n\n1. a distributed processing of the vast (many PBs) Common Crawl datasets, which produces a collection of matching URL and caption\n2. a single node much lighter post-processing of the data that anyone can run in a few days and which produces the final dataset\n\n### 1. Distributed processing of Common Crawl\n\nWe acquire the raw web data for the creation of our dataset from Common Crawl. Common Crawl is a non-profit organisation dedicated to providing a copy of the internet to internet researchers, companies, and individuals at no cost for research and analysis. They regularly release dumps of HTML-like data parsed from billions of public websites found [on the Common Crawl website](https://commoncrawl.org/the-data/get-started/). To create image-text pairs, we parse through the data from Common Crawl and parse out all HTML IMG tags containing an [alt text attribute](https://en.wikipedia.org/wiki/Alt_attribute). Common Crawl provides its data in several formats. For our purpose, we chose to use the data in the WAT format. The WAT files contain only the metadata of the crawled sites, which includes all links and IMG tags contained in the website. Parsing only this metadata is much faster than parsing the whole HTML text (provided in the WARC format).\n\n#### Downloading original images\n\nWe download the raw images from the URLs we parsed from Common Crawl with asynchronous requests using the libraries [Trio](https://github.com/python-trio/trio) and [Asks](https://github.com/theelous3/asks). They allow us to go multithreading for a single CPU. Usually, a home internet link will be exhausted by a single or two CPUs. A data centre node can scale up benefits from guaranteed internet speed with a multiprocessing pool much faster than a single CPU node. At this time, we were able to use 50 cores with a full, secured 1Gbps connection to the public internet. This bandwidth must be available to the downloading node, not shared among many nodes or apps. We have optimised the script for speed while mitigating various errors we encountered. Usually, to satisfy a high-end demanding node such as above, we must take additional steps to provide DNS caching capabilities. We found that the knot-resolver ran with two processes and configured with caching option can solve this problem.\n\n#### Filtering out unsuitable image-text pairs\n\nAfter downloading the WAT files from Common Crawl, we filter the samples in the following steps:\n\n1. We dropped all samples with less than five character alt text length\n2. We dropped all samples with less than 5 KB image size\n3. We use continuously updated bloom filters to drop samples that are already in our dataset. The bloom filters deduplicate by concatenating the URL and the alt text.\n4. We use continuously updated bloom filters to drop samples from URLs that had timed out previously and therefore seem unreachable (or at least not reachable in an efficient way)\n5. We use OpenAI’s CLIP model (the ‘_ViT-B-32_‘ version) to compute the image and alt text embeddings. Then we calculate the cosine similarity of both embedding vectors and drop all samples with a similarity below 0.3. We chose this threshold after trying different values and using human evaluations of how well the texts fit the images. Lower values like 0.28 or 0.29 also seemed okay in many cases, but after further inspections, we decided to choose the conservative value of 0.3.\n6. We use the CLIP embeddings of the images to estimate if their contents contain NSFW content. We do this by calculating CLIP embeddings for a list of image categories like, e.g. “selfie”, “illustration”, or “landscape”, which also contains categories that indicate NSFW content like “porn” and “sex”.\n7. Then we compute the cosine similarities between the embedding image we are currently filtering and each of these category keywords. If the category with the highest similarity and the keyword with the second-highest similarity belong both to NSFW keywords, we tag the sample as “NSFW”. If only one of them belongs to an NSFW keyword, we categorise the sample as “UNSURE”. If both keywords with the highest similarities are not NSFW, we tag the sample as “UNLIKELY”.\n8. In the next step, we look at all samples with either the “NSFW” or “UNSURE” tag and drop those with any keywords in their text related to kids, teens, or other semantically related content.\n9. In step 8, we repeat the procedure of computing the cosine similarities from step 6 with the difference that we now use category texts that indicate contents semantically related to kids and teens on a CLIP embedding level. If either the highest similarity or the second-highest similarity between a sample’s image embedding and a text of the precomputed categories belongs to a text that indicates content related to under-aged persons, we drop this sample.\n10. Finally, we repeat the procedure from step 8 with texts semantically related to animal categories like e.g. “animal”, “bird”, etc.\n\nWe perform these rigorous filtering steps for NSFW with potentially illegal content because we cannot guarantee that the contents of Common Crawl are free of such. We feel obligated to try our best to filter out such content. Inspections of samples filtered out by steps 7 to 9 have shown that our filtering procedure is very conservative and produces many false positives (samples it drops, which are not problematic). This process is okay because the number of potential samples waiting for us to crawl is vast.\n\n#### System Architecture\n\nTo orchestrate the interactions of the many crawling scripts (called _workers_) in our project, we use a server that keeps track of processed WAT files and of which worker gets which unprocessed WAT. We call this orchestrating server a _tracker_. Its functions are offering jobs to both download workers and inference workers, confirming cleanup requests from the DL staging server, maintaining ACLs for the Bloom server, and some more. We also employ several staging servers as buffers for jobs on their way to the storage location. The staging servers continuously update filters in the central bloom server where we use RedisBloom for high-performance reasons.![](https://i.imgur.com/kxl4jJe.png)\n\n#### Workflow\n\nDuring the evolution of our crawling project, we applied two different workflows:\n\n##### Workflow 1 (_“Hybrid”_ – workers)\n\nThis worker performs all computation steps during one job and then submits the result to the staging server. It then queues the results for release to the storage area.\n\n##### Workflow 2 (_“CPU – GPU – 2 stages”_ – workflow)\n\nWe soon discovered that the best way to utilise resources is to split the workload into CPU + networking tasks (downloading steps) and GPU tasks (CLIP inference steps). Hence, the 2 stage approach uses “CPU workers” to download images, create image-text pairs, and save the intermediate result to a staging server. Then “GPU workers” pick up jobs, concatenate a number of them to group around 20000 pairs per final result file. The 2 stage workflow proved to be most efficient, with speeds up to 25 million pairs added to the dataset per day when using 100 CPU workers with one core and one GPU worker employing an NVidia RTX 3090 graphic card utilising all 16 lanes of PCIe bus. The GPU node also needs about CPU 24 threads to keep up with the GPU processing capacity.\n\n#### Removing abuse alerts\n\nDuring downloading, we encountered abuse alerts from manual and automated tools that protect websites. After some learning curve, we reduced most of the issues by employing these mitigation techniques:\n\n- By far, the most efficient one was to use centralised bloom filters that eliminate requests going to the duplicate URLs over and over. Of course, the efficiency of these filters dramatically depends on how fast they are updated and used by the workers. By definition, having multiple downloading workers performing jobs in parallel makes them prone to overlap requests to the same URL even if the bloom filters are up to date at the beginning of the job.\n- Therefore the second technique significantly reduced the problem of parallel workers via randomising the jobs at the tracker server level. While executing jobs in sequence (with the oldest WAT files from 2013), we discovered that adjacent jobs were overlapping considerably. When we randomised jobs, we saw a dramatic decrease in such overlapping.\n\n#### Who ran this?\n\nWe want to thank :\n\n- the [LAION folks](https://laion.ai/#team), via so many worker nodes everywhere in the cloud\n- [the data hoarders](https://www.reddit.com/r/DataHoarder/comments/oyta8q/crawlinghome_help_build_the_worlds_largest/) Reddit community\n- as well as all our friends and relatives that did not know what they were helping with\n\nfor running the workers to produce this vast dataset in a few months.\n\n### 2. Post-processing of the dataset\n\nOnce the distributed pipeline has run, resulting in a sizeable caption+url dataset, it’s time to package it in the best way. The objective of this second pipeline is to produce a version of the dataset that is easy to use for multimodal training. For this, we built tools that anyone can run out of a collection of caption+url. The exact command line to run is available in [cah-prepro](https://github.com/rom1504/cah-prepro) (which uses mainly [img2dataset](https://github.com/rom1504/img2dataset) and [clip-retrieval](https://github.com/rom1504/clip-retrieval) )\n\n#### Pyspark preprocessing of the CSV files\n\nAfter a fast run of a script to [download the CSV files,](https://github.com/rom1504/cah-prepro/tree/main/download_csv) the first step of this post-processing pipeline is to do deduplication by url+caption. The first pipeline does some partial deduplication using a bloom filter, but it is approximate, and some duplicates remain. Doing that pyspark post-processing also makes it possible to reduce the number of metadata files from hundred of thousands to 32 parquet files of size 1.7GB. See this [deduplication script there](https://github.com/rom1504/cah-prepro/blob/main/deduplicate/cah_stats_spark.py). Pyspark would be an excellent way to do any further filtering, and we [provide](https://github.com/rom1504/cah-prepro/blob/main/deduplicate/compute_more_stats.py) an example to compute some statistics. The resulting output is 32 parquet files containing columns such as URL, text, NSFW described at the beginning of the post.\n\n#### Img2dataset\n\nOnce this set of 50GB parquet files has is ready, we can use the [img2dataset](https://github.com/rom1504/img2dataset) tool to download, resize and store the images and captions as [webdataset](https://github.com/webdataset/webdataset). This tool can download 100M images in 20h in a single node (1Gbps 32GB of ram 16 i7 cores), so anyone can run this for the whole dataset or a smaller subset. The format this tool outputs is a collection of tar files (that dataset format is called webdataset) containing images, captions, and metadata and corresponding parquet files containing the same metadata\n\n- 00000.tar of size 270MB containing at most 10k samples\n  - 0.jpg\n  - 0.txt containing the caption\n  - 0.json containing metadata such as the URL, the original width, the EXIF data, whether the image is NSFW\n- 00000.parquet of size 1.6MB containing the same metadata as the JSON file. Useful to compute statistics without reading all the tar files\n\nThe size of the tars of 270MB is when using the options of img2dataset indicated there [download_images.sh](https://github.com/rom1504/cah-prepro/blob/main/download_images/download_images.sh) (resizing all images to 256×256 with padding for maximum file uniformity and avoid losing information). If using different options, you may have larger or smaller tar files.\n\n#### Clip retrieval and autofaiss\n\nFinally, the tar dataset aims to compute and package clip embeddings and compute a KNN index over the clip embeddings. The [clip-retrieval](https://github.com/rom1504/clip-retrieval/) tool makes it fast to compute 100M embeddings per 20h with a single 3080 GPU, so it’s possible to rerun this part on the whole dataset or a subset at a low cost. The embeddings are stored in NPY files next to parquet files in the same order. Since this dataset is much smaller than image one, each NPY file stores 1M samples. NPY files are 1GB in size, and parquet files are 150MB. There are a total of 400 such files. These embeddings help build text and an image knn index using the [autofaiss](https://github.com/criteo/autofaiss) tool, making it possible to produce a quantised index of an arbitrary file. The chosen index type is 6GB, so it’s cheap for anyone to load and run fast (10ms) queries over the whole dataset. We also generated another kind of index of size 16GB. Thanks to memory mapping, it’s also possible to load it at no ram usage. A simple [web demo](https://rom1504.github.io/clip-retrieval/) shows the results.\n\n![](https://i.imgur.com/6bEztg9.png)\n\n### License\n\nWe distribute the metadata dataset (the parquet files) under the most open [Creative Common CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) license, which poses no particular restriction. The images are under their copyright.\n\n## Contributing\n\nYou can contribute to the project to help us release the following dataset sizes at 1 billion pairs, 2 billion pairs and so on.\n\nChoose one or more methods that suit you or your company:\n\n1. donate either [cash](https://laion.ai/laion-400-open-dataset/#) or [computing time](https://laion.ai/how-to-donate-computing-time/). We also launched a [Go Get Funding campaign](https://gogetfunding.com/help-us-build-the-worlds-largest-open-billion-scale-image-text-dataset-perfect-for-training-dall-e-clip-other-multimodal-models/).\n2. participate in the development effort\n3. spread the word. At best, use the dataset, get nice results and mention it in your papers\n\nUseful links:\n\n- Dataset progress [Crawling@Home Dashboard](http://crawling.at/) and [leaderboard](http://crawling.at/leaderboard)\n- Reddit [post](https://www.reddit.com/r/DataHoarder/comments/oyta8q/crawlinghome_help_build_the_worlds_largest/?utm_source=share\u0026utm_medium=web2x\u0026context=3)\n- DALLE-PyTorch [Discord server](https://discord.gg/mVcgxMPD7e)\n- DALLE-PyTorch [GitHub Repository](https://github.com/lucidrains/DALLE-pytorch)\n\n[  \n](https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/)\n\n### Sponsors\n\nWe made it so far due to the generosity of these donors:\n| ![](https://i.imgur.com/z6K7kSq.png) |![](https://i.imgur.com/KYvncYl.png)|\n|--|--|\n|[doodlebot.ai](http://doodlebot.ai/)|[Gentec Data](https://gentec.ro/)|\n","date":1629417600000}]},"__N_SSG":true},"page":"/blog","query":{},"buildId":"B54m5e8vIbpJDP9geUnol","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>