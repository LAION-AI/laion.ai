{"pageProps":{"frontmatter":{"title":"Open Scientific Summaries at Scale: The Inference.net × LAION × Grass Initiative","author":"FULL AUTHOR LIST TBD","date":"October 28 2025","previewImg":"public/images/blog/sci3.jpg"},"content":"\n\n## Abstract\n\nWe present a comprehensive approach to democratizing access to scientific knowledge through large-scale, **structured summarization** of academic literature. We retrieved and processed ~**100 million** research papers from the public internet, leveraging existing datasets from **bethgelab**, **PeS2o**, **Hugging Face**, and **Common Pile**. We designed a standardized **JSON schema** for scientific paper summaries and **post-trained two models**—**Qwen 3 14B** and **Nemotron 12B**—to produce summaries in this format.\n\nOur evaluation combines **LLM-as-a-Judge** and a **QA dataset**. Fine-tuned models achieve performance on our evals comparable to leading closed models (e.g., GPT-5, Claude 4.5). **Nemotron 12B** offers ~**2.25×** higher throughput than Qwen 3 14B, making it attractive for large-scale processing.\n\nWith this preliminary blog post, we **release a fine-tuned model and 100k paper summaries**. A live **visualization tool** at [https://laion.inference.net/](https://laion.inference.net/) demonstrates the utility of structured summaries. We plan to release structured summaries for the full **100M** paper corpus.\n\n---\n\n## Introduction\n\nAccess to scientific knowledge remains constrained by paywalls, licensing, and copyright, slowing research and education. Our **Project Alexandria** ([arXiv:2502.19413](https://arxiv.org/abs/2502.19413)) showed that it is legally and technically feasible to **extract factual knowledge** while respecting copyright via **Knowledge Units**—structured, style-agnostic representations of content.\n\nHowever, research-paper corpora vary in format and structure, making it hard to compare similar claims or retrieve knowledge efficiently. Building on Alexandria, we introduce a **pipeline** to collect, process, and summarize papers into **structured outputs** consumable by humans and AI systems alike. Our aims:\n\n* **Create** a massive, openly accessible, well-structured summary dataset of scientific literature\n* **Develop** models capable of generating **structured, factual** summaries\n* **Demonstrate** the utility of these summaries for scientific tasks\n* **Explore** decentralized computing to process at global scale\n\nThis brief outlines **methodology**, **results**, and **implications** for the scientific community—and humanity.\n\n---\n\n## Methodology\n\n### 2.1 Dataset Collection & Processing\n\nPrimary corpus: ~**100M** research papers retrieved via collaboration with **Wynd Labs** using the **Grass** network. After deduplication, we **supplemented** with:\n\n* **bethgelab**: *paper_parsed_jsons* ([dataset](https://huggingface.co/datasets/bethgelab/paper_parsed_jsons))\n* **LAION**: *COREX-18text* ([dataset](https://huggingface.co/datasets/laion/COREX-18text))\n* **Common Pile**: *PubMed* subset ([dataset](https://huggingface.co/datasets/common-pile/pubmed))\n* **LAION**: *PeS2oX-fulltext* ([dataset](https://huggingface.co/datasets/laion/Pes2oX-fulltext))\n\n**Post-training subset (110k papers)**: 40% from the retrieved corpus, **15% each** from the four sources above. Split: **100k train / 10k val**.\n**Length stats**: mean **81,334** characters, median **45,025** characters.\n\n### 2.2 Structured Summary Schema\n\nInspired by Alexandria’s **Knowledge Units**, our **JSON schema** first **classifies** content:\n\n* `SCIENTIFIC_TEXT` — complete research articles\n* `PARTIAL_SCIENTIFIC_TEXT` — partial scientific content\n* `NON_SCIENTIFIC_TEXT` — non-research content\n\nFor scientific texts, the schema extracts: **title, authors, year, field/subfield, paper type, executive summary, research context, RQs & hypotheses, methods, procedures/architectures, key results (with numbers), interpretation, contradictions/limitations, claims (with supporting/contradicting evidence), data/code availability, robustness/ablations, ethics, key figures/tables, three takeaways**. (See **Appendix A**.)\n\n### 2.3 Model Post-Training\n\nWe post-trained:\n\n* **Qwen 3 14B** (dense Transformer)\n* **Nemotron 12B** (hybrid Mamba-Transformer)\n\nTargets were **GPT-5-generated** structured reports. A strict prompt guided **classification**, then **schema-aligned extraction** (executive summary, context, methods, procedures/architectures, key results, interpretations, contradictions, claims, data/code, robustness, ethics, key visuals, and three takeaways). See **Appendix A** for prompt.\n\n### 2.4 Evaluation\n\nWe used **two complementary approaches**:\n\n1. **LLM-as-a-Judge** — Ensemble of GPT-5, Gemini 2.5 Pro, and Claude 4.5 Sonnet, rating student outputs vs. GPT-5 references on a **1–5** rubric (accuracy, completeness, structure, clarity; hallucination checks). See survey [6].\n2. **QA Dataset** — For a holdout set, we generated **5 MCQs per paper** with GPT-5 and measured models’ ability to answer **using their own generated summaries** (truncated to **10,000 chars**), providing a proxy for **factual utility** (cf. Alexandria [1]).\n\n---\n\n## Results\n\n### 3.1 LLM-as-a-Judge\n\n<p align=\"center\">\n  <img src=\"public/images/blog/sci2.jpg\" alt=\"LLM-as-a-Judge scores chart\" width=\"600\">\n</p>\n\n| Model                 | Score (1–5) |\n| --------------------- | :---------: |\n| GPT-5                 |  **4.805**  |\n| **Qwen 3 14B (FT)**   |  **4.207**  |\n| **Nemotron 12B (FT)** |  **4.095**  |\n| Gemini 2.5 Flash      |    4.052    |\n| Claude 4.5 Sonnet     |    3.521    |\n| GPT OSS 120B          |    3.273    |\n| Qwen 3 14B (Base)     |    3.015    |\n| GPT OSS 20B           |    2.903    |\n| Nemotron 12B (Base)   |    2.179    |\n\n*Figure 1.* Average LLM-as-a-Judge scores; **95% CIs via bootstrap**.\n\n### 3.2 QA Accuracy\n\n<p align=\"center\">\n  <img src=\"public/images/blog/sci.jpg\" alt=\"QA evaluation accuracy chart\" width=\"600\">\n</p>\n\n| Model                 | Accuracy (%) |\n| --------------------- | :----------: |\n| GPT-5                 |   **74.6**   |\n| **Qwen 3 14B (FT)**   |   **73.9**   |\n| Gemini 2.5 Flash      |     73.9     |\n| Claude 4.5 Sonnet     |     72.9     |\n| **Nemotron 12B (FT)** |     71.3     |\n| Nemotron 12B (Base)   |     70.1     |\n| Qwen 3 14B (Base)     |     68.3     |\n| GPT OSS 120B          |     63.9     |\n| GPT OSS 20B           |     58.8     |\n\n*Figure 2.* QA evaluation over **1,270 MCQs** (multiple-choice accuracy).\n\n### 3.3 Throughput on 8×H200 (TP=8, vLLM)\n\n| Model            | Requests/sec | Input tok/sec | Output tok/sec | Single-req tok/sec |\n| ---------------- | :----------: | :-----------: | :------------: | :----------------: |\n| **Nemotron 12B** |   **0.97**   | **16,943.69** |  **4,880.76**  |      **76.17**     |\n| **Qwen 3 14B**   |     0.43     |    7,516.54   |    2,588.30    |        39.59       |\n\n**Nemotron 12B** delivers ~**2.25×** the throughput of **Qwen 3 14B**, favoring **large-scale** runs.\n\n### 3.4 Visualization Tool\n\nExplore **100k** structured summaries (Qwen 3 14B FT outputs) at **[https://laion.inference.net/](https://laion.inference.net/)**. We compute **Qwen 3 Embedding 4B** embeddings on summaries and use **UMAP** for clustering; **cosine similarity** supports nearest-neighbor exploration.\n\n---\n## Discussion ### 4.1 Implications Structured summaries enable: * Faster **retrieval** across the literature * Better **machine reasoning** on scientific content * Improved **accessibility** where full texts are unavailable * Novel **visual analytics** for mapping scientific landscapes * **Standardized English** representations to simplify cross-domain search Fine-tuned **open** models, correctly trained and formatted, are **competitive** for this task. \n\n\n### 4.2 Decentralized Compute Processing **100M** papers is compute-intensive. The **Inference.net** **permissionless GPU network** (with **verification**) harnesses **idle global compute** at low cost, offering resilient infrastructure for science. Rough estimates: **>$5M** at current **GPT-5** pricing vs. **< $100k** via decentralized nodes and our models. \n\n\n### 4.3 Limitations * **Hallucinations** remain possible, especially for fine-grained details (Ns, effect sizes, CIs, dates, units). * **LLM-as-a-Judge** compresses multiple desiderata into one score; high scores don’t guarantee **line-by-line fidelity**. * **QA** tests whether a smaller model can use a **generated** summary—not whether every atomic claim is exact. * **Context limits** (e.g., **128k tokens**) may force **selective reading** on very long papers. * **Domain heterogeneity** can reduce recall in specialized subfields without further tuning. * **LLM-generated targets** risk **propagating upstream biases**. **Appropriate use:** Treat summaries as **high-quality overviews** for search/triage/review—not as substitutes for the source in **high-stakes** contexts. Verify numbers, dates, and terms in the original paper when precision is critical. \n\n\n### 4.4 Outlook & Future Work * **Scale to 100M summaries + metadata**: Join each summary to **OpenAlex** metadata (authors, venues, concepts, references, citations) for **graph-native** exploration at scale ([https://docs.openalex.org/](https://docs.openalex.org/)). * **Release permissive full texts + summaries**: For permissively licensed papers (e.g., **PeS2o**, **Common Pile PubMed**), pair **full text** with structured summaries to support **long-context** training and **grounded retrieval**. * **From summaries to Knowledge Units**: Iteratively convert summaries into **Alexandria-style Knowledge Units** ([arXiv:2502.19413](https://arxiv.org/abs/2502.19413)) to create a **shareable factual substrate** suited for open dissemination. (More compute-intensive; we will prioritize scaled summaries + metadata first.) --- \n\n\n## Conclusion Open, large-scale **structured summarization** can significantly **accelerate** scientific discovery and education. **Nemotron 12B** provides **superior throughput** for at-scale processing; our **fine-tuned models** and **released datasets** show that open approaches can be both **practical** and **competitive**. Our **visualizer** demonstrates real applications of structured summaries, and collaboration with **Inference.net** highlights how **decentralized compute** can tackle the processing challenges ahead. **Call to action:** We invite **researchers, librarians, and open-access advocates** to help us **gather more papers** for large-scale knowledge extraction. We also invite **engineers and compute providers** to help **optimize** our paragraph-level pipeline and **contribute GPU capacity** (decentralized nodes, clusters, credits) so we can run inference over the **full corpus** and convert it into **Alexandria-style Knowledge Units**—**freeing factual scientific knowledge** for education and accelerated research. \n\n--- \n\n## Acknowledgments This is a collaboration between **LAION**, **Grass**, and **Inference.net**. We thank all contributors, especially **Tawsif Ratul** for data collection, and **Prof. Sören Auer**, **Dr. Gollam Rabby**, and the **TIB – Leibniz Information Centre for Science and Technology** for scientific advice and support. --- ## References 1. **Alexandria Project** (2023). *Democratizing access to scientific knowledge.* [https://projects.laion.ai/project-alexandria/](https://projects.laion.ai/project-alexandria/) 2. **bethgelab Paper Dataset** (2024). [https://huggingface.co/datasets/bethgelab/paper_parsed_jsons](https://huggingface.co/datasets/bethgelab/paper_parsed_jsons) 3. **LAION COREX-18text** (2024). [https://huggingface.co/datasets/laion/COREX-18text](https://huggingface.co/datasets/laion/COREX-18text) 4. **Common Pile PubMed** (2024). [https://huggingface.co/datasets/common-pile/pubmed](https://huggingface.co/datasets/common-pile/pubmed) 5. **LAION PeS2oX-fulltext** (2024). [https://huggingface.co/datasets/laion/Pes2oX-fulltext](https://huggingface.co/datasets/laion/Pes2oX-fulltext) 6. **A Survey on LLM-as-a-Judge** (2025). [https://arxiv.org/abs/2411.15594](https://arxiv.org/abs/2411.15594) 7. **Inference.net Paper Visualizer** (2025). [https://laion.inference.net/](https://laion.inference.net/) 8. **Qwen 3 Embedding 4B** (2025). [https://huggingface.co/Qwen/Qwen3-Embedding-4B](https://huggingface.co/Qwen/Qwen3-Embedding-4B)\n\n---\n\n## Appendix A — Implementation Details\n\n### A.1 LLM-as-a-Judge Prompt\n\n```text\nYou are an expert judge evaluating the quality of AI-generated summarizations of scientific research articles...\n[truncated for brevity – keep your full prompt here as in the original]\n```\n\n### A.2 JSON Schema (Article Response)\n\n```json\n{\n  \"name\": \"article_response\",\n  \"schema\": {\n    \"$defs\": {\n      \"...\": \"Keep your full schema here exactly as in the original post\"\n    }\n  }\n}\n```\n\n","slug":"summaries"},"__N_SSG":true}