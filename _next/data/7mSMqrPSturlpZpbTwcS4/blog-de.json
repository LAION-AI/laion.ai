{"pageProps":{"posts":[{"slug":"leo-lm","frontmatter":{"title":"LeoLM: Ein Impuls f√ºr Deutschsprachige LLM-Forschung","author":"Bj√∂rn Pl√ºster","date":"September 28 2023","previewImg":"/images/blog/leolm-banner.jpg"},"content":"\nLernen Sie LeoLM kennen, das erste offen und kommerziell verf√ºgbare deutsche Foundation Language Model, das auf Llama-2 basiert.\nUnsere Modelle erweitern die F√§higkeiten von Llama-2 durch ein fortgesetztes Training auf einem gro√üen Korpus von hochwertigen deutschen und gr√∂√ütenteils lokal spezifischen Texten.\nDank eines Compute-Grants auf dem neuen Supercomputer **42** von [HessianAI](https://hessian.ai/) ver√∂ffentlichen wir zwei Foundation-Modelle, die mit einer Kontextl√§nge von 8k trainiert wurden,\n[`LeoLM/leo-hessianai-7b`](https://huggingface.co/LeoLM/leo-hessianai-7b) und [`LeoLM/leo-hessianai-13b`](https://huggingface.co/LeoLM/leo-hessianai-13b) (70b folgt auch bald! üëÄ) unter der [Llama-2 Community-Lizenz](https://ai.meta.com/llama/license/). Zus√§tzlich konstruieren wir einen Evaluierungssatz f√ºr Benchmarks zur √úberpr√ºfung der F√§higkeiten deutscher Sprachmodelle, um den Modellvergleich zu standardisieren, √§hnlich zu den weit verbreiteten auf Englisch basierten Evaluierungen, wie sie beispielsweise von [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) oder [LLM-Foundry](https://github.com/mosaicml/llm-foundry) bereitgestellt werden.\nMit dieser Ver√∂ffentlichung sind LAION und Hessian.AI bereit, die deutsche Open-Source und kommerzielle LLM-Forschung erheblich zu verbessern und so neue M√∂glichkeiten zu f√∂rdern und die weite Verbreitung zu beschleunigen.\n\nProbieren Sie [**LeoLM/leo-hessianai-7b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-7b-chat) und [**LeoLM/leo-hessianai-13b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-13b-chat) auf HuggingFace Spaces aus!\n\n*[[Auf Englisch lesen]](/blog/leo-lm)*\n\n## Einleitung\n\nSeit der Ver√∂ffentlichung der urspr√ºnglichen Llama Foundation Models <sup>1</sup> im Januar 2023 hat die Open-Source und wissenschaftliche Forschungsgemeinschaft\neine rasante Beschleunigung in der Entwicklung von immer f√§higeren Sprachmodellen erlebt. Die Fortschritte\nder letzten Wochen haben die leistungsf√§higsten Llama-2 <sup>2</sup> basierten Modelle n√§her an die Konkurrenz zu OpenAI's ChatGPT auf Basis von GPT-3.5 oder sogar dem st√§rkeren GPT4 gebracht.\nDennoch besteht eine bemerkenswerte Einschr√§nkung fort: Die Mehrheit dieser bahnbrechenden Fortschritte bleibt auf den Bereich der englischen Sprache beschr√§nkt.\nDiese Einschr√§nkung resultiert haupts√§chlich daraus, dass gro√üe Open-Source-Modelle √ºberwiegend auf monolingualen englischen Daten trainiert wurden. Obwohl es einige\nForschungen zum Zweitsprachen- oder Mehrsprachen-Finetuning gibt, sind die meisten resultierenden Modelle in ihren F√§higkeiten beschr√§nkt und leiden unter grammatikalischen Schw√§chen und der US-zentrischen Voreingenommenheit, die den englischen Daten inh√§rent ist.\n\nWir wollen diese Probleme im Fallbeispiel der deutschen Sprache durch die Anwendung vieler heutiger Spitzentechniken l√∂sen, um ein wirklich leistungsf√§higes,\nlokales und zweisprachiges LLM zu entwickeln.\nZu diesem Zweck pr√§sentieren wir LeoLM (**L**inguistisch **E**rweitertes **O**ffenes **L**anguage **M**odel), eine Suite von auf Llama-2 basierenden deutschen Foundation-\nModellen und eine Auswahl begleitender Feinabstimmungen.\nDes Weiteren pr√§sentieren wir GermanBench, eine Sammlung der relevantesten ins Deutsche √ºbersetzten englischen Benchmarks, die es uns erm√∂glichen in √§hnlichem Ausma√ü wie im Englischen, die F√§higkeiten von LeoLM gr√ºndlich zu bewerten.\n\n<sup>1</sup>: [Touvron et al. 2023a](https://arxiv.org/abs/2302.13971)\n<sup>2</sup>: [Touvron et al. 2023b](https://arxiv.org/abs/2307.09288)\n\n## Vorverarbeitung in Phase 2\n\nLlama-2-Modelle werden auf 2 Billionen Tokens √ºberwiegend englischen Textes vortrainiert. Um ihre Kompetenz in der deutschen Sprache zu erh√∂hen, verwenden wir ein fortgesetztes Vortraining, welches wir als \"Stage 2 Pretraining\" bezeichnen.\nWir initialisieren LeoLM mit Llama-2-Gewichten und setzen das Training des Modells auf einem gro√üen deutschen Textkorpus von 65 Milliarden Tokens fort, die rigoros gefiltert und dedupliziert wurden und gr√∂√ütenteils aus dem [OSCAR-2301-Korpus](https://huggingface.co/datasets/oscar-corpus/OSCAR-2301) stammen.\nEin wesentlicher Aspekt dieses Ansatzes besteht darin, das Vergessen oder den Verlust von zuvor erlerntem Wissen oder F√§higkeiten zu minimieren. Wir folgen den Erkenntnissen von [Gupta et al. (2023)](https://arxiv.org/abs/2308.04014) in unserer Wahl der Hyperparameter, um das Risiko des Vergessens zu minimieren.\nZus√§tzlich folgen wir der Arbeit von [Together](https://huggingface.co/togethercomputer/LLaMA-2-7B-32K) bei der Anwendung von [linearer RoPE-Skalierung](https://kaiokendev.github.io/til#extending-context-to-8k) und [Flash Attention 2](https://tridao.me/publications/flash2/flash2.pdf), um die Trainingseffizienz zu verbessern und die Kontextl√§nge auf 8k Tokens zu verdoppeln.\nSiehe Abbildung 1 f√ºr einen √úberblick √ºber alle Training-Hyperparameter.\n\n![training_parameters](/images/blog/training_params.png \"Training-Hyperparameters\")\n\n## Feinabstimmungsdatens√§tze\n\nEs gibt viel Diskussion dar√ºber, was ein guter Chat/Instruktionstuning-Datensatz bieten muss, was zur Entwicklung einer Vielzahl verschiedener, erfolgreicher Ans√§tze gef√ºhrt hat. Wir lassen uns von dieser Vielfalt inspirieren und √ºbersetzen, um √§hnliche F√§higkeiten auf Deutsch zu bringen, eine Auswahl hochwertiger Instruktionsdatens√§tze ins Deutsche mit OpenAI's `gpt-3.5-turbo` API. Die Verwendung von `gpt-3.5-turbo` stellt sicher, dass der Zusammenhang zwischen Aufforderungen und Antworten intakt bleibt und dass komplexe Anweisungen, die m√∂glicherweise Code, Gleichungen oder formatierte Daten enthalten, korrekt √ºbersetzt werden.\nAufbauend auf den Erkenntnissen der Community, w√§hlen wir eine Vielzahl von Datens√§tzen aus, die wir √ºbersetzen und f√ºr das Training unseres Chat-Modells verwenden.\nDie √ºbersetzten Datens√§tze sind:\n\n- [OpenPlatypus](https://huggingface.co/datasets/garage-bAInd/Open-Platypus) -> [OpenSchnabeltier](https://huggingface.co/datasets/LeoLM/OpenSchnabeltier)\n- [OpenAssistant OASST1](https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25) -> [OpenAssistant-DE](https://huggingface.co/datasets/OpenAssistant/OASST-DE)\n\nAu√üerdem verwenden wir [`FreedomIntelligence/evol-instruct-deutsch`](https://huggingface.co/datasets/FreedomIntelligence/evol-instruct-deutsch) und [`FreedomIntelligence/alpaca-gpt4-deutsch`](https://huggingface.co/datasets/FreedomIntelligence/alpaca-gpt4-deutsch) aus dem [MultilingualSIFT](https://github.com/FreedomIntelligence/MultilingualSIFT)-Projekt. Vielen Dank an die Autoren, dass sie ihre Daten geteilt haben!\nUm die zweisprachige Nutzung zu erleichtern, trainieren wir auch Modelle auf einer Kombination dieser √ºbersetzten Datens√§tze und ihren urspr√ºnglichen, englischen Gegenst√ºcken.\n\nSchlie√ülich erstellen wir, um Schw√§chen beim kreativen Schreiben und Reimen, die bei fr√ºhen Tests festgestellt wurden, auszugleichen, zwei weitere Datens√§tze:\n\n- [GPT4 Gedichte](https://huggingface.co/datasets/LeoLM/German_Poems): Eine Sammlung deutscher Gedichte zu verschiedenen Themen, geschrieben von GPT4\n- [GPT4 Lieder](https://huggingface.co/datasets/LeoLM/German_Songs): Eine Sammlung deutscher Lieder und nachfolgende Analysen, geschrieben von GPT4.\n\n## Evaluation und Ergebnisse\n\nDie Evaluierung der F√§higkeiten von LLMs, insbesondere von Chat-Modellen, ist komplex und die besten Methoden sind noch umstritten. Benchmarks, die auf Multiple-Choice basieren und anhand der Protokoll-Wahrscheinlichkeiten des Modells ausgewertet werden (wie im [Open LLM Leaderboard]()), sind eine derzeit beliebte Methode. Eine andere Methode bewertet Antworten automatisch mit GPT4, wie bei AlpacaEval oder MT-Bench. Dieser Ansatz richtet sich eher an Chat-Modelle, da er die Qualit√§t von Modellantworten in realen Aufgaben ber√ºcksichtigt. Um so vergleichbar wie m√∂glich zu sein, √ºbersetzen wir eine Reihe von englischen Benchmarks direkt ins Deutsche. Wir ver√∂ffentlichen diese Datens√§tze in unserer [HF-Organisation](https://huggingface.co/LeoLM) und mit ausf√ºhrlicher Dokumentation [auf GitHub](https://github.com/bjoernpl/GermanBenchmark), und Sie finden den entsprechende `lm-evaluation-harness`-Branch [hier](https://github.com/bjoernpl/lm-evaluation-harness-de/tree/mmlu_de) und den `FastEval`-Branch [hier](https://github.com/bjoernpl/FastEval).\n\nIn Abbildung 3 k√∂nnen Sie einen Vergleich von LeoLM gegen√ºber den Basis-Llama-2-Modellen auf einer Auswahl von Benchmarks mit sowohl der englischen Version (blau) als auch unserer √ºbersetzten Version (gr√ºn) sehen. Unser Trainging verbessert die Benchmark-Ergebnisse f√ºr die deutschen Aufgaben, w√§hrend die Ergebnisse f√ºr die englischen Aufgaben leicht reduziert werden. Bemerkenswert ist, dass der durchschnittliche Anstieg der deutschen Benchmark-Ergebnisse die durchschnittliche Abnahme der Leistung auf den englischen Benchmarks deutlich √ºberwiegt, was zeigt, dass unser Ansatz das Erlernen einer neuen Sprache erm√∂glicht, ohne zu vergessen, was zuvor gelernt wurde. Warum die Ergebnisse in Deutsch niedriger bleiben als in Englisch, ist eine offene Frage, kann aber zum Teil auf eine Qualit√§tsminderung bei der √úbersetzung zur√ºckgef√ºhrt werden.\n\n![](/images/blog/benchmarks.png)\n\nDie folgende Tabelle zeigt die Ergebnisse auf unserer √ºbersetzten Version von MT-Bench. MT-Bench ist ein Benchmark, der die Multi-Turn-Leistung auf einem kuratierten Satz von 80 Fragen aus mehreren Kategorien mit GPT-4 als Richter bewertet. Dabei bewertet GPT-4 die Aufforderungen auf einer Skala von 1-10 hinsichtlich der wahrgenommenen Hilfsbereitschaft, Relevanz, Genauigkeit, Tiefe, Kreativit√§t und Detailliertheit der Antwort. Das monolinguale Modell `leo-hessianai-13b-chat` schneidet insgesamt am besten ab und kommt sogar dem GPT-3.5 im Thema \"Geisteswissenschaften\" nahe. Es erzielt auffallend schlechte Ergebnisse in Mathematik und Codierung, was zu erwarten ist, da die Llama-2-Modelle in diesem Bereich ohne sehr explizites Finetuning von vornherein Schw√§chen aufweisen. Die zweisprachigen Modelle erzielen in einigen Kategorien wie Mathematik und Logik leicht unter ihren monolingualen Gegenst√ºcken, w√§hrend sie in Codierung und Extraktion √ºbertreffen.\n\n![](/images/blog/mt_bench.png)\nF√ºr eine detailliertere Evaluierung, bleiben Sie dran f√ºr unser Paper!\n\n## Qualitative Ergebnisse\n\nBenchmarks neigen dazu, ziemlich abstrakt zu sein. Um ein besseres Gef√ºhl f√ºr LeoLM's zu bekommen, schauen Sie sich unsere Demos an und probieren Sie es selbst aus: [**LeoLM/leo-hessianai-7b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-7b-chat) und den gr√∂√üeren Bruder [**LeoLM/leo-hessianai-13b-chat`**](https://huggingface.co/spaces/LeoLM/leo-hessianai-13b-chat). Alternativ k√∂nnen Sie das Modell selbst mit ü§óTransformers ausf√ºhren. Weitere Informationen zur Einrichtung finden Sie auf der [Modellkarte](https://huggingface.co/LeoLM/leo-hessianai-13b-chat).\n\n## Fazit\n\nUnsere Forschung hat mehrere Schl√ºsselbeitr√§ge:\n\n- Wir ver√∂ffentlichen eine Suite von deutschen Foundation-Sprachmodellen unter einer offenen Lizenz.\n- Wir √ºbertragen einen gr√ºndlichen und vielseitigen Evaluierungsansatz f√ºr Basis- und Chat-Modelle ins Deutsche.\n- Wir zeigen, dass eine gro√ü angelegte Fortbildung auch f√ºr datenges√§ttigte Modelle wie Llama-2 ohne signifikantes Vergessen oder Verlust von fr√ºheren F√§higkeiten m√∂glich ist.\n- Wir pr√§sentieren eine vielf√§ltige Suite von Instruktions-/Chat-Tuning-Datens√§tzen, die vom Englischen ins Deutsche √ºbersetzt wurden, um als Basis f√ºr die deutsche Open-Source-LLM-Forschungsgemeinschaft zu dienen.\n\nInsgesamt ist die LeoLM-Modellsuite ein Proof-of-Concept f√ºr den Spracherwerb f√ºr vortrainierte Modelle. Dar√ºber hinaus pr√§sentiert sie sich als das erste offen verf√ºgbare deutsche Foundation-Modell, das den heutigen Standards entspricht. Wir bei LAION hoffen, die deutsche Open-Source-Forschungsgemeinschaft ansto√üen zu k√∂nnen, um die Abh√§ngigkeit von geschlossenen kommerziellen Quellen wie OpenAI zu verringern. Viel Spa√ü mit LeoLM!\n\n## Danksagungen\n\nDieses Projekt wurde von Bj√∂rn Pl√ºster (Universit√§t Hamburg) geleitet mit Unterst√ºtzung, Anleitung und\nSupervision von Christoph Schuhmann (LAION), Patrick Schramowski (LAION, Hessian AI, DFKI) und Jenia Jitsev (LAION, Juelich Supercomputing Center, Helmholtz Research Center Juelich).\nWir danken den Leuten bei hessian.AI, insbesondere Wolfgang Stille, Kristian Kersting, Florian Kieser und Patrick Diem\nf√ºr ihre gro√üz√ºgige Rechenzeitvergabe √ºber das Hessian AI Innovation Lab (gef√∂rdet vom Hessischen Ministerium f√ºr Digitale Strategie und Entwicklung) und das AI Service Center hessian.AISC (gef√∂rdert vom Bundesministerium f√ºr Bildung und Forschung (BMBF)), und Unterst√ºtzung w√§hrend des gesamten Trainingsprozesses!  Vielen Dank auch an Jae Hee Lee von der Universit√§t Hamburg, Andreas K√∂pf von Open-Assistant und Community-Mitglieder bei LAION f√ºr ihr hilfreiches Feedback. Weiterhin danken wir dem hessian.AI's Innovationslabor, dem AI Servicezentrum hessian.AISC und dem Deutschen Zentrum f√ºr K√ºnstliche Intelligenz (DFKI) f√ºr ihre Unterst√ºtzung.\n\nAu√üerdem danken wir den Gruppen, die die Datens√§tze f√ºr unser Chat-finetuning urspr√ºnglich erstellt haben: [OpenAssistant](https://huggingface.co/OpenAssistant), [FreedomIntelligence](https://huggingface.co/FreedomIntelligence), [garage-bAInd](https://huggingface.co/garage-bAInd), [WizardLM](https://huggingface.co/WizardLM) and [Instruction-Tuning-with-GPT-4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM).\n\n---\n\n### An Example\n\n**User:**  Warum ist open-source besonders in der KI-Forschung wichtig?\n\n**LeoLM:**\nOpen-Source spielt eine wichtige Rolle in der KI-Forschung, da sie Zusammenarbeit, Innovation und Zugang zur Technologie f√∂rdert. Hier sind einige Gr√ºnde, warum Open-Source in der KI-Forschung besonders wichtig ist:\n\n- Zusammenarbeit: Durch Open-Source k√∂nnen Forscher gemeinsam an Projekten arbeiten, Ideen austauschen und voneinander lernen. Dies f√∂rdert die Zusammenarbeit zwischen Forschern und kann zu schnelleren und effizienteren Fortschritten in der KI-Forschung f√ºhren.\n\n- Transparenz: Open-Source erm√∂glicht Transparenz und Reproduzierbarkeit in der KI-Forschung. Forscher k√∂nnen den Code und die Methoden anderer sehen, reproduzieren und darauf aufbauen, was die Validierung von Ergebnissen und die Identifizierung potenzieller Schwachstellen erleichtert.\n\n- Zugang zur Technologie: Open-Source macht KI-Technologien und -Werkzeuge einem breiteren Publikum zug√§nglich. Dies erm√∂glicht kleineren Forschungseinrichtungen und Einzelpersonen den Zugang zu KI-Werkzeugen, die ihnen sonst m√∂glicherweise nicht zur Verf√ºgung st√ºnden. Dadurch kann Innovation und Forschung auch au√üerhalb traditioneller Zentren gef√∂rdert werden.\n\n- Innovation: Open-Source f√∂rdert Innovation in der KI-Forschung, indem sie Raum f√ºr Experimente und neue Ideen bietet. Da der Code offen und f√ºr alle zug√§nglich ist, k√∂nnen Forscher ihn an ihre spezifischen Bed√ºrfnisse anpassen und neue KI-Werkzeuge und -Technologien entwickeln.\n\n- Lernen: Open-Source erm√∂glicht es Forschern, voneinander zu lernen. Durch die gemeinsame Nutzung von Code und Methoden k√∂nnen Forscher aus den Erfahrungen anderer lernen und ihre eigenen Forschungsprojekte verbessern.\n\nZusammenfassend ist Open-Source ein wichtiger Aspekt der KI-Forschung, da er Zusammenarbeit, Transparenz, Zugang zur Technologie, Innovation und Lernen f√∂rdert. Dadurch wird die KI-Forschung effizienter, innovativer und zug√§nglicher f√ºr eine breitere Palette von Forschern und Institutionen\n\n| [Link zu einer PDF mit mehr Beispielen](/documents/leo-lm.pdf) |\n|----------|\n","date":1695859200000}]},"__N_SSG":true}