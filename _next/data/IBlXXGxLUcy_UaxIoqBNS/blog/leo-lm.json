{"pageProps":{"frontmatter":{"title":"LeoLM: Igniting German-Language LLM Research","author":"Bj√∂rn Pl√ºster","date":"September 28 2023","previewImg":"/images/blog/leolm-banner.jpg"},"content":"\nWe proudly introduce LeoLM (**L**inguistically **E**nhanced **O**pen **L**anguage **M**odel), the first comprehensive suite of German-language Foundation Language Models trained in collaboration with HessianAI on their new supercomputer **42**! Built on Llama-2 and trained on a large-scale, high-quality German text corpus, we present LeoLM-7B and 13B, with LeoLM-70B on the horizon, accompanied by a collection of exceptionally proficient German and bilingual chat models.\n\nMeet LeoLM, the first open and commercially available German Foundation Language Model built on Llama-2.\nOur models extend Llama-2's capabilities into German through continued pretraining on a large corpus of high-quality German and mostly locality-specific text.\nThanks to a compute grant at [HessianAI](https://hessian.ai/)'s new supercomputer **42**, we release two foundation models trained with 8k context length,\n[`LeoLM/leo-hessianai-7b`](https://huggingface.co/LeoLM/leo-hessianai-7b) and [`LeoLM/leo-hessianai-13b`](https://huggingface.co/LeoLM/leo-hessianai-13b) (70b also coming soon! üëÄ) under the [Llama-2 community license](https://ai.meta.com/llama/license/). In addition, we construct evaluation set for benchmarking capabilities of german language models to standardize model comparison, similar to widely adopted english based evals, as provided for instance by [lm-harness-eval](https://github.com/EleutherAI/lm-evaluation-harness) or [LLM-Foundry](https://github.com/mosaicml/llm-foundry).\nWith this release, LAION and Hessian.AI are poised to significantly enhance German open-source and commercial LLM research, fostering new opportunities and expediting widespread adoption.\n\n*[[Read in German]](/blog-de/leo-lm)*\n\n## Introduction\n\nSince the release of the original Llama Foundation Models <sup>1</sup> in January of 2023, the open-source and academic research community\nhas experienced a rapid acceleration in the development of increasingly capable language models. The advances\nof the past weeks have brought the strongest Llama-2 <sup>2</sup>  based models closer to competing with OpenAI's ChatGPT based on GPT-3.5 or even the stronger GPT4 in some cases.\nNevertheless, a noteworthy constraint persists: the majority of these groundbreaking advancements remain confined to the realm of the English language.\nThis limitation stems mainly from large open-source models having been trained on predominantly mono-lingual English data. While there has been some\nresearch into second-language or multilingual finetuning, most resulting models are limited in their capabilities and suffer from the US-centric bias inherent to English data.\n\nWe seek to alleviate these issues in the case study of the German language by applying many of today's state-of-the-art techniques to develop a truly capable,\nlocalized, and bilingual LLM.\nTo this end, we present LeoLM (**L**inguistically **E**nhanced **O**pen **L**anguage **M**odel), a suite of Llama-2-based German foundation\nmodels, and an assortment of accompanying finetunes.\nFurther, we present GermanBench, a collection of the most relevant English Benchmarks translated into German, allowing us to evaluate LeoLM's capabilities thoroughly.\n\n<sup>1</sup> [Touvron et al. 2023a](https://arxiv.org/abs/2302.13971)\n<sup>2</sup> [Touvron et al. 2023b](https://arxiv.org/abs/2307.09288)\n\n## Stage-2 Pretraining\n\nLlama-2 models are pretrained on 2 trillion tokens of predominantly English text. To enhance their proficiency in the German language, we employ a Stage 2 pretraining methodology.\nWe initialize LeoLMs using Llama-2 weights and continue training the model on a large German text corpus of 65 billion tokens of deliberately filtered and deduplicated web text built from the [OSCAR-2301 corpus](https://huggingface.co/datasets/oscar-corpus/OSCAR-2301).\nA significant aspect of this approach is mitigating the forgetting or loss of previously learned knowledge or capabilities. We follow the findings by [Gupta et al. (2023)](https://arxiv.org/abs/2308.04014) in our choice of hyperparameters to minimize the risk of forgetting.\nAdditionally, we follow work by [Together](https://huggingface.co/togethercomputer/LLaMA-2-7B-32K) in employing [linear RoPE scaling](https://kaiokendev.github.io/til#extending-context-to-8k) and [Flash Attention 2](https://tridao.me/publications/flash2/flash2.pdf) to improve training efficiency and double context length to 8k tokens.\nSee Figure 1 for an overview of all training hyperparameters.\n\n![training_parameters](/images/blog/training_params.png \"Training Hyperparameters\")\n\n## Finetuning Datasets\n\nThere is much debate on what a good chat/instruction tuning dataset must offer, sparking the development of a plethora of different, successful approaches. We take inspiration from this diversity and, to bring similar capabilities to German, translate an assortment of high-quality instruction datasets to German using OpenAI's `gpt-3.5-turbo` API. Using `gpt-3.5-turbo` ensures that the context between prompts and responses remains intact and that complex instructions, potentially containing code, equations, or formatted data, are accurately translated.\nBuilding on community findings, we select a variety of datasets to translate and use to train our chat model.\nThe translated datasets are:\n\n- [OpenPlatypus](https://huggingface.co/datasets/garage-bAInd/Open-Platypus) -> [OpenSchnabeltier](https://huggingface.co/datasets/LeoLM/OpenSchnabeltier)\n- [OpenAssistant OASST1](https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25) -> [OpenAssistant-DE](https://huggingface.co/datasets/OpenAssistant/OASST-DE)\n\nMoreover, we use [`FreedomIntelligence/evol-instruct-deutsch`](https://huggingface.co/datasets/FreedomIntelligence/evol-instruct-deutsch) and [`FreedomIntelligence/alpaca-gpt4-deutsch`](https://huggingface.co/datasets/FreedomIntelligence/alpaca-gpt4-deutsch) from the [MultilingualSIFT](https://github.com/FreedomIntelligence/MultilingualSIFT) project. Thanks to the authors for sharing their data!\nTo facilitate bilingual use, we also train models on a combination of these translated datasets and their original, English counterparts.\n\nFinally, to make up for weaknesses in creative writing and rhyming identified during early tests, we curate two more datasets:\n\n- [GPT4 Poems](https://huggingface.co/datasets/LeoLM/German_Poems): A set of German poems about different topics written by GPT4\n- [GPT4 Songs](https://huggingface.co/datasets/LeoLM/German_Songs): A collection of German songs and following analyses written by GPT4.\n\n## Evaluation and Results\n\nEvaluating the capabilities of LLMs, especially chat models, is complex, and the best methods are still up for debate. Benchmarks based on multiple choice that are evaluated via the model's log-probabilities (as in the [Open LLM Leaderboard]()) are one currently popular method. Another method automatically evaluates responses using GPT4, as in AlpacaEval or MT-Bench. This approach is more geared toward chat models, as it considers the quality of model responses in real-life tasks. To be as comparable as possible, we directly translate a set of English benchmarks to German. We release these datasets in our [HF Organization](https://huggingface.co/LeoLM) and with more detailed documentation [on GitHub](https://github.com/bjoernpl/GermanBenchmark), and you can find the corresponding `lm-evaluation-harness` fork [here](https://github.com/bjoernpl/lm-evaluation-harness-de/tree/mmlu_de) and the `FastEval` fork [here](https://github.com/bjoernpl/FastEval).\n\nIn Figure 3, you can see a comparison of LeoLM versus the base Llama-2 models on a selection of Benchmarks with both the English version (blue) and our translated version (green). Our training improves benchmark scores on the German tasks while slightly reducing scores on English tasks. Notably, the mean increase in German benchmark scores significantly outweighs the mean decrease in performance on English benchmarks, showing that our approach enables learning a new language without forgetting what was previously learned. Why the scores remain lower in German than English is an open question but may be partially attributed to quality degradation during translation.\n\n![](/images/blog/benchmarks.png)\n\nThe following table shows the results on our translated version of MT-Bench. MT-Bench is a benchmark that evaluates multi-turn performance on a curated set of 80 questions from multiple categories using GPT-4 as a judge. In this, GPT-4 evaluates the prompts on a scale from 1-10 with regard to perceived helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. The monolingual `leo-hessianai-13b-chat` model performs best overall, even coming close to GPT-3.5 in the \"humanities\" topic. It scores noticeably poorly in math and coding, which is somewhat to be expected given that the Llama-2 models inherently fall short on this without very explicit finetuning. The bilingual models score slightly below their monolingual counterparts in some categories such math and reasoning while exceeding in coding and extraction.\n\n![](/images/blog/mt_bench.png)\nFor a more detailed evaluation, stay tuned for our paper!\n\n## Qualitative Results\n\nBenchmarks tend to be pretty abstract. To get a better feeling for LeoLM's check out our demos and try it yourself: [**LeoLM/leo-hessianai-7b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-7b-chat) and the bigger sibling [**LeoLM/leo-hessianai-13b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-13b-chat). Alternatively, you can run the model yourself using ü§óTransformers. Find more info on how to set this up on the [model card](https://huggingface.co/LeoLM/leo-hessianai-13b-chat).\n\n## Conclusion\n\nOur research has several key contributions:\n\n- We release a suite of German Foundation Language Models with a permissive license.\n- We transfer a thorough and multi-faceted evaluation approach for base and chat models into German.\n- We demonstrate that large-scale continued pretraining is possible even for data-saturated models such as Llama-2 without significant forgetting or loss of previous capabilities.\n- We present a diverse suite of instruction/chat-tuning datasets translated from English to German to serve as a basis for Germany's open-source LLM research community.\n\nOverall, the LeoLM model suite is a proof-of-concept of language acquisition for pretrained models. Moreover, it presents itself as the first openly available German Foundation Model that meets today's standards. We at LAION hope to kickstart the German open-source research community to reduce dependence on closed-source commercial sources such as OpenAI. Have fun with LeoLM!\n\n## Acknowledgments\n\nThis project was led by Bj√∂rn Pl√ºster with support, guidance and supervision by Christoph Schuhmann (LAION), Patrick Schramowski (LAION, Hessian.AI, DFKI) and Jenia Jitsev (LAION, Juelich Supercomputing Center, Helmholtz Research Center Juelich).\nWe would like to thank the folks at hessian.AI, especially Wolfgang Stille, Kristian Kersting, Florian Kieser and Patrick Diem\nfor their generous compute sponsorship via the Hessian.AI Innovation Lab (funded by the Hessian Ministry for Digital Strategy and Innovation) and the hessian.AISC Service Center (funded by the Federal Ministry of Education and Research (BMBF)), and their support throughout the training process!  Many thanks also to Jae Hee Lee from the\nUniversity of Hamburg and community members at LAION for their helpful feedback. Special thanks also go to hessian.AI's\nInnovation Lab, the AI service centre hessian.AISC and the German Center for Artificial Intelligence (DFKI).\n\n---\n\n### An Example\n\n**User:**  Warum ist open-source besonders in der KI-Forschung wichtig?\n\n**LeoLM:**\nOpen-Source spielt eine wichtige Rolle in der KI-Forschung, da sie Zusammenarbeit, Innovation und Zugang zur Technologie f√∂rdert. Hier sind einige Gr√ºnde, warum Open-Source in der KI-Forschung besonders wichtig ist:\n\n- Zusammenarbeit: Durch Open-Source k√∂nnen Forscher gemeinsam an Projekten arbeiten, Ideen austauschen und voneinander lernen. Dies f√∂rdert die Zusammenarbeit zwischen Forschern und kann zu schnelleren und effizienteren Fortschritten in der KI-Forschung f√ºhren.\n\n- Transparenz: Open-Source erm√∂glicht Transparenz und Reproduzierbarkeit in der KI-Forschung. Forscher k√∂nnen den Code und die Methoden anderer sehen, reproduzieren und darauf aufbauen, was die Validierung von Ergebnissen und die Identifizierung potenzieller Schwachstellen erleichtert.\n\n- Zugang zur Technologie: Open-Source macht KI-Technologien und -Werkzeuge einem breiteren Publikum zug√§nglich. Dies erm√∂glicht kleineren Forschungseinrichtungen und Einzelpersonen den Zugang zu KI-Werkzeugen, die ihnen sonst m√∂glicherweise nicht zur Verf√ºgung st√ºnden. Dadurch kann Innovation und Forschung auch au√üerhalb traditioneller Zentren gef√∂rdert werden.\n\n- Innovation: Open-Source f√∂rdert Innovation in der KI-Forschung, indem sie Raum f√ºr Experimente und neue Ideen bietet. Da der Code offen und f√ºr alle zug√§nglich ist, k√∂nnen Forscher ihn an ihre spezifischen Bed√ºrfnisse anpassen und neue KI-Werkzeuge und -Technologien entwickeln.\n\n- Lernen: Open-Source erm√∂glicht es Forschern, voneinander zu lernen. Durch die gemeinsame Nutzung von Code und Methoden k√∂nnen Forscher aus den Erfahrungen anderer lernen und ihre eigenen Forschungsprojekte verbessern.\n\nZusammenfassend ist Open-Source ein wichtiger Aspekt der KI-Forschung, da er Zusammenarbeit, Transparenz, Zugang zur Technologie, Innovation und Lernen f√∂rdert. Dadurch wird die KI-Forschung effizienter, innovativer und zug√§nglicher f√ºr eine breitere Palette von Forschern und Institutionen\n\n| [Link to the PDF with more examples](/documents/leo-lm.pdf) |\n|----------|\n","slug":"leo-lm"},"__N_SSG":true}