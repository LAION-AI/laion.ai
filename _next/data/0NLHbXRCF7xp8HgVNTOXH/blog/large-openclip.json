{"pageProps":{"frontmatter":{"title":"Large scale openCLIP: L/14, H/14 and g/14 trained on LAION-2B","author":"Romain Beaumont","date":"Sep 15, 2022","previewImg":"/images/blog/compare3.png"},"content":"\nWe trained three large CLIP models with [OpenCLIP](https://github.com/mlfoundations/open_clip): ViT-L/14, ViT-H/14 and ViT-g/14 (ViT-g/14 was trained only for about a third the epochs compared to the rest). The H/14 model achieves **78.0%** zero shot top-1 accuracy on ImageNet and **73.4%** on zero-shot image retrieval at Recall@5 on MS COCO. As of September 2022, this is the best open source CLIP model.\n\nCLIP makes it possible to compute representations of images and texts to measure how **similar** they are. It can be used for \n\n\n\n* Zero shot classification: compare an image with the text of the class to know which class is most similar (e.g., ImageNet classification)\n* Retrieval: compare an image or a text to billions of text or images to find the most similar (e.g. as in [clip-retrieval](https://rom1504.github.io/clip-retrieval/) )\n* Generation\n    * CLIP guidance: decide a text you want to generate, then use an image generator model, and use the CLIP distance between whatâ€™s generated and the text to generate a better image (e.g., VQGAN + CLIP)\n    * CLIP conditioning: use a clip text embedding as input of a generator to make it generate this text directly (e.g., stable diffusion)\n\nCLIP models are trained in a self supervised fashion on hundreds of millions or billions of (image, text) pairs.\n\nWith LAION, we produced the LAION-5B dataset that contains 5 billions of closely related image and text pairs.\n\nThe CLIP model ViT B/32, released by OpenAI, was initially used to filter this dataset out of common crawl.\n\nProducing the best open source CLIP model out of this data set completes the open source replication of the [excellent](https://openai.com/blog/clip/) clip paper that OpenAI released one year ago.\n\n\n## Results\n\nWe replicated the results from openai CLIP in models of different sizes, then trained bigger models. The full evaluation suite on 39 datasets ([vtab+](https://github.com/LAION-AI/CLIP_benchmark)) are available in this [results notebook](https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/results.ipynb) and show consistent improvements over all datasets.\n\nThe larger models we release today are L/14, H/14 and g/14.\n\nL/14 was trained on JUWELS Booster supercomputer by [Ross wightman](https://github.com/rwightman). H/14 and g/14 were trained on stability cluster by [Romain Beaumont](https://github.com/rom1504) . While L/14 and H/14 were trained using 34B samples from LAION-2b, g/14 used a substantially smaller sample scale for training, seeing only 12B samples (see tables for more details).\n\n\n#### 32B samples seen\n\n\n<table>\n  <tr>\n   <td><strong>Model name</strong>\n   </td>\n   <td><strong>Batch size</strong>\n   </td>\n   <td><strong>Samples seen</strong>\n   </td>\n   <td><strong>Text Params</strong>\n   </td>\n   <td><strong>Image params</strong>\n   </td>\n   <td><strong>Imagenet top1</strong>\n   </td>\n   <td><strong>Mscoco image retrieval at 5</strong>\n   </td>\n   <td><strong>Flickr30k image retrieval at 5</strong>\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://wandb.ai/rom1504/eval_openclip/reports/B-32-2B--VmlldzoyNDkwNDMy\">B/32</a>\n   </td>\n   <td>79k\n   </td>\n   <td>34B (16 epochs of laion2B)\n   </td>\n   <td>63.43M\n   </td>\n   <td>87.85M\n   </td>\n   <td>66.6%\n   </td>\n   <td>65.4%\n   </td>\n   <td>88.4%\n   </td>\n  </tr>\n  <tr>\n   <td>L/14\n   </td>\n   <td>79k for 14B samples, 86K for 18B\n   </td>\n   <td>32B\n   </td>\n   <td>123.65M\n   </td>\n   <td>303.97M\n   </td>\n   <td>75.3%\n   </td>\n   <td>71.1%\n   </td>\n   <td>92.9%\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://wandb.ai/rom1504/eval_openclip/reports/H-14--VmlldzoyNDAxODQ3\">H/14</a>\n   </td>\n   <td>79k\n   </td>\n   <td>32B (16 epochs of laion2B)\n   </td>\n   <td>354.03M\n   </td>\n   <td>632.08M\n   </td>\n   <td>78.0%\n   </td>\n   <td>73.4%\n   </td>\n   <td>94%\n   </td>\n  </tr>\n</table>\n\n\n\n#### 12B samples seen\n\n\n<table>\n  <tr>\n   <td><strong>Model name</strong>\n   </td>\n   <td><strong>Batch size</strong>\n   </td>\n   <td><strong>Samples seen</strong>\n   </td>\n   <td><strong>Text Params</strong>\n   </td>\n   <td><strong>Image params</strong>\n   </td>\n   <td><strong>Imagenet top1</strong>\n   </td>\n   <td><strong>Mscoco image retrieval at 5</strong>\n   </td>\n   <td><strong>Flickr30k image retrieval at 5</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>B/32\n   </td>\n   <td>32k\n   </td>\n   <td>12B (32 epochs of laion400m)\n   </td>\n   <td>63.43M\n   </td>\n   <td>87.85M\n   </td>\n   <td>62.9%\n   </td>\n   <td>60.8%\n   </td>\n   <td>85.5%\n   </td>\n  </tr>\n  <tr>\n   <td>B/16\n   </td>\n   <td>32k\n   </td>\n   <td>12B (32 epochs of laion400m)\n   </td>\n   <td>91.16M\n   </td>\n   <td>86.19M\n   </td>\n   <td>69%\n   </td>\n   <td>63.6%\n   </td>\n   <td>85.5%\n   </td>\n  </tr>\n  <tr>\n   <td>L/14\n   </td>\n   <td>32k\n   </td>\n   <td>12B (32 epochs of laion400m)\n   </td>\n   <td>123.65M\n   </td>\n   <td>303.97M\n   </td>\n   <td>72%\n   </td>\n   <td>68.1%\n   </td>\n   <td>90.8%\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://wandb.ai/rom1504/eval_openclip/reports/slow-g-14--VmlldzoyNTMwMjg5\">g/14</a>\n   </td>\n   <td>32k for 8B samples then 64k for 4B samples\n   </td>\n   <td>12B (similar to 32 epochs on laion400m)\n   </td>\n   <td>354.03M\n   </td>\n   <td>1012.65M\n   </td>\n   <td>76.6%\n   </td>\n   <td>72.4%\n   </td>\n   <td>93.5%\n   </td>\n  </tr>\n</table>\n\n\nIn addition to having overall better results, we hope the larger text encoder will help improve text understanding. The good performance on the retrieval metrics seems to be a good indicator of this property.\n\nNote the difference in samples seen between the H/14 and the g/14 model. This explains the difference in performance. We picked this lower number to try and fix the stability issue at a lower cost. Eventually they were fixed (by using bfloat16). The performance of this model falls in the scaling curve of 12B sample seen (similar to 32 epochs of laion400m), and a g/14 trained on 32B samples of laion2B would most likely follow the same trends as the other models and get better performance as H/14.\n\n\n![alt_text](public/images/blog/images/compare3.png \"image_tooltip\")\n\n\n\n## Released checkpoints\n\nWe release the checkpoints for the models, they are available through [openclip](https://github.com/mlfoundations/open_clip) and in HuggingFace hub at [B/32](https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K) [L/14](https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K) [H/14](https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K) and [g/14](https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K)\n\n\n## Related works\n\nRelated work results:\n\n\n<table>\n  <tr>\n   <td><strong>Model name</strong>\n   </td>\n   <td><strong>Samples seen</strong>\n   </td>\n   <td><strong>Imagenet top1</strong>\n   </td>\n   <td><strong>Mscoco image retrieval at 5</strong>\n   </td>\n   <td><strong>Flickr30k image retrieval at 5</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>Openai B/32\n   </td>\n   <td>12B (32 epochs of WIT)\n   </td>\n   <td>62%\n   </td>\n   <td>\n   </td>\n   <td>\n   </td>\n  </tr>\n  <tr>\n   <td>Openai B/16\n   </td>\n   <td>12B (32 epochs of WIT)\n   </td>\n   <td>69%\n   </td>\n   <td>\n   </td>\n   <td>\n   </td>\n  </tr>\n  <tr>\n   <td>Openai L/14\n   </td>\n   <td>12B (32 epochs of WIT)\n   </td>\n   <td>75.4%\n   </td>\n   <td>61%\n   </td>\n   <td>87%\n   </td>\n  </tr>\n  <tr>\n   <td>ALIGN\n   </td>\n   <td>20B\n   </td>\n   <td>76.4%\n   </td>\n   <td>69.8%\n   </td>\n   <td>93.3%\n   </td>\n  </tr>\n  <tr>\n   <td>BASIC\n   </td>\n   <td>32B\n   </td>\n   <td>85.7%\n   </td>\n   <td>\n   </td>\n   <td>\n   </td>\n  </tr>\n  <tr>\n   <td>CoCa\n   </td>\n   <td>32B\n   </td>\n   <td>86.3%\n   </td>\n   <td>74.2%\n   </td>\n   <td>95.7%\n   </td>\n  </tr>\n</table>\n\n\n[BASIC](https://arxiv.org/abs/2111.10050) and  [ALIGN](https://arxiv.org/abs/2102.05918) got excellent imagenet results. They used either different image encoder architecture (EfficientNet, CoAtNet), a larger network scale (BASIC-L with 2.4B params) or pre trained their network with supervised learning on a large dataset (BASIC CoAtNet vision encoder).\n\n[COCA](https://arxiv.org/abs/2205.01917) additionally used captioning during training and got 86.3% top1.\n\n\n## Scaling up notes\n\nDuring these training runs, we encountered several interesting issues:\n\n\n\n* Using many GPUs means many of them can have hardware issues and can freeze, crash or even just be slow. This is a particularly annoying problem to handle as if one GPU has an issue, the synchronized nature of distributed training means that all GPUs get stuck. I created [https://github.com/rom1504/gpu-tester](https://github.com/rom1504/gpu-tester) to figure out what are the bad GPUs and exclude them\n* Stability issues! When scaling up the model size, the batch size and the dataset size, at around half the training the loss starts increasing until it reaches a plateau. We tried many possible things (find the list [there](https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c/edit)) and eventually concluded on a surprisingly simple solution: **using amp bfloat16 instead of amp float16 made the training fully stable**\n\nAnd also made some discoveries:\n\n\n\n* It seems using a very large batch size (up to 159k) can help reach even higher performance. This is most likely due to the fact that contrastive learning provides information to the loss as a logit matrix, hence having N times more samples in a batch means N square logits. We did not verify this systematically but BASIC paper provides more experiments and a theoretical justification for this result.\n* Itâ€™s possible to get a reasonably performing g/14 CLIP by doing a much shorter cosine decay => getting a 68% g/14 in 10k gpu hours.\n* Grad checkpointing allows to do 10x on the batch size\n\n\n### Training stability issues\n\nStability of training was the main problem we solved in this iteration of the scaling up of OpenCLIP. At around half the training (for L/14, H/14 and g/14), the loss started going up until it plateaued very high (11) and didnâ€™t go down anymore.\n\nWe tried many possible fixes (decreasing lr, gradient shrinking, gradient clipping, cosine attention, post layer norm, â€¦) with little to no effect when trying to resume from before the crash. \n\nEventually only 2 things worked:\n\n\n\n* Finishing the  lr decay very fast : in 8 epochs (compared to the planned 256 epochs). That managed to get most of the performance out of clip H. \n* Switching from float16 to bfloat16 solved the problem while being faster for clip g. We then applied the same fix for clip H and finished its training properly.\n\n[See all the training notes](https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c/edit) with all the details on all the possible ideas that didnâ€™t work.\n\n\n### Training speeds\n\nTo better understand the cost and length of training of clip, we provide these training speed numbers. All numbers assume a100 with 40GB of VRAM. We used gradient checkpointing.\n\n\n<table>\n  <tr>\n   <td>Model\n   </td>\n   <td>Batch size per gpu\n   </td>\n   <td>Precision\n   </td>\n   <td>Number of gpus\n   </td>\n   <td>Sample per second per gpu\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://wandb.ai/rom1504/open-clip/runs/rnxrp6k7?workspace=user-rom1504\">B/32</a>\n   </td>\n   <td>96\n   </td>\n   <td>float16\n   </td>\n   <td>824\n   </td>\n   <td>228\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://wandb.ai/rom1504/open-clip/runs/2zphcgkn?workspace=user-rom1504\">H/14</a>\n   </td>\n   <td>96\n   </td>\n   <td>float16\n   </td>\n   <td>824\n   </td>\n   <td>30\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://wandb.ai/rom1504/open-clip/runs/21cpomx2?workspace=user-rom1504\">g/14</a>\n   </td>\n   <td>40\n   </td>\n   <td>float16\n   </td>\n   <td>800\n   </td>\n   <td>20\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://wandb.ai/rom1504/open-clip/runs/3l7ppqh3?workspace=user-rom1504\">H/14</a>\n   </td>\n   <td>96\n   </td>\n   <td>bfloat16\n   </td>\n   <td>824\n   </td>\n   <td>42\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://wandb.ai/rom1504/open-clip/runs/1pby5fkb?workspace=user-rom1504\">g/14</a>\n   </td>\n   <td>80\n   </td>\n   <td>bfloat16\n   </td>\n   <td>800\n   </td>\n   <td>31\n   </td>\n  </tr>\n</table>\n\n\nThe speed usually increases with batch size per gpu until a plateau is reached. The speed also increases with the number of gpu. After a certain number of gpus, the curve becomes slower than linear.\n\nBfloat16 which we used in the second part of training provides both better stability and faster sample/s for clip models.\n\n\n## Whatâ€™s next\n\nThe models will be used for many applications, including clip guiding and conditioning. Even better results could be reached on models like stable diffusion by using a better clip model!\n\nNow that the scaling properties of clip are proven in an open source reproduction, a lot of doors open. Here are some ideas of next steps:\n\n\n\n* Changing the text encoder to work in the multilingual setting (to get a model like [Multilingual-CLIP](https://github.com/FreddeFrallan/Multilingual-CLIP) but trained contrastively, with hopefully even better results!) and scale it up\n* Can we get clip models while using less gpu hours ? extracting the knowledge from smaller clips into a bigger one may help bootstrap the learning process (see [encoder-distill](https://github.com/iejMac/encoder-distill) from [iejMac](https://github.com/iejMac) getting some preliminary results on this)  \n* The clip idea can be expanded to other modalities, see [CLAP](https://github.com/LAION-AI/CLAP) for text-audio alignment\n\nIf you have ideas or want to help out, feel free to reach out in laion server.\n\n\n## Contributions\n\nThanks to\n\n\n\n* [Romain Beaumont](https://github.com/rom1504) for running the experiments on H/14 and g/14\n* [Ross wightman](https://github.com/rwightman) for conducting all the openclip experiments at JUWELS Booster (Juelich Supercomputing Center) up to L/14 and providing valuable feedback during these H and g clip trainings\n* [Phil Wang](https://github.com/lucidrains) for providing ideas and code (cosine attention, post layer norm, ..) during the stability issues\n* [Boris Dayma](https://github.com/borisdayma) and [Mitchell Wortsman](https://mitchellnw.github.io/) for both proposing to try float32 that showed precision was an issue and eventually lead to trying bfloat16\n* [Blinkdl](https://github.com/Blinkdl) for proposing interesting ideas regarding tuning the learning rate\n* [Christoph Schuhmann](https://github.com/christophschuhmann) for following up on all these experiments, and finding very early that training were frozen, saving some valuable timeuiding Stable Diffusion with CLIP\n* [Jenia Jitsev](https://github.com/JeniaJitsev) for providing ideas and feedback during the training issues, supervision and coordination of the compute grants at JUWELS Booster\n* [Ludwig Schmidt](https://github.com/ludwigschmidt) for reviewing this post and giving many ideas about laion datasets and clip \n* [Mehdi Cherti](https://github.com/mehdidc) for helping to debug the evaluation scripts and getting comparable results for mscoco\n\nAnd of course [Emad](https://twitter.com/EMostaque) (Stability AI) for providing the many GPUs used during these experiments! (g/14 and H/14!)\n\nFor the L/14 training, we gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at JÃ¼lich Supercomputing Centre (JSC), Germany.\n","slug":"large-openclip"},"__N_SSG":true}