{"pageProps":{"posts":[{"slug":"admin_bud-e","frontmatter":{"title":"Admin Bud-E V1.0 ‚Äì Datenschutzfreundliche KI-Assistenz f√ºr Schulen, Universit√§ten & Unternehmen","author":"Christoph Schuhmann, Robert Kaczmarczyk","date":"October 9 2025","previewImg":"/images/blog/admin-bud-e+.jpg"},"content":"----------------------------------------\n\n<img width=\"1024\" height=\"1024\" alt=\"image\" src=\"https://github.com/user-attachments/assets/babd1bd7-cca5-470e-ab5e-335a49585f4d\" />\n\n**LAION** verfolgt seit seiner Gr√ºndung ein klares Ziel: die **Demokratisierung von K√ºnstlicher Intelligenz** und **zug√§ngliche, faire Bildung** f√ºr alle. Mit **[Bud-E 1.0](https://laion.ai/blog/bud-e-release/)** und **[School Bud-E 1.0](https://laion.ai/blog/bud-e-release/)**  haben wir zu Jahresbeginn zwei **browserbasierte Sprachassistenten** bereitgestellt, die konsequent auf **Privatsph√§re**, **Offenheit** und **Datenhoheit** setzen. Gespr√§che bleiben **lokal im Browser**, die Architektur ist **modular**, und der Betrieb ist mit **selbst gehosteten** oder **DSGVO-konformen** APIs m√∂glich.\nMit **Admin Bud-E V1.0** komplettieren wir nun das System: Gemeinsam mit **School Bud-E** entsteht eine **durchg√§ngige L√∂sung** f√ºr Organisationen, die **volle Kontrolle √ºber Speicher- und Verarbeitungsort** ihrer Daten behalten ‚Äì und dabei **einen Gro√üteil der Kosten** im Vergleich zu klassischen, kommerziellen Angeboten einsparen.\n\n## Einleitung\n\n### Was ist Admin Bud-E?\n\n**Admin Bud-E** ist eine **Open-Source-Administrationsschicht** zwischen dem Frontend (zum Beispiel School Bud-E) und den von Ihnen gew√§hlten KI-Anbietern. Die Middleware nimmt Anfragen entgegen, leitet sie an die konfigurierten Dienste weiter, misst die Nutzung (zum Beispiel in Tokens, Zeichen oder Sekunden), bucht die entsprechenden Credits pro Nutzerin oder Nutzer und pro Projekt und erzeugt bei Bedarf Berichte. Das Frontend kann auf Ihrer eigenen Infrastruktur laufen oder von uns bereitgestellt werden. Die Gespr√§chsverl√§ufe verbleiben **ausschlie√ülich im Browser** der Anwenderinnen und Anwender. Auf dem Server wird **nichts dauerhaft gespeichert**.\n\n### Warum jetzt?\n\nViele Schultr√§ger, Hochschulen und Organisationen w√ºnschen sich **volle Datenkontrolle**, **sp√ºrbare Kostenvorteile** gegen√ºber geschlossenen Komplettl√∂sungen und ein **schnelles Onboarding** ganzer Gruppen ohne komplizierte Kontenerstellung. **Admin Bud-E** liefert genau das: Administratorinnen und Administratoren erzeugen **anonyme API-Schl√ºssel** in Sekunden, exportieren diese als **CSV**, verteilen **projektbezogene Budgets** und stellen das **Routing** zu unterschiedlichen Modellen und Anbietern mit wenigen Klicks ein ‚Äì **ohne** dass die Clients ge√§ndert werden m√ºssen.\n\n## Datenschutz & Architektur\n\n* **Lokale Speicherung:** Sch√ºler- und Nutzerdaten bleiben im **Browser**; die Middleware leitet Anfragen **fl√ºchtig im RAM** weiter. Eine **persistente Server-Ablage** personenbezogener Inhalte findet nicht statt.\n* **TLS-gesicherte Verarbeitung:** Bei Cloud-Anbietern (z. B. Vertex, Mistral) erfolgt die √úbertragung **verschl√ºsselt**; es wird **kein Logging** personengebundener Inhalte erzwungen. Ein **DSGVO-konformer** Betrieb ist m√∂glich.\n* **Offen & modular:** Alle Bud-E-Varianten (School, Web, Desktop) basieren auf einer **flexiblen Client-Server-Architektur**. Komponenten wie **ASR**, **LLM**, **TTS** oder **Vision** sind austauschbar und k√∂nnen an Ihre Umgebung angepasst werden.\n\nAuf dieser Basis ‚Äì **lokale Speicherung**, **TLS-gesicherte Verarbeitung** und **offene, modulare Architektur** ‚Äì f√ºgt sich **Admin Bud-E** sauber in die Anforderungen der **Datenschutz-Grundverordnung (DSGVO)** und die **Leitlinien zur Nutzung von KI im Bildungsbereich** (LI Hamburg) ein. Ein zentrales Problem vieler kommerzieller Dienste besteht darin, dass **personenbezogene Daten** auf den Servern der Anbieter **persistiert und weiterverarbeitet** werden. Schulen, Beh√∂rden und √∂ffentliche Tr√§ger nennen genau das regelm√§√üig als Grund f√ºr Zur√ºckhaltung. Unternehmen wiederum sorgen sich, dass **sensible Informationen** oder **Betriebsgeheimnisse** in **Trainingsdaten** gro√üer Anbieter einflie√üen k√∂nnten. **Admin Bud-E** setzt hier einen anderen Schwerpunkt: Verantwortliche behalten **volle Kontrolle** dar√ºber, **wo** Daten **gespeichert und verarbeitet** werden ‚Äì lokal, auf eigener Infrastruktur oder in **eindeutig DSGVO-konfigurierten EU-Cloud-Instanzen**. Das schafft **Vertrauen** und erm√∂glicht den **rechtskonformen** und **verantwortlichen** Einsatz von KI-Assistenz.\n\n## Kernfunktionen in Admin Bud-E\n\n**Users (Nutzerverwaltung):**\nAdministratorinnen und Administratoren legen **Nutzerkonten** an, **aktivieren** sie und vergeben **individuelle Credits** ‚Äì also Nutzungsbudgets ‚Äì pro Person. **API-Schl√ºssel** lassen sich mit einem Klick **erzeugen**, **rotieren** und als **CSV** exportieren. Ein Klick auf eine Tabellenzeile √ºbernimmt die Daten bequem in die Eingabefelder, damit √Ñnderungen **schnell und fehlerarm** erfolgen.\n\n**Projects (Projekte & Budgets):**\nEin **Projekt** fasst eine organisatorische Einheit zusammen ‚Äì etwa eine **Abteilung**, eine **Fakult√§t** oder eine **Schule**. F√ºr jedes Projekt definieren Sie **Allowance-Intervalle** (t√§glich, w√∂chentlich oder monatlich), also wie oft sich pers√∂nliche Budgets automatisch erneuern. Optional aktivieren Sie den **Common Pool**: Nicht verbrauchte Credits von Personen, die wenig nutzen, flie√üen in einen **gemeinsamen Topf** und stehen dann anderen mit h√∂herem Bedarf **innerhalb desselben Projekts** zur Verf√ºgung. So entsteht **Fairness**, w√§hrend die **Gesamtausgaben** des Projekts **nie das definierte Budget √ºberschreiten**. Ein **Wizard** richtet auf Wunsch in einem Durchgang ein neues Projekt **inklusive N Nutzern und Schl√ºsseln** ein. Exportfunktionen (z. B. `user_id, email, api_key`) und **Key-Rotation** sind integriert.\n\n**Pricing (Abrechnung & Einheiten):**\nDie Preislogik ist **klar und einheitlich**. **LLM/VLM-Dienste** werden pro **1 000 000 Tokens** gerechnet, wobei **Eingabe** und **Ausgabe** getrennt erfasst werden. **TTS** wird **pro Zeichen** abgerechnet, **ASR** vorzugsweise **pro Tokens** (Fallback: **pro Zeit**), sodass auch **Whisper-√§hnliche Endpunkte** korrekt erfasst sind. Damit verstehen auch Nicht-Informatikerinnen und -Informatiker, **wo Kosten entstehen** und **welche Stellschrauben** (Antwortl√§nge, TTS-Anteil) sie zur Steuerung haben.\n\n**Providers & Routes (Anbieter & Weiterschaltung):**\nSie hinterlegen **mehrere Anbieter** mit **Name**, **Basis-URL** und **API-Key**. F√ºr jeden Aufgabentyp ‚Äì also **LLM/VLM**, **TTS**, **ASR** ‚Äì definieren Sie **Priorit√§tenketten**. F√§llt ein Dienst aus oder ist ausgelastet, greift automatisch ein **Failover** zur n√§chsten Route. Das Frontend kann einfach auf **‚Äûauto‚Äú** stehen. Ein **Reset-to-defaults** stellt die Standardkonfiguration wieder her. Praktisch l√§sst sich das so denken: Die **Provider** sind die **Stromleitungen** (die Dienste), und die **Routes** bestimmen die **Reihenfolge**, in der diese Leitungen genutzt werden.\n\n**Usage (Nutzung & Exporte):**\nEine **transparente Nutzungs√ºbersicht** unterst√ºtzt **Audit-Anforderungen** und die **Kostenkontrolle**. Bei Bedarf exportieren oder restaurieren Sie den **SQLite-Datenstand** per Klick. Ein **Hard-Reset** ist m√∂glich, wenn eine Instanz vollst√§ndig neu aufgesetzt werden soll.\n\n<img width=\"1374\" height=\"588\" alt=\"image\" src=\"https://github.com/user-attachments/assets/6849a076-eead-4ed7-a4be-116436a1f950\" />\n\n## Kosten ‚Äì fair & planbar\n\nWenn eine Nachricht durch Bud-E l√§uft, passiert immer dasselbe in drei Schritten: Zuerst verwandelt die **Spracherkennung (ASR)** Gesprochenes in Text. Dann ‚Äûdenkt‚Äú das **Sprachmodell (LLM)** nach und formuliert eine Antwort. Auf Wunsch liest die **Vorlesestimme (TTS)** die Antwort h√∂rbar vor. Die Abrechnung erfolgt je nach Teilkomponente unterschiedlich. Das **LLM** rechnet in **Tokens** ab ‚Äì das sind kleine Textst√ºcke, in die W√∂rter zerlegt werden. Als Faustregel gilt: **Ein deutsches Wort entspricht im Schnitt 1‚Äì2 Tokens**. Eine Antwort mit etwa **250 Tokens** umfasst grob **150‚Äì200 W√∂rter**, **500 Tokens** entsprechen ungef√§hr **300‚Äì400 W√∂rtern**. **TTS** rechnet dagegen **Zeichen** (also Buchstaben). Dadurch wird **TTS** bei l√§ngeren Antworten schnell der **gr√∂√üte Kostenblock**, w√§hrend **ASR** sehr g√ºnstig bleibt und **LLM** moderat ist. Deshalb ist im **Bud-E-Frontend** die **Sprachausgabe standardm√§√üig ausgeschaltet**. Im Unterricht, bei Recherche oder beim Verfassen von E-Mails ist **stilles Lesen** meist schneller und praktischer. F√ºr **kurze Hinweise**, **freundliche Best√§rkungen** oder **lockeres Plaudern** l√§sst sich **TTS gezielt manuell einschalten**.\nZur **Modellwahl**: Der Betrieb ist **beispielsweise** mit Modellen wie **‚ÄûGemini Flash‚Äú** und **‚ÄûGemini Pro‚Äú** √ºber die **Google Cloud (Vertex-Plattform)** m√∂glich. Die **Abrechnung** l√§uft dann **zentral √ºber Ihren Google-Cloud-Account**. In den Einstellungen kann festgelegt werden, dass **s√§mtliche Anfragen ausschlie√ülich an EU-Regionen** gesendet werden und dass **keine Daten zu Trainingszwecken gespeichert** werden. Die Daten werden **nur kurzfristig** f√ºr die eigentliche **Inferenz** im Arbeitsspeicher gehalten. Alternativ k√∂nnen Sie **andere KI-Service-Provider** nutzen ‚Äì etwa **Microsoft Azure**, **Mistral** ‚Äì oder **eigene Modelle selbst hosten**. **Admin Bud-E** gibt Ihnen die Wahl und die Steuerbarkeit.\n\n**Konkrete Gr√∂√üenordnung (Beispielrechnung mit Gemini + Chirp 3 HD):** Eine **kurze Antwort (~250 Tokens)** kostet **mit Vorlesen** etwa **$0.030‚Äì$0.034**; **ohne Vorlesen** fallen nur die kleinen **ASR-/LLM-Anteile** an (**‚âà $0.00019‚Äì$0.00351**, je nach Modellstufe). Auf **1 000 Nachrichten** pro Person umgerechnet hei√üt das: **ohne TTS** insgesamt **‚âà $0.19 (Flash-Lite)** bis **‚âà $3.51 (Pro)**; **mit TTS nur in 25 % der F√§lle** liegen die Gesamtkosten bei **‚âà $7.70‚Äì$11.00**. Geht man davon aus, dass eine durchschnittliche Person **~1 000 Nachrichten pro Jahr** versendet, sind die j√§hrlichen Kosten **deutlich geringer** als bei vielen Abo-Modellen **ohne** TTS (typisch **10‚Äì15 ‚Ç¨ pro Monat**). Die wichtigsten Stellschrauben bleiben **Antwortl√§nge** (Tokens) und **TTS-Anteil** (Zeichen) ‚Äì beides ist in **Admin Bud-E** **gezielt steuerbar**.\n\n## Skalierung & Betrieb\n\n**Admin Bud-E** w√§chst mit Ihren Bed√ºrfnissen ‚Äì vom **Pilot mit zehn** Nutzenden bis zur **Instanz mit zehntausend**. Die Middleware bleibt **schlank**, weil sie Anfragen √ºberwiegend **durchreicht**. Es sind **keine schweren Zusatzdienste** und **keine komplizierten Serverfarmen** notwendig. H√§ufig gen√ºgt ein **einzelner, kosteng√ºnstiger Server**. Starten Sie beispielsweise auf einem **VPS in Deutschland** (z. B. Hetzner), profitieren Sie von **EU-Standort**, **geringer Latenz** und **klarem Datenschutz**. Ihre **Datenbank** l√§sst sich **per Klick exportieren**, sodass Sie St√§nde sichern oder √ºbertragen k√∂nnen. **Modelle und Anbieter** wechseln Sie komfortabel in der **Admin-Oberfl√§che**, **ohne** dass an den Clients etwas ge√§ndert werden muss. Auf diese Weise behalten Sie **technische und finanzielle Beweglichkeit**, auch wenn Anforderungen wachsen oder sich √§ndern.\n\n## Praxis: So l√§uft‚Äôs im Alltag\n\nDer Einstieg ist **unkompliziert**. Die IT richtet **Admin Bud-E** auf einem gemieteten Cloud-Server ein (in der Regel ein einmaliger Aufwand von etwa einem Nachmittag). Anschlie√üend erzeugt sie mit wenigen Klicks **anonyme API-Schl√ºssel** und verteilt diese als **CSV** an die Lehrkr√§fte oder Mitarbeitenden ‚Äì **ohne** dass pers√∂nliche Konten angelegt werden m√ºssen. Nutzende √∂ffnen die **Einstellungen**, f√ºgen ihren Schl√ºssel ein, laden **PDFs oder Bilder** hoch und erhalten **sofort Antworten** ‚Äì wahlweise still lesbar oder, wenn gew√ºnscht, **vorgelesen**. √úber **Allowance** (ein pers√∂nliches, automatisch erneuerbares Guthaben) und den **Common Pool** (ein gemeinsamer Topf im Projekt) bleibt die Nutzung **fair**: Wer wenig braucht, verschenkt ungenutzte Credits nicht, sondern h√§lt sie **im Projekt nutzbar**; wer viel arbeitet, hat **genug Spielraum**, ohne dass das **Gesamtbudget** √ºberschritten wird. **Obergrenzen** stellen Organisationen selbst ein, damit die Kosten **stabil** bleiben. So wird KI im Unterricht und Arbeitsalltag **planbar, leise, wirksam**.\n\n<img width=\"1484\" height=\"851\" alt=\"image\" src=\"https://github.com/user-attachments/assets/e2b2044c-ed0e-4dc8-a32e-0e88eb93d98e\" />\n\n## Ressourcen & Links\n\n* **School Bud-E Frontend (Live):** [https://school.bud-e.ai/](https://school.bud-e.ai/)\n* **Admin Bud-E Repository (GitHub):** [https://github.com/LAION-AI/Admin_Bud-E](https://github.com/LAION-AI/Admin_Bud-E)\n* **School Bud-E Frontend Repository (GitHub):** [https://github.com/LAION-AI/school-bud-e-frontend](https://github.com/LAION-AI/school-bud-e-frontend)\n\n## Mitmachen\n\n<img width=\"1024\" height=\"1024\" alt=\"image\" src=\"https://github.com/user-attachments/assets/126d983a-febd-4afb-9808-ba02f4953631\" />\n\nWir laden **Schulen, Hochschulen, Unternehmen und √∂ffentliche Einrichtungen** ein, **Admin Bud-E** in der Praxis zu erproben, zu evaluieren und gemeinsam weiterzuentwickeln. Wir bieten **regelm√§√üig kostenlose Webinare** zum Einrichten von **Admin Bud-E** und **School Bud-E** f√ºr die eigene Organisation auf eigenen Servern an. Bei Interesse schreiben Sie uns bitte an **[contact@laion.ai](mailto:contact@laion.ai)**. Gemeinsam bringen wir **faire, offene und empathische KI-Assistenz** unter **Ihrer** Kontrolle in den Alltag.\n","date":1759968000000},{"slug":"leo-lm","frontmatter":{"title":"LeoLM: Ein Impuls f√ºr Deutschsprachige LLM-Forschung","author":"Bj√∂rn Pl√ºster","date":"September 28 2023","previewImg":"/images/blog/leolm-banner.jpg"},"content":"\nLernen Sie LeoLM kennen, das erste offen und kommerziell verf√ºgbare deutsche Foundation Language Model, das auf Llama-2 basiert.\nUnsere Modelle erweitern die F√§higkeiten von Llama-2 durch ein fortgesetztes Training auf einem gro√üen Korpus von hochwertigen deutschen und gr√∂√ütenteils lokal spezifischen Texten.\nDank eines Compute-Grants auf dem neuen Supercomputer **42** von [HessianAI](https://hessian.ai/) ver√∂ffentlichen wir zwei Foundation-Modelle, die mit einer Kontextl√§nge von 8k trainiert wurden,\n[`LeoLM/leo-hessianai-7b`](https://huggingface.co/LeoLM/leo-hessianai-7b) und [`LeoLM/leo-hessianai-13b`](https://huggingface.co/LeoLM/leo-hessianai-13b) (70b folgt auch bald! üëÄ) unter der [Llama-2 Community-Lizenz](https://ai.meta.com/llama/license/). Zus√§tzlich konstruieren wir einen Evaluierungssatz f√ºr Benchmarks zur √úberpr√ºfung der F√§higkeiten deutscher Sprachmodelle, um den Modellvergleich zu standardisieren, √§hnlich zu den weit verbreiteten auf Englisch basierten Evaluierungen, wie sie beispielsweise von [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) oder [LLM-Foundry](https://github.com/mosaicml/llm-foundry) bereitgestellt werden.\nMit dieser Ver√∂ffentlichung sind LAION und Hessian.AI bereit, die deutsche Open-Source und kommerzielle LLM-Forschung erheblich zu verbessern und so neue M√∂glichkeiten zu f√∂rdern und die weite Verbreitung zu beschleunigen.\n\nProbieren Sie [**LeoLM/leo-hessianai-7b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-7b-chat) und [**LeoLM/leo-hessianai-13b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-13b-chat) auf HuggingFace Spaces aus!\n\n*[[Auf Englisch lesen]](/blog/leo-lm)*\n\n## Einleitung\n\nSeit der Ver√∂ffentlichung der urspr√ºnglichen Llama Foundation Models <sup>1</sup> im Januar 2023 hat die Open-Source und wissenschaftliche Forschungsgemeinschaft\neine rasante Beschleunigung in der Entwicklung von immer f√§higeren Sprachmodellen erlebt. Die Fortschritte\nder letzten Wochen haben die leistungsf√§higsten Llama-2 <sup>2</sup> basierten Modelle n√§her an die Konkurrenz zu OpenAI's ChatGPT auf Basis von GPT-3.5 oder sogar dem st√§rkeren GPT4 gebracht.\nDennoch besteht eine bemerkenswerte Einschr√§nkung fort: Die Mehrheit dieser bahnbrechenden Fortschritte bleibt auf den Bereich der englischen Sprache beschr√§nkt.\nDiese Einschr√§nkung resultiert haupts√§chlich daraus, dass gro√üe Open-Source-Modelle √ºberwiegend auf monolingualen englischen Daten trainiert wurden. Obwohl es einige\nForschungen zum Zweitsprachen- oder Mehrsprachen-Finetuning gibt, sind die meisten resultierenden Modelle in ihren F√§higkeiten beschr√§nkt und leiden unter grammatikalischen Schw√§chen und der US-zentrischen Voreingenommenheit, die den englischen Daten inh√§rent ist.\n\nWir wollen diese Probleme im Fallbeispiel der deutschen Sprache durch die Anwendung vieler heutiger Spitzentechniken l√∂sen, um ein wirklich leistungsf√§higes,\nlokales und zweisprachiges LLM zu entwickeln.\nZu diesem Zweck pr√§sentieren wir LeoLM (**L**inguistisch **E**rweitertes **O**ffenes **L**anguage **M**odel), eine Suite von auf Llama-2 basierenden deutschen Foundation-\nModellen und eine Auswahl begleitender Feinabstimmungen.\nDes Weiteren pr√§sentieren wir GermanBench, eine Sammlung der relevantesten ins Deutsche √ºbersetzten englischen Benchmarks, die es uns erm√∂glichen in √§hnlichem Ausma√ü wie im Englischen, die F√§higkeiten von LeoLM gr√ºndlich zu bewerten.\n\n<sup>1</sup>: [Touvron et al. 2023a](https://arxiv.org/abs/2302.13971)\n<sup>2</sup>: [Touvron et al. 2023b](https://arxiv.org/abs/2307.09288)\n\n## Vorverarbeitung in Phase 2\n\nLlama-2-Modelle werden auf 2 Billionen Tokens √ºberwiegend englischen Textes vortrainiert. Um ihre Kompetenz in der deutschen Sprache zu erh√∂hen, verwenden wir ein fortgesetztes Vortraining, welches wir als \"Stage 2 Pretraining\" bezeichnen.\nWir initialisieren LeoLM mit Llama-2-Gewichten und setzen das Training des Modells auf einem gro√üen deutschen Textkorpus von 65 Milliarden Tokens fort, die rigoros gefiltert und dedupliziert wurden und gr√∂√ütenteils aus dem [OSCAR-2301-Korpus](https://huggingface.co/datasets/oscar-corpus/OSCAR-2301) stammen.\nEin wesentlicher Aspekt dieses Ansatzes besteht darin, das Vergessen oder den Verlust von zuvor erlerntem Wissen oder F√§higkeiten zu minimieren. Wir folgen den Erkenntnissen von [Gupta et al. (2023)](https://arxiv.org/abs/2308.04014) in unserer Wahl der Hyperparameter, um das Risiko des Vergessens zu minimieren.\nZus√§tzlich folgen wir der Arbeit von [Together](https://huggingface.co/togethercomputer/LLaMA-2-7B-32K) bei der Anwendung von [linearer RoPE-Skalierung](https://kaiokendev.github.io/til#extending-context-to-8k) und [Flash Attention 2](https://tridao.me/publications/flash2/flash2.pdf), um die Trainingseffizienz zu verbessern und die Kontextl√§nge auf 8k Tokens zu verdoppeln.\nSiehe Abbildung 1 f√ºr einen √úberblick √ºber alle Training-Hyperparameter.\n\n![training_parameters](/images/blog/training_params.png \"Training-Hyperparameters\")\n\n## Feinabstimmungsdatens√§tze\n\nEs gibt viel Diskussion dar√ºber, was ein guter Chat/Instruktionstuning-Datensatz bieten muss, was zur Entwicklung einer Vielzahl verschiedener, erfolgreicher Ans√§tze gef√ºhrt hat. Wir lassen uns von dieser Vielfalt inspirieren und √ºbersetzen, um √§hnliche F√§higkeiten auf Deutsch zu bringen, eine Auswahl hochwertiger Instruktionsdatens√§tze ins Deutsche mit OpenAI's `gpt-3.5-turbo` API. Die Verwendung von `gpt-3.5-turbo` stellt sicher, dass der Zusammenhang zwischen Aufforderungen und Antworten intakt bleibt und dass komplexe Anweisungen, die m√∂glicherweise Code, Gleichungen oder formatierte Daten enthalten, korrekt √ºbersetzt werden.\nAufbauend auf den Erkenntnissen der Community, w√§hlen wir eine Vielzahl von Datens√§tzen aus, die wir √ºbersetzen und f√ºr das Training unseres Chat-Modells verwenden.\nDie √ºbersetzten Datens√§tze sind:\n\n- [OpenPlatypus](https://huggingface.co/datasets/garage-bAInd/Open-Platypus) -> [OpenSchnabeltier](https://huggingface.co/datasets/LeoLM/OpenSchnabeltier)\n- [OpenAssistant OASST1](https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25) -> [OpenAssistant-DE](https://huggingface.co/datasets/OpenAssistant/OASST-DE)\n\nAu√üerdem verwenden wir [`FreedomIntelligence/evol-instruct-deutsch`](https://huggingface.co/datasets/FreedomIntelligence/evol-instruct-deutsch) und [`FreedomIntelligence/alpaca-gpt4-deutsch`](https://huggingface.co/datasets/FreedomIntelligence/alpaca-gpt4-deutsch) aus dem [MultilingualSIFT](https://github.com/FreedomIntelligence/MultilingualSIFT)-Projekt. Vielen Dank an die Autoren, dass sie ihre Daten geteilt haben!\nUm die zweisprachige Nutzung zu erleichtern, trainieren wir auch Modelle auf einer Kombination dieser √ºbersetzten Datens√§tze und ihren urspr√ºnglichen, englischen Gegenst√ºcken.\n\nSchlie√ülich erstellen wir, um Schw√§chen beim kreativen Schreiben und Reimen, die bei fr√ºhen Tests festgestellt wurden, auszugleichen, zwei weitere Datens√§tze:\n\n- [GPT4 Gedichte](https://huggingface.co/datasets/LeoLM/German_Poems): Eine Sammlung deutscher Gedichte zu verschiedenen Themen, geschrieben von GPT4\n- [GPT4 Lieder](https://huggingface.co/datasets/LeoLM/German_Songs): Eine Sammlung deutscher Lieder und nachfolgende Analysen, geschrieben von GPT4.\n\n## Evaluation und Ergebnisse\n\nDie Evaluierung der F√§higkeiten von LLMs, insbesondere von Chat-Modellen, ist komplex und die besten Methoden sind noch umstritten. Benchmarks, die auf Multiple-Choice basieren und anhand der Protokoll-Wahrscheinlichkeiten des Modells ausgewertet werden (wie im [Open LLM Leaderboard]()), sind eine derzeit beliebte Methode. Eine andere Methode bewertet Antworten automatisch mit GPT4, wie bei AlpacaEval oder MT-Bench. Dieser Ansatz richtet sich eher an Chat-Modelle, da er die Qualit√§t von Modellantworten in realen Aufgaben ber√ºcksichtigt. Um so vergleichbar wie m√∂glich zu sein, √ºbersetzen wir eine Reihe von englischen Benchmarks direkt ins Deutsche. Wir ver√∂ffentlichen diese Datens√§tze in unserer [HF-Organisation](https://huggingface.co/LeoLM) und mit ausf√ºhrlicher Dokumentation [auf GitHub](https://github.com/bjoernpl/GermanBenchmark), und Sie finden den entsprechende `lm-evaluation-harness`-Branch [hier](https://github.com/bjoernpl/lm-evaluation-harness-de/tree/mmlu_de) und den `FastEval`-Branch [hier](https://github.com/bjoernpl/FastEval).\n\nIn Abbildung 3 k√∂nnen Sie einen Vergleich von LeoLM gegen√ºber den Basis-Llama-2-Modellen auf einer Auswahl von Benchmarks mit sowohl der englischen Version (blau) als auch unserer √ºbersetzten Version (gr√ºn) sehen. Unser Trainging verbessert die Benchmark-Ergebnisse f√ºr die deutschen Aufgaben, w√§hrend die Ergebnisse f√ºr die englischen Aufgaben leicht reduziert werden. Bemerkenswert ist, dass der durchschnittliche Anstieg der deutschen Benchmark-Ergebnisse die durchschnittliche Abnahme der Leistung auf den englischen Benchmarks deutlich √ºberwiegt, was zeigt, dass unser Ansatz das Erlernen einer neuen Sprache erm√∂glicht, ohne zu vergessen, was zuvor gelernt wurde. Warum die Ergebnisse in Deutsch niedriger bleiben als in Englisch, ist eine offene Frage, kann aber zum Teil auf eine Qualit√§tsminderung bei der √úbersetzung zur√ºckgef√ºhrt werden.\n\n![](/images/blog/benchmarks.png)\n\nDie folgende Tabelle zeigt die Ergebnisse auf unserer √ºbersetzten Version von MT-Bench. MT-Bench ist ein Benchmark, der die Multi-Turn-Leistung auf einem kuratierten Satz von 80 Fragen aus mehreren Kategorien mit GPT-4 als Richter bewertet. Dabei bewertet GPT-4 die Aufforderungen auf einer Skala von 1-10 hinsichtlich der wahrgenommenen Hilfsbereitschaft, Relevanz, Genauigkeit, Tiefe, Kreativit√§t und Detailliertheit der Antwort. Das monolinguale Modell `leo-hessianai-13b-chat` schneidet insgesamt am besten ab und kommt sogar dem GPT-3.5 im Thema \"Geisteswissenschaften\" nahe. Es erzielt auffallend schlechte Ergebnisse in Mathematik und Codierung, was zu erwarten ist, da die Llama-2-Modelle in diesem Bereich ohne sehr explizites Finetuning von vornherein Schw√§chen aufweisen. Die zweisprachigen Modelle erzielen in einigen Kategorien wie Mathematik und Logik leicht unter ihren monolingualen Gegenst√ºcken, w√§hrend sie in Codierung und Extraktion √ºbertreffen.\n\n![](/images/blog/mt_bench.png)\nF√ºr eine detailliertere Evaluierung, bleiben Sie dran f√ºr unser Paper!\n\n## Qualitative Ergebnisse\n\nBenchmarks neigen dazu, ziemlich abstrakt zu sein. Um ein besseres Gef√ºhl f√ºr LeoLM's zu bekommen, schauen Sie sich unsere Demos an und probieren Sie es selbst aus: [**LeoLM/leo-hessianai-7b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-7b-chat) und den gr√∂√üeren Bruder [**LeoLM/leo-hessianai-13b-chat`**](https://huggingface.co/spaces/LeoLM/leo-hessianai-13b-chat). Alternativ k√∂nnen Sie das Modell selbst mit ü§óTransformers ausf√ºhren. Weitere Informationen zur Einrichtung finden Sie auf der [Modellkarte](https://huggingface.co/LeoLM/leo-hessianai-13b-chat).\n\n## Fazit\n\nUnsere Forschung hat mehrere Schl√ºsselbeitr√§ge:\n\n- Wir ver√∂ffentlichen eine Suite von deutschen Foundation-Sprachmodellen unter einer offenen Lizenz.\n- Wir √ºbertragen einen gr√ºndlichen und vielseitigen Evaluierungsansatz f√ºr Basis- und Chat-Modelle ins Deutsche.\n- Wir zeigen, dass eine gro√ü angelegte Fortbildung auch f√ºr datenges√§ttigte Modelle wie Llama-2 ohne signifikantes Vergessen oder Verlust von fr√ºheren F√§higkeiten m√∂glich ist.\n- Wir pr√§sentieren eine vielf√§ltige Suite von Instruktions-/Chat-Tuning-Datens√§tzen, die vom Englischen ins Deutsche √ºbersetzt wurden, um als Basis f√ºr die deutsche Open-Source-LLM-Forschungsgemeinschaft zu dienen.\n\nInsgesamt ist die LeoLM-Modellsuite ein Proof-of-Concept f√ºr den Spracherwerb f√ºr vortrainierte Modelle. Dar√ºber hinaus pr√§sentiert sie sich als das erste offen verf√ºgbare deutsche Foundation-Modell, das den heutigen Standards entspricht. Wir bei LAION hoffen, die deutsche Open-Source-Forschungsgemeinschaft ansto√üen zu k√∂nnen, um die Abh√§ngigkeit von geschlossenen kommerziellen Quellen wie OpenAI zu verringern. Viel Spa√ü mit LeoLM!\n\n## Danksagungen\n\nDieses Projekt wurde von Bj√∂rn Pl√ºster (Universit√§t Hamburg) geleitet mit Unterst√ºtzung, Anleitung und\nSupervision von Christoph Schuhmann (LAION), Patrick Schramowski (LAION, Hessian AI, DFKI) und Jenia Jitsev (LAION, Juelich Supercomputing Center, Helmholtz Research Center Juelich).\nWir danken den Leuten bei hessian.AI, insbesondere Wolfgang Stille, Kristian Kersting, Florian Kieser und Patrick Diem\nf√ºr ihre gro√üz√ºgige Rechenzeitvergabe √ºber das Hessian AI Innovation Lab (gef√∂rdet vom Hessischen Ministerium f√ºr Digitale Strategie und Entwicklung) und das AI Service Center hessian.AISC (gef√∂rdert vom Bundesministerium f√ºr Bildung und Forschung (BMBF)), und Unterst√ºtzung w√§hrend des gesamten Trainingsprozesses!  Vielen Dank auch an Jae Hee Lee von der Universit√§t Hamburg, Andreas K√∂pf von Open-Assistant und Community-Mitglieder bei LAION f√ºr ihr hilfreiches Feedback. Weiterhin danken wir dem hessian.AI's Innovationslabor, dem AI Servicezentrum hessian.AISC und dem Deutschen Zentrum f√ºr K√ºnstliche Intelligenz (DFKI) f√ºr ihre Unterst√ºtzung.\n\nAu√üerdem danken wir den Gruppen, die die Datens√§tze f√ºr unser Chat-finetuning urspr√ºnglich erstellt haben: [OpenAssistant](https://huggingface.co/OpenAssistant), [FreedomIntelligence](https://huggingface.co/FreedomIntelligence), [garage-bAInd](https://huggingface.co/garage-bAInd), [WizardLM](https://huggingface.co/WizardLM) and [Instruction-Tuning-with-GPT-4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM).\n\n---\n\n### An Example\n\n**User:**  Warum ist open-source besonders in der KI-Forschung wichtig?\n\n**LeoLM:**\nOpen-Source spielt eine wichtige Rolle in der KI-Forschung, da sie Zusammenarbeit, Innovation und Zugang zur Technologie f√∂rdert. Hier sind einige Gr√ºnde, warum Open-Source in der KI-Forschung besonders wichtig ist:\n\n- Zusammenarbeit: Durch Open-Source k√∂nnen Forscher gemeinsam an Projekten arbeiten, Ideen austauschen und voneinander lernen. Dies f√∂rdert die Zusammenarbeit zwischen Forschern und kann zu schnelleren und effizienteren Fortschritten in der KI-Forschung f√ºhren.\n\n- Transparenz: Open-Source erm√∂glicht Transparenz und Reproduzierbarkeit in der KI-Forschung. Forscher k√∂nnen den Code und die Methoden anderer sehen, reproduzieren und darauf aufbauen, was die Validierung von Ergebnissen und die Identifizierung potenzieller Schwachstellen erleichtert.\n\n- Zugang zur Technologie: Open-Source macht KI-Technologien und -Werkzeuge einem breiteren Publikum zug√§nglich. Dies erm√∂glicht kleineren Forschungseinrichtungen und Einzelpersonen den Zugang zu KI-Werkzeugen, die ihnen sonst m√∂glicherweise nicht zur Verf√ºgung st√ºnden. Dadurch kann Innovation und Forschung auch au√üerhalb traditioneller Zentren gef√∂rdert werden.\n\n- Innovation: Open-Source f√∂rdert Innovation in der KI-Forschung, indem sie Raum f√ºr Experimente und neue Ideen bietet. Da der Code offen und f√ºr alle zug√§nglich ist, k√∂nnen Forscher ihn an ihre spezifischen Bed√ºrfnisse anpassen und neue KI-Werkzeuge und -Technologien entwickeln.\n\n- Lernen: Open-Source erm√∂glicht es Forschern, voneinander zu lernen. Durch die gemeinsame Nutzung von Code und Methoden k√∂nnen Forscher aus den Erfahrungen anderer lernen und ihre eigenen Forschungsprojekte verbessern.\n\nZusammenfassend ist Open-Source ein wichtiger Aspekt der KI-Forschung, da er Zusammenarbeit, Transparenz, Zugang zur Technologie, Innovation und Lernen f√∂rdert. Dadurch wird die KI-Forschung effizienter, innovativer und zug√§nglicher f√ºr eine breitere Palette von Forschern und Institutionen\n\n| [Link zu einer PDF mit mehr Beispielen](/documents/leo-lm.pdf) |\n|----------|\n","date":1695859200000}]},"__N_SSG":true}