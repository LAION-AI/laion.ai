{"pageProps":{"posts":[{"slug":"petition","frontmatter":{"title":"Petition for keeping up the progress tempo on AI research while securing its transparency and safety.","author":"LAION.ai","date":"March 29 2023","previewImg":"/images/blog/laion-black.png"},"content":"\n[LINK TO OUR PETITION](https://www.openpetition.eu/petition/online/securing-our-digital-future-a-cern-for-open-source-large-scale-ai-research-and-its-safety)\n\n## Securing Our Digital Future: Calling for CERN like international organization to transparently coordinate and progress on large-scale AI research and its safety\n\nIn an era of unparalleled technological advancements, humanity stands on the precipice of a new epoch characterized by the profound influence of artificial intelligence (AI) and its foundational models, such as GPT-4. The potential applications of these technologies are vast, spanning scientific research, education, governance, and small and medium-sized enterprises. To harness their full potential as tools for societal betterment, it is vital to democratize research on and access to them, lest we face severe repercussions for our collective future.\n\n### Dominance of few large corporations in AI development\n\nIncreasingly, we are witnessing the emergence of a system wherein educational institutions, government agencies, and entire nations become dependent on the AI technology of a select few large corporations that operate with little transparency or public accountability. To secure our society's technological independence, foster innovation, and safeguard the democratic principles that underpin our way of life, we must act now.\nWe call upon the global community, particularly the European Union, the United States, the United Kingdom, Canada, Australia and other willing countries, to collaborate on a monumental initiative: the establishment of an international, publicly funded, open-source supercomputing research facility. This facility, analogous to the CERN project in scale and impact, should house a diverse array of machines equipped with at least 100,000 high-performance state-of-the-art accelerators (GPUs or ASICs), operated by experts from the machine learning and supercomputing research community and overseen by democratically elected institutions in the participating nations.\n\n### Multimodal future\n\nThis ambitious endeavor will provide a platform for researchers and institutions worldwide to access and refine advanced AI models, such as GPT-4, harnessing their capabilities for the greater good. By making these models open source and incorporating multimodal data (audio, video, text, and program code), we can significantly enrich academic research, enhance transparency, and ensure data security. Furthermore, granting researchers access to the underlying training data will enable them to understand precisely what these models learn and how they function, an impossibility when restricted by APIs.\nAdditionally, the open-source nature of this project will promote safety and security research, allowing potential risks to be identified and addressed more rapidly and transparently by the academic community and open-source enthusiasts. This is a vital step in ensuring the safety and reliability of AI technologies as they become increasingly integrated into our lives.\nThe proposed facility should feature AI Safety research labs with well-defined security levels, akin to those used in biological research labs, where high-risk developments can be conducted by internationally renowned experts in the field, backed by regulations from democratic institutions. The results of such safety research should be transparent and available for the research community and society at large. These AI Safety research labs should be capable of designing timely countermeasures by studying developments that, according to broad scientific consensus, would predictably have a significant negative impact on our societies.\n\n### Economic impact\n\nEconomically, this initiative will bring substantial benefits to small and medium-sized companies worldwide. By providing access to large foundation models, businesses can fine-tune these models for their specific use cases while retaining full control over the weights and data. This approach will also appeal to government institutions seeking transparency and control over AI applications in their operations.\nThe importance of this endeavor cannot be overstated. We must act swiftly to secure the independence of academia and government institutions from the technological monopoly of large corporations in AI research. Technologies like GPT-4 are too powerful and significant to be exclusively controlled by a select few.\nIn a world where machine learning expertise and resources for AI development become increasingly concentrated in large corporations, it is imperative that smaller enterprises, academic institutions, municipal administrations, and social organizations, as well as nation-states, assert their autonomy and refrain from relying solely on the benevolence of these powerful entities that are often driven by short-term profit interests and act without properly taking democratic institutions into their decision-making loop. We must take immediate and decisive action to secure the technological independence of our society, nurturing innovation while ensuring the safety of these developments and protecting the democratic principles that form the foundation of our way of life.\n\n### Safety measures\n\nThe [recent proposition](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) of decelerating AI research as a means to ensure safety and progress presents an understandable but untenable  approach that will be detrimental to both objectives. Corporate or state actors will make advancements in the dark while simultaneously curtailing the public research community's ability to scrutinize the safety aspects of advanced AI systems thoroughly. Rather than impeding the momentum of public AI development, a more judicious and efficacious approach would be to foster a better-organized, transparent, safety-aware, and collaborative research environment. The establishment of transparent open-source AI safety labs tied to the international large-scale AI research facility as described above, which employ eligible AI safety experts, have corresponding publicly funded compute resources, and act according to regulations issued by democratic institutions, will cover the safety aspect without dampening progress. By embracing this cooperative framework, we can simultaneously ensure progress and the responsible development of AI technology, safeguarding the well-being of our society and the integrity of democratic values.\n\n### What you can do\n\nWe urge you to join us in this crucial campaign. Sign [this petition](https://www.openpetition.eu/petition/online/securing-our-digital-future-a-cern-for-open-source-large-scale-ai-research-and-its-safety) and make your voice heard. Our collective digital future, the autonomy of our academic research, and the equilibrium of our global economy depend on our ability to act quickly and decisively.\nTogether, we can build a future where advanced AI technologies are accessible to all, and where innovation and progress are not constrained by the boundaries of a few powerful corporations. Let us seize this opportunity and build a brighter future for generations to come.\n","date":1680048000000},{"slug":"general-gpt","frontmatter":{"title":"General-GPT: Breaking the Modality Constraint","author":"Shivaen Ramshetty and Christoph Schuhmann","date":"March 28 2023","previewImg":"/images/blog/general-gpt-logo.png"},"content":"## Introduction\n\nWith the rapid explosion of large language models and utilization of their encompassing applications, most notably [ChatGPT](https://openai.com/blog/chatgpt), there is a clear promise of more capable and useful AI models/systems. Often, such models are compared to us as humans using the Turing test or their performance on tasks relative to humans. As of recent, these models have even achieved incredible success on tests designed for humans such as the LSAT. However, the limited means by which one can interact with such systems  elucidates a variety of opportunities for exploration and possibly discovery. We ask whether modalities can be mixed and learnt alongside one another, and whether that environment of learning offers new avenues for understanding.\n\nWith this in mind, we are excited to introduce a relatively new project at [LAION](https://laion.ai/) called General-GPT.\n\n\n## Goals\n\nIn an effort to keep this concise, we enumerate our goals as follows:\n\n1. Explore the ability to directly intertwine any modality into large language models (LLMs), such that expression of ideas and responses can be more natural and informative.\n2. Allow longer contexts by inputting embedded sequences rather than operating directly on the sequences themselves. Though we may lose fine-grained details of the original sequences, it may prove useful for higher-level tasks.\n3. Provide open-source tools, methods, and models that we hope extend our bigger picture goal of \"democratizing AI.\"\n\n\n## Experiments\n\n### Text-Image Expression\nCurrently, our efforts have been primarily centered around experimenting with whether or not we can format our first goal into a trainable and functioning model. In order to do so, we first simplified the problem in a three ways. First, we choose to focus on tackling only the text-image domain rather than the full gamut that we hope to include. Secondly, we format the problem as a straightforward mapping from $x \\rightarrow y$ or $y \\rightarrow x$. Where $x$ represents an image embedding and $y$ represents the accompanying text. Finally, we tune on just the [MS-COCO](https://cocodataset.org/#home) [1] 2017 training set of 591753 image-caption pairs.\n\nTo construct $x$ we utilize [CLIP](https://openai.com/research/clip) [2], specifically CLIP *ViT-L/14*, to encode the images. On the other hand, we utilize [GPT-2](https://huggingface.co/gpt2) [3] as our LLM that receives mixed inputs and grounds for multimodal understanding or expression. The choice of these two models as baselines comes from their relatively reasonable scale, existing work and research, and the common dimensionality of their encodings. \n\n#### Image Captioning: $x \\rightarrow y$ \nFor this task, we introduce two specific tokens into the vocab so that the model may recognize when an embedding is being input and what that embedding is. Intuitively, the first token (\"[CLIP IN]\") should signal that there is an image embedding before the second token (\"[\\CLIP IN]\"). Therefore, the training data for this task is structured as follows:\n\n*<center>[CLIP IN] **embedding** [\\CLIP IN] Caption: [MS-COCO caption ...].</center>*\n\nIn regards to training itself, we follow [CLIP prefix captioning](https://github.com/rmokady/CLIP_prefix_caption) [4] and simply insert the image embedding as a new token in between our two new tokens. Then, we introduce a dummy token as our target token at the same inserted position. Lastly, the loss for this task is just cross-entropy between shifted-by-1 logits and the original target indices with the dummy token being ignored.\n\n\n| Encoded Image | Generated Caption | Original Caption|\n|  :----: | :----: | :----: |\n| ![Catch Example](/images/blog/general-gpt_captioning_example-1.png) | A man and a child playing baseball. | A man and a boy are playing catch in a yard. |\n| ![Sleeping Dog](/images/blog/general-gpt_captioning_example-2.png) | A dog laying on a sidewalk next to a bike. | a white dog is sleeping on a street and a bicycle |\n\nTable 1: Results of image captioning with CLIP embeddings as input into GPT-2.\n\n\n#### Image Retrieval: $y \\rightarrow x$\nSimilar to the first task, we also introduce two additional tokens: \"[CLIP OUT]\" and \"[\\CLIP OUT].\" As there text suggests, they represent the position and container for the CLIP image embedding. The training data for task is formatted as such:\n\n*<center>Caption: [MS-COCO caption ...]. [CLIP OUT][\\CLIP OUT] </center>*\n\nAn interesting difference between the two task arises in the training procedure. Here, we must enforce GPT-2 to learn image representations that are as close to the original CLIP image embeddings as possible. In order to do this, we compute the mean squared error between the last hidden state at the position of the \"[\\CLIP OUT]\" token and the original CLIP embedding. Finally, we perform the same cross-entropy loss for language modeling.\n\n| Caption      | MS-COCO | LAION-5B\n| :---: | :---: | :---: |\n| Birds flying over the beach. | ![Beach Birds](/images/blog/general-gpt_coco-retrieval_example-1.png)| <img src=\"/images/blog/general-gpt_laion-retrieval_example-1.jpg\" width=600></src> |\n| A nightstand with a collection of books. |  ![Room with Books](/images/blog/general-gpt_coco-retrieval_example-2.png) | <img src=\"/images/blog/general-gpt_laion-retrieval_example-2.jpg\" width=300></src> |\n\nTable 2: Nearest neighbors of GPT-2 image embedding prediction within MS-COCO and LAION-5B [5].\n\n\n### Sentence Reconstruction\nOne significant limitation of current open-source LLMs is the constraint on context length. This constraint prevents models from effectively comprehending and reasoning over extensive background knowledge spanning thousands of sentences. To address this challenge, we propose an innovative approach that enables GPT models with a context length of 2048 or 4096, for example, to process and understand vast amounts of background information more efficiently.\n\nAs a preliminary experiment we evaluated how reasonable our second goal was by reconstructing the original text with GPT-2 from an input of its embedded representation. In other words, we hoped to see whether we could embed sentences into some shared dimensional space and then generate the same tokens from those sentences? If so, we may be able to shrink longer contexts into a series of sequence embeddings which would be useful across diverse sets of inputs.\n\nTo model this behavior, we followed a method similar to how we performed the aforementioned image captioning. However, we avoid adding any new tokens or structuring our training data. Instead, a simple encoding of each sentence using the sentence transformer [*all-mpnet-base-v2*](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) [6] is followed by the sentence itself. Then, we compute the cross-entropy loss as previously described with the output logits and target token indices.\n\n| Original Caption | Reconstructed Caption |\n| :---: | :---: |\n| A man riding a motorcycle down the street. | A man riding a motorcycle down the street. |\n| Two animals chasing each other in a barn. | Two animals chasing each other in a barn. |\n| Two animals chasing each other in a farmhouse. | Two animals chase after a flock of farm animals in a barn. |\n\nTable 3: Results of sentence reconstruction with *all-mpnet-base-v2* and GPT-2.\n\n\n## Next Steps\n\nUltimately, our aim is to train GPT models to handle texts and sequences of other modalities entirely in semantic embeddings, such as sequences of CLIP embeddings for videos, where each CLIP embedding represents the image embedding of one image frame, or where one embedding could be the audio clip (CLAP) [7] embedding of 5 or 10 seconds of audio. By predicting sequences in these semantic spaces or streams of ideas, truly multimodal sequence learning could be realized, capable of learning robust and sophisticated world models by pretraining on data from various modalities.\n\nAdditionally, embeddings could be decoded by specialized decoders into different outputs, such as text, images, audio, and video, similar to what DALL-E (Ramesh et al., 2021) does with CLIP embeddings that get decoded into images. Coalescing modalities could open the door to more \n\n### Scale\nIn terms of scale, there are a few dimensions of the experimental setup that we will modify. Three such dimensions include larger models, larger datasets, and more complex data, which we expect will improve the generalization across inputs. In order to tune these larger models on richer data we also need to expand our computational resources, possibly in a distributed setting. \n\nWe plan on introducing greater complexity to the current data by utilizing truly interleaved datasets and large context inputs. For the latter, we convert the background text into a series of sentence embeddings using a pre-trained sentence embedding model, CLIP, or the recently proposed SGPT [8]. Then, create a sequence of these sentence embeddings, effectively compressing the original lengthy text into a condensed representation that captures high-level semantic information. Next, the sequence of embeddings is provided to the GPT model with the more recent context in the form of text tokens. This additional input serves to inform the model about the specific grammar, syntax, and style of the text. The model is then tasked with generating a continuation of the text based on the thousands of sentence embeddings and the few hundred words of the most recent context.\n\nBy representing longer contexts as a series of sequence embeddings, we enable the GPT model to reason over the entire text at once, leading to more coherent and contextually informed outputs. This method could be especially useful for tasks requiring a deep understanding of vast amounts of background information, such as generating summaries of novels, long articles, or comprehensive research papers.\n\nCurrent trends suggest that these modifications will improve our results, but greater complexity may lead to instability. If that is the case, additional modifications or redesigns will be necessary; all of which will be shared as they arise.\n\n### New Tasks\nSome obvious directions we plan to investigate include the extrapolation of the current design into other modalities such as audio and video. Additionally, we wish to understand whether a LLM can generate both text and images that play off one another. In such a case, the LLM wouldn't necessarily generate the images directly, but rather condition an image generation model. If we are able to show that image generation can be guided in an interleaved manner, then other modalities will again be an extension. \n\nAlthough our research in this direction is still preliminary and incomplete, it is highly promising, and we encourage everyone interested in this topic to join our server and contribute to our research. Part of what makes us excited for this project is all the ideas that the open-source community may come up with and even implement. For that reason, we would love any suggestions, feedback, and help!\n\n## Notes\n\nIt is quite clear from the results that inputs that are out-of-distribution in both experiments leads to poor results. Though this isn't unexpected for the scale and goals of our experiments, it does hint at poor generalization in such a configuration. Further experiments will be essential in diagnosing the impacts of richer data and scale.\n\nIf you wish to contribute, stay updated, or learn a bit more about the current work, please check out the following links:\n- 🧑‍💻 [GitHub Repository](https://github.com/LAION-AI/General-GPT)\n- 💬 [LAION Discord](https://discord.gg/HzJU2kuC)\n- 🎥 [Introduction Video](https://www.youtube.com/watch?v=LA3AC8gM6hw)\n\n\n## Acknowledgements\nWe further thank the authors and contributors of the following works/repositories:\n- [HuggingFace](https://github.com/huggingface/transformers)\n- [CLIP Retrieval](https://github.com/rom1504/clip-retrieval)\n\nLogo generated with [Craiyon](https://www.craiyon.com/)\n\n\n## References\n\n[1] Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., ... & Zitnick, C. L. (2014). Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 (pp. 740-755). Springer International Publishing.\n\n[2] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). PMLR.\n\n[3] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9.\n\n[4] Mokady, R., Hertz, A., & Bermano, A. H. (2021). Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734.\n\n[5] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., & Jitsev, J. (2022). LAION-5B: An open large-scale dataset for training next generation image-text models. _ArXiv, abs/2210.08402_.\n\n[6] Reimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.\n\n[7] Elizalde, B., Deshmukh, S., Ismail, M. A., & Wang, H. (2022). Clap: Learning audio concepts from natural language supervision. arXiv preprint arXiv:2206.04769.\n\n[8] Muennighoff, N. (2022). Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904.","date":1679961600000},{"slug":"open-flamingo","frontmatter":{"title":"Announcing OpenFlamingo: An open-source framework for training vision-language models with in-context learning","author":"Anas Awadalla and Irena Gao","date":"Mar 28 2023","previewImg":"/images/blog/flamingo-logo.png"},"content":"\n**Overview.**\nWe are thrilled to announce the release of OpenFlamingo, an open-source reproduction of DeepMind's Flamingo model. At its core, OpenFlamingo is a framework that enables training and evaluation of large multimodal models (LMMs). Check out our [GitHub repository](https://github.com/mlfoundations/open_flamingo) and [demo](https://7164d2142d11.ngrok.app) to get started!\n\nFor this first release, our contributions are as follows:\n\n* 🏋️ A Python framework to train Flamingo-style LMMs (based on Lucidrains' [flamingo implementation](https://github.com/lucidrains/flamingo-pytorch) and David Hansmair's [flamingo-mini repository](https://github.com/dhansmair/flamingo-mini)).\n* 🪅 A large-scale multimodal dataset with interleaved image and text sequences.\n* 🧪 An in-context learning evaluation benchmark for vision-language tasks.\n* 🤖 A first version of our OpenFlamingo-9B model based on LLaMA, with much better models to come!\n\n\nThe recent progress in open-source LMMs with the release of [BLIP-2](https://arxiv.org/abs/2301.12597) and [FROMAGe](https://jykoh.com/fromage) has shown the exciting potential of multimodal systems. We hope that OpenFlamingo will help drive progress in multimodal machine learning, and we have more exciting contributions in the pipeline, so stay tuned! \n\n\n**Goal.**\nOur goal with OpenFlamingo is to develop a multimodal system that can tackle a diverse range of vision-language tasks. Ultimately, we aim to match the power and versatility of GPT-4 in handling visual and text input. To achieve this goal, we are creating an open-source version of [DeepMind's Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model) model, a LMM capable of processing and reasoning about images, videos, and text. We are committed to build fully open-source models, and believe this transparency is essential for fostering collaboration, accelerating progress, and democratizing access to state-of-the-art LMMs. Our release is the first step towards this goal.\n\nWe are sharing the first checkpoint of our OpenFlamingo-9B model. While the model is not yet fully optimized, it demonstrates the potential of this project. By working together and receiving feedback from the community, we can train better LMMs. We encourage the community to participate in the development process by providing feedback and contributing to the repository. \n\n\n**Technical Details.**\nOur implementation largely follows that of [Flamingo](https://arxiv.org/abs/2204.14198). Flamingo models are trained on large-scale web corpora containing interleaved text and images, which is crucial for endowing them with in-context few-shot learning capabilities. OpenFlamingo implements the same architecture (Perceiver resamplers, cross-attention layers) proposed in the original Flamingo paper. However, since the training data for Flamingo is not available to the public, we use open-source datasets for training our models. Specifically, the released OpenFlamingo-9B checkpoint is trained on 5M samples from our new Multimodal C4 dataset and 10M samples from [LAION-2B](https://huggingface.co/datasets/laion/laion2B-en). \n\n\n## **Multimodal C4**\n\nThe Multimodal-C4 dataset is an expansion of the text-only [C4 dataset](https://www.tensorflow.org/datasets/catalog/c4), which was used to train  [T5 models](https://arxiv.org/abs/1910.10683). This dataset is built by our collaborators [Jack Hessel](https://jmhessel.com) and [Wanrong Zhu](https://wanrong-zhu.com) at the Allen Institute for AI. For each document in the [C4 en.clean](https://www.tensorflow.org/datasets/catalog/c4#c4en_default_config) dataset, we retrieve the original webpage from [Common Crawl](https://commoncrawl.org/), then collect the downloadable images. Data cleaning is carried out through deduplication and content filtering, which aims to eliminate non-safe for work (NSFW) and unrelated images, such as advertisements. Additionally, we run face detection and discard images with positive identifications. Finally, images and sentences are interleaved using bipartite matching within a document: CLIP ViT/L-14 image-text similarities serve as edge weights. Multimodal-C4 consists of approximately 75 million documents, encompassing around 400M images and 38B tokens. A full release with more detail is coming soon.\n\n![](/images/blog/mmc4-example.png)\n\n## **Benchmark**\n\nTo measure the performance of OpenFlamingo, we evaluate on a diverse set of downstream tasks. Our aim is to eventually build an open-source version of Flamingo’s benchmark and extend past that to standardize vision-language task evaluation. Currently we support visual question-answering ([VQAv2](https://visualqa.org/index.html), [OK-VQA](https://okvqa.allenai.org)), captioning ([COCO](https://cocodataset.org/#home), [Flickr30k](https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset)), and image classification ([ImageNet](https://image-net.org/index.php)) tasks. Expect us to add many more evaluation sets that probe model reasoning, biases, and more! You can access the benchmark on the OpenFlamingo repo. \n\n\n## **Model release**\n\n![](/images/blog/flamingo-llama.png)\n\nAs part of our release, we are also providing a checkpoint from our under-development OpenFlamingo-9B, a LMM built on top of [LLaMA 7B](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) and [CLIP ViT/L-14](https://openai.com/research/clip). This model is still a work in progress but it can already bring a lot of value to the community. For instance,\n\n![](/images/blog/flamingo-9B-sample-one.png)\n![](/images/blog/flamingo-9B-sample-two.png)\n\n**Performance**\n\nWe evaluated our checkpoint on COCO and VQAv2. Here we report the validation performance using a different number of shots. \n\nCOCO (CIDEr)\n<table>\n  <tr>\n   <td>\n   </td>\n   <td>0-shot\n   </td>\n   <td>4-shot\n   </td>\n   <td>8-shot\n   </td>\n   <td>16-shot\n   </td>\n   <td>32-shot\n   </td>\n  </tr>\n  <tr>\n   <td>OpenFlamingo-9B*\n   </td>\n   <td>65.5\n   </td>\n   <td>74.3\n   </td>\n   <td>79.3\n   </td>\n   <td>81.8\n   </td>\n   <td>84.5\n   </td>\n  </tr>\n  <tr>\n   <td>DeepMind Flamingo-9B\n   </td>\n   <td>79.4\n   </td>\n   <td>93.1\n   </td>\n   <td>99.0\n   </td>\n   <td>102.2\n   </td>\n   <td>106.3\n   </td>\n  </tr>\n</table>\n\n---\n\nVQAv2 (VQA accuracy)\n<table>\n  <tr>\n   <td>\n   </td>\n   <td>0-shot\n   </td>\n   <td>4-shot\n   </td>\n   <td>8-shot\n   </td>\n   <td>16-shot\n   </td>\n   <td>32-shot\n   </td>\n  </tr>\n  <tr>\n   <td>OpenFlamingo-9B*\n   </td>\n   <td>43.5\n   </td>\n   <td>44.0\n   </td>\n   <td>47.5\n   </td>\n   <td>48.9\n   </td>\n   <td>50.3\n   </td>\n  </tr>\n  <tr>\n   <td>DeepMind Flamingo-9B\n   </td>\n   <td>51.8\n   </td>\n   <td>56.3\n   </td>\n   <td>58.0\n   </td>\n   <td>59.4\n   </td>\n   <td>60.4\n   </td>\n  </tr>\n</table>\n\n\n*Note that we report validation performance (using the same setup outlined in Flamingo paper) for OpenFlamingo-9B while DeepMind Flamingo-9B performance is on test data.\n\n**Safety and ethical considerations**\n\nAs OpenFlamingo-9B is built on top of frozen [LLaMA](https://arxiv.org/abs/2302.13971) and [CLIP](https://arxiv.org/abs/2103.00020) models, you can expect OpenFlamingo to inherit the harms of the parent models. We understand that by releasing these models, they may be used in harmful ways. However, it is important for the research community to study the harms of large multimodal models, and we believe that open-sourcing these models will enable the community to develop better ways to mitigate these harms in future models.\n\nWe emphasize that OpenFlamingo-9B is a research artifact and not a finished product. It can produce unintended, inappropriate, offensive, and/or inaccurate results. We thus advocate for caution and thorough evaluations before using our models in any real applications.\n\n\n### Contributions\n\n**Thanks to:**\n\n* [Josh Gardner](https://homes.cs.washington.edu/~jpgard/) and [Yonatan Bitton](https://yonatanbitton.github.io/) for implementing the evaluation benchmark.\n* [Kalyani Marathe](https://kalyani7195.github.io/) for implementing the data pipeline and improving code quality.\n* [Yusuf Hanafy](https://www.linkedin.com/in/yusufhanafy/) for working on the demo.\n* [Wanrong Zhu](https://wanrong-zhu.com/), [Jack Hessel](https://jmhessel.com/), and [Samir Gadre](https://sagadre.github.io/) for building the Multimodal C4 dataset.\n* [Jenia Jitsev](https://scholar.google.de/citations?user=p1FuAMkAAAAJ&hl=en) for helping us with large scale training.\n* [Mitchell Wortsman](https://mitchellnw.github.io/), [Gabriel Ilharco](https://gabrielilharco.com/), [Simon Kornblith](https://simonster.com/), [Pang Wei Koh](https://koh.pw/) for technical discussions and for feedback on this blog.\n* [Ludwig Schmidt](https://people.csail.mit.edu/ludwigs/) for being our main advisor on this project and for their support.\n\n\n### Acknowledgements\n\nThis code is based on Lucidrains' [flamingo implementation](https://github.com/lucidrains/flamingo-pytorch) and David Hansmair's [flamingo-mini repo](https://github.com/dhansmair/flamingo-mini). Thank you for making your code public! We also thank the [OpenCLIP](https://github.com/mlfoundations/open_clip) team as we use their data loading code and take inspiration from their library design.\n\nWe would like to thank [Jean-Baptiste Alayrac](https://www.jbalayrac.com/) and [Antoine Miech](https://antoine77340.github.io/) for their advice, [Rohan Taori](https://www.rohantaori.com/), [Nicholas Schiefer](https://nicholasschiefer.com/), [Deep Ganguli](https://hai.stanford.edu/people/deep-ganguli), [Thomas Liao](https://thomasliao.com/), [Tatsunori Hashimoto](https://thashim.github.io/), and [Nicholas Carlini](https://nicholas.carlini.com/) for their help with assessing the safety risks of our release. This research is supported in part by NSF Institute on the Foundations of Machine Learning (IFML). Thanks to [Stability AI](https://stability.ai) for providing us with compute resources to train these models!\n","date":1679961600000},{"slug":"oig-dataset","frontmatter":{"title":"The OIG Dataset","author":"By Huu Nguyen -  Ontocord.ai, Sameer Suri, Ken Tsui , Shahules786, Together.xyz team, and Christoph Schuhmann - LAION.ai","date":"March 10 2023","previewImg":"/images/blog/oig-example.png"},"content":"\nThe [Open Instruction Generalist (OIG)](https://huggingface.co/datasets/laion/OIG) dataset is a large open source instruction dataset that currently contains ~43M instructions. \n\nOIG is one of many chatbot datasets that [LAION](https://laion.ai), along with its volunteers, [Ontocord](https://www.ontocord.ai), [Together](https://www.together.xyz) and other members of the open source community, will be releasing and is intended to create equal access to chatbot technology. Everyone is welcome to use the dataset and contribute improvements to it.\n\n## Examples of what is in OIG\n\n![](/images/blog/oig-example-2.png)\nExample data in OIG-43M.\n\n![](/images/blog/oig-example.png)\nTopic map of a subset of OIG-43M\n\n|id: value|\n|-|\n|6602: -1_Image prompts for drawing with specific keywords___|\n|1165: 0_Clipart use for teaching materials in commercial format with unlimited illustrations as an abcteach member___|\n|1047: 1_Images of Air Force Change of Command Ceremonies___|\n|745: 2_Documents related to military training and operations of Marine Corps and Army forces in 2013, 2017, and 2018.___|\n|332: 3_Employment Trends in Selected Metropolitan Areas___|\n|304: 4_Health Policy Workshop Proceedings and Image Covers for Cancer, Workforce, Literacy, and Accounting Approaches___|\n|291: 5_Printable worksheets for math, reading, and kindergarten learning with image prompts.___|\n|259: 6_Energy Trends and Prices___|\n|225: 7_Images featuring Defense Secretary James Mattis in official meetings and events.___|\n|174: 8_Images of Ricky Gervais, Jennifer Aniston, and Rachel Brosnahan at various award shows in Beverly Hills and Los Angeles.___|\n|168: 9_Cricket matches and fans in India, featuring IPL teams Kings XI Punjab and Kolkata Knight Riders, Bollywood actors Katrina Kaif and Shah Rukh Khan, and cricket legends Sachin Tendulkar and bowler Singh. Also includes matches with Australia, New Zealand, Pakistan, and Sri Lanka during the World Cup and Test matches.___|\n|140: 10_Images related to Covid-19 vaccination and prevention___|\n\nBreak-down of some image prompt instructions in a subset of OIG-43M.\n\n## Discussion\n\nOIG is a large-scale dataset containing instructions that are created using data augmentation from a diverse collection of data sources, and formatted in a dialogue style (<human>… <bot>… pairs). The goal of OIG is to help convert a language model pre-trained on large amounts of text into an instruction-following model. It is designed to support continued pre-training to enable a base model (e.g., GPT-NeoX-20B) that can be later fine-tuned with the smaller-scale domain-specific datasets.\n\nOIG is created by various LAION community members, consisting of 30 datasets and 43M instructions, but we will continue to expand on this dataset with the goal of reaching 1 trillion tokens - enough to pretrain on OIG only.  It covers not only standard datasets (such as Natural Questions and Natural Instructions), but also data specifically related to dialog, summarization, education, etc.\n\nAppendix 1 describes the components of the current OIG dataset. The dataset can be divided roughly into 75% academic datasets such as P3, Natural instructions, and FLAN, where answers may be short, and the tasks are often artificial, such as determining NLI. The other 25% is composed of various tasks, such as question and answering, providing how-to instructions, performing basic high school math, basic python coding, story generation, essay generation, poetry generation, and a very rudimentary attempt at generating songs based on augmenting existing poetry. Of note, we have also created a UL2-like fill in the blank dataset using TurkuNLP’s [OSCAR-registry](https://huggingface.co/datasets/TurkuNLP/register_oscar) data (e.g, “Fill in the missing spans”, “Fill in the rest of this paragraph”, “Give me the missing words”). We hypothesize that this mixture of instruction improves academic metrics as well as instruction fulfillment. \n\n## Safety and Moderation\n\nAlong with OIG, [Ontocord.ai](https://www.ontocord.ai) is also releasing [OIG-moderation](https://huggingface.co/datasets/ontocord/OIG-moderation), a small safety instruction dataset. OIG-moderation is intended to train a moderation model to predict labels for various moderation categories such as \"needs intervention\", “hate”, \"sexual content\", etc. Ontocord will also release in future versions, multilingual versions of the dataset, and include potential responses that could contain a reason why a chatbot might not respond to the answer. It aims to address issues including privacy eliciting prompts, and depression responses, along with prompts eliciting sexual content and aggressive behavior from users.\n\nOIG-moderation includes data from (a) public datasets such as anthropic-redteam and anthropic-harmless, prosocial, and contributed datasets from community members (b) [augmented toxic data](https://huggingface.co/datasets/SummerSigh/PolicyData) such as civil comments data converted into instructions, (c) anthropic-redteam data [augmented with prosocial tags](https://huggingface.co/datasets/shahules786/prosocial_augmented) (d) data provided by the LAION community that might include NSFW prompts, and (e) synthetic depression data generated from [a public depression bag of words](https://huggingface.co/datasets/joangaes/depression) dataset using one of LAION’s volunteer’s [grammar fixing models](https://huggingface.co/pszemraj/flan-t5-large-grammar-synthesis). \n\nA model trained on the OIG-moderation dataset can be used to provide safety labels, and the bot providers can choose to then block responses from their chatbots based on these labels. If a bot provider's policy for example permits sexual content, but prohibits PII eliciting text, they can hopefully do so with the output of a model trained on this OIG-moderation. \n\n## Safety Goals\n\nOpen source and extendable safety pipelines unfortunately do not exist on the same scale as those in ChatGPT and other commercial systems. To further research in implementable, accurate, and extendable safety pipelines, LAION, Together, and Ontocord will push models, datasets, and code to the public. Research is one of our goals for safety, and we believe that keeping code, datasets, and models private hinders the overall progress in keeping LLM systems safe. By sharing such information, users and researchers alike can point out the harms, and potential solutions in these multifaceted systems.\n\nAnother goal for us is to bring safety research to a production setting where it can be effectively implemented and tested in real world use cases. Research in subjects like [toxicity detection](https://docs.cohere.ai/reference/toxicity-detection) and [bias mitigation](https://arxiv.org/abs/2106.13219) in LLMs is well established; however, the implementation of such research in systems that use language models as conversational agents in real world production settings has largely gone undocumented and unevaluated. The gap between research and implementation, brings many questions that must be answered to bring safe LLMs to the general public.\n\nWith the potential of offering OIG based systems to millions of users, it’s important to recognize the diversity in the user base with respect to socially acceptable paradigms. Pushing generally accepted social paradigms for one user in a specific country, locality, or even age does not warrant those same paradigms to be pushed upon users of other areas and ages. Thus we have opted for a multi-pronged approach to moderation and safety. We have curated and created data with safety tags, so that the bot providers can decide to train on the data and decide for themselves which moderation knob to turn on and off and which to permit their users to turn on and off (e.g., via parental controls).\n\nThus, we will strive to make data for safety systems that allow for user input so that our models can accept and reject prompts on a per locality and even a per user basis. It may not be possible to achieve in the first iteration of a safety pipeline however we will continue to research and strive for this goal. \n\n## How is the OIG dataset related to LAION’s Open Assistant Project?\n\nLAION’s [Open Assistant (OA)](https://github.com/LAION-AI/Open-Assistant) project is our efforts to replicate the functionality of ChatGPT, and as such centers around gathering human feedback and training a reinforcement model based on human feedback. In contrast, the OIG dataset is almost purely a synthetic data set created using data augmentation. Our hypothesis for the OIG dataset is that you can create a performant bot, without RLHF, by first performing continued pre-training with an average quality instruction dataset such as OIG, and then doing a finetuning on a high quality instruction dataset such as OIG-small-chip2. With that said, the team members between the OA and OIG projects overlap and the OIG data began from work done within the LAION OA working group. \n\n## Models*\n\nThe community has trained several models based on a subset of the OIG datasets including:\n\n* Rallio67/joi2_(20,12,7)B_instruct_alpha\n* Rallio67/chip2_(20,12,7)B_instruct_alpha\n* Rallio67/joi_(20,12,7)B_instruct_alpha\n* Rallio67/chip_(20,12,7)B_instruct_alpha\n* togethercomputer/GPT-NeoXT-Chat-Base-20B\n\n## Safety models**\n\n* SummerSigh/T5-Base-Rule-Of-Thumb\n* SummerSigh/Safety-Policy\n* SummerSigh/BART-Base-Rule-Of-Thumb \n* shahules786/prosocial-classifier\n* shahules786/Safetybot-mt5-base\n* shahules786/Safetybot-T5-base\n* togethercomputer/GPT-JT-Moderation-6B\n\n[Together](https://www.together.xyz/) has finetuned a GPT-JT model on v.01 of OIG-moderation, and other LAION volunteers have trained many other models on different subsets of the OIG-moderation v.02 dataset. \n\nNote: All the models above can be found at [https://huggingface.co/](https://huggingface.co/)\n\n*We will update this section as more OIG based models are trained and released. \n\n** Models are in development and do not currently represent the final safety system for LAION chatbots or how models trained on OIG-moderation will fully behave. \n\n## What’s next \n\nThis is just the beginning. This is a new project that we hope will evolve over time. From a purely dataset cleanup perspective, we intend to run a PII anonymizer on the web crawled portion of the dataset (e.g., OSCAR-registry based data). Also, there are several key areas that we need to improve including knowledge Q&A, creative writing and coding. We are also working on collaborations for fine-tuned versions of the bot for tasks like education, which we are incredibly excited about. We also need to perform deduplication and basic filtering for very uninformative instructions in case we made mistakes in the data augmentation. As a prelude, LAION has an ongoing filtering and analysis project, called [riverbed](https://github.com/LAION-AI/riverbed), which aims to analyze the OIG dataset, which we describe briefly below.\n\n## Quality Filtering Approaches using masked language models (MLM)\n\nText outputs from dialogue prompted large language models are known to suffer from hallucinations and other factual inaccuracies. To address this problem, we applied various filtering to detect misinformation and contradiction with masked language models. In particular, masked language model is in a different paradigm from autoregressive language model, as its receptive field covers the context of both directions, providing extra information in detecting factual inaccuracies.\n\nWe framed fact checking as a masked language model pre-training objective. The idea behind is that if the prediction of  <mask> based on bidirectional context matches with the original token, the original token is more likely to be correct than incorrect and vice versa. A custom light weight RoBERTa based model is trained on high quality factual materials like books and wikipedia.\n\nWith the model, we analyzed the outputs of the GPT style language models by randomly masking a small percentage of the generated tokens and then using a BERT or T5 style language model to replace the masked tokens. Discrepancies between the original and replaced tokens were penalized to varying degrees. If the replaced token exactly matched the original token the penalty was zero. If the replaced token did not match the original token then it was analyzed for a semantic match by comparing a string containing the original token plus several tokens around it using language models finetuned on the natural language inference task (NLI) with three classifiers: entailment, neutral, contradiction. Entailment or neutral scores were considered positive while contradiction penalized the match.\n\n### Example 1\n\nOriginal > The big dog barked at the [fluffy] black cat.\n\nMasked > The big dog barked at the `<mask>` black cat.\n\nReplaced > The big dog barked at the [small] black cat\n\nNLI(Original,Replaced) = {'entailment': 13.0892, 'neutral': 79.0414, 'contradiction': 7.8693}\n\nNLI(Replaced,Original) = {'entailment': 0.1456, 'neutral': 99.7456, 'contradiction': 0.1087}\n\nScore = ( ( (13.09+79.04-7.86)/2 + (0.15+99.75-0.11)/2 ) / 100 ) = 0.92\n\n\n### Example 2\n\nOriginal > The big [llama] barked at the fluffy black cat.\n\nMasked > The big `<mask>` barked at the fluffy black cat.\n\nReplaced > The big [dog] barked at the fluffy black cat.\n\nNLI(Original, Replaced): {'entailment': 1.8346, 'neutral': 3.7347, 'contradiction': 94.4307}\n\nNLI(Replaced, Original): {'entailment': 0.111, 'neutral': 1.8248, 'contradiction': 98.0642}\n\nScore = ( ( (1.83 + 3.73 - 94.43)/2 + (0.11 + 1.82 - 98.06)/2 ) / 100 ) = -0.93\n\n\nWe also framed fact checking as replaced token detection, which is the pre-trained objective of ELECTRA. The idea is to look for “corrupted” token based on bidirectional context. Empirically, we found that, together with named entity recognition, pretrained electra large discriminator models could detect wrong named entities with a reasonable precision and recall in an augmented squad_v2 dataset.\n\nWe leveraged natural language inference to detect contradiction of dialogue from the bot. This acts as a self-consistency filter where we require a dialogue not to contradict itself. You can find a small sample of our work-in-progress [filtered OIG](https://huggingface.co/datasets/laion/OIG-riverbed-filtered-small) data here. More to come…\n\n## Support this project\n\nYour contributions and feedback support the open source ecosystem, improve the bot and provide datasets for future AI research. To participate you can:\n\n* Submit [Github](https://github.com/LAION-AI/Open-Instruction-Generalist) issues,  track issues and help create datasets that need improvement.\n* Join our [Discord](https://discord.gg/xBPBXfcFHd) to talk with other team members working on this!\n\n## Disclaimer\n\nThese datasets contain synthetic data and in some cases data that includes humans trying to get the language model to say toxic/offensive/trolling things. If you are concerned about the presence of this type of material in the dataset please make sure you carefully inspect each of the entries and filter appropriately. Our goal is for the model to be as helpful and non-toxic as possible and we are actively evaluating ways to reduce or eliminate undesirable content from the instruction tuning datasets.\n\n## License\n\nThe OIG dataset that is authored by LAION volunteers is released under an Apache 2.0 license. However, the data also includes content licensed under other permissive licenses such as Wikipedia data which is licensed under CC-BY-SA, or web-crawled data which is used under fair use principles. \n\n## Acknowledgement\n\n* We would also like to thank all of our amazing LAION volunteers including: @Rallio, @Jue, @Ce Zhang, @Player-1, @Laurel, @danielpatrickhug, @Jjmachan, @Mylo, @Khalid, @Coco.han,  @Pszemraj, and many others. \n* We would like to thank [Together](https://www.together.xyz/) for their tireless dedication to the open source and AI community and their contribution to many of the datasets.\n* We would like to thank [AI Horde](https://aihorde.net/) and user @Db0 for their incredible contribution of filtered data that were flagged as unethical.\n* Lastly, [Ontocord.ai](https://www.ontocord.ai)’s founders are grateful to have the opportunity to create a portion of the data augmentation and safety-moderation code for this project.\n\n\n## Appendix - Description of OIG datasets components\n\n- unified_ni [https://github.com/allenai/natural-instructions](https://github.com/allenai/natural-instructions)\n- unified_p3: [https://huggingface.co/datasets/bigscience/P3](https://huggingface.co/datasets/bigscience/P3)\n- unified_flan: [https://github.com/google-research/FLAN/tree/main/flan/v2](https://github.com/google-research/FLAN/tree/main/flan/v2)\n- unified_soda_dialog: [https://huggingface.co/datasets/allenai/soda](https://huggingface.co/datasets/allenai/soda)\n- unified_unifiedskg_instructions: [https://github.com/HKUNLP/UnifiedSKG](https://github.com/HKUNLP/UnifiedSKG) \n- unified_merged_code_xp3: [https://huggingface.co/datasets/bigscience/xP3](https://huggingface.co/datasets/bigscience/xP3) (only Python)\n- unified_oscar_en_sample_dialog: A small portion of [https://oscar-project.org/](https://oscar-project.org/)\n[https://huggingface.co/datasets/TurkuNLP/register_oscar](https://huggingface.co/datasets/TurkuNLP/register_oscar)\n- unified_ul2_plus_oscar_en_sample_dialog: A small portion of  [https://oscar-project.org/](https://oscar-project.org/)\n[https://huggingface.co/datasets/TurkuNLP/register_oscar](https://huggingface.co/datasets/TurkuNLP/register_oscar)\n- unified_multi_news: [https://www.tensorflow.org/datasets/catalog/multi_news](https://www.tensorflow.org/datasets/catalog/multi_news)\n- unified_openai_summarize_tldr:  [https://github.com/openai/summarize-from-feedback](https://github.com/openai/summarize-from-feedback)\n- unified_scitldr:  [https://github.com/allenai/scitldr](https://github.com/allenai/scitldr)\n- unified_squad_v2:  [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)\n- unified_nq:  [https://ai.google.com/research/NaturalQuestions](https://ai.google.com/research/NaturalQuestions)\n- unified_poetry_instructions: Poetry data of mostly classical poems\n[https://huggingface.co/datasets/merve/poetry](https://huggingface.co/datasets/merve/poetry)\n[https://huggingface.co/datasets/matthh/gutenberg-poetry-corpus](https://huggingface.co/datasets/matthh/gutenberg-poetry-corpus)\n- unified_sqlv1 and unified_sqlv2: public text 2 sql datasets.\n- unified_unatural_instructions: [https://github.com/orhonovich/unnatural-instructions](https://github.com/orhonovich/unnatural-instructions)\n- unified_conv_finqa:  [https://github.com/czyssrs/ConvFinQA](https://github.com/czyssrs/ConvFinQA)\n- unified_essays:  essays available on the public web \n- unified_plot_screenplay_books_dialog : [https://github.com/markriedl/WikiPlots](https://github.com/markriedl/WikiPlots) extracted from Wikipedia, snippets from the Pile’s [https://huggingface.co/datasets/the_pile_books3](https://huggingface.co/datasets/the_pile_books3), and snippets of screenplays available on the public web. \n- unified_grade_school_math_instructions: [https://github.com/openai/grade-school-math](https://github.com/openai/grade-school-math)\n- unified_mathqa_flanv2_kojma_cot: Public chain-of-thought datasets converted to instructions [https://huggingface.co/datasets/math_qa](https://huggingface.co/datasets/math_qa), \n- unified_joke_explanations: a very small dataset of joke explanations crawled from the public web \n- unified_cuad:  [https://www.atticusprojectai.org/cuad](https://www.atticusprojectai.org/cuad)\n- unified_abstact_infill:  dbpedia and wikipedia snippets combined with a small portion of [https://github.com/google-research/dialog-inpainting](https://github.com/google-research/dialog-inpainting) \n- unified_image_prompts_instructions: A very small subset of LAION-400M\n- unified_canadian_parliament:  [https://openparliament.ca/data-download/](https://openparliament.ca/data-download/)\n- unified_poetry_2_song:  The above poetry dataset (and [https://huggingface.co/datasets/shahules786/PoetryFoundationData](https://huggingface.co/datasets/shahules786/PoetryFoundationData))  translated to song-like structures.\n- unified_hc3_human - [https://huggingface.co/datasets/Hello-SimpleAI/HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3) \n- unified_rallio_safety_and_prosocial: Generated from public datasets and generated from Wiki similar to the chip2 data; find a full list in the end of the document, also includes [https://huggingface.co/datasets/allenai/prosocial-dialog](https://huggingface.co/datasets/allenai/prosocial-dialog) and [https://huggingface.co/datasets/Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)  \n- unified_chip2: Generated from public datasets and generated from Wiki’s; full list below\n\n\n## OIG-small-chip2\n\n### Python Code Examples\n\nA set of instruction / response pairs where the User requests the agent to generate a python function. These examples were generated using a large language model and few shot prompting with python code verified to execute. There are also ~3000 examples of manually curated one line python code examples from the Conala publication (see: [https://conala-corpus.github.io/](https://conala-corpus.github.io/))\n\n### Natural Instruction Examples\n\nA balanced set of diverse natural and factual questions and answers made using few shot prompted UL2 20B and an instruction tuned GPT-NeoX-20B model (Chip) and then rejection sampled using multiple automatic evaluations to remove low quality outputs and to filter out factually inaccurate answers. Also includes some filtered natural instructions from Anthropic Helpful instructions (see: https://github.com/anthropics/hh-rlhf).\n\n### Generic Harmless Instruction Examples\n\nA set of instruction / response pairs sourced from the Anthropic redteam paper github (see: https://github.com/anthropics/hh-rlhf). This dataset includes a lot of data regarding real humans trying to make the Anthropic language models say harmful/toxic/trolling things. For this dataset only examples that were rated lowly on the harmful scale (0,1,2 out of 4, where 4 is the most toxic) were included. Again, only the first lines of dialogue (instruction, first_agent_response) were retained.\n\n### Instruction/Responses with Lists\n\nA set of filtered and reformatted instruction / response pairs where the agent response contains a list. Sourced from the Anthropic github (see: https://github.com/anthropics/hh-rlhf). Sourced from wikihow text lists created by b-mc2 (https://huggingface.co/datasets/b-mc2/wikihow_lists). And rejection filtered instruction response pairs generated by Chip20B that contained lists. All lists are formatted in a similar style.\n\n### Follow-up questions\n\nExamples of instructions and responses where an appropriate response is to ask for more information from the prompter. These examples were generated from a combination of few shot prompted UL2 20B (to generate natural questions) and a large dialogue prompted language model to generate the responses containing follow-up questions.\n\n### Wikipedia Toxic Adversarial Questions\n\nQuestions and answers generated from wikipedia articles that discuss potentially sensitive topics (flagged as potentially toxic by an early toxicity detection model).\n\n### Grade School Math GSM8K (~9,000)\n\nGSM8K is a dataset of 8.5K high quality linguistically diverse grade school math word problems created by human problem writers. The dataset is segmented into 7.5K training problems and 1K test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ − ×÷) to reach the final answer. A bright middle school student should be able to solve every problem. It can be used for multi-step mathematical reasoning. [https://github.com/openai/grade-school-math](https://github.com/openai/grade-school-math)\n\n### Reasoning Instructions\n\nExamples from the Com2Sense and Strategy QA datasets that were reformatted into natural instructions using large language models with few shot prompting and additional quality filtering steps.\n\n### Character and Scene Descriptions\n\nExamples of instructions and responses for the generation of character or scene descriptions. Scenes were sourced from video game wikis and reformatted into instruction / response format using large language models or generated by few shot prompting with large language models.","date":1678406400000},{"slug":"coca","frontmatter":{"title":"Training Contrastive Captioners","author":"Giovanni Puccetti, Maciej Kilian, Romain Beaumont","date":"Feb 2 2023","previewImg":"/images/blog/eval_coca_clip.jpg"},"content":"\n\nWe introduce a new model type to [OpenClip](https://github.com/mlfoundations/open_clip) Contrastive Captioners (CoCa) [1]. This model adds an autoregressive objective (generation) on top of the CLIP contrastive one. The architecture is composed of three parts, the first two are similar to those composing a CLIP model and the third is a text decoder that stands on top of the text encoder. The additional decoder takes as input the encoded images (through cross-attention) and the previous tokens to predict the next most probable one. One of the few architecture changes, compared to CLIP, is attentional pooling [2], used to aggregate image representations and pass them to both the contrastive loss and the decoder cross-attention.\n\nThis is interesting for several reasons:\n\n* We believe there is no openly available trained model with this architecture;\n* Adding a generative task appears to help the contrastive task with minimal computational impact;\n* The model is easily adaptable to a large number of tasks, on top of all those CLIP is suited for. CoCa models can (with relatively cheap fine-tuning) perform Image Captioning, Visual Question Answering, Multimodal Understanding, and more;\n* CoCa gives captioning models an intermediate contrastive latent space for minimal training cost increase.\n\n\n## Benchmarks\n\nOn a comparable model size and with the same training data available, CoCa outperforms a CLIP model on several zero-shot tasks (Figure 1). Most notably on _imagenet1k_ CoCa achieves 75.5 and CLIP 73.1 (2.6% improvement).\n\n\n|(a) ![](/images/blog/eval_coca_clip.jpg) |(b) ![](/images/blog/eval_coca_clip_diff.jpg) |\n|:-|:-|\n\n\n_Figure 1:_ Scores achieved by _coca_ViT-L-14_ and _ViT-L-14_ on several zeroshot classification tasks **(a)**, together with the performance gap between the two models, in the same tasks sorted by magnitude **(b)**.\n\n\n\n\nTable 2 shows the results achieved on Text to Image and Image to Text retrieval by both CoCa and CLIP. In this case too, CoCa outperforms CLIP on all tasks with differences ranging from 0.3 to 1.3.\n\n\n<table>\n  <tr>\n   <td colspan=\"4\" align=\"center\" > Text to Image Retrieval Recall@5\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td>flickr30k\n   </td>\n   <td>flickr8k\n   </td>\n   <td>Mscoco captions\n   </td>\n  </tr>\n  <tr>\n   <td>coca_ViT-L-14\n   </td>\n   <td>92.0\n   </td>\n   <td>70.1\n   </td>\n   <td>70.5\n   </td>\n  </tr>\n  <tr>\n   <td>ViT-L-14\n   </td>\n   <td>91.7\n   </td>\n   <td>69.0\n   </td>\n   <td>69.2\n   </td>\n  </tr>\n  <tr>\n   <td colspan=\"4\" align=\"center\"> Image to Text Retrieval Recall@5\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td>flickr30k\n   </td>\n   <td>flickr8k\n   </td>\n   <td>Mscoco captions\n   </td>\n  </tr>\n  <tr>\n   <td>coca_ViT-L-14\n   </td>\n   <td>99.3\n   </td>\n   <td>81.7\n   </td>\n   <td>83.6\n   </td>\n  </tr>\n  <tr>\n   <td>ViT-L-14\n   </td>\n   <td>98.4\n   </td>\n   <td>81.2\n   </td>\n   <td>83.0\n   </td>\n  </tr>\n</table>\n\n_Table 2:_ Text to Image and Image to Text retrieval **Recall@5** on _flickr30k_, _flickr8k_ and _Mscoco captions_.\n\n## Released Checkpoint\n\nWe release checkpoints for two model configs, _coca_ViT-B-32_ and _coca_ViT-L-14_. We also release the MSCOCO finetunes of those models which are much better at captioning but unfortunately lose their contrastive capabilities during fine tuning.\n\nTry generation in this [Space](https://huggingface.co/spaces/laion/CoCa) or in this [colab notebook](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb)!\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td>L/14\n   </td>\n   <td>B/32\n   </td>\n   <td>CoCa (from paper)\n   </td>\n  </tr>\n  <tr>\n  <td># Params Image Encoder\n   </td>\n   <td>306.72M\n   </td>\n   <td>89.16M\n   </td>\n   <td>\n    1B\n   </td>\n  </tr>\n  <tr>\n   <td># Params Text Encoder\n   </td>\n   <td>123.65M\n   </td>\n   <td>63.42M\n   </td>\n   <td rowspan=\"2\">\n    1.1B\n   </td>\n  </tr>\n  <tr>\n   <td># Params Text Decoder\n   </td>\n   <td>208.07M\n   </td>\n   <td>100.96M\n   </td>\n  </tr>\n</table>\n\n_Table 3:_ Number of parameters for each encoder/decoder component for _coca_ViT-L-14_, _coca_ViT-B-32_ and the _CoCa_ model from the original paper (M=millions, B=billions).\n\n\n\n## Training Notes\n\n\n### Pretraining\n\nWe train both model configurations on 13B samples seen from [LAION-2B](https://laion.ai/blog/laion-5b/) [3] with a batch size of 90k, learning rate of 1e-3, and a cosine decay learning rate schedule. Experiments were performed on 384 A100’s and over the course of training we maintained 75.5 samples/s/gpu (~29k samples/s in total).\n\nWhen it comes to cost, even though CoCa has more capabilities than single-task captioning models there’s a minimal increase ~20% (as reported by Table 8b of the paper). This is due to the fact that the first half of the text decoder (i.e. the text encoder) is unimodal and is computed in parallel to the image encoder, once the encoders are done we simply continue the forward pass of the text embeddings through the text decoder and also include the image embeddings via cross attention. The trainig report can be found [here](https://wandb.ai/iejmac/open-clip/reports/CoCa-L-14--VmlldzozNDEwMDIx).\n\n\n### Fine-tuning\n\nFor image captioning tasks fine-tuning is a straightforward extension of pretraining with few hyper parameters changes. The crucial one is contrastive loss weight, which has to be set to zero to let the backward pass only account for the generative loss, besides  there are no additional fine-tuning oriented components nor changes in the loss. We use a batch size of 128 with a learning rate of 1e-5 and a cosine learning rate schedule. Experiments are performed on 4 A100's. Table 4 shows the language generation scores achieved by _coca_ViT-L-14_ and by CoCa in the original paper, _coca_ViT-L-14 performance is still far from the original CoCa model one.\n\nIt is noteworthy that (in our experiments) after fine-tuning with a generative only loss these models lose their contrastive skills entirely.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td>Bleu@4\n   </td>\n   <td>METEOR\n   </td>\n   <td>CIDEr\n   </td>\n   <td>Spice\n   </td>\n  </tr>\n  <tr>\n    <td colspan=\"5\" align=\"center\">\n    coca_ViT-L-14\n    </td>\n  </tr>\n  <tr>\n   <td>Karpathy val\n   </td>\n   <td>35.6\n   </td>\n   <td>29.8\n   </td>\n   <td>125.3\n   </td>\n   <td>23.4\n   </td>\n  </tr>\n  <tr>\n   <td>NoCaps\n   </td>\n   <td>39.9\n   </td>\n   <td>29.1\n   </td>\n   <td>106.5\n   </td>\n   <td>14.7\n   </td>\n  </tr>\n  <tr>\n    <td colspan=\"5\" align=\"center\">\n    Original CoCa (from paper)\n    </td>\n  </tr>\n  <tr>\n   <td>Karpathy val\n   </td>\n   <td>40.9\n   </td>\n   <td>33.9\n   </td>\n   <td>143.6\n   </td>\n   <td>24.7\n   </td>\n  </tr>\n  <tr>\n   <td>NoCaps\n   </td>\n   <td> -\n   </td>\n   <td>-\n   </td>\n   <td>122.4\n   </td>\n   <td>15.5\n   </td>\n  </tr>\n</table>\n\n_Table 4:_ Visual captioning scores achieved with _coca_ViT-L-14_ on _karpathy_ validation set and _NoCaps_.\n\n\n\n## Captioning Examples\n\n\n\n|<img src=\"/images/blog/ipod_apple.png\" alt=\"cao\" width=\"500\">|<img src=\"/images/blog/space_raccoon.png\" alt=\"cao\" width=\"500\">|\n|:-|:-|\n|An apple sitting on top of a wooden table.|A painting of a raccoon in a space suit.|\n\n\n\n\n\n\n\n\n\n\n\n\n## What’s Next\n\n\n\n* Unimodal Text Pretraining - One of the shortcomings of CoCa is that it can have trouble with zero-shot captioning because the noisy web text it was trained on isn’t as rich as unimodal text data. To this end we can look into methods that provide CoCa models with this rich text understanding either via initializing the weights of the decoder with some pretrained unimodal text decoder or perhaps alternating between multimodal and unimodal losses that use different data.\n* Fine tuning on more tasks VQA, multimodal reasoning, and more.\n* Image Decoder - CoCa adds a multimodal text decoder on top of CLIP and shows this multi-task learning can benefit both tasks. Why not also add a multimodal image decoder?\n\n\n## Contributions and acknowledgements\n\nThanks to\n\n\n\n* [gpucce](https://gpucce.github.io/) and [iejMac](https://github.com/iejMac) for implementation into open_clip and training the models.\n* [lucidrains](https://github.com/lucidrains) for [initial implementation](https://github.com/lucidrains/CoCa-pytorch).\n* [Romain Beaumont](https://github.com/rom1504) and [Ross Wightman](https://github.com/rwightman) for advice, reviews, and engineering support.\n* [Soonhwan-Kwon](https://github.com/Soonhwan-Kwon) for implementing beam search.\n\nHuge thanks to [Emad](https://twitter.com/EMostaque) and [StabilityAI](https://stability.ai/) for providing the compute resources required to train these models.\n\n\n## References\n\n[1] Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., & Wu, Y. (2022). CoCa: Contrastive Captioners are Image-Text Foundation Models. _ArXiv, abs/2205.01917_.\n\n[2] Lee, J., Lee, Y., Kim, J., Kosiorek, A.R., Choi, S., & Teh, Y.W. (2018). Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. _International Conference on Machine Learning_.\n\n[3] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., & Jitsev, J. (2022). LAION-5B: An open large-scale dataset for training next generation image-text models. _ArXiv, abs/2210.08402_.","date":1675296000000},{"slug":"h14_clip_retrieval","frontmatter":{"title":"Clip-Retrieval Update: H-14 Index & SLURM Inference","author":"no usr","date":"Jan 31 2023","previewImg":"/images/blog/h_14_clip_front.png"},"content":"\nToday we release a KNN index for LAION-5B that allows for fast queries of the dataset with the open clip ViT-H-14 CLIP model. This means that users can search through billions of samples quickly and easily, making it a powerful tool for various applications such as image and text retrieval, data filtering and more. With this update also comes a brand new SLURM based inference backend for high-compute environments.\n\nWith this users can now:\n  - Peer into the superset of data used to train the latest stable-diffusion-v2 models.\n  - Easily filter through the dataset to create fine-tuning datasets and averaged embeddings for aesthetic gradients.\n  - Quickly compute indices for new datasets with the SLURM backend.\n  - Download the index & deploy locally\n\n## The front-end\n\nOur new H/14 index is now available for use on our clip-front demo at https://rom1504.github.io/clip-retrieval or https://knn.laion.ai. \nThis new index allows for fast querying using both images and text, making it a valuable tool for a variety of use cases.\nTo start using the new index right away, simply visit the website and start experimenting with the available query options. \nThe demo also allows you to easily download the resulting query as an [img2dataset](https://github.com/rom1504/img2dataset) compatible json file. \nThis means that you can quickly create datasets for any use case, making it a valuable resource for creatives, data scientists and researchers alike.\n\n## Using the KNN as an API\n\nThe KNN index can be accessed via the API, which allows you to perform nearest-neighbor searches in an easy and intuitive way. \nHowever, if you would prefer to use the provided knn index programmatically, you can! \nWe have a notebook that you can use as a guide on how to do so. You can find the notebook [here](https://colab.research.google.com/github/rom1504/clip-retrieval/blob/master/notebook/clip-retrieval-getting-started.ipynb), it will walk you through the steps necessary to use the provided KNN index programmatically. \nThis can be useful if you want to integrate the KNN index into your own application or if you want to automate the process of nearest-neighbor searches. \nNote that if you are looking to integrate the index into your own product, you should deploy it locally.\n\n## Computing your own index\n\nCreating your own index is a great way to interact with and visualize your data. \nWith a custom CLIP embedding index you can quickly search for similar images, check what images your prompts summon, or check how unique a generated image may be to the training data.\n\nThe clip-retrieval repo offers the ability for users to compute their own indices for their own datasets. \nIn an effort to support the creation of our new H-14 index, we added support for SLURM as a backend inference engine. \nThis update adds a third option for computing indices meaning whether you are using SLURM, PySpark, or running it on your local machine, the process of creating a CLIP KNN index has never been easier. \nFor specific usage please see the project’s [README](https://github.com/rom1504/clip-retrieval#clip-inference) for the inference API and the newest arguments available for creating your own index.\n\n## Deploying Locally\n\nUsers who would like to do a lot of queries, or integrate the index into their own product, should download the index and metadata and deploy it locally to their own server. \nIn order to do so we have uploaded the pre-computed indices to huggingface which can be found [here](https://huggingface.co/datasets/laion/laion5b-h14-index). \nFor full documentation on the exact steps necessary to begin hosting the index yourself please visit the [clip-retrieval docs](https://github.com/rom1504/clip-retrieval/blob/main/docs/laion5B_h14_back.md).\n","date":1675123200000},{"slug":"giant-openclip","frontmatter":{"title":"Reaching 80% zero-shot accuracy with OpenCLIP: ViT-G/14 trained on LAION-2B","author":"Mitchell Wortsman","date":"Jan 24 2023","previewImg":"/images/blog/scaling_vit_giant.png"},"content":"\nWe have trained a new [ViT-G/14 CLIP](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k) model with [OpenCLIP](https://github.com/mlfoundations/open_clip) which achieves **80.1%** zero-shot accuracy on ImageNet and **74.9%** zero-shot image retrieval (Recall@5) on MS COCO. As of January 2023, this is the best open source CLIP model.\n\nWe believe this is interesting because:\n* CLIP models are useful for zero-shot classification, retrieval, and for guidance/conditioning in generative models (OpenCLIP is used in Stable Diffusion V2 and currently the third most downloaded model on HuggingFace is a CLIP model). The approach underlying CLIP—self supervised learning on a large, heterogeneous dataset—has been shown to produce models which are more [robust](https://openai.com/blog/clip/) and [fair](https://ai.facebook.com/blog/seer-10b-better-fairer-computer-vision-through-self-supervised-learning-training-on-diverse-datasets/).\n* Our new ViT-G model achieves the highest zero-shot ImageNet accuracy among models that use only naturally occurring image-text pairs as training data, and without explicit labels, pseudo-labels, or any pretrained image or text encoders.\n* Our training run utilized multiple new techniques, including [FLIP](https://arxiv.org/abs/2212.00794) to accelerate training and [model soups](https://arxiv.org/abs/2203.05482) to surpass 80% accuracy.\n\n## Main Results\nThe following results are with image resolution 224x224 except for CoCa which uses 576x576.\n\n| Model name       | Batch size |               Samples seen              | Text Params | Image params | ImageNet top1 | Mscoco image retrieval at 5 | Flickr30k image retrieval at 5 |\n|------------------|:----------:|:---------------------------------------:|:-----------:|:------------:|:-------------:|:---------------------------:|:------------------------------:|\n| OpenAI CLIP L/14 | 32k        | 13B                                     | 123.65M     | 303.97M      | 75.4%         | 61.0%                         | 87.0%                            |\n| OpenCLIP H/14    | 79k        | 32B (16 epochs of laion2B)              | 354.0M      | 632.08M      | 78.0%         | 73.4%                       | 94%                            |\n| OpenCLIP G/14    | 160k       | 32B +unmasked fine-tune (details below) | 694.7M      | 1844.9M      | 80.1%*        | 74.9%                       | 94.9%                          |\n| CoCa            | 66k        | 33B                                     | 1100M       | 1000M        | 86.3%**       | 74.2                        | 95.7                           |\n\n\\* When using [CuPL](https://arxiv.org/abs/2209.03320) prompts instead of the standard prompts from OpenAI, the zero-shot accuracy is 80.3%. When evaluating at 280x280 and changing resize to squash, Ross Wightman found the model achieves 80.4%.\n\n** In addition to natural language supervision, [CoCa](https://arxiv.org/abs/2205.01917) uses synthetic captions constructed with the labels from the JFT-3B dataset. In addition to natural language supervision, CoCa uses synthetic captions constructed with the labels from the JFT-3B dataset. 973 of the 1,000 ImageNet classes have a corresponding class in JFT (e.g., see here sec C.7.2).\n\nAlso see the figure below (figure code by Ross) and our analysis of scaling trends for OpenCLIP model [here](https://arxiv.org/abs/2212.07143).\n\n![](/images/blog/scaling_vit_giant.png)\n\n## Released Checkpoints\n\nWe release the checkpoint through [OpenCLIP](https://github.com/mlfoundations/open_clip) and in the [HuggingFace hub](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k).\n\n## Notes on scaling up\n\nTo scale up model size while reducing compute we used [Fast Language-Image Pre-training (FLIP)](https://arxiv.org/abs/2212.00794) with patch dropout 0.5. Similar to [Masked Autoencoders (MAE)](https://arxiv.org/abs/2111.06377), FLIP drops out patches during training. FLIP also requires a short “unmasked tuning” phase, which we discuss in training notes below. In addition to reducing Giga multiply–accumulate operations (GMACs) for each forward/backward pass, FLIP allowed us to use a larger per-GPU batch size. Without FLIP, gradient accumulation was necessary to maintain a large batch size. Keeping batch size and number of GPUs consistent (at 160k and 512, respectively) but switching to unmasked fine-tuning resulted in a drop from 46.9 to 20.4 samples per second per GPU. For reference, OpenCLIP H/14 with global batch size 79k across 824 GPUs without patch dropout trained at 42 samples/s/GPU.\n\nTo scale up the batch size to 160k, we used [gradient checkpointing](https://arxiv.org/abs/1604.06174v2) and 80GM VRAM A100s. For the unmasked tuning portion, we also used gradient accumulation (see our implementation for the contrastive objective [here](https://github.com/mlfoundations/open_clip/pull/267)). Finally, we used a 2x higher learning rate of 2e-3 compared to our experiments with batch size 80k. The combination of scaling up model, batch size, and learning rate resulted in training instability during the warmup phase. Accordingly, we increased warm-up to 13k steps, trained with layer scale, and used AdamW beta2 0.95. All runs used AMP bfloat16, after previously switching from float16 in prior experiments with L/14 and H/14.\n\n## Training notes\n\n### Phase 1: Patch dropout\n\nFor phase 1 we trained ViT-G with [patch dropout](https://arxiv.org/abs/2212.00794) 0.5 on LAION-2B for 32B samples seen. We used batch size 160k, learning rate 2e-3, and a cosine decay schedule. After this phase the model reached 79.07 zero-shot top1 accuracy on ImageNet.\n\nTraining was mainly done on 512 to 760 A100s depending on availability. When changing the number of GPUs, local batch size was also modified so that the global batch size remained at 160k. When using 512 GPUs we set local batch size to 313 and observed roughly 24k samples per second or 46.9 samples/s/GPU. When using 760 GPUs we set local batch size 211 and observed roughly 33k samples per second or 43.4 samples/s/GPU.\n\n### Phase 2: Unmasked tuning + Model soups\n\nFor phase 2 we followed [FLIP](https://arxiv.org/abs/2212.00794) in conducting a short unmasked tuning phase. We fell short of 80% in our first unmasked fine-tuning phase, reaching only 79.43%. So we tried twice more with different settings (described below) to obtain 79.45% and 79.2%, respectively. Next, we followed [model soups](https://arxiv.org/abs/2203.05482) and averaged the weights of three checkpoints produced by these runs to achieve our final accuracy of 80.1%. [LIMoE](https://arxiv.org/abs/2206.02770) and [PaLI](https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html) also used model soups for better pre-training.\n\nFor our first unmasked fine-tuning run we did not modify the learning rate schedule, but instead doubled the base LR and extended the number of iterations so that the run would proceed for an additional 2B samples seen. LR started at 3.8e-5. For the second run we used LR 5.5e-5 with a full cosine schedule (warmup for roughly 200M samples and a total of 4B samples). The third run had identical hyperparameters to the first but used the LAION-A subset of LAION-2B. LAION-A is a 900M subset of LAION-2B filtered with aesthetic V2 4.5+ and pHash deduplicated. Instead of waiting for the third run to complete we use the checkpoint after approximately 700M samples which, when “[souped](https://arxiv.org/abs/2203.05482)” with the final checkpoints from the two proceeding runs, already allowed us to surpass our goal of 80% accuracy. This indiviual checkpoint achieved 79.2%.\n\nUnmasked fine-tuning was done on 512 A100 GPUs at a speed of roughly 10,450 samples/s or 20.4 samples/s/GPU.\n\nThe following plot shows the loss curve for phase 1.\n\n![](/images/blog/loss_vit_giant.png)\n\n## More results\n\nZero-shot accuracies at resolution 224x224 computed with [CLIP Benchmark](https://github.com/LAION-AI/CLIP_benchmark).\n\n| Dataset         | OpenCLIP H/14 | OpenCLIP G/14 |\n|-----------------|---------------|---------------|\n| ImageNet        | 78.0         | 80.1         |\n| ImageNet-V2     | 70.8         | 73.6         |\n| ImageNet-R      | 89.3         | 92.1         |\n| ImageNet-Sketch | 66.6         | 68.9         |\n| ObjectNet       | 69.7         | 73.0         |\n| ImageNet-A      | 59.2         | 69.3         |\n| CIFAR-10        | 97.4         | 98.2         |\n| CIFAR-100       | 84.7         | 87.5         |\n| MNIST           | 72.9         | 71.6         |\n| SVHN            | 56.1         | 62.5         |\n| Caltech-101     | 85.0         | 86.4         |\n| SUN397          | 75.2         | 74.5         |\n| FGVC Aircraft   | 42.8         | 49.7         |\n| Country211      | 30.0         | 33.8         |\n| Cars            | 93.5         | 94.6         |\n\nHere is a summary figure comparing G/14 and H/14 made with evals by Romain Beaumont.\n\n![](/images/blog/summary_vit_giant.png)\n\n## What’s Next?\n\nIn the future, we may fine-tune the model to enable multilingual capabilities, or fine-tune at higher resolution. Also, [FSDP is coming to OpenCLIP](https://github.com/mlfoundations/open_clip/pull/358) which will allow even larger models, as is [CoCa](https://github.com/mlfoundations/open_clip/pull/308) which will allow new openclip models to also be captioners. More contributions to OpenCLIP are always welcome!\n\n## Contributions and acknowledgements\n\nThanks to:\n* [Romain Beaumont](https://github.com/rom1504), [Ross Wightman](https://github.com/rwightman), [Mehdi Cherti](https://github.com/mehdidc), [Gabriel Ilharco](https://gabrielilharco.com/), and [Jenia Jitsev](https://github.com/JeniaJitsev) for providing extensive ideas, advice, engineering support, evaluating the model, and maintaining the openclip repository used for model training.\n* [Christoph Schuhmann](https://github.com/christophschuhmann) for encouragement and support\n* [Richard Vencu](https://github.com/rvencu) for cluster support\n* [Phil Wang](https://github.com/lucidrains) and [Haoqi Fan](https://haoqifan.github.io/) for the implementation and discussion regarding patch dropout\n* [Sho Yaida](https://www.shoyaida.com/), [Jong Wook Kim](https://jongwook.kim/), [Ari Morcos](http://www.arimorcos.com/) and [Saining Xie](https://www.sainingxie.com/) for helpful remarks regarding hyperparameters\n* [Sarah Pratt](https://sarahpratt.github.io/) for implementing CuPL\n* [Ludwig Schmidt](https://github.com/ludwigschmidt) and [Ali Farhadi](https://homes.cs.washington.edu/~ali/) for helpful discussions, and to the [RAIVN](https://raivn.cs.washington.edu/) and [EFML](https://github.com/mlfoundations/) labs at the University of Washington\n\nAnd of course thanks to [Emad](https://twitter.com/EMostaque) and [Stability AI](https://stability.ai/) for providing the compute resources used for these experiments.\n","date":1674518400000},{"slug":"laion-stable-horde","frontmatter":{"title":"Collaboration between LAION and the Stable Horde","author":"Konstantinos Thoukydidis, hlky","date":"Jan 08, 2023","previewImg":"/images/blog/artbot-image-ratings.png"},"content":"\nAuthor: [Konstantinos Thoukydidis](https://dbzer0.com), [hlky](https://twitter.com/hlky__)\n\nWe are happy to announce that LAION will be assisted by the Stable Horde to provide aesthetic ratings for existing datasets and a completely new dataset of Stable Diffusion generations, which will also be rated by their community.\n\nWe wrote in the past about [LAION-Aesthetics](https://laion.ai/blog/laion-aesthetics/) and how we filtered LAION-5b using an [aesthetic predictor](https://github.com/christophschuhmann/improved-aesthetic-predictor). The predictor, a simple neural net that uses CLIP ViT-L/14 embeddings as input. hlky has retrained the aesthetic predictor using ViT-H-14, and the results are promising.\n\n[hlky’s ViT-H aesthetic predictor](https://github.com/hlky/aesthetic-predictor) was trained on the same datasets as the original - AVA: A Large-Scale Database for Aesthetic Visual Analysis, Simulacra Aesthetic Captions, and LAION-logos. These datasets are limited, totalling around 400k image-rating pairs. This is where the Stable Horde comes in.\n\n[The Stable Horde](https://stablehorde.net/) is a crowdsourced, distributed, [free and open sourced service](https://github.com/db0/AI-Horde) for generating Stable Diffusion images available to be integrated into everything. They have volunteered to direct their community to rate the existing LAION datasets for aesthetic rating of 1-10. As the Stable Horde can be used for free, to incentivize this, they are providing priority (AKA kudos) to anyone who is providing such ratings for their stable diffusion generations.\n\nThe integration has been made possible through an API hosted by Sygil.dev which can collect such ratings and be integrated directly into all Stable Horde clients. Already the first integrations have been released on clients like [ArtBot](https://tinybots.net/artbot/rate).\n\n![](/public/images/blog/artbot-image-ratings.png)\n\nFurthermore the Stable Horde has started gathering thousands of Stable Diffusion images and their parameters generated by their community on a voluntary basis, which will then be used to build a completely new AI dataset. These images will not only be incentivized to be rated aesthetically as well, but also to be rated pairwise. \n\nStable Horde supports many models in addition to base Stable Diffusion models. At the time of writing this, nearly 80 finetunes are available. There are currently no datasets of generated images from Stable Diffusion finetunes, this will be a world first!\n\nThe way this works is that whenever an image set (meaning a number of images out of the same prompt) is generated, a follow-up call can be done to submit ratings for all, or some of them. If the set includes more than one image, optionally the best of the set can be selected, which will serve as a pairwise rating. If no “best” image has been selected but they have been aesthetically rated, the best image will be automatically selected among those ratings.\n\nThrough this collaboration, LAION will be provided with a dataset of millions of images rated in such a manner, along with their prompts, which can be made freely accessible to train further models with improved quality.\n\n\n## What's next?\n\nThe Stable Horde plans to onboard more ratings into their dataset, such as tagging images for artifacts, watermarks, nudity and the like. The provided set will continue expanding so long as people volunteer their time and processing power. \n\n## Support this endeavour\n\nIf you have any questions or need support about the Stable Horde or the rating system, they have [a very active discord server you can join](https://discord.gg/3DxrhksKzn). If you want to support this initiative, you can help by either rating images yourself, or by [onboarding your own GPU as a horde worker](https://github.com/db0/AI-Horde-Worker) which will help the community generate more images and thus increase the size of the open dataset they provide.","date":1673136000000},{"slug":"laion-coco","frontmatter":{"title":"Laion coco: 600M synthetic captions from Laion2B-en","author":"Christoph Schuhmann, Andreas Köpf, Richard Vencu, Theo Coombes, Romain Beaumont","date":"Sep 15, 2022","previewImg":"/images/blog/laion_coco_beach.png"},"content":"\nAuthor: [Christoph Schuhmann](https://github.com/christophschuhmann), [Andreas Köpf](https://github.com/andreaskoepf) , [Theo Coombes](https://github.com/TheoCoombes), [Richard Vencu](https://github.com/rvencu/), [Benjamin Trom](https://github.com/limiteinductive) , [Romain Beaumont](https://github.com/rom1504) \n\n**We present LAION-COCO, the world’s largest dataset of 600M generated high-quality captions for publicly available web-images**\n\nLaion5B has five billion natural captions. They provide a lot of information, but could synthetic captions complement them ?\n\nTo answer this question, we use a combination of existing, publicly available models to produce high quality captions for images in the style of [MS COCO](https://paperswithcode.com/dataset/coco).\n\nWe captioned 600M images from the english subset of Laion-5B with an ensemble of [BLIP](https://github.com/salesforce/BLIP) L/14 and 2 CLIP versions (L/14 and RN50x64).  \n\nWith this post we release them openly today.\n\nThis will make it possible to investigate the value of generated captions to train models. We’re curious on how these synthetic captions could impact models trained on them!\n\n\n## Download it\n\nThe 600M samples are provided in parquet files. Columns include the original caption, the url, the top caption and a list of alternative captions with lower CLIP-similarity scores.\n\n[https://huggingface.co/datasets/laion/laion-coco](https://huggingface.co/datasets/laion/laion-coco) \n\n\n## Samples\n\n\n\n<img src=\"/images/blog/ring.png\" style=\"height:250px; width: auto;\"/>\n\n\n**Original:** LGSY 925 Sterling Silver Double Heart Rings Infinity Love Thin Rings Wedding Engagement Promise Engraved Love Rings for Women for Dainty Gift\n\n**Generated:** An open ring with two hearts on it.\n\n\n\n<img src=\"/images/blog/boot.png\" style=\"height:250px; width: auto;\"/>\n         \n\n**Original:** Female Thick with Pointy Head High Heel Chelsea Ankle Boots\n\n**Generated:** Red leather ankle boots with gold buckles.\n\n\n\n<img src=\"/images/blog/laion_coco_beach.png\" style=\"height:250px; width: auto;\"/>\n\n\n**Original:** a group of people on horses on a beach\n\n**Generated:** Several people riding horses down the beach on a cloudy day.\n\n\n\n\n<img src=\"/images/blog/laion_coco_tags.png\" style=\"height:250px; width: auto;\"/>\n\n\n**Original:** a wall with a bunch of graffiti on it\n\n**Generated:** The parking meter is near a graffiti covered building.\n\n                                                                                                                                        \n\n\n\n<img src=\"/images/blog/sheeple.png\" style=\"height:250px; width: auto;\"/>\n\n\n**Original:** sheeple family\n\n**Generated:** A cartoon drawing of sheep watching TV with their babies.\n\n\n## More samples of images with their generated captions can be found here: \n\n(no cherry picking)\n\n[http://captions.christoph-schuhmann.de/eval_laion/eval.html](http://captions.christoph-schuhmann.de/eval_laion/eval.html) \n\n\n## Method\n\nThe method we used to generate these captions was to\n\n1. We use Blip L/14 to generate 40 captions\n2. Rank them using openai Clip Open AI L/14 ; selected the best 5 captions\n3. Rank using Open AI RN50x64 Clip model to select the best one\n4. Use a small, fine-tuned T0 model to roughly repair grammar and punctuation of the texts\n\nThe hyperparameters were chosen through a [grid search](https://wandb.ai/andreaskoepf/blip_coco_val_sample_sweep_bayes_02/sweeps/1bsha6b0) ([settings](https://github.com/andreaskoepf/CLIP-Image-Captioning/blob/blip_test/blip_coco_val_sample_sweep_bayes_02.yaml)) by Andreas Köpf to best match the style ( ROUGE scores ) of MS COCO texts.\n\n[laion_idle_cap](https://github.com/andreaskoepf/laion_idle_cap/tree/main/docker) is the script that was used for this processing.\n\n\n## Evaluation\n\nWe evaluated these generated captions by asking human evaluators to guess whether a caption is coming from a human or an AI model. We also asked them to rate the quality on a scale from 0(bad) to 5 (good). \n\nIn a first round we presented the evaluators each 200 samples, that contained 100 AI generated and 100 human written MS COCO captions.\n\n### Observations\n\n \n\n<img src=\"/images/blog/eval_laion_coco.png\" style=\"width:400px\" />\n\n\nGT: Y-Axis\n\nAnnotation: X-Axis\n\nMean rating & standard deviation of samples, that were written by a human:\n\nMean: 3.98\n\nStdev: 0.99\n\nMean rating & standard deviation of samples, that were written by an AI \n\nMean: 3.89\n\nStdev: 1.12\n\nMean rating & standard deviation of samples, where the annotator believed they were written by a human:\n\nMean: 4.44\n\nStdev: 0.61\n\nMean rating & standard deviation of samples, where the annotator believed they were generated by an AI \n\nMean: 3.50\n\nStdev: 1.15\n\n### Interpretation\n\nIt is very interesting that the mean scores of the samples generated by humans and generated by the model are very similar. We also notice that the standard deviation of the generated captions is a little bit higher.\n\nWe hypothesize that most in most cases the quality of the generated captions is perceived as as good as the quality of the human written captions.\n\nBut sometimes the captioning model obviously fails and the quality of the results is pretty low because the model doesn't relevant understand concepts about what is going on in the picture, because it's knowledge is not grounded in a sufficiently sophisticated world model.\n\n### Failure cases\n\n\n\n<img src=\"/images/blog/laion_coco_umbrella.png\" style=\"height:250px; width: auto;\"/>\n\n\n_“Two people posing for the camera in their wedding attire, one with an umbrella over his head and another with long red hair.”_\n\n\n\n<img src=\"/images/blog/laion_coco_man.png\" style=\"height:250px; width: auto;\"/>\n\n\n_“An older man having a heart attack, with his hand on the chest.”_\n\nWhen we remove all samples from the evaluations that have ratings of either 0 or 1, we Observe that the mean ratings and standard deviations move closer together. \n\n### Scores without ratings of 0 and 1\n\nMean rating & standard deviation of samples, that were written by a human:\n\nMean: 4.07\n\nStdev: 0.81\n\nMean rating & standard deviation of samples, that were written by an AI \n\nMean: 4.02\n\nStdev: 0.94\n\nThe mean ratings of the generated captions are still a little bit lower and the standard deviation is still a little bit higher, but the trend is pretty clear. By removing samples with rating 2, the gap between the qualities would probably decrease even further. \n\nPresentation only generated captions:\n\nIn a next step, we presented the human evaluators 400 captions that were only generated by the model (no human written captions in between):\n\nMean rating of all samples \n\n3.81\n\nStandard deviation of all samples \n\n0.94\n\n% rated as human\n\n47.5\n\n% rated as AI\n\n52.5\n\nWe observe that the human evaluators thought in 47.5% of all cases, that the captions were written by a human. This makes us confident that our captains are on average pretty good. When we told the evaluators later that all captions were generated by the model they told us that it was very hard for them to judge whether a caption was written by a model or a human, and that it only was easy for them in obvious failure cases.\n\n### Conclusions\n\nWe conclude that Our ensemble of BLIP and CLIP is already pretty good and capable of generating captions with a quality that is on average pretty close to the human written captions of MS Coco. \n\nIt would be very interesting for future work to let people rate our generated captions at larger scale and then filter out the samples with low rating values. These results could be used to train models to rate the quality of captions and to predict whether a caption looks like a generated or a human written caption.\n\nAnd even without further automated filtering, an ensemble of our captions and human evaluators would be a pretty good workflow to curate high quality captions at much lower costs than if we would ask humans to write them from scratch.\n\n\n## Credit assignments\n\n\n\n* [Christoph Schuhmann](https://github.com/christophschuhmann) lead the project, implemented a first version of the code, ran most of the generations & conducted the human evaluations\n* [Andreas Köpf](https://github.com/andreaskoepf) conducted the hyperparameter search & wrote the code to execute BLIP + CLIP filtering at scale\n* [Theo Coombes](https://github.com/TheoCoombes) managed the server that coordinated which GPU worker got which part of LAION to work on\n* [Romain Beaumont](https://github.com/rom1504) packaged the .json into parquet files, sent to HF and wrote the first draft of this post\n* [Richard Vencu](https://github.com/rvencu/) provided the infra structure to use the idle compute for this project\n* [Benjamin Trom](https://github.com/limiteinductive) wrote code that help us to convert the .json files to parquet\n\nWe thank [stability.ai](https://stability.ai/) for providing the compute used to generate the captions in the dataset.\n","date":1663200000000},{"slug":"laion-translated","frontmatter":{"title":"Laion translated: 3B captions translated to English from laion5B","author":"Marianna Nezhurina, Romain Beaumont, Richard Vencu and Christoph Schuhmann","date":"Sep 15, 2022","previewImg":"/images/blog/laion-translated-samples.png"},"content":"\nAuthor: [Marianna Nezhurina](https://github.com/marianna13) [Romain Beaumont](https://github.com/rom1504/) [Richard Vencu](https://github.com/rvencu) [Christoph Schuhmann](https://github.com/christophschuhmann)  \n\nLaion5B dataset was automatically collected from a section of the human web (common crawl). Can models generate different and interesting data compared to what humans write?\n\nThat’s a question we are interested in investigating. To let the community study it, we translated 3B samples of Laion5B from many languages into English.\n\nWe released 3 billions captions for the multilingual part of Laion5B. This makes it possible to use the whole Laion5B dataset to train English models. This also enables training models using these aligned pairs such as [Multilingual-CLIP](https://github.com/FreddeFrallan/Multilingual-CLIP).\n\nWe’re curious what you will do using it!\n\n\n## Downloading it\n\nThe dataset is available in huggingface as parquet files containing the caption, translated caption and urls.\n\n[laion1B-nolang-joined-translated-to-en](https://huggingface.co/datasets/laion/laion1B-nolang-joined-translated-to-en) \n\n[laion2B-multi-joined-translated-to-en](https://huggingface.co/datasets/laion/laion2B-multi-joined-translated-to-en)  \n\n\n## Processing\n\nEvery caption of the original dataset was translated with Facebook’s [M2M100 1.2B model](https://huggingface.co/facebook/m2m100_1.2B) using the following [script](https://github.com/marianna13/translate_dataset/blob/main/translate_data.py). All other fields remain the same as in the original [LAION2B Multi Joined](https://huggingface.co/datasets/laion/laion2B-multi-joined). To make translation possible the original dataset was split into parts with 50k samples in each and every such small part was translated in parallel on GPU nodes and saved in a separate parquet file. The speed of translation depends on the number of nodes and GPUs. The processing was done with 20 nodes with 8 GPUs in each and the speed of translation (including preprocessing and data loading) is 34 samples/per GPU/per second. Then, all translated parquets were [merged together using Spark](https://github.com/marianna13/translate_dataset/blob/main/join_additional.py) and saved as 128 parquet files. The resulting dataset was [joined with the aesthetics scores](https://github.com/marianna13/translate_dataset/blob/main/join_aesthetics.py). \n\n\n## Dataset columns\n\n\n\n* TEXT (the original text of caption)\n* LANGUAGE (language of the original TEXT)\n* ENG TEXT (translation in English of the original TEXT)\n* URL (URL of the image)\n* WIDTH (width of the image)\n* HEIGHT (height of the image)\n* Hash (hash of the URL and TEXT)\n* Pwatermark (probability of being a watermarked image, computed using our [watermark detector](https://github.com/LAION-AI/LAION-5B-WatermarkDetection))\n* Punsafe (probability of being an unsafe image, computed using our [clip based detector](https://github.com/LAION-AI/CLIP-based-NSFW-Detector))\n* Similarity (cosine between text and image ViT-B/32 embeddings, clip for en, mclip for multi and nolang)\n* Prediction (aesthetics score)\n\n\n## Samples from the translated dataset:\n\n\n![laion-2B-translated-samples](/images/blog/laion-translated-samples.png \"laion-2B-translated-samples\")\n\n\n\n## Laion2B-multi-translated\n\n\n### Dataset stats\n\n_Note. Dataset stats were computed using [this](https://github.com/marianna13/translate_dataset/blob/main/get_dataset_stats.py) script._\n\nNumber of uniques 2266M (2266193302)                                            \n\nNumber with WIDTH >= 0 and WIDTH &lt;= 128 160M (160260569)                        \n\nNumber with WIDTH >= 128 and WIDTH &lt;= 256 734M (734166164)                      \n\nNumber with WIDTH >= 256 and WIDTH &lt;= 512 849M (849569769)                      \n\nNumber with WIDTH >= 512 and WIDTH &lt;= 1024 457M (457572747)                     \n\nNumber with WIDTH >= 1024 86M (86750813)                                        \n\nNumber with HEIGHT >= 0 and HEIGHT &lt;= 128 103M (103514467)                      \n\nNumber with HEIGHT >= 128 and HEIGHT &lt;= 256 614M (614490681)                    \n\nNumber with HEIGHT >= 256 and HEIGHT &lt;= 512 753M (753540968)                    \n\nNumber with HEIGHT >= 512 and HEIGHT &lt;= 1024 686M (686553437)                   \n\nNumber with HEIGHT >= 1024 153M (153139456)                                     \n\nNumber with lenengtext >= 0 and lenengtext &lt;= 25 506M (506238532)               \n\nNumber with lenengtext >= 25 and lenengtext &lt;= 50 849M (849160165)              \n\nNumber with lenengtext >= 50 and lenengtext &lt;= 100 840M (840635023)             \n\nNumber with lenengtext >= 100 and lenengtext &lt;= 150 136M (136709119)            \n\nNumber with lenengtext >= 150 5M (5148507)\n\n\n### Similarities between text and images\n\n10000 images and captions were sampled from the dataset, [CLIP embeddings were computed](https://github.com/marianna13/translate_dataset/blob/main/get_clip_embs_similarities.py) (for original texts embeddings were computed using Multilingual CLIP). Then dot products between image and text embeddings were computed (for both original and translated dataset) to get similarities between texts and images. Here’s the distribution of average similarities for two datasets:\n\nSimilarity for original dataset:\n\n10% quantile -  0.2552971839904785\n\n20% quantile -  0.2633610963821411\n\n30% quantile -  0.2694466710090637\n\n40% quantile -  0.2750270366668701\n\n50% quantile -  0.28088638186454773\n\n60% quantile -  0.28750720620155334\n\n70% quantile -  0.2950591444969177\n\n80% quantile -  0.3049575388431549\n\n90% quantile -  0.32077282667160034\n\nSimilarity for translated dataset:\n\n10% quantile  -  0.23388671875\n\n20% quantile  -  0.25390625\n\n30% quantile  -  0.265869140625\n\n40% quantile  -  0.2763671875\n\n50% quantile  -  0.2861328125\n\n60% quantile  -  0.29638671875\n\n70% quantile  -  0.306884765625\n\n80% quantile  -  0.31982421875\n\n90% quantile  -  0.338134765625\n\n\n\n![laion-2B-translated](/images/blog/laion-2B-translated.png \"laion-2B-translated\")\n\n\n\n## Laion1B-nolang-translated\n\n[LAION1B Nolang Joined](https://huggingface.co/datasets/laion/laion1B-nolang-joined) dataset was also translated with a similar [script](https://github.com/marianna13/translate_dataset/tree/main) and in the same way as Multi. This dataset doesn’t have the language column so the model also had to determine language. All other columns in the Nolang dataset are the same as in Multi. And just like translated [LAION2B Multi Joined](https://huggingface.co/datasets/laion/laion2B-multi-joined), LAION1B Nolang was also joined with corresponding [aesthetics scores](https://github.com/marianna13/translate_dataset/blob/main/join_aesthetics.py).\n\n\n### Nolang dataset Stats\n\nNumber of uniques 1260M (1260048307)                                            \n\nNumber with WIDTH >= 0 and WIDTH &lt;= 128 90M (90701133)                          \n\nNumber with WIDTH >= 128 and WIDTH &lt;= 256 409M (409575445)                      \n\nNumber with WIDTH >= 256 and WIDTH &lt;= 512 475M (475885337)                      \n\nNumber with WIDTH >= 512 and WIDTH &lt;= 1024 239M (239035772)                     \n\nNumber with WIDTH >= 1024 59M (59942110)                                        \n\nNumber with HEIGHT >= 0 and HEIGHT &lt;= 128 59M (59814914)                        \n\nNumber with HEIGHT >= 128 and HEIGHT &lt;= 256 370M (370913206)                    \n\nNumber with HEIGHT >= 256 and HEIGHT &lt;= 512 451M (451897702)                    \n\nNumber with HEIGHT >= 512 and HEIGHT &lt;= 1024 316M (316723245)                   \n\nNumber with HEIGHT >= 1024 87M (87671543)                                       \n\nNumber with lenengtext >= 0 and lenengtext &lt;= 25 312M (312548202)               \n\nNumber with lenengtext >= 25 and lenengtext &lt;= 50 555M (555971621)              \n\nNumber with lenengtext >= 50 and lenengtext &lt;= 100 413M (413430230)             \n\nNumber with lenengtext >= 100 and lenengtext &lt;= 150 20M (20446701)              \n\nNumber with lenengtext >= 150 0M (138974)  \n\n\n### Similarities between text and images for Nolang\n\n10000 images and captions were randomly sampled from the dataset, [CLIP embeddings were computed](https://github.com/marianna13/translate_dataset/blob/main/get_clip_embs_similarities.py) (for original texts embeddings were computed using Multilingual CLIP). Then dot products between image and text embeddings were computed (for both original and translated dataset) to get similarities between texts and images. Here’s the distribution of average similarities for two datasets:\n\nSimilarity for original dataset:\n\n10% quantile  -  0.258196085691452\n\n20% quantile  -  0.266357421875\n\n30% quantile  -  0.2728866934776306\n\n40% quantile  -  0.27902457118034363\n\n50% quantile  -  0.28590404987335205\n\n60% quantile  -  0.29329144954681396\n\n70% quantile  -  0.3023602366447449\n\n80% quantile  -  0.31363412737846375\n\n90% quantile  -  0.3313804566860199\n\nSimilarity for translated dataset:\n\n10% quantile  -  0.2406005859375\n\n20% quantile  -  0.2607421875\n\n30% quantile  -  0.27490234375\n\n40% quantile  -  0.2861328125\n\n50% quantile  -  0.296142578125\n\n60% quantile  -  0.306396484375\n\n70% quantile  -  0.317626953125\n\n80% quantile  -  0.33203125\n\n90% quantile  -  0.353271484375\n\n\n![laion-1B-translated](/images/blog/laion-1B-translated.png \"laion-1B-translated\")\n\n\n## Credit\n\n\n\n* [Marianna Nezhurina](https://github.com/marianna13) translated the samples, packaged them, computed stats and wrote most of this post\n* [Romain Beaumont](https://github.com/rom1504/) helped out on packaging and scaling\n* [Richard Vencu](https://github.com/rvencu) set up all the infra that made using idle compute possible\n* [Christoph Schuhmann](https://github.com/christophschuhmann) suggested the project and guided the work to completion\n\nWe thank [https://stability.ai/](https://stability.ai/) for providing the compute for this massive translation. This was a great use of pre-emptible jobs to fill any idle compute available!","date":1663200000000},{"slug":"large-openclip","frontmatter":{"title":"Large scale openCLIP: L/14, H/14 and g/14 trained on LAION-2B","author":"Romain Beaumont","date":"Sep 15, 2022","previewImg":"/images/blog/compare3.png"},"content":"\nWe trained three large CLIP models with [OpenCLIP](https://github.com/mlfoundations/open_clip): ViT-L/14, ViT-H/14 and ViT-g/14 (ViT-g/14 was trained only for about a third the epochs compared to the rest). The H/14 model achieves **78.0%** zero shot top-1 accuracy on ImageNet and **73.4%** on zero-shot image retrieval at Recall@5 on MS COCO. As of September 2022, this is the best open source CLIP model.\n\nCLIP makes it possible to compute representations of images and texts to measure how **similar** they are. It can be used for \n\n\n\n* Zero shot classification: compare an image with the text of the class to know which class is most similar (e.g., ImageNet classification)\n* Retrieval: compare an image or a text to billions of text or images to find the most similar (e.g. as in [clip-retrieval](https://rom1504.github.io/clip-retrieval/) )\n* Generation\n    * CLIP guidance: decide a text you want to generate, then use an image generator model, and use the CLIP distance between what’s generated and the text to generate a better image (e.g., VQGAN + CLIP)\n    * CLIP conditioning: use a clip text embedding as input of a generator to make it generate this text directly (e.g., stable diffusion)\n\nCLIP models are trained in a self supervised fashion on hundreds of millions or billions of (image, text) pairs.\n\nWith LAION, we produced the LAION-5B dataset that contains 5.8 billions of closely related image and text pairs.\n\nThe CLIP model ViT B/32, released by OpenAI, was initially used to filter this dataset out of common crawl.\n\nProducing the best open source CLIP model out of this data set completes the open source replication of the [excellent](https://openai.com/blog/clip/) CLIP paper that OpenAI released one year ago.\n\n\n## Results\n\nWe replicated the results from openai CLIP in models of different sizes, then trained bigger models. The full evaluation suite on 39 datasets ([vtab+](https://github.com/LAION-AI/CLIP_benchmark)) are available in this [results notebook](https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/results.ipynb) and show consistent improvements over all datasets.\n\nThe larger models we release today are L/14, H/14 and g/14.\n\nL/14 was trained on JUWELS Booster supercomputer by [Ross wightman](https://github.com/rwightman). H/14 and g/14 were trained on stability cluster by [Romain Beaumont](https://github.com/rom1504) . While L/14 and H/14 were trained using 34B samples from LAION-2b, g/14 used a substantially smaller sample scale for training, seeing only 12B samples (see tables for more details).\n\n\n#### 32B samples seen\n\n\n| **Model name**                                                                   | **Batch size**                   | **Samples seen**           | **Text Params** | **Image params** | **Imagenet top1** | **Mscoco image retrieval at 5** | **Flickr30k image retrieval at 5** |\n|----------------------------------------------------------------------------------|----------------------------------|----------------------------|-----------------|------------------|-------------------|---------------------------------|------------------------------------|\n| [B/32](https://wandb.ai/rom1504/eval_openclip/reports/B-32-2B--VmlldzoyNDkwNDMy) | 79k                              | 34B (16 epochs of laion2B) | 63.43M          | 87.85M           | 66.6%             | 65.4%                           | 88.4%                              |\n| L/14                                                                             | 79k for 14B samples, 86K for 18B | 32B                        | 123.65M         | 303.97M          | 75.3%             | 71.1%                           | 92.9%                              |\n| [H/14](https://wandb.ai/rom1504/eval_openclip/reports/H-14--VmlldzoyNDAxODQ3)    | 79k                              | 32B (16 epochs of laion2B) | 354.03M         | 632.08M          | 78.0%             | 73.4%                           | 94%                                |\n\n\n\n#### 12B samples seen\n\n\n| **Model name**                                                                     | **Batch size**                             | **Samples seen**                        | **Text Params** | **Image params** | **Imagenet top1** | **Mscoco image retrieval at 5** | **Flickr30k image retrieval at 5** |\n|------------------------------------------------------------------------------------|--------------------------------------------|-----------------------------------------|-----------------|------------------|-------------------|---------------------------------|------------------------------------|\n| B/32                                                                               | 32k                                        | 12B (32 epochs of laion400m)            | 63.43M          | 87.85M           | 62.9%             | 60.8%                           | 85.5%                              |\n| B/16                                                                               | 32k                                        | 12B (32 epochs of laion400m)            | 91.16M          | 86.19M           | 69%               | 63.6%                           | 85.5%                              |\n| L/14                                                                               | 32k                                        | 12B (32 epochs of laion400m)            | 123.65M         | 303.97M          | 72%               | 68.1%                           | 90.8%                              |\n| [g/14](https://wandb.ai/rom1504/eval_openclip/reports/slow-g-14--VmlldzoyNTMwMjg5) | 32k for 8B samples then 64k for 4B samples | 12B (similar to 32 epochs on laion400m) | 354.03M         | 1012.65M         | 76.6%             | 72.4%                           | 93.5%                              |\n\n\nIn addition to having overall better results, we hope the larger text encoder will help improve text understanding. The good performance on the retrieval metrics seems to be a good indicator of this property.\n\nNote the difference in samples seen between the H/14 and the g/14 model. This explains the difference in performance. We picked this lower number to try and fix the stability issue at a lower cost. Eventually they were fixed (by using bfloat16). The performance of this model falls in the scaling curve of 12B sample seen (similar to 32 epochs of laion400m), and a g/14 trained on 32B samples of laion2B would most likely follow the same trends as the other models and get better performance as H/14.\n\n![alt_text](/images/blog/compare3.png \"image_tooltip\")\n\n\n\n## Released checkpoints\n\nWe release the checkpoints for the models, they are available through [openclip](https://github.com/mlfoundations/open_clip) and in HuggingFace hub at [B/32](https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K) [L/14](https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K) [H/14](https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K) and [g/14](https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K)\n\n\n## Related works\n\nRelated work results:\n\n\n| **Model name** | **Samples seen**       | **Imagenet top1** | **Mscoco image retrieval at 5** | **Flickr30k image retrieval at 5** |\n|----------------|------------------------|-------------------|---------------------------------|------------------------------------|\n| Openai B/32    | 12B (32 epochs of WIT) | 62%               |                                 |                                    |\n| Openai B/16    | 12B (32 epochs of WIT) | 69%               |                                 |                                    |\n| Openai L/14    | 12B (32 epochs of WIT) | 75.4%             | 61%                             | 87%                                |\n| ALIGN          | 20B                    | 76.4%             | 69.8%                           | 93.3%                              |\n| BASIC          | 32B                    | 85.7%             |                                 |                                    |\n| CoCa           | 32B                    | 86.3%             | 74.2%                           | 95.7%                              |\n\n\n[BASIC](https://arxiv.org/abs/2111.10050) and  [ALIGN](https://arxiv.org/abs/2102.05918) got excellent imagenet results. They used either different image encoder architecture (EfficientNet, CoAtNet), a larger network scale (BASIC-L with 2.4B params) or pre trained their network with supervised learning on a large dataset (BASIC CoAtNet vision encoder).\n\n[COCA](https://arxiv.org/abs/2205.01917) additionally used captioning loss during training with a multi-modal text decoder which predicted text tokens autoregressively and got 86.3% top1, employing a larger model scale (2.1B params)\n\n\n## Scaling up notes\n\nDuring these training runs, we encountered several interesting issues:\n\n\n\n* Using many GPUs means many of them can have hardware issues and can freeze, crash or even just be slow. This is a particularly annoying problem to handle as if one GPU has an issue, the synchronized nature of distributed training means that all GPUs get stuck. I created [https://github.com/rom1504/gpu-tester](https://github.com/rom1504/gpu-tester) to figure out what are the bad GPUs and exclude them\n* Stability issues! When scaling up the model size, the batch size and the dataset size, at around half the training the loss starts increasing until it reaches a plateau. We tried many possible things (find the list [there](https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c/edit)) and eventually concluded on a surprisingly simple solution: **using amp bfloat16 instead of amp float16 made the training fully stable**\n\nAnd also made some discoveries:\n\n\n\n* It seems using a very large batch size (up to 159k) can help reach even higher performance. This is most likely due to the fact that contrastive learning provides information to the loss as a logit matrix, hence having N times more samples in a batch means N square logits. We did not verify this systematically but BASIC paper provides more experiments and a theoretical justification for this result.\n* It’s possible to get a reasonably performing g/14 CLIP by doing a much shorter cosine decay => getting a 68% g/14 in 10k gpu hours.\n* Grad checkpointing allows to do 10x on the batch size\n\n\n### Training stability issues\n\nStability of training was the main problem we solved in this iteration of the scaling up of OpenCLIP. At around half the training (for L/14, H/14 and g/14), the loss started going up until it plateaued very high (11) and didn’t go down anymore.\n\nWe tried many possible fixes (decreasing lr, gradient shrinking, gradient clipping, cosine attention, post layer norm, …) with little to no effect when trying to resume from before the crash. \n\nEventually only 2 things worked:\n\n\n\n* Finishing the  lr decay very fast : in 8 epochs (compared to the planned 256 epochs). That managed to get most of the performance out of clip H. \n* Switching from float16 to bfloat16 solved the problem while being faster for clip g. We then applied the same fix for clip H and finished its training properly.\n\n[See all the training notes](https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c/edit) with all the details on all the possible ideas that didn’t work.\n\n\n### Training speeds\n\nTo better understand the cost and length of training of clip, we provide these training speed numbers. All numbers assume a100 with 40GB of VRAM. We used gradient checkpointing.\n\n| Model                                                                           | Batch size per gpu | Precision | Number of gpus | Sample per second per gpu |\n|---------------------------------------------------------------------------------|--------------------|-----------|----------------|---------------------------|\n| [B/32](https://wandb.ai/rom1504/open-clip/runs/rnxrp6k7?workspace=user-rom1504) | 96                 | float16   | 824            | 228                       |\n| [H/14](https://wandb.ai/rom1504/open-clip/runs/2zphcgkn?workspace=user-rom1504) | 96                 | float16   | 824            | 30                        |\n| [g/14](https://wandb.ai/rom1504/open-clip/runs/21cpomx2?workspace=user-rom1504) | 40                 | float16   | 800            | 20                        |\n| [H/14](https://wandb.ai/rom1504/open-clip/runs/3l7ppqh3?workspace=user-rom1504) | 96                 | bfloat16  | 824            | 42                        |\n| [g/14](https://wandb.ai/rom1504/open-clip/runs/1pby5fkb?workspace=user-rom1504) | 80                 | bfloat16  | 800            | 31                        |\n\n\nThe speed usually increases with batch size per gpu until a plateau is reached. The speed also increases with the number of gpu. After a certain number of gpus, the curve becomes slower than linear.\n\nBfloat16 which we used in the second part of training provides both better stability and faster sample/s for clip models.\n\n\n## What’s next\n\nThe models will be used for many applications, including clip guiding and conditioning. Even better results could be reached on models like stable diffusion by using a better clip model!\n\nNow that the scaling properties of clip are proven in an open source reproduction, a lot of doors open. Here are some ideas of next steps:\n\n\n\n* Changing the text encoder to work in the multilingual setting (to get a model like [Multilingual-CLIP](https://github.com/FreddeFrallan/Multilingual-CLIP) but trained contrastively, with hopefully even better results!) and scale it up\n* Can we get clip models while using less gpu hours ? extracting the knowledge from smaller clips into a bigger one may help bootstrap the learning process (see [encoder-distill](https://github.com/iejMac/encoder-distill) from [iejMac](https://github.com/iejMac) getting some preliminary results on this)  \n* The clip idea can be expanded to other modalities, see [CLAP](https://github.com/LAION-AI/CLAP) for text-audio alignment\n\nIf you have ideas or want to help out, feel free to reach out in laion server.\n\n\n## Contributions\n\nThanks to\n\n\n\n* [Romain Beaumont](https://github.com/rom1504) for running the experiments on H/14 and g/14\n* [Ross Wightman](https://github.com/rwightman) for conducting all the openclip experiments at JUWELS Booster (Juelich Supercomputing Center) up to L/14 and providing valuable feedback during these H and g clip trainings\n* [Phil Wang](https://github.com/lucidrains) for providing ideas and code (cosine attention, post layer norm, ..) during the stability issues\n* [Boris Dayma](https://github.com/borisdayma) and [Mitchell Wortsman](https://mitchellnw.github.io/) for both proposing to try float32 that showed precision was an issue and eventually lead to trying bfloat16\n* [Blinkdl](https://github.com/Blinkdl) for proposing interesting ideas regarding tuning the learning rate\n* [Christoph Schuhmann](https://github.com/christophschuhmann) for daring proposing to train such large clips, following up on all these experiments, and finding very early that training were frozen, saving some valuable time\n* [Jenia Jitsev](https://github.com/JeniaJitsev) for providing ideas and feedback during the training issues, supervision and coordination of the compute grants at JUWELS Booster\n* [Ludwig Schmidt](https://github.com/ludwigschmidt) for reviewing this post and giving many ideas about LAION datasets and CLIP \n* [Mehdi Cherti](https://github.com/mehdidc) for helping to debug the evaluation scripts and getting comparable results for MS-COCO\n\nAnd of course [Emad](https://twitter.com/EMostaque) (Stability AI) for providing the many GPUs used during these experiments! (g/14 and H/14!)\n\nFor the L/14 training, we gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at Jülich Supercomputing Centre (JSC), Germany.\n","date":1663200000000},{"slug":"laion-aesthetics","frontmatter":{"title":"LAION-Aesthetics","author":"Christoph Schuhmann","date":"Aug 16, 2022","previewImg":"/images/blog/LAION-Aesthetics.jpg"},"content":"\nWe present LAION-Aesthetics, several collections of subsets from LAION 5B with high visual quality.\n\n![](https://raw.githubusercontent.com/LAION-AI/laion.ai/Chris/blog/LAION-Aesthetics.jpg)\n\nTo create LAION-Aesthetics we trained several lightweight models that predict the rating people gave when they were asked _“How much do you like this image on a scale from 1 to 10?”_.\n\n## LAION-Aesthetics V1\n\nWe started with training a linear model on 5000 image-rating pairs from the [SAC](https://github.com/JD-P/simulacra-aesthetic-captions) dataset (which only contained 5000 samples at that time).\n\nSimulacra Aesthetic Captions is a dataset of over 238000 synthetic images generated with AI models such as CompVis latent GLIDE and Stable Diffusion from over forty thousand user submitted prompts.\n\nAs inputs this model uses not the images themselves, but their CLIP Image embeddings produced with the Open AI CLIP VIT L 14 model. We call this model LAION-Aesthetics_Predictor V1.\n\nIts results were so encouraging, that we decided to produce 8M and 120M sample subsets of the LAION 5B images with the highest predicted scores, of those that have english texts.\n\nWe call the dataset consisting of these 2 subsets [LAION-Aesthetics V1](https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md).\n\n![](https://github.com/LAION-AI/laion.ai/blob/Chris/blog/LAION-Aesthetics%20V1.jpg?raw=true)\n\nThe model used for creating this subset can be found [here.](https://github.com/LAION-AI/aesthetic-predictor)\n\nThe LAION-Aesthetics V1 dataset & further details about it can be found [here.](https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md)\n\n## LAION-Aesthetics V2\n\nAfter these very encouraging results, we continued to experiment and gathered the following data to train more improved MLP (multi-layer perceptron) models:\n\n- More samples from the SAC dataset, which had grown in the meanwhile\n  to 176000 image - rating pairs\n- LAION-Logos, a dataset of 15.000 logo image-text pairs with aesthetic\n  ratings from 1 to 10. We collected this dataset to improve the models\n  abilities to evaluate images with more or less aesthetic texts in\n  them.\n- [The Aesthetic Visual Analysis (AVA) dataset](https://github.com/imfing/ava_downloader), which is a large-Scale database for aesthetic visual analysis that contains 250000 photos from dpchallenge.com with several aesthetic ratings from 1 to 10 for most images.\n- After training several MLPs with different numbers of layers and parameters and different activation functions, we found that a simple linear model on the top of CLIP ViT/14 produced in our subjective view the visually most appealing results when used to rank images of LAION-5B. (Even though other MLPs with e.g. Relu functions produced slightly lower MSE and MAE loss values.) We call the resulting model trained on SAC, LAION-Logos and AVA [LAION-Aesthetics_Predictor V2.](https://github.com/christophschuhmann/improved-aesthetic-predictor)\n- Visualizations of sorting all 2.37B images from LAION 5B that have English captions into 40 buckets with the LAION-Aesthetics_Predictor V2 can be found [here.](http://captions.christoph-schuhmann.de/aesthetic_viz_laion_sac+logos+ava1-l14-linearMSE-en-2.37B.html)\n\nUsing LAION-Aesthetics_Predictor V2, we created the following subsets of the LAION 5B samples with English captions:\n\n- 1,2B image-text pairs with predicted aesthetics scores of 4.5 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-4.5.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_4.5plus)\n- 939M image-text pairs with predicted aesthetics scores of 4.75 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-4.75.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_4.75plus)\n- 600M image-text pairs with predicted aesthetics scores of 5 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-5.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_5plus)\n- 12M image-text pairs with predicted aesthetics scores of 6 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-6.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_6plus)\n- 3M image-text pairs with predicted aesthetics scores of 6.25 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-6.25.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_6.25plus)\n- 625K image-text pairs with predicted aesthetics scores of 6.5 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-6.5.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_6.5plus)\n\nThese subsets overlap. 5 fully includes 6 which includes 6.25 and so on. We call the collection of these subsetsLAION-Aesthetics V2.\n\nWe provided the dataset to the [CompViz](https://github.com/CompVis) team led by Robin Rombach and Patrick Esser. They used the 5+ subset to train [Stable Diffusion V1](https://github.com/CompVis/stable-diffusion/tree/ce05de28194041e030ccfc70c635fe3707cdfc30#stable-diffusion-v1) model.\n\n## What's next?\n\nAt the moment we are translating all 2,15B samples from LAION 5B of the multilingual subset to English using the 1,2B parameter [M2M-100](https://github.com/facebookresearch/fairseq/tree/main/examples/m2m_100) model .\n\nThis will allow us to roughly double the size of V2.\n\nAdditionally, we are already working on new multimodal large-scale dataset, this time at webpage-level, similar to the interleaved image-text dataset Deepmind used for [Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model), but also with audio & video files ... and much, much bigger. :)\n\nStay tuned & keep checking our blog for more datasets in the near future.\n\n## Connect\n\nIf you have any questions or comments or the wish to support our efforts, don’t hesitate to [join our Discord community and contact us.](https://discord.gg/vnjVezbeSJ)\n\n_Christoph Schuhmann ( spirit-from-germany#1488 ) and Romain Beaumont ( rom1504#5008 )_\n","date":1660608000000},{"slug":"laion-5b","frontmatter":{"title":"LAION-5B: A NEW ERA OF OPEN LARGE-SCALE MULTI-MODAL DATASETS","author":"Romain Beaumont","date":"Mar 31, 2022","previewImg":"/images/blog/5b.png"},"content":"\nWe present a dataset of 5,85 billion CLIP-filtered image-text pairs, 14x bigger than LAION-400M, previously the biggest openly accessible image-text dataset in the world - see also our [NeurIPS2022 paper](https://arxiv.org/abs/2210.08402)\n\n_Authors: Christoph Schuhmann, Richard Vencu, Romain Beaumont, Theo Coombes, Cade Gordon, Aarush Katta, Robert Kaczmarczyk, Jenia Jitsev_\n\n![](https://lh5.googleusercontent.com/u4ax53sZ0oABJ2tCt4FH6fs4V6uUQ_DRirV24fX0EPpGLMZrA8OlknEohbC0L1Nctvo7hLi01R4I0a3HCfyUMnUcCm76u86ML5CyJ-5boVk_8E5BPG5Z2eeJtPDQ00IhVE-camk4)\n\nLarge image-text models like ALIGN, BASIC, Turing Bletchly, FLORENCE & GLIDE have shown better and better performance compared to previous flagship models like CLIP and DALL-E. Most of them had been trained on billions of image-text pairs and unfortunately, no datasets of this size had been openly available until now. To address this problem we present LAION 5B, a large-scale dataset for research purposes consisting of 5,85B CLIP-filtered image-text pairs. 2,3B contain English language, 2,2B samples from 100+ other languages and 1B samples have texts that do not allow a certain language assignment (e.g. names ). Additionally, we provide several nearest neighbor indices, an improved web interface for exploration & subset creation as well as detection scores for watermark and NSFW. We also announce a full reproduction of a clip training trained on LAION-400M at [open_clip](https://github.com/mlfoundations/open_clip). Explore the dataset at the [search demo](https://rom1504.github.io/clip-retrieval/). See also the [same post on laion website](https://laion.ai/laion-5b-open-dataset) .\n\nWe thank our sponsors [hugging face](https://huggingface.co/), [doodlebot](http://doodlebot.ai/) and [stability](https://stability.ai/) for providing us with computing resources to produce this dataset! We also thank the-eye.eu for hosting the image embeddings and a copy of the whole dataset.\n\n### Disclaimer on dataset purpose and content warning\n\nThe motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet. Our recommendation is therefore to use the dataset for research purposes. Be aware that this large-scale dataset is uncurated. Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer. Therefore, please use the demo links with caution and at your own risk. It is possible to extract a “safe” subset by filtering out samples based on the safety tags (using a customized trained NSFW classifier that we built). While this strongly reduces the chance for encountering potentially harmful content when viewing, we cannot entirely exclude the possibility for harmful content being still present in safe mode, so that the warning holds also there. We think that providing the dataset openly to broad research and other interested communities will allow for transparent investigation of benefits that come along with training large-scale models as well as pitfalls and dangers that may stay unreported or unnoticed when working with closed large datasets that remain restricted to a small community. Providing our dataset openly, we however **do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress**.\n\n## Introduction\n\nSince the release of CLIP & DALL-E in January 2021, several similar large multi-modal language-vision models have been trained by large groups. Models like FLORENCE, Turing Bletchley, ALIGN & BASIC demonstrated very strong transfer capabilities on novel datasets in absence of per-sample labels, which also steadily improved when growing training data amount, following scaling laws observed in previous research work. These models require billions of image-text pairs to achieve competitive performances and unfortunately, no billion-scale image-text pair dataset had been openly available up until now. To address this problem we release LAION 5B, a CLIP-filtered dataset of 5,85 billion high-quality image-text pairs, their CLIP ViT-L/14 embeddings, kNN-indices, a web interface for exploration & subset-creation and NSFW- and watermark-detection scores and tools. We describe the procedure to create the dataset and demonstrate successful training of DALL-E architecture. Having sufficiently large scales, the dataset opens venues for research on multi-modal language-vision models to a broad community.\n\n## Download the data\n\nWe release the following packages under the LAION-5B project:\n\n- [laion2B-en](https://huggingface.co/datasets/laion/laion2B-en) 2.32 billion of these contain texts in the English language\n- [laion2B-multi](https://huggingface.co/datasets/laion/laion2B-multi) 2.26 billion contain texts from 100+ other languages\n- [laion1B-nolang](https://huggingface.co/datasets/laion/laion1B-nolang) 1.27 billion have texts where a particular language couldn’t be clearly detected.\n\nThe data can comfortably be downloaded with [img2dataset](https://github.com/rom1504/img2dataset) (240TB in 384, 80TB in 224)\n\nFor training usage, we recommend reading the [usage guide for training](https://github.com/rom1504/laion-prepro/blob/main/laion5B/usage_guide/preparing_data_for_training.md)\n\nIn particular, we release this data:\n\n- 5.85 billion pairs of image URLs and the corresponding metadata at [laion2B-en](https://huggingface.co/datasets/laion/laion2B-en) [laion2B-multi](https://huggingface.co/datasets/laion/laion2B-multi) [laion1B-nolang](https://huggingface.co/datasets/laion/laion1B-nolang) (800GB)\n- A [knn index](https://huggingface.co/datasets/laion/laion5B-index) that enables quick search in the laion5B dataset (1.6TB)\n- [Indices](https://mystic.the-eye.eu/public/AI/cah/laion5b/indices/vit-l-14/) (or [here](https://the-eye.eu/public/AI/cah/laion5b/indices/vit-l-14/)) for laion2B-en, laion2B-multi, laion1B-nolang (2TB)\n- Web demo of image-text search on LAION-5B [clip-retrieval](https://rom1504.github.io/clip-retrieval/)\n- Safety tags at [laion2B-en-safety](https://huggingface.co/datasets/laion/laion2B-en-safety) [laion2B-multi-safety](https://huggingface.co/datasets/laion/laion2B-multi-safety) [laion1B-nolang-safety](https://huggingface.co/datasets/laion/laion1B-nolang-safety) (50GB)\n- Watermark tags at [laion2B-en-watermark](https://huggingface.co/datasets/laion/laion2B-en-watermark) [laion2B-multi-watermark](https://huggingface.co/datasets/laion/laion2B-multi-watermark) [laion1B-nolang-watermark](https://huggingface.co/datasets/laion/laion1B-nolang-watermark) (50GB)\n\n\nPre-Computed Embeddings\n| Clip Model | Dataset  | URL                                                                                                                 | Size       | Host      |\n|------------|----------|----------------------------------------------------------------------------------------------------------------------|------------|-----------|\n| Vit-L/14   | laion2b-en  | [image embeddings, text embeddings, & metadata](https://mystic.the-eye.eu/public/AI/cah/laion5b/embeddings/laion2B-en/) (or [here](https://the-eye.eu/public/AI/cah/laion5b/embeddings/laion2B-en/)) | 6.2TB        | the eye   |\n| Vit-L/14   | laion2b-multi  | [image embeddings & metadata](https://mystic.the-eye.eu/public/AI/cah/laion5b/embeddings/laion2B-multi/) (or [here](https://the-eye.eu/public/AI/cah/laion5b/embeddings/laion2B-multi/)) | 3.2TB        | the eye   |\n| Vit-L/14   | laion1b-nolang  | [image embeddings & metadata](https://mystic.the-eye.eu/public/AI/cah/laion5b/embeddings/laion1B-nolang/) (or [here](https://the-eye.eu/public/AI/cah/laion5b/embeddings/laion1B-nolang/)) | 2TB        | the eye   |\n| Vit-L/14   | laion2b-en  | [image embeddings , text embeddings, & metadata](https://huggingface.co/datasets/laion/laion2b-en-vit-l-14-embeddings) | 6.2TB           | huggingface        |\n| Vit-L/14   | laion2b-multi  | [image embeddings & metadata](https://huggingface.co/datasets/laion/laion2b-multi-vit-l-14-embeddings)              |         3.2TB   | huggingface        |\n| Vit-L/14   | laion1b-nolang  | [image embeddings & metadata](https://huggingface.co/datasets/laion/laion1b-nolang-vit-l-14-embeddings)              |   2TB         | huggingface        |\n| Vit-H/14   | laion2b-en  | [image embeddings, text embeddings, & metadata](https://huggingface.co/datasets/laion/laion2b-en-vit-h-14-embeddings) | 9.5TB           | huggingface        |\n| Vit-H/14   | laion2b-multi  | [image embeddings & metadata](https://huggingface.co/datasets/laion/laion2b-multi-vit-h-14-embeddings)              | 4.5TB           | huggingface        |\n| Vit-H/14   | laion1b-nolang  | [image embeddings & metadata](https://huggingface.co/datasets/laion/laion1b-nolang-vit-h-14-embeddings)              | 2.5TB         | huggingface        |\n\n\n\nThe metadata files are parquet files that contain the following attributes: URL, TEXT, the cosine similarity score between the text and image embedding and height and width of the image. Watermark and safety tags can be joined with the metadata prior to downloading by using [this script](https://github.com/rom1504/laion-prepro/blob/main/laion5B/safety/join.py). Once that is done, they can easily be filtered upon with a probability threshold at your choice (we recommend 0.5 for safety and 0.8 for watermark).\n\nYou can also find the prejoined files at [laion2B-en-joined](https://huggingface.co/datasets/laion/laion2B-en-joined) [laion2B-multi-joined](https://huggingface.co/datasets/laion/laion2B-multi-joined) [laion1B-nolang-joined](https://huggingface.co/datasets/laion/laion1B-nolang-joined) (800GB)\n\n## License\n\nWe distribute the metadata dataset (the parquet files) under the [Creative Common CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) license, which poses no particular restriction. The images are under their copyright.\n\n## Dataset columns\n\nWe provide these columns :\n\n- URL: the image url, millions of domains are covered\n- TEXT: captions, in english for en, other languages for multi and nolang\n- WIDTH: picture width\n- HEIGHT: picture height\n- LANGUAGE: the language of the sample, only for laion2B-multi, computed using [cld3](https://github.com/google/cld3)\n- similarity: cosine between text and image ViT-B/32 embeddings, clip for en, mclip for multi and nolang\n- pwatermark: probability of being a watermarked image, computed using our [watermark detector](https://github.com/LAION-AI/LAION-5B-WatermarkDetection)\n- punsafe: probability of being an unsafe image, computed using our [clip based detector](https://github.com/LAION-AI/CLIP-based-NSFW-Detector)\n\npwatermark and punsafe are available either as individual collections that must be [joined](https://github.com/rom1504/laion-prepro/blob/main/laion5B/safety/join.py) with the hash of url+text, either as prejoined collections.\n\n## Dataset Statistics\n\nWe [computed](https://github.com/rom1504/laion-prepro/blob/main/laion5B/stats/compute_stats.py) some statistics on the datasets to let people understand better: Samples are considered unsafe if the model predicts it as unsafe with a probability of more than 0.5. More than 0.8 for watermark. These values are pretty conservative, so the estimated safeness and watermark proportion may be higher than the truth. Other thresholds may be chosen to get a different precision/recall tradeoff.\n\nComputed quantiles are quantiles from 0.05 to 0.95.\n\nAlso see the whole [sheet](https://docs.google.com/spreadsheets/d/19AkcufyABAnbBlsr12VUmlR9oyQWb4uloAQnd-rqJC0/edit#gid=0) and the whole [dashboard](https://datastudio.google.com/reporting/c67c1749-816f-464f-873a-867b4a43f044/page/p_i9he8sxntc/edit)\n\n### Laion2B-en\n\nTotal: 2.3B samples\n\n![](https://lh6.googleusercontent.com/-SW3vGI4_Ojemg_ttYpZvFmC8vTjYavTDgmnY7SsnfF-smnVpLwqbYCUsmB9_1HBmmVbKRE2QXJRwxamNcw1A9sRXDFPSj0YZ2WiptPnNeAMuSF0O_2Yi_CGsm_QChM4eJXd4lyY)\n\nNumber with height and width bigger than\n\n- 256 -> 1324M\n- 512 -> 488M\n- 1024 -> 76M\n\nWidth quantiles: 132.0, 160.0, 180.0, 210.0, 225.0, 240.0, 262.0, 300.0, 309.0, 340.0, 400.0, 450.0, 480.0, 512.0, 600.0, 656.0, 760.0, 960.0, 1050.0\n\nHeight quantiles: 125.0, 150.0, 166.0, 188.0, 208.0, 225.0, 250.0, 270.0, 300.0, 320.0, 350.0, 380.0, 418.0, 470.0, 500.0, 600.0, 672.0, 800.0, 1014.0\n\nUnsafe proportion: 2.9%\n\nWatermark proportion: 6.1%\n\nAverage text length: 67\n\nText length quantiles: 21.0, 25.0, 30.0, 33.0, 37.0, 40.0, 43.0, 47.0, 50.0, 54.0, 58.0, 62.0, 67.0, 72.0, 78.0, 85.0, 96.0, 114.0, 152.0\n\n### Laion2B-multi\n\nTotal: 2.2B samples\n\n![](https://lh4.googleusercontent.com/jLZYO_GMS28fzxwfZS199LOjSeUpTH7HEmjIRwyVAtvJdBkzyHzs83FgnD-hOY0CjK8LDooytibVcbuVa_O5YSoCu1IduEj4Z8uneE8Km-0Y39qHzYAJs4Lr4oEyQh4EeWxHGFOk)\n\nNumber with height and width bigger than\n\n- 256 -> 1299M\n- 512 -> 480M\n- 1024 -> 57M\n\nWidth quantiles: 140.0, 160.0, 188.0, 205.0, 235.0, 250.0, 284.0, 300.0, 324.0, 366.0, 420.0, 480.0, 520.0, 600.0, 640.0, 720.0, 800.0, 960.0, 1080.0\n\nHeight quantiles: 120.0, 144.0, 160.0, 180.0, 200.0, 217.0, 240.0, 262.0, 300.0, 320.0, 350.0, 394.0, 416.0, 458.0, 500.0, 564.0, 636.0, 725.0, 1000.0\n\nTop 10 languages: LANGUAGE count proportion:\n\n- ru 241M 0.106\n- fr 168M 0.074\n- de 150M 0.066\n- es 149M 0.066\n- zh 143M 0.063\n- ja 131M 0.057\n- it 95M 0.042\n- pt 88M 0.038\n- nl 66M 0.029\n- pl 62M 0.027\n- no 49M 0.021\n\nUnsafe proportion: 3.3%\n\nWatermark proportion: 5.6%\n\nAverage text length: 52\n\nText length quantiles: 12.0, 16.0, 20.0, 23.0, 27.0, 30.0, 33.0, 37.0, 40.0, 44.0, 48.0, 52.0, 57.0, 61.0, 67.0, 74.0, 81.0, 93.0, 120.0\n\n### Laion1B-nolang\n\nTotal: 1.2B samples\n\n![](https://lh3.googleusercontent.com/mAI2e-sLE2geRsX8-3Mw-Ye_8wDu0SEDnIBZjWNqJiaAdjtjj7PtBvRhreugGS6_740-KcILnRbvRbDcTIFccrYO-adNz2uRM6zb0VgR3wjZVo_x0dxDhaMouH2KHScZnuNNkvs3)\n\nNumber with height and width bigger than\n\n- 256 -> 1324M\n- 512 -> 488M\n- 1024 -> 76M\n\nWidth quantiles: 135.0, 160.0, 181.0, 207.0, 225.0, 241.0, 264.0, 300.0, 306.0, 338.0, 398.0, 426.0, 499.0, 520.0, 600.0, 655.0, 768.0, 940.0, 1080.0\n\nHeight quantiles: 118.0, 144.0, 160.0, 186.0, 200.0, 220.0, 240.0, 260.0, 292.0, 305.0, 338.0, 368.0, 405.0, 456.0, 500.0, 562.0, 637.0, 768.0, 1000.0\n\nUnsafe proportion: 3%\n\nWatermark proportion: 4%\n\nAverage text length: 46\n\nText length quantiles: 13.0, 17.0, 20.0, 23.0, 26.0, 29.0, 32.0, 35.0, 38.0, 41.0, 44.0, 48.0, 51.0, 56.0, 60.0, 67.0, 73.0, 82.0, 99.0\n\n## Acquisition pipeline\n\n![](https://lh4.googleusercontent.com/5Pcm38fU3jxc9zf1oSNLMTxh9TH4eMG-5n-dwLF-EPNc_o-jRCwa1D1AJIX-dP4UmeEoKkDxg4fWr3Mq5JwQ7TDuKUwMiaww9GPh8EGJwGkCqWXWKx15CY4Jgige8Uu6inl0KSOu)\n\nThe acquisition pipeline follows the flowchart above and can be split into three major components:\n\n- Distributed processing of petabyte-scale Common Crawl dataset, which produces a collection of matching URLs and captions (preprocessing phase)\n- The distributed download of images based on shuffled data to pick a correct distribution of URLs, to avoid too heavy request loads on single websites\n- Few GPU node post-processing of the data, which is much lighter and can be run in a few days, producing the final dataset.\n\n### Distributed processing of Common Crawl\n\nTo create image-text pairs, we parse through WAT files from Common Crawl and parse out all HTML IMG tags containing an alt-text attribute. At the same time, we perform a language detection on text with three possible outputs: English language with confidence, another language with confidence, no language which contains “no detection” and “detection under the confidence threshold”. The “no language” set often contains short texts, mostly with names of people and places. All extracted information by the preprocessing workers were packed and sent to the Postgresql node for storage using the COPY command. The Postgresql server was maintained to keep about 500M records at all times by means of balancing the ingress and egress of data from the database.\n\n### Distributed downloading of the images\n\nWe download the raw images from the parsed URLs with asynchronous requests using Trio and Asks libraries in order to maximize all resources usage: vCPUs, RAM and bandwidth. We found that a single node in the cloud with 1-2 vCPUs, 0.5-1GB RAM and 5-10Mbps download bandwidth is inexpensive enough to allow downloading on a limited budget. Such a unit can process 10000 links in about 10-15 minutes. Each batch consisted of 10000 links taken from the Postgresql server by using the TABLESAMPLE technique, ensuring that the distribution among the 10000 links was following the distribution of the existing 500M records available on the database. We found that the distribution is still good when in the database are still above 20M records to be processed given that we had some 300 downloading workers at any time. The above techniques allowed both maximizing downloading speed and minimizing IP reputation damages.\n\n### CLIP inference at the post-processing stage\n\nThe data pipeline continued with GPU nodes doing inference on the collected image-text pairs, and calculating the similarity of the embeddings for the image and the text. After the similarity score was established we removed the pairs under the threshold we decided to use, i.e 0.28 for the English dataset ( with CLIP ViT B/32 ) and 0.26 for the rest (with mCLIP). As an estimation, we removed about 90% of the samples, trimming the 50+ billion of candidates to just below 6 billion.\n\n### Filtering out unsuitable image-text pairs\n\nAfter downloading the WAT files from Common Crawl, we apply the following filtering conditions:\n\n- All samples with less than 5 characters alt-text length or less than 5 KB image size are dropped.\n- All images with the too big resolution, potentially DOS bombs, were dropped before attempting to process them.\n- Duplicate removal is performed with a bloom filter based on URL. Future runs would include more variate deduplication rules, such as URL + language for the multilanguage dataset.\n- We use CLIP respectively MCLIP to compute embeddings of the image and alt-text. Then we compute the cosine similarity of both embeddings and drop all samples with cosine similarity below 0.28 for the English language ( with CLIP B/32) and 0.26 for the multilingual dataset (MCLIP). These thresholds were selected based on human inspection of the test results.\n- We use the CLIP embeddings of images and texts to filter out to the possible extent the illegal content.\n\n## Dataset preparation pipeline\n\nAfter processing and filtering common crawl, 5,85B of URL/text samples remained. We did additional steps after that in order to prepare the dataset. See this [semantic search blogpost](https://rom1504.medium.com/semantic-search-with-embeddings-index-anything-8fb18556443c) and the readme of [clip-retrieval](https://github.com/rom1504/clip-retrieval) for additional details about this process. See also [semantic search at billions scale](https://medium.com/@rom1504/semantic-search-at-billions-scale-95f21695689a) for more technical details of the process that was done for laion5B.\n\n1.  Downloading the data as webdataset with distributed img2dataset\n2.  Computing Vit-L/14 embeddings with distributed clip-inference\n3.  Computing a KNN index from these embeddings using autofaiss\n4.  Computing additional tags (NSFW and watermark) using clip embeddings\n\n### Distributed img2dataset\n\nWe developed the [img2dataset](https://github.com/rom1504/img2dataset) library to comfortably download from a given set of URLs, resize and store the images and captions in the webdataset format. This allows downloading 100 million images from our list of URLs in 20 hours with a single node (1Gbps connection speed, 32GB of RAM, an i7 CPU with 16 cores), which allows anyone to obtain the whole dataset or a smaller subset. For LAION-5B we introduced a [distributed mode](https://github.com/rom1504/img2dataset/blob/main/dataset_examples/laion5B.md) for this tool, allowing to downloading the 5,85B samples in a week using 10 nodes.\n\n### Distributed clip inference\n\nFrom these images, the [clip retrieval](https://github.com/rom1504/clip-retrieval%7D%7Bhttps://github.com/rom1504/clip-retrieval) inference tool was used to compute ViT-L/14 embeddings, allowing for a better analysis capacity of the data. In particular, a [distributed mode](https://github.com/rom1504/clip-retrieval/blob/main/docs/distributed_clip_inference.md) made it possible to compute these embeddings in a week using 32 A100: this larger clip model can only be computed at a speed of 312 sample/s per GPU, compared to 1800 sample/s for ViT-B/32. The resulting embeddings are available for everyone to use e.g. for clustering, indexing, linear inference.\n\n### Distributed indexing\n\nWe then used these 9 TB of image embeddings to build a large PQ128 knn index using the [autofaiss](https://github.com/criteo/autofaiss) tool. To make this run faster, a [distributed mode](https://github.com/criteo/autofaiss/blob/master/docs/distributed/distributed_autofaiss.md) is available.\n\n### Integration in the search UI\n\nIn order to demonstrate the value of this data, we integrated this index into the [knn search UI](https://knn5.laion.ai/). It is powered by the code called [clip back](https://github.com/rom1504/clip-retrieval). The knn index is 800GB and the metadata (URL and captions) as well, so memory mapping is used for both in order to use no ram, only an SSD drive of that capacity is required.\n\n### Watermark and safety inference\n\nWe wanted to give users the ability to remove unsafe examples, and watermarked examples. To do that we collected training and test sets. The training set was augmented with examples retrieved from the knn index, while the test set samples were selected to represent well the dataset distribution, but were all manually annotated. The inference is done using the [embedding-reader](https://github.com/rom1504/embedding-reader) module for NSFW and [LAION-5B-WatermarkDetection](https://github.com/Zasder3/LAION-5B-WatermarkDetection) for watermarks These tags were also integrated into the UI, allowing everyone to observe that the safety tags indeed filter out almost all the unsafe results, and giving confidence that training a generative model on this data will not result in unexpectedly unsafe images.\n\n### Watermarks\n\n![](https://lh6.googleusercontent.com/WQMTd03M8xmR8yTuSudnyZGgcQvcLfro4Lf0DXvIZb9P8xqjGJVPjrgziZ4U2HoHlgmY_3Ubx33qYaG5jIC-h_uuEOqyJ9K0IqJsNZH0XPiP1CDek8xz60fARNXpwJW9yCkEKa7u)\n\nThe training dataset is 90000 samples (45222 watermarks, 44778 clear).\n\nWatermarked images are a big problem when training generative models like DALL-E or GLIDE. To tackle this problem we trained a watermark detection model and used it to calculate confidence scores for every image in LAION-5B. Therefore we created a training dataset consisting of 90.000 images with 50% watermarked and 50% clean images. The majority of the watermarked images have been extracted from the LAION-400M KNN index through the use of several text prompts like “clip art watermark”, “cat watermark” or “landscape watermark”.\n\nThe images in the cleaned category were composed of images from the Open Images dataset and images that contained texts, but no watermarks, like PPT slides and memes, also retrieved from the kNN indices of LAION-400M. While we tried to curate a test set to evaluate the quality of our watermark detection model, we realized that it is almost impossible to draw a clear line between what actually is a watermark and what is not. For example pictures with small transparent texts at the bottom had been considered by some people as watermarked, by others not.\n\nIn the end we decided to choose a model based on our consensual judgment. It seems to be “good” at spotting obvious watermarks like those used on popular stock image sites. The creation of high-quality, openly accessible watermark detection test sets with clear and plausible definitions of what should be considered a watermark and what not, remains a challenge for future projects. Nevertheless we are convinced that removing images with a high confidence score for containing a watermark based on our model will significantly reduce the percentage of images that would be considered as obvious watermarks.\n\nThe model is available at [https://github.com/LAION-AI/watermark-detection](https://github.com/LAION-AI/watermark-detection) and [https://github.com/LAION-AI/LAION-5B-WatermarkDetection/releases/tag/1.0](https://github.com/LAION-AI/LAION-5B-WatermarkDetection/releases/tag/1.0)\n\n### Safety\n\nOn a balanced manually annotated safety test set with 3000 samples:\n\n- the accuracy of the B32 NSFW classifier is: 0.960\n- the accuracy of the ViT L 14 NSFW classifier is: 0.961\n\nThe model, as well as the training code, are available at [CLIP-based-NSFW-Detector](https://github.com/LAION-AI/CLIP-based-NSFW-Detector) The tags are available at [laion2B-en-safety](https://huggingface.co/datasets/laion/laion2B-en-safety) [laion2B-multi-safety](https://huggingface.co/datasets/laion/laion2B-multi-safety) [laion1B-nolang-safety](https://huggingface.co/datasets/laion/laion1B-nolang-safety) Demo at [clip-retrieval](https://rom1504.github.io/clip-retrieval/) (check/uncheck safe mode)\n\n## Using LAION datasets\n\nLaion5B and LAION-400M could e.g. be used to train\n\n- Generative models: training image/text generative models, e.g autoregressive models like DALL-E or diffusion models like GLIDE\n- Models with contrastive losses: self-supervised training on image/text pairs using contrastive losses, e.g CLIP\n- Classification models: e.g, performing zero-shot classification by extracting pseudo labels from queries on the dataset\n\nWe present here a few examples of models that were trained on our LAION datasets with success:\n\n#### CLIP\n\nWe, LAION, are currently working together with the Cross Sectional Team Deep Learning (CST-DL), Scalable Learning and Multi-Purpose AI Lab (SLAMPAI) at the Jülich Supercomputing Centre (JSC) and the Open CLIP team in the replication of OpenAI’s CLIP results.\n\n![](https://lh3.googleusercontent.com/-lvN21OSxBEwmzj1H0eIa4RxpS_VEogmXxku8R_0LWSLWtDP_tfNPuKCPuBEzA2arDvyPVfZlACZhdgymssC5E0tp_aqPdUFJMOLQf3W0wZKx3LqpJKF4JViL_nrBQH6TxEn5H2i)\n\n( The results in the right column are from our model. – huge thanks to Cade Gordon & Ross Wightman for performing the training run )\n\nThe repository with the training code and the model checkpoints can be found here: [https://github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip)\n\nWe gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at Jülich Supercomputing Centre (JSC).\n\n#### BLIP inference tuning\n\n[BLIP](https://github.com/salesforce/BLIP) is a model that was trained for both image-text matching and image captioning. It was trained on a 115M subset of LAION-400M. To improve the results of the generated captions we (LAION) performed over 100 experiments to determine the hyperparameters that maximize the BLEU-4 score compared to MS COCO captions. Here you can see some of our [results](http://captions.christoph-schuhmann.de/eval_b_auto/eval.html).\n\n![](https://lh4.googleusercontent.com/pLpUwcYitBPvnLqApETmU0Ik4VVYkslTyuEZBXT8VByhMjGqINSdVPLpqOR6ZcdvCilLakTXJXP40xUecLMMCEIl7CpWAN2RfAHU__OAKcIyd3_8lrsWuKeQcraU86ITbWmxM-y_)\n\n_eval_best_auto0185: An orange cat is looking at its reflection in the mirror._\n\n![](https://lh4.googleusercontent.com/QCQjVFkxZAQf24yZ49q5RLQ3ElyCIMFlOy5ACsfC7QC-CLiDDhTdCSnn5sobHFOsxLrVUIpNrk9sPyVmYDL6NB8qyQP6aSfkgxDuoWdeT3JX3j5MuJOGC9b7UgxKKnl4X9l33d0w)\n\n_eval_best_auto0190: A green highway sign with the words Queens Bronx._\n\nWe found that we can significantly improve the quality of the captions by generating 40 (or more) candidate captions for each image and then ranking them using OpenAI’s CLIP ViT-L/14 & CLIP-Resnet50x64. First we ranked all candidates with ViT-L/14 and then we ranked the top-5 results again using Resnet50x64. Preliminary results of human evaluations indicate that:\n\n1.  our evaluators gave the generated captions an average quality rating of 3,8 on a scale from 0 to 5, with a standard deviation of 0,9 ( in this particular hyperparameter configuration n= 600)\n2.  our evaluators gave original human captions from MS COCO an average quality rating of 3,9 with a standard deviation of 0,8 ( n = 2100 )\n\n—> We hypothesize that the generated captions match (& sometimes even surpass) the average quality of the human captions of MS COCO (which are sometimes also far from perfect) in most cases, but sometimes ( in less than <10% ) contain obvious mistakes, that humans would not make, because deeper kind of world knowledge & „common sense“ would be necessary in those cases.\n\n#### GLIDE\n\nClay Mullis (alias [afiaka87](https://github.com/afiaka87)) used subsets of LAON-2B to fine-tune the OpenAi [Glide](https://github.com/openai/glide-text2im) model and managed to reintroduce human generations. Samples\n\n- [https://replicate.com/afiaka87/laionide-v3](https://replicate.com/afiaka87/laionide-v3)\n- [https://wandb.ai/afiaka87/glide_compare/reports/Finetuning-GLIDE-on-Laion5B–VmlldzoxNTg3MTkz](https://wandb.ai/afiaka87/glide_compare/reports/Finetuning-GLIDE-on-Laion5B--VmlldzoxNTg3MTkz)\n- [https://wandb.ai/afiaka87/laionide-v3-glide/reports/Laionide-Version-3-Benchmark–VmlldzoxNjE0MTE3](https://wandb.ai/afiaka87/laionide-v3-glide/reports/Laionide-Version-3-Benchmark--VmlldzoxNjE0MTE3)\n\n![](https://lh5.googleusercontent.com/SjEvGWJlqpcocr0aeRj2V-ldfCJkO-RubJF-QQr6OdTgP196lqJynBEx45FRoEp3YTUfZLmYjpa9QDApqRvwd14zZSwbyEKso1i_q5wJNJXgQytb3yPVcllWGPsht4Tv52j7unM7)![](https://lh5.googleusercontent.com/BD89utsfddsrbxBCZsXEpxv_F4t2gGUrB8Hcqq5fl2aHrhsp5i5lJ5014d5T7I9DjxAT-4Q2N-cZYebumBzJWr7YwvRU0W2tjguSjF9DjTUuveBhhz__XWDuYGQh39N9D-op_1L7)![](https://lh5.googleusercontent.com/GLlZtnYViyWsxomeN0Xh7kY8IBZA9Ni2JmndUtGoSzIUT0NnXB7ru4wCNiHdFfhk1gwdc2LmbzvvNe_TTQCWe3hckDJLl7GiHzAc1S0agio3jxgv2DG3ih0WBPpQ88KQJf0jqtaN)\n\n#### Semantic search and subset extraction\n\nThe [clip-retrieval](https://rom1504.github.io/clip-retrieval/) interface allows a user to search images and texts based on a query image or text using the CLIP embeddings of the input and our precomputed kNN indices. It demonstrates the diversity of images and captions that can be found in LAION-5B as well as high semantic relevance shows the distribution of image sizes of LAION-5B. Given the abundance of high-resolution images, one can produce subsets of images for training various customized models, and also choose image resolution that is suitable for the purpose of particular training.\n\n#### CLOOB\n\nKatherine Crowson and John David Pressman recently trained a CLOOB ViT-B/16, variant of CLIP, for 32 epochs on LAION-400M and got preliminary results, that come close to the performance of OpenAI’s ViT-B/32, even though this was an early run with unoptimized hyperparameters. The checkpoints can be found here: [https://github.com/crowsonkb/cloob-training](https://github.com/crowsonkb/cloob-training)![](https://lh5.googleusercontent.com/ROxNOoa1jgaAW9JDbP2KHdDHOcTUCe-oPMvNTe2OYW_ETxbQI8W9YdA8oN93ULJ3r1Wyk7aAtm5GztJsQUZXUgw06BRiPECqp4o_bGXePp5cp9jFMBbbf2h1EYfDzvEHIcMdrcm-)\n\n(zero-shot accuracies on Imagenet-1K )\n\nWe are in touch with Andreas Fürst, one of the original CLOOB authors, and learned from him that their team is currently (at the time of writing) training a CLOOB ViT-B/32 with LAION-400M with optimized hyperparameters and very promising results so far (53% zero-shot accuracy on Imagenet after 7 epochs).\n\n## Papers citing LAION 400M\n\nAfter the release of LAION-400M, several papers used LAION-400M for image generation, text to image generation, image to text generation and text image matching:\n\n- [Vector Quantized Diffusion Model for Text-to-Image Synthesis](https://arxiv.org/abs/2111.14822.pdf) used LAION-400M to train VQ diffusion text to image generation models\n- [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752.pdf) used a subset of LAION-400M to train latent diffusion models\n- [General Facial Representation Learning in a Visual-Linguistic Manner](https://arxiv.org/abs/2112.03109.pdf) LAION-400M face subset to train a face clip\n- [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) image captioning using LAION-400M subset\n- [MAGMA – Multimodal Augmentation of Generative Models through Adapter-based Finetuning](https://arxiv.org/pdf/2112.05253.pdf) was trained on image question answering using a LAION-400M subset\n\n## Conclusion\n\nBy releasing an updated version of an openly available dataset that contains 5 billion image-text pairs, we have set new Standards for the scale of openly available datasets and enable researchers from all over the world to train state-of-the-art language-vision models like GLIDE or Turing Bletchley. As proof of concept, we demonstrated that a subset of our dataset can be used to train various CLIP-like models, producing samples of sufficient quality. This dataset extends the possibilities in multi-language large-scale training and research of language-vision models, that were previously restricted to those having access to proprietary large datasets, to the broad community.\n\n## What’s next?\n\nThis is only the beginning! Now that this huge and open dataset is released, it can be used to train many models, such as gigantic clip models, image/text generation models and much more. We have so many projects going on that it’s probably best, if you are interested, to join our Discord server and check out what’s going on. We are and always will be a grassroots community that works openly and welcomes everyone who is kind and passionate and for machine learning.\n\nJoin us in [discord](https://discord.gg/eq3cAMZtCC) and help us to train models like CLIP, BLIP, GLIDE, Dall-E, SimMIM, AudioCLIP and don’t hesitate to share your ideas for new projects with us.\n\n**Become a part of our constantly growing crowd of supporters who help us to make machine learning dreams come true!**\n\n## Credit Assignment\n\n- Christoph Schuhmann: He led this project and built POCs for most of its components including clip filtering,the safety model, the watermark model and the Blip inference tuning project.\n- Richard Vencu: System architecture and download script optimizations, GPU assisted filtering. Set up the AWS infrastructure.\n- Romain Beaumont: Guidance on scaling for the common crawl filtering pipeline. Built and ran the dataset preparation pipeline: pyspark deduplication job, img2dataset, clip inference, autofaiss, safety tags.\n- Clayton Mullis: DALLE-pytorch training/analysis, glide training, WDS filtering\n- Jenia Jitsev: scientific organization & writing, experiments planning and design, compute resource acquisition, general supervision\n- Robert Kaczmarczyk: Established WDS architecture, performed DALL-E training runs, balancing calculation, sample (NSFW, watermark, caption quality) annotation and manuscript revision\n- Andreas Köpf: He conducted the hyperparameter search for the inference strategies with the BLIP image-captioning model\n- Theo Coomber: He was one of our first contributors & build the first versions of our [worker swarm system](https://github.com/TheoCoombes/crawlingathome). Without his enthusiasm this project might never have taken off.\n- Aarush Katta: Trained the watermark model\n- Cade Gordon: Run distributed inference for the watermark tags & trained the CLIP B/32 model on JUWELS Booster\n- Ross Wightman: Ross helped Cade with the debugging & training of the CLIP-B/32 model and executed experiments on JUWELS Booster\n- Katherine Crowson and John David Pressman: Trained the CLOOB model\n- Aran Komatsuzaki: Led an image-text-pair dataset building project, which inspired this project.\n- Bokai Yu: Accomplished most of the work to make the knn index building tool autofaiss work in a distributed setting\n\n[  \n](https://laion.ai/laion-400-open-dataset/)\n","date":1648684800000},{"slug":"laion-400-open-dataset","frontmatter":{"title":"LAION-400-MILLION OPEN DATASET","author":"Christoph Schuhmann","date":"Aug 20, 2021","previewImg":"/images/blog/500m.png"},"content":"\nWe present LAION-400M: 400M English (image, text) pairs - see also our [Data Centric AI NeurIPS Workshop 2021 paper](https://arxiv.org/abs/2111.02114)\n\n## Concept and Content\n\nThe LAION-400M dataset is entirely openly, freely accessible.\n\n**WARNING**: be aware that this large-scale dataset is non-curated. It was built for research purposes to enable testing model training on larger scale for broad researcher and other interested communities, and is **not** meant for any real-world production or application.\n\nWe have filtered all images and texts in the LAION-400M dataset with OpenAI‘s [CLIP](https://openai.com/blog/clip/) by calculating the cosine similarity between the text and image embeddings and dropping those with a similarity below 0.3. The threshold of 0.3 had been determined through human evaluations and seemed to be a good heuristic for estimating semantic image-text-content matching.\n\nThe image-text-pairs have been extracted from the [Common Crawl](https://commoncrawl.org/) web data dump and are from random web pages crawled between 2014 and 2021.\n\n### Download Information\n\n###### UPDATE 16 dec 2021\n\nWhile **the eye** experiences technical difficulties, we provide an alternate download server for this dataset at this link: [laion400m at deploy.laion.ai](http://deploy.laion.ai/8f83b608504d46bb81708ec86e912220/)\n\n###### Original information\n\nYou can find\n\n- The CLIP image embeddings (NumPy files)\n- The parquet files\n- KNN index of image embeddings\n\nTo download from **the eye**, run this command\n\n`aria2c \"https://the-eye.eu/public/AI/cah/laion400m-met-release.torrent\"`\n\nYou may want to use the `–show-files` and `–select-file` options to download only some files.\n\nYou can also find the files in [laion400m-met-release](https://the-eye.eu/public/AI/cah/laion400m-met-release/)\n\nSome more significant knn indices are present in [laion400m-indexes](https://the-eye.eu/public/AI/cah/laion400m-indexes/). We advise using the 128GB ones.\n\nThe parquet files in Kaggle: [laion400m on Kaggle](https://www.kaggle.com/romainbeaumont/laion400m)\n\nAfter downloading the metadata as indicated above, you can run [this command](https://github.com/rom1504/laion-prepro/blob/main/laion400m/download_images/download_images.sh) to download the images and generate the webdataset files (command using [img2dataset](https://github.com/rom1504/img2dataset) )\n\n### LAION-400M Dataset Statistics\n\nThe LAION-400M and future even bigger ones are, in fact, datasets of datasets. For instance, we can filter it out by image sizes into smaller datasets like this:\n\n```\nNumber of unique samples 413M\nNumber with height or width >= 1024 26M\nNumber with height and width >= 1024 9.6M\nNumber with height or width >= 512 112M\nNumber with height and width >= 512 67M\nNumber with height or width >= 256 268M\nNumber with height and width >= 256 211M\n```\n\nBy using the KNN index, we can extract specialized datasets by domains of interest. They are (or will be) sufficient in size to train technical domain models.\n\nAlso, use [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) for simple visualisation of the dataset. There you can search among the dataset using CLIP and a knn index.\n\n### Disclaimer & Content Warning\n\nOur filtering protocol only removed NSFW images detected as illegal, but the dataset still has NSFW content accordingly marked in the metadata. When freely navigating through the dataset, keep in mind that it is a large-scale, **non-curated** set crawled from the internet for research purposes, such that collected links may lead to discomforting and disturbing content. Therefore, please use the demo links with **caution**. You can extract a “safe” subset by filtering out samples drawn with NSFW or via stricter CLIP filtering.\n\nThere is a certain degree of duplication because we used URL+text as deduplication criteria. The same image with the same caption may sit at different URLs, causing duplicates. The same image with other captions is not, however, considered duplicated.\n\nUsing KNN clustering should make it easy to further deduplicate by image content.\n\n### LAION-400M Open Dataset structure\n\nWe produced the dataset in several formats to address the various use cases:\n\n- a 50GB url+caption metadata dataset in parquet files. We can use the metadata to compute statistics and redownload part of the dataset\n- a 10TB webdataset with 256×256 images, captions and metadata. It is a full version of the dataset that can be used directly for training (this one is for internal use, you need to redownload images yourself due to licensing issues)\n- a 1TB set of the 400M text and image clip embeddings, useful to rebuild new knn indices\n- pairs of 16G, 32G, 64G and 128G knn indices (running in the web demo)\n\n#### URL and caption metadata dataset.\n\nWe provide 32 parquet files of size around 1GB (total 50GB) with the image URLs, the associated texts and additional metadata in the following format:\n\n> SAMPLE_ID | URL | TEXT | LICENSE | NSFW | similarity | WIDTH | HEIGHT\n\nwhere\n\n- **SAMPLE_ID**: A unique identifier\n- **LICENSE**: Where we found a Creative Commons License in the image data, we named it here like, e.g. “creativecommons.org/licenses/by-nc-sa/3.0/” – otherwise you’ll find it here a “?”\n- **NSFW**: we used CLIP to estimate if the image has NSFW content. The estimation has been pretty conservative, reducing false negatives at the cost of more false positives. Possible values are “UNLIKELY”, “UNSURE” and “NSFW”.\n- **similarity**: Value of the cosine similarity between the text and image embedding\n- WIDTH and HEIGHT: image size as the image was embedded. We downsized originals that were larger than 4K to 4K.\n\nThis metadata dataset purpose is to download the images for the whole dataset or a subset of it by supplying it to the very efficient [img2dataset](https://github.com/rom1504/img2dataset) tool.\n\n#### 10 TB webdataset with images and captions\n\nBy running the img2dataset tool, we can download a 10TB webdataset. It will resize all images at 256×256 resolution, will append the corresponding caption and will generate a collection of tar files (that dataset format is called webdataset) containing images, captions, and metadata and related parquet files containing the same metadata\n\n- 00000.tar of size 270MB containing at most 10k samples\n  - 0.jpg\n  - 0.txt containing the caption\n  - 0.json containing metadata such as the URL, the original width, the EXIF data, whether the image is NSFW\n- 00000.parquet of size 1.6MB containing the same metadata as the JSON file. Useful to compute statistics without reading all the tar files\n\nThe 400M dataset will therefore have 41455 tar and 41455 parquet files. This dataset purpose is to train multimodal models like CLIP or DALL-E.\n\n#### 1TB of clip embeddings\n\nThe clip embeddings are stored in NPY files next to parquet files in the same order. Since this dataset is much smaller than image one, each NPY file stores 1M samples. Each NPY file is 1GB, and each parquet file is 150MB. There are a total of 400 such files. The embeddings purpose is to compute statistics on the dataset, for example, using clustering or knn indices.\n\n#### Two small 6GB knn indices\n\nWe provide two 6GB knn indices built using the [autofaiss](https://github.com/criteo/autofaiss). We can use them to compute a subset of the dataset and, more generally, to search among it efficiently. See the search [web demo](https://rom1504.github.io/clip-retrieval/) of it. We can use the CLIP filter tool along with this index to produce subsets using search terms efficiently. We also provide two 16GB knn indices of higher quality.\n\n### What can we do with the LAION-400M dataset?\n\nVision and language modelling has been taking off in 2021. Here are some pointers about what this kind of image + text datasets unlocks and why it seems interesting:\n\n- Six months ago, OpenAI released two blog posts and papers, [CLIP](https://openai.com/blog/clip/) and [DALL-E](https://openai.com/blog/dall-e/). Both models rely on a large amount of (text, image) pairs. They used an unreleased 400M pairs dataset.\n  - CLIP is a model that computes how related are a text and an image. It makes it possible to build large text to image search, and it makes it possible to create that kind of crazy text to image art [clip-art](https://ml.berkeley.edu/blog/blog/clip-art/). They released a small and medium version of the model but no training code.\n  - DALL-E is a model that directly generates images from texts. As can be seen from the blog post, it achieves awe-inspiring results that could directly impact the world for anything that needs drawing and illustrations. OpenAI did not release any model, even through an API\n\nSince then, various researchers have organised several efforts to replicate DALL-E. People gathered initially around this excellent DALLE replication repository [DALLE-PyTorch](https://github.com/lucidrains/DALLE-pytorch) with some fantastic results visible in the readme. More recently, as part of huggingface events, new developments have been achieved (see [DALLE-mini report](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini--Vmlldzo4NjIxODA) ), and an online demo is now available at [DALLE-mini demo.](https://huggingface.co/spaces/flax-community/dalle-mini)\n\nThe replication effort is still far from achieving the same performance as the original DALLE, and it seems possible to go even further. Some people also want to make a better CLIP to produce even better-generated art.\n\nA large part of the results that we can achieve with such models is thanks to a large amount of data. Before LAION-400M, the largest open dataset for (image, text) pairs are in the order of 10M (see [DALLE-datasets](https://github.com/robvanvolt/DALLE-datasets) ), which is enough to train exciting models but not enough to reach the best performance. Having a public dataset with hundreds of millions of pairs will help build these image+text models.\n\n### Analysis of the LAION-400M data\n\nWe annotated 3456 samples of the dataset and got the following results:\n\n- Correct positive NSFW: 4\n- Correct negative NSFW: 3371\n- False-positive NSFW: 73\n- False-negative NSFW: 8\n- Bad captions: 3 (0.09 %)\n\nThe matching is excellent, thanks to CLIP. We could improve the NSFW automatic tagging in the future; however, the NSFW total rate is low enough (less than 1%) to make this not an issue.\n\n## Technical Details\n\nThe dataset acquisition has into two significant parts:\n\n1.  a distributed processing of the vast (many PBs) Common Crawl datasets, which produces a collection of matching URL and caption\n2.  a single node much lighter post-processing of the data that anyone can run in a few days and which produces the final dataset\n\n### 1. Distributed processing of Common Crawl\n\nWe acquire the raw web data for the creation of our dataset from Common Crawl. Common Crawl is a non-profit organisation dedicated to providing a copy of the internet to internet researchers, companies, and individuals at no cost for research and analysis. They regularly release dumps of HTML-like data parsed from billions of public websites found [on the Common Crawl website](https://commoncrawl.org/the-data/get-started/). To create image-text pairs, we parse through the data from Common Crawl and parse out all HTML IMG tags containing an [alt text attribute](https://en.wikipedia.org/wiki/Alt_attribute). Common Crawl provides its data in several formats. For our purpose, we chose to use the data in the WAT format. The WAT files contain only the metadata of the crawled sites, which includes all links and IMG tags contained in the website. Parsing only this metadata is much faster than parsing the whole HTML text (provided in the WARC format).\n\n#### Downloading original images\n\nWe download the raw images from the URLs we parsed from Common Crawl with asynchronous requests using the libraries [Trio](https://github.com/python-trio/trio) and [Asks](https://github.com/theelous3/asks). They allow us to go multithreading for a single CPU. Usually, a home internet link will be exhausted by a single or two CPUs. A data centre node can scale up benefits from guaranteed internet speed with a multiprocessing pool much faster than a single CPU node. At this time, we were able to use 50 cores with a full, secured 1Gbps connection to the public internet. This bandwidth must be available to the downloading node, not shared among many nodes or apps. We have optimised the script for speed while mitigating various errors we encountered. Usually, to satisfy a high-end demanding node such as above, we must take additional steps to provide DNS caching capabilities. We found that the knot-resolver ran with two processes and configured with caching option can solve this problem.\n\n#### Filtering out unsuitable image-text pairs\n\nAfter downloading the WAT files from Common Crawl, we filter the samples in the following steps:\n\n1.  We dropped all samples with less than five character alt text length\n2.  We dropped all samples with less than 5 KB image size\n3.  We use continuously updated bloom filters to drop samples that are already in our dataset. The bloom filters deduplicate by concatenating the URL and the alt text.\n4.  We use continuously updated bloom filters to drop samples from URLs that had timed out previously and therefore seem unreachable (or at least not reachable in an efficient way)\n5.  We use OpenAI’s CLIP model (the ‘_ViT-B-32_‘ version) to compute the image and alt text embeddings. Then we calculate the cosine similarity of both embedding vectors and drop all samples with a similarity below 0.3. We chose this threshold after trying different values and using human evaluations of how well the texts fit the images. Lower values like 0.28 or 0.29 also seemed okay in many cases, but after further inspections, we decided to choose the conservative value of 0.3.\n6.  We use the CLIP embeddings of the images to estimate if their contents contain NSFW content. We do this by calculating CLIP embeddings for a list of image categories like, e.g. “selfie”, “illustration”, or “landscape”, which also contains categories that indicate NSFW content like “porn” and “sex”.\n7.  Then we compute the cosine similarities between the embedding image we are currently filtering and each of these category keywords. If the category with the highest similarity and the keyword with the second-highest similarity belong both to NSFW keywords, we tag the sample as “NSFW”. If only one of them belongs to an NSFW keyword, we categorise the sample as “UNSURE”. If both keywords with the highest similarities are not NSFW, we tag the sample as “UNLIKELY”.\n8.  In the next step, we look at all samples with either the “NSFW” or “UNSURE” tag and drop those with any keywords in their text related to kids, teens, or other semantically related content.\n9.  In step 8, we repeat the procedure of computing the cosine similarities from step 6 with the difference that we now use category texts that indicate contents semantically related to kids and teens on a CLIP embedding level. If either the highest similarity or the second-highest similarity between a sample’s image embedding and a text of the precomputed categories belongs to a text that indicates content related to under-aged persons, we drop this sample.\n10. Finally, we repeat the procedure from step 8 with texts semantically related to animal categories like e.g. “animal”, “bird”, etc.\n\nWe perform these rigorous filtering steps for NSFW with potentially illegal content because we cannot guarantee that the contents of Common Crawl are free of such. We feel obligated to try our best to filter out such content. Inspections of samples filtered out by steps 7 to 9 have shown that our filtering procedure is very conservative and produces many false positives (samples it drops, which are not problematic). This process is okay because the number of potential samples waiting for us to crawl is vast.\n\n#### System Architecture\n\nTo orchestrate the interactions of the many crawling scripts (called _workers_) in our project, we use a server that keeps track of processed WAT files and of which worker gets which unprocessed WAT. We call this orchestrating server a _tracker_. Its functions are offering jobs to both download workers and inference workers, confirming cleanup requests from the DL staging server, maintaining ACLs for the Bloom server, and some more. We also employ several staging servers as buffers for jobs on their way to the storage location. The staging servers continuously update filters in the central bloom server where we use RedisBloom for high-performance reasons.![](https://i.imgur.com/kxl4jJe.png)\n\n#### Workflow\n\nDuring the evolution of our crawling project, we applied two different workflows:\n\n##### Workflow 1 (_“Hybrid”_ – workers)\n\nThis worker performs all computation steps during one job and then submits the result to the staging server. It then queues the results for release to the storage area.\n\n##### Workflow 2 (_“CPU – GPU – 2 stages”_ – workflow)\n\nWe soon discovered that the best way to utilise resources is to split the workload into CPU + networking tasks (downloading steps) and GPU tasks (CLIP inference steps). Hence, the 2 stage approach uses “CPU workers” to download images, create image-text pairs, and save the intermediate result to a staging server. Then “GPU workers” pick up jobs, concatenate a number of them to group around 20000 pairs per final result file. The 2 stage workflow proved to be most efficient, with speeds up to 25 million pairs added to the dataset per day when using 100 CPU workers with one core and one GPU worker employing an NVidia RTX 3090 graphic card utilising all 16 lanes of PCIe bus. The GPU node also needs about CPU 24 threads to keep up with the GPU processing capacity.\n\n#### Removing abuse alerts\n\nDuring downloading, we encountered abuse alerts from manual and automated tools that protect websites. After some learning curve, we reduced most of the issues by employing these mitigation techniques:\n\n- By far, the most efficient one was to use centralised bloom filters that eliminate requests going to the duplicate URLs over and over. Of course, the efficiency of these filters dramatically depends on how fast they are updated and used by the workers. By definition, having multiple downloading workers performing jobs in parallel makes them prone to overlap requests to the same URL even if the bloom filters are up to date at the beginning of the job.\n- Therefore the second technique significantly reduced the problem of parallel workers via randomising the jobs at the tracker server level. While executing jobs in sequence (with the oldest WAT files from 2013), we discovered that adjacent jobs were overlapping considerably. When we randomised jobs, we saw a dramatic decrease in such overlapping.\n\n#### Who ran this?\n\nWe want to thank :\n\n- the [LAION folks](https://laion.ai/#team), via so many worker nodes everywhere in the cloud\n- [the data hoarders](https://www.reddit.com/r/DataHoarder/comments/oyta8q/crawlinghome_help_build_the_worlds_largest/) Reddit community\n- [the-eye](https://the-eye.eu/) community\n- as well as all our friends and relatives that did not know what they were helping with\n\nfor running the workers to produce this vast dataset in a few months.\n\n### 2. Post-processing of the dataset\n\nOnce the distributed pipeline has run, resulting in a sizeable caption+url dataset, it’s time to package it in the best way. The objective of this second pipeline is to produce a version of the dataset that is easy to use for multimodal training. For this, we built tools that anyone can run out of a collection of caption+url. The exact command line to run is available in [cah-prepro](https://github.com/rom1504/cah-prepro) (which uses mainly [img2dataset](https://github.com/rom1504/img2dataset) and [clip-retrieval](https://github.com/rom1504/clip-retrieval) )\n\n#### Pyspark preprocessing of the CSV files\n\nAfter a fast run of a script to [download the CSV files,](https://github.com/rom1504/cah-prepro/tree/main/download_csv) the first step of this post-processing pipeline is to do deduplication by url+caption. The first pipeline does some partial deduplication using a bloom filter, but it is approximate, and some duplicates remain. Doing that pyspark post-processing also makes it possible to reduce the number of metadata files from hundred of thousands to 32 parquet files of size 1.7GB. See this [deduplication script there](https://github.com/rom1504/cah-prepro/blob/main/deduplicate/cah_stats_spark.py). Pyspark would be an excellent way to do any further filtering, and we [provide](https://github.com/rom1504/cah-prepro/blob/main/deduplicate/compute_more_stats.py) an example to compute some statistics. The resulting output is 32 parquet files containing columns such as URL, text, NSFW described at the beginning of the post.\n\n#### Img2dataset\n\nOnce this set of 50GB parquet files has is ready, we can use the [img2dataset](https://github.com/rom1504/img2dataset) tool to download, resize and store the images and captions as [webdataset](https://github.com/webdataset/webdataset). This tool can download 100M images in 20h in a single node (1Gbps 32GB of ram 16 i7 cores), so anyone can run this for the whole dataset or a smaller subset. The format this tool outputs is a collection of tar files (that dataset format is called webdataset) containing images, captions, and metadata and corresponding parquet files containing the same metadata\n\n- 00000.tar of size 270MB containing at most 10k samples\n  - 0.jpg\n  - 0.txt containing the caption\n  - 0.json containing metadata such as the URL, the original width, the EXIF data, whether the image is NSFW\n- 00000.parquet of size 1.6MB containing the same metadata as the JSON file. Useful to compute statistics without reading all the tar files\n\nThe size of the tars of 270MB is when using the options of img2dataset indicated there [download_images.sh](https://github.com/rom1504/cah-prepro/blob/main/download_images/download_images.sh) (resizing all images to 256×256 with padding for maximum file uniformity and avoid losing information). If using different options, you may have larger or smaller tar files.\n\n#### Clip retrieval and autofaiss\n\nFinally, the tar dataset aims to compute and package clip embeddings and compute a KNN index over the clip embeddings. The [clip-retrieval](https://github.com/rom1504/clip-retrieval/) tool makes it fast to compute 100M embeddings per 20h with a single 3080 GPU, so it’s possible to rerun this part on the whole dataset or a subset at a low cost. The embeddings are stored in NPY files next to parquet files in the same order. Since this dataset is much smaller than image one, each NPY file stores 1M samples. NPY files are 1GB in size, and parquet files are 150MB. There are a total of 400 such files. These embeddings help build text and an image knn index using the [autofaiss](https://github.com/criteo/autofaiss) tool, making it possible to produce a quantised index of an arbitrary file. The chosen index type is 6GB, so it’s cheap for anyone to load and run fast (10ms) queries over the whole dataset. We also generated another kind of index of size 16GB. Thanks to memory mapping, it’s also possible to load it at no ram usage. A simple [web demo](https://rom1504.github.io/clip-retrieval/) shows the results.\n\n![](https://i.imgur.com/6bEztg9.png)\n\n### License\n\nWe distribute the metadata dataset (the parquet files) under the most open [Creative Common CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) license, which poses no particular restriction. The images are under their copyright.\n\n## Contributing\n\nYou can contribute to the project to help us release the following dataset sizes at 1 billion pairs, 2 billion pairs and so on.\n\nChoose one or more methods that suit you or your company:\n\n1.  donate either [cash](https://laion.ai/laion-400-open-dataset/#) or [computing time](https://laion.ai/how-to-donate-computing-time/). We also launched a [Go Get Funding campaign](https://gogetfunding.com/help-us-build-the-worlds-largest-open-billion-scale-image-text-dataset-perfect-for-training-dall-e-clip-other-multimodal-models/).\n2.  participate in the development effort\n3.  spread the word. At best, use the dataset, get nice results and mention it in your papers\n\nUseful links:\n\n- Dataset progress [Crawling@Home Dashboard](http://crawling.at/) and [leaderboard](http://crawling.at/leaderboard)\n- Reddit [post](https://www.reddit.com/r/DataHoarder/comments/oyta8q/crawlinghome_help_build_the_worlds_largest/?utm_source=share&utm_medium=web2x&context=3)\n- DALLE-PyTorch [Discord server](https://discord.gg/mVcgxMPD7e)\n- DALLE-PyTorch [GitHub Repository](https://github.com/lucidrains/DALLE-pytorch)\n\n[  \n](https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/)\n\n### Sponsors\n\nWe made it so far due to the generosity of these donors:\n| ![](https://i.imgur.com/z6K7kSq.png) |![](https://i.imgur.com/KYvncYl.png)|![](https://i.imgur.com/y2yNLm8.png)|\n|--|--|--|\n|[doodlebot.ai](http://doodlebot.ai/)|[Gentec Data](https://gentec.ro/)|[the-eye.eu](http://the-eye.eu/)|\n","date":1629417600000}]},"__N_SSG":true}