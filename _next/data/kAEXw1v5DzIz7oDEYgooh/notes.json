{"pageProps":{"posts":[{"slug":"summaries","frontmatter":{"title":"Open Scientific Summaries at Scale: The Inference.net × LAION × Grass Initiative","author":"FULL AUTHOR LIST TBD","date":"October 28 2025","previewImg":"https://github.com/LAION-AI/laion.ai/blob/58dd33f12bf32a95b49fc6a430f9abc3cde127ae/public/images/blog/sci3.jpg"},"content":"\n\n## Abstract\n\nWe present a comprehensive approach to democratizing access to scientific knowledge through large-scale, **structured summarization** of academic literature. We retrieved and processed ~**100 million** research papers from the public internet, leveraging existing datasets from **bethgelab**, **PeS2o**, **Hugging Face**, and **Common Pile**. We designed a standardized **JSON schema** for scientific paper summaries and **post-trained two models**—**Qwen 3 14B** and **Nemotron 12B**—to produce summaries in this format.\n\nOur evaluation combines **LLM-as-a-Judge** and a **QA dataset**. Fine-tuned models achieve performance on our evals comparable to leading closed models (e.g., GPT-5, Claude 4.5). **Nemotron 12B** offers ~**2.25×** higher throughput than Qwen 3 14B, making it attractive for large-scale processing.\n\nWith this preliminary blog post, we **release a fine-tuned model and 100k paper summaries**. A live **visualization tool** at [https://laion.inference.net/](https://laion.inference.net/) demonstrates the utility of structured summaries. We plan to release structured summaries for the full **100M** paper corpus.\n\n---\n\n## Introduction\n\nAccess to scientific knowledge remains constrained by paywalls, licensing, and copyright, slowing research and education. Our **Project Alexandria** ([arXiv:2502.19413](https://arxiv.org/abs/2502.19413)) showed that it is legally and technically feasible to **extract factual knowledge** while respecting copyright via **Knowledge Units**—structured, style-agnostic representations of content.\n\nHowever, research-paper corpora vary in format and structure, making it hard to compare similar claims or retrieve knowledge efficiently. Building on Alexandria, we introduce a **pipeline** to collect, process, and summarize papers into **structured outputs** consumable by humans and AI systems alike. Our aims:\n\n* **Create** a massive, openly accessible, well-structured summary dataset of scientific literature\n* **Develop** models capable of generating **structured, factual** summaries\n* **Demonstrate** the utility of these summaries for scientific tasks\n* **Explore** decentralized computing to process at global scale\n\nThis brief outlines **methodology**, **results**, and **implications** for the scientific community—and humanity.\n\n---\n\n## Methodology\n\n### 2.1 Dataset Collection & Processing\n\nPrimary corpus: ~**100M** research papers retrieved via collaboration with **Wynd Labs** using the **Grass** network. After deduplication, we **supplemented** with:\n\n* **bethgelab**: *paper_parsed_jsons* ([dataset](https://huggingface.co/datasets/bethgelab/paper_parsed_jsons))\n* **LAION**: *COREX-18text* ([dataset](https://huggingface.co/datasets/laion/COREX-18text))\n* **Common Pile**: *PubMed* subset ([dataset](https://huggingface.co/datasets/common-pile/pubmed))\n* **LAION**: *PeS2oX-fulltext* ([dataset](https://huggingface.co/datasets/laion/Pes2oX-fulltext))\n\n**Post-training subset (110k papers)**: 40% from the retrieved corpus, **15% each** from the four sources above. Split: **100k train / 10k val**.\n**Length stats**: mean **81,334** characters, median **45,025** characters.\n\n### 2.2 Structured Summary Schema\n\nInspired by Alexandria’s **Knowledge Units**, our **JSON schema** first **classifies** content:\n\n* `SCIENTIFIC_TEXT` — complete research articles\n* `PARTIAL_SCIENTIFIC_TEXT` — partial scientific content\n* `NON_SCIENTIFIC_TEXT` — non-research content\n\nFor scientific texts, the schema extracts: **title, authors, year, field/subfield, paper type, executive summary, research context, RQs & hypotheses, methods, procedures/architectures, key results (with numbers), interpretation, contradictions/limitations, claims (with supporting/contradicting evidence), data/code availability, robustness/ablations, ethics, key figures/tables, three takeaways**. (See **Appendix A**.)\n\n### 2.3 Model Post-Training\n\nWe post-trained:\n\n* **Qwen 3 14B** (dense Transformer)\n* **Nemotron 12B** (hybrid Mamba-Transformer)\n\nTargets were **GPT-5-generated** structured reports. A strict prompt guided **classification**, then **schema-aligned extraction** (executive summary, context, methods, procedures/architectures, key results, interpretations, contradictions, claims, data/code, robustness, ethics, key visuals, and three takeaways). See **Appendix A** for prompt.\n\n### 2.4 Evaluation\n\nWe used **two complementary approaches**:\n\n1. **LLM-as-a-Judge** — Ensemble of GPT-5, Gemini 2.5 Pro, and Claude 4.5 Sonnet, rating student outputs vs. GPT-5 references on a **1–5** rubric (accuracy, completeness, structure, clarity; hallucination checks). See survey [6].\n2. **QA Dataset** — For a holdout set, we generated **5 MCQs per paper** with GPT-5 and measured models’ ability to answer **using their own generated summaries** (truncated to **10,000 chars**), providing a proxy for **factual utility** (cf. Alexandria [1]).\n\n---\n\n## Results\n\n### 3.1 LLM-as-a-Judge\n\n<p align=\"center\">\n  <img src=\"public/images/blog/sci2.jpg\" alt=\"LLM-as-a-Judge scores chart\" width=\"600\">\n</p>\n\n| Model                 | Score (1–5) |\n| --------------------- | :---------: |\n| GPT-5                 |  **4.805**  |\n| **Qwen 3 14B (FT)**   |  **4.207**  |\n| **Nemotron 12B (FT)** |  **4.095**  |\n| Gemini 2.5 Flash      |    4.052    |\n| Claude 4.5 Sonnet     |    3.521    |\n| GPT OSS 120B          |    3.273    |\n| Qwen 3 14B (Base)     |    3.015    |\n| GPT OSS 20B           |    2.903    |\n| Nemotron 12B (Base)   |    2.179    |\n\n*Figure 1.* Average LLM-as-a-Judge scores; **95% CIs via bootstrap**.\n\n### 3.2 QA Accuracy\n\n<p align=\"center\">\n  <img src=\"public/images/blog/sci.jpg\" alt=\"QA evaluation accuracy chart\" width=\"600\">\n</p>\n\n| Model                 | Accuracy (%) |\n| --------------------- | :----------: |\n| GPT-5                 |   **74.6**   |\n| **Qwen 3 14B (FT)**   |   **73.9**   |\n| Gemini 2.5 Flash      |     73.9     |\n| Claude 4.5 Sonnet     |     72.9     |\n| **Nemotron 12B (FT)** |     71.3     |\n| Nemotron 12B (Base)   |     70.1     |\n| Qwen 3 14B (Base)     |     68.3     |\n| GPT OSS 120B          |     63.9     |\n| GPT OSS 20B           |     58.8     |\n\n*Figure 2.* QA evaluation over **1,270 MCQs** (multiple-choice accuracy).\n\n### 3.3 Throughput on 8×H200 (TP=8, vLLM)\n\n| Model            | Requests/sec | Input tok/sec | Output tok/sec | Single-req tok/sec |\n| ---------------- | :----------: | :-----------: | :------------: | :----------------: |\n| **Nemotron 12B** |   **0.97**   | **16,943.69** |  **4,880.76**  |      **76.17**     |\n| **Qwen 3 14B**   |     0.43     |    7,516.54   |    2,588.30    |        39.59       |\n\n**Nemotron 12B** delivers ~**2.25×** the throughput of **Qwen 3 14B**, favoring **large-scale** runs.\n\n### 3.4 Visualization Tool\n\nExplore **100k** structured summaries (Qwen 3 14B FT outputs) at **[https://laion.inference.net/](https://laion.inference.net/)**. We compute **Qwen 3 Embedding 4B** embeddings on summaries and use **UMAP** for clustering; **cosine similarity** supports nearest-neighbor exploration.\n\n---\n## Discussion ### 4.1 Implications Structured summaries enable: * Faster **retrieval** across the literature * Better **machine reasoning** on scientific content * Improved **accessibility** where full texts are unavailable * Novel **visual analytics** for mapping scientific landscapes * **Standardized English** representations to simplify cross-domain search Fine-tuned **open** models, correctly trained and formatted, are **competitive** for this task. \n\n\n### 4.2 Decentralized Compute Processing **100M** papers is compute-intensive. The **Inference.net** **permissionless GPU network** (with **verification**) harnesses **idle global compute** at low cost, offering resilient infrastructure for science. Rough estimates: **>$5M** at current **GPT-5** pricing vs. **< $100k** via decentralized nodes and our models. \n\n\n### 4.3 Limitations * **Hallucinations** remain possible, especially for fine-grained details (Ns, effect sizes, CIs, dates, units). * **LLM-as-a-Judge** compresses multiple desiderata into one score; high scores don’t guarantee **line-by-line fidelity**. * **QA** tests whether a smaller model can use a **generated** summary—not whether every atomic claim is exact. * **Context limits** (e.g., **128k tokens**) may force **selective reading** on very long papers. * **Domain heterogeneity** can reduce recall in specialized subfields without further tuning. * **LLM-generated targets** risk **propagating upstream biases**. **Appropriate use:** Treat summaries as **high-quality overviews** for search/triage/review—not as substitutes for the source in **high-stakes** contexts. Verify numbers, dates, and terms in the original paper when precision is critical. \n\n\n### 4.4 Outlook & Future Work * **Scale to 100M summaries + metadata**: Join each summary to **OpenAlex** metadata (authors, venues, concepts, references, citations) for **graph-native** exploration at scale ([https://docs.openalex.org/](https://docs.openalex.org/)). * **Release permissive full texts + summaries**: For permissively licensed papers (e.g., **PeS2o**, **Common Pile PubMed**), pair **full text** with structured summaries to support **long-context** training and **grounded retrieval**. * **From summaries to Knowledge Units**: Iteratively convert summaries into **Alexandria-style Knowledge Units** ([arXiv:2502.19413](https://arxiv.org/abs/2502.19413)) to create a **shareable factual substrate** suited for open dissemination. (More compute-intensive; we will prioritize scaled summaries + metadata first.) --- \n\n\n## Conclusion Open, large-scale **structured summarization** can significantly **accelerate** scientific discovery and education. **Nemotron 12B** provides **superior throughput** for at-scale processing; our **fine-tuned models** and **released datasets** show that open approaches can be both **practical** and **competitive**. Our **visualizer** demonstrates real applications of structured summaries, and collaboration with **Inference.net** highlights how **decentralized compute** can tackle the processing challenges ahead. **Call to action:** We invite **researchers, librarians, and open-access advocates** to help us **gather more papers** for large-scale knowledge extraction. We also invite **engineers and compute providers** to help **optimize** our paragraph-level pipeline and **contribute GPU capacity** (decentralized nodes, clusters, credits) so we can run inference over the **full corpus** and convert it into **Alexandria-style Knowledge Units**—**freeing factual scientific knowledge** for education and accelerated research. \n\n--- \n\n## Acknowledgments This is a collaboration between **LAION**, **Grass**, and **Inference.net**. We thank all contributors, especially **Tawsif Ratul** for data collection, and **Prof. Sören Auer**, **Dr. Gollam Rabby**, and the **TIB – Leibniz Information Centre for Science and Technology** for scientific advice and support. --- ## References 1. **Alexandria Project** (2023). *Democratizing access to scientific knowledge.* [https://projects.laion.ai/project-alexandria/](https://projects.laion.ai/project-alexandria/) 2. **bethgelab Paper Dataset** (2024). [https://huggingface.co/datasets/bethgelab/paper_parsed_jsons](https://huggingface.co/datasets/bethgelab/paper_parsed_jsons) 3. **LAION COREX-18text** (2024). [https://huggingface.co/datasets/laion/COREX-18text](https://huggingface.co/datasets/laion/COREX-18text) 4. **Common Pile PubMed** (2024). [https://huggingface.co/datasets/common-pile/pubmed](https://huggingface.co/datasets/common-pile/pubmed) 5. **LAION PeS2oX-fulltext** (2024). [https://huggingface.co/datasets/laion/Pes2oX-fulltext](https://huggingface.co/datasets/laion/Pes2oX-fulltext) 6. **A Survey on LLM-as-a-Judge** (2025). [https://arxiv.org/abs/2411.15594](https://arxiv.org/abs/2411.15594) 7. **Inference.net Paper Visualizer** (2025). [https://laion.inference.net/](https://laion.inference.net/) 8. **Qwen 3 Embedding 4B** (2025). [https://huggingface.co/Qwen/Qwen3-Embedding-4B](https://huggingface.co/Qwen/Qwen3-Embedding-4B)\n\n---\n\n## Appendix A — Implementation Details\n\n### A.1 LLM-as-a-Judge Prompt\n\n```text\nYou are an expert judge evaluating the quality of AI-generated summarizations of scientific research articles...\n[truncated for brevity – keep your full prompt here as in the original]\n```\n\n### A.2 JSON Schema (Article Response)\n\n```json\n{\n  \"name\": \"article_response\",\n  \"schema\": {\n    \"$defs\": {\n      \"...\": \"Keep your full schema here exactly as in the original post\"\n    }\n  }\n}\n```\n\n","date":1761609600000},{"slug":"rook","frontmatter":{"title":"ROOK: Reasoning Over Organized Knowledge","author":"Jonathan Rahn, Jenia Jitsev & Qi Sun","date":"January 20 2025","previewImg":"/images/blog/laion-blue.png"},"content":"\nThe field of artificial intelligence has long used strategic reasoning tasks as benchmarks for measuring and advancing AI capabilities. Chess, with its intricate rules and vast decision space, stands out as a particularly challenging domain. The game's complexity stems from its branching factor—the number of possible moves at each turn — which averages around 35, creating a search space that quickly becomes intractable for brute-force approaches.\n\nToday, we're excited to introduce ROOK (Reasoning Over Organized Knowledge), a suite of innovative language models that push the boundaries of strategic reasoning in chess. Our approach diverges from traditional chess engines that rely heavily on search algorithms. Instead, we explore the potential of language models to capture chess knowledge and reasoning processes in a more human-like manner.\n\nOur project comprises three chess-playing transformer models:\n\n1. ROOK-CLF  \n2. ROOK-LM  \n3. RookWorld-LM\n\n|  | Experts | Step \\#1 | Step \\#2 | Step \\#3 |\n| :---- | :---: | :---: | :---: | :---: |\n| **Policy** | Stockfish 16.1 | ROOK-CLF | ROOK-LM | RookWorld-LM |\n| **Environment** | python-chess | python-chess | python-chess |  |\n\n**Figure 1**: Overview\n\nIn this blog post, we'll introduce each of these models, explore their capabilities, methodologies, and implications for AI research. For those interested in the technical details and implementation, we encourage you to take a look at our public GitHub repositories.\n\n## 2\\. ROOK-CLF: Replicating State-of-the-Art Chess Policy\n\nOur journey began with ROOK-CLF, an experiment aimed at reproducing a key result from the Google DeepMind (GDM) paper \"Grandmaster-Level Chess Without Search\" ([Ruoss et al., 2024](https://arxiv.org/abs/2402.04494)). This paper set a new state-of-the-art for chess policies without search, and replicating its results serves as an important validation of their findings and our own methodologies. They published parts of their training code (in JAX) and their datasets ([link](https://github.com/google-deepmind/searchless_chess)) as ChessBench.\n\nWe focused on a small ablation study from the paper, training a 9 million parameter model ROOK-CLF-9M based on the LLaMA architecture with a classification head. Mirroring the approach detailed in section B.5 and Figure A5 of the paper, key aspects of our implementation include:\n\n* A tokenizer with a vocabulary of 32 characters  \n* A fixed context length of 78 tokens  \n* An output space of 1968 classes, representing all possible chess moves in Universal Chess Interface (UCI) notation\n\nWe trained on the same 40 million data samples used in the GDM paper, employing a behavior cloning setup. Our results were encouraging, with ROOK-CLF-9M showing:\n\n* 49% accuracy in predicting correct actions from the GDM dataset after 195,000 training steps (compared to approximately 55% reported in Figure A6 of the GDM paper)  \n* 57% accuracy on the [BIG-bench Checkmate-in-One](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/checkmate_in_one) benchmark (after converting the positions from Standard Algebraic Notation to Forsyth–Edwards Notation).\n\nThese results, while not quite matching those reported in the original paper, demonstrate the replicability of the findings and provide a solid foundation for our subsequent experiments.\n\n## 3\\. ROOK-LM: Incorporating Chain-of-Thought Reasoning\n\nBuilding on the success of ROOK-CLF, we next developed ROOK-LM, inspired by recent work on pre-training language models on Chain-of-Thought data for reasoning ([Ye et al., 2024](https://arxiv.org/abs/2407.20311)). This transition marked a shift from a pure classification approach to a more flexible language modeling paradigm, allowing us to explore the generation of reasoning traces — intermediate step-by-step thought processes that lead to a final decision.\n\nTo facilitate this, we created a novel dataset rook-40m of up to 40 million chess positions in Forsyth–Edwards Notation (FEN), a standard method for describing chess positions. Each position was extracted from chess games played on Lichess in 2022, by players with ELO over 2000 and annotated with:\n\n* The top five candidate moves  \n* The evaluation score for each candidate move  \n* The best move\n\nThese annotations were generated using Stockfish 16.1, one of the strongest chess engines available, running on the [Tsubame 4.0 Supercomputer](https://www.hpcwire.com/off-the-wire/tokyo-techs-tsubame4-0-supercomputer-now-operational/) at Tokyo Institute of Technology.\n\nHere's an example of our data format:\n\n| Example Data | P: | 6k1/7p/ 4P1q1 /1pb1Q2p/ 2p1b3 /2P4P/ PP4P1 /R6K w \\- \\- 9 38 | M: | e5g5 a1g1 e5b8 e5h2 e5e4 | E: | \\-999.97 \\-2.97 \\-1.63 \\-6.59 \\-5.95 | B: | e5b8 |\n| :---- | ----- | :---- | ----- | :---- | :---- | :---- | :---- | ----- |\n| Field Explanation | Prefix | State ([FEN](https://en.wikipedia.org/wiki/Forsyth%E2%80%93Edwards_Notation)) \\+ padding | Delimiter | Top 5 Moves, shuffled \\+ padding | Delimiter | Top 5 Moves Eval \\+ padding | Delimiter | Best Move |\n| Inference | Prompt |  | Generated Chain-of-Thought Tokens |  |  |  |  | Action |\n\n**Figure 2**: rook-40m data representation.\n\nThis rich dataset allowed us to train ROOK-LM-124M \\-  a 124 million parameter GPT-2 model for autoregressive text generation using the [karpathy/LLM.c](https://github.com/karpathy/llm.c) library, which leverages the original GPT2 tokenizer. We obtain 6 billion tokens from the whole rook-40m dataset.\n\nCompared to ROOK-CLF-9M, ROOK-LM-124M increases model scale, adds more metadata for the moves in the chess parties using external data generator (Stockfish 16.1), and changes learning loss type:\n\n* Over 13 times more parameters (124M vs. 9M)  \n* A more information-rich dataset (including top 5 moves and their evaluations, see also the discussion of Predictor-targets in sections 3.4 and B.5 of Ruoss et al.)  \n* A shift from purely supervised classification loss based on move class labels to self-supervised next token prediction with autoregressive modeling\n\nDue to more generic loss nature compared to narrow supervised classification, we observe ROOK-LM-124M strongly underperforming ROOK-CLF-9M on benchmarks related to best move execution:\n\n* 22.2% action accuracy on a validation split of the rook-40m dataset  \n* 24.4% accuracy on the Checkmate-in-One benchmark\n\nThis drop in performance is not unexpected. While ROOK-CLF was optimized solely for best move prediction via supervised classification and can only solve this narrow task, ROOK-LM learns to predict and generate all aspects of our data, including the reasoning process behind move execution. Further, the scales of our experiments are still very small.The advantage of specialized models over generically  pre-trained ones is often observed at smaller scales on which we operated in our experiments, where generic pre-training procedure, being more flexible and scalable, results usually in more powerful models also for specialized tasks when further increasing scales, and the diversity of the pretraining data (eg incorporating other board games apart from chess). Testing this is the subject of our future work.\n\n## 4\\. RookWorld-LM: Unifying Policy and World Model\n\nOur final model, RookWorld-LM, represents a further step towards a model capable of representing full game state and reasoning about it. Inspired by recent research on using language models as world models ([Li et al., 2023](https://openreview.net/forum?id=DeG07_TcZvT), [Wang et al., 2024](https://arxiv.org/abs/2406.06485)), we recognized that a truly comprehensive chess AI system should not only decide on moves but also understand the consequences of those moves — effectively modeling the entire game environment.\n\nWe began by creating a new dataset arbiter-6m (inspired by the interface design of [Gymnasium](https://github.com/Farama-Foundation/Gymnasium)) and trained a separate GPT-2 model to act as an environment simulator (dubbed “Arbiter”). Given a chess position (state) and a move (action), this model generates:\n\n* The resulting position (observation)  \n* A score-signal for reinforcement learning (reward)  \n* Whether the game ended and if the move was legal (termination, truncation)\n\n| Example Data | 5R2/6R1/8/ 3P4/p7/ 1b2R2P/ 2p3P1 /6K1 b \\- \\- 0 58 | b3d5 | e1e3 a2b3 f7f2 b5b4 g5g7 b4c3 f2f8 c3c2 d4d5 b3d5 | 5R2/6R1/ 8/3b4/ p7/4R2P/ 2p3P1/ 6K1 w \\- \\- 0 59 | 0.001 | 0 | 0 |\n| :---- | ----- | :---- | :---- | ----- | :---- | :---- | :---- |\n| Field Explanation | Last State ([FEN](https://en.wikipedia.org/wiki/Forsyth%E2%80%93Edwards_Notation)) | Action | Action History (maxlen 10\\) for [repetitions](https://en.wikipedia.org/wiki/Threefold_repetition) | Observation (new State) | Reward (-1 loss or illegal, 0.5 draw, 1 win, 0.001 valid) | Termination (bool, True if game ends by [WLD](https://en.wikipedia.org/wiki/Chess_scoring)) | Truncation (bool, True if game ends by illegal action) |\n| Inference | Prompt |  |  | Generated Environment Update |  |  |  |\n\n**Figure 3**: arbiter-6m chess environment data representation. All fields concatenated into a string, delimited by “+”.\n\n| Source Datasets & Splits | Combined Dataset | Combined Dataset Splits |\n| :---: | :---: | :---: |\n| rook-40m Valid (10k Samples) | \\- | \\- |\n| rook-40m Train (40m Samples) | rookworld-46m | rookworld-46m Train (46m Samples) |\n| arbiter-6m Train (6m Samples) |  | rookworld-46m Valid (15k Samples) |\n| arbiter-6m Valid (10k Samples) | \\- | \\- |\n\n**Table 1:** Dataset composition\n\nThis environment model achieved promising results:\n\n* 92% accuracy in generating correct observations on the validation set of arbiter-6m  \n* 99.8% normalized Levenshtein similarity on generated observations on the validation set of arbiter-6m\n\nEncouraged by these results, we then trained a unified GPT-2 model using multi-task learning. This model, which we call RookWorld-LM, can perform both the policy task (deciding on moves) and the world-modeling task (simulating game outcomes) based on a prompt prefix. Interleaving the rook-40m and arbiter-6m datasets to rookworld-46m, we obtained 6.9 billion tokens for training RookWorld-LM.\n\n![Figure 4](/images/blog/rook-1.png)  \n**Figure 4**: RookWorld-LM Self-Play Loop\n\nThe results of this unified model significantly improved upon the results of the separated models:\n\n* 26.2% action accuracy on a validation split of the rook-40m dataset  \n* 32.1% accuracy on the Checkmate-in-One benchmark  \n* 99.9% accuracy on generated observations on the arbiter-6m validation set\n\nNotably, RookWorld-LM's performance on the Checkmate-in-One benchmark surpasses that of ChessGPT-Base ([Feng et al., 2023](https://arxiv.org/pdf/2306.09200)), a fine-tune of the Red Pajama 3b language model on a wide variety of chess-content, which achieved 26.5% accuracy.\n\nRookWorld-LM represents, to our knowledge, one of the strongest chess policies without search implemented as an autoregressive decoder-transformer model to date. Its ability to self-play extended sequences of legal chess moves, combining policy decisions with environment simulation, opens up exciting new possibilities in AI research. You can try it yourself in this [Colab Notebook](https://colab.research.google.com/drive/1YDzcGWotOYNaIBc6iVLcCCK3O-16W8_M) and this [Hugging Face Space](https://huggingface.co/spaces/jrahn/RookWorld).\n\n## 5\\. Implications and Future Directions\n\nThe ROOK project has several significant implications for AI research:\n\n1. **Strategic Reasoning**: Our models confirm previous evidence ([Li et al., 2023](https://openreview.net/forum?id=DeG07_TcZvT)) that language models can learn complex strategic reasoning tasks without explicit coding of domain rules. This suggests potential applications far beyond chess, in areas requiring sophisticated decision-making and planning.  \n2. **World Modeling**: RookWorld-LM's success in accurately simulating the chess environment points to the potential of language models in modeling complex, rule-based systems. This could lead to AI systems capable of reasoning about hypothetical scenarios and even judging the validity of their own outputs.  \n3. **Unified Architectures**: The performance improvements seen in RookWorld-LM highlight the benefits of multi-task learning in creating versatile, multi-purpose AI systems. This approach could be extended to other domains, potentially leading to more general and capable AI systems.\n\nAs we release our research, including code, data, and evaluations, **we invite collaboration from the community**. Some promising directions for future research include:\n\n* **Scaling**: Exploring the limits of language models for chess policies without search by scaling up model parameters, dataset size, and training duration. RookWorld-LM was trained on only 2x RTX 4090 GPUs and every increase in compute translated into improvements on benchmark performance (see Table 1). This could involve modifying the vocabulary of the GPT2-tokenizer to more efficiently capture chess-specific patterns.  \n* **Self-Improvement**: Investigating whether RookWorld-LM can improve its play through fine-tuning on filtered self-play games, keeping only the moves from winning games.  \n* **Multi-Task Learning**: Extending our multi-task approach to incorporate additional synthetically generated data from other strategic games or reasoning tasks (see [Zhang et al., 2024](https://www.arxiv.org/abs/2410.02536)).  \n* **Transfer Learning**: Exploring whether the strategic reasoning capabilities learned in the chess domain can transfer to language tasks or other problem-solving domains.\n\nWe're excited about the potential of this work and look forward to seeing how the AI research community builds upon and extends these ideas. The intersection of strategic games, language models, and reasoning systems remains a fertile ground for advancing our understanding of artificial intelligence.\n\n#### **Contributions:**\n\n**Jonathan Rahn:** led the project, designed experiments, executed experiments, developed the code bases, small scale dataset generation, analysis of results and writing  \n**Qi Sun:** co-design experiments and large scale dataset generation  \n**Jenia Jitsev:** scientific supervision and advising\n\n# Appendix\n\nOpen-source releases:\n\n* Code:  \n  * ROOK-CLF code (HF Transformers): [jorahn/rook on GitHub](https://github.com/jorahn/rook)\n  * ROOK-LM and RookWorld-LM code (LLM.c): [jorahn/RookWorld on GitHub](https://github.com/jorahn/RookWorld)  \n* Datasets:  \n  * ROOK-40m policy dataset: [lfsm/rook-40m on Hugging Face Datasets](https://huggingface.co/datasets/lfsm/rook-40m)  \n  * Arbiter-6m environment dataset: [jrahn/arbiter\\_6m on Hugging Face Datasets](https://huggingface.co/datasets/jrahn/arbiter_6m)  \n* Trained Models:  \n  * ROOK-CLF-9M trained model: [jrahn/ROOK-CLF-9m on Hugging Face](https://huggingface.co/jrahn/ROOK-CLF-9m)\n  * ROOK-LM-124M trained model: [jrahn/ROOK-LM-124m on Hugging Face](https://huggingface.co/jrahn/ROOK-LM-124m)\n  * RookWorld-LM-124M trained model: [jrahn/RookWorld-LM-124M on Hugging Face](https://huggingface.co/jrahn/RookWorld-LM-124M)  \n* Inference Demos:  \n  * Play RookWorld HF Space: [jrahn/RookWorld on Hugging Face Spaces](https://huggingface.co/spaces/jrahn/RookWorld)\n  * Inference ROOK-CLF, ROOK-LM, RookWorld-LM Colab: [Colab Notebook](https://colab.research.google.com/drive/1YDzcGWotOYNaIBc6iVLcCCK3O-16W8_M)\n\n| Model | Dataset (Samples) | Steps (Epochs) | Action Accuracy | Checkmate in One Accuracy |\n| :---: | :---: | :---: | :---: | :---: |\n| ROOK-CLF (9M) | GDM 40M | 195,000 (5) | 49% | 57% |\n| Ruoss et al., 2024 (9M, BC) | GDM 40M |  | \\~55% |  |\n| Ruoss er al., 2024 (270M, AV) | GDM 15.3B |  | 69.4% |  |\n| ROOK-LM (124M) | ROOK 20k | 5,000  (2) | 0.6% | 0.0% |\n| ROOK-LM (124M) | rook-260k | 18,752  (1) | 3.8% | 4.7% |\n| ROOK-LM (124M) | rook-709k | 51,481  (1) | 7.4% | 4.8% |\n| ROOK-LM (124M) | rook-709k | 102,962  (2) | 7.8% | 5.5% |\n| ROOK-LM (124M) | rook-709k | 154,443  (3) | 8.8% | 7.0% |\n| ROOK-LM (124M) | rook-5m | 11,646  (1) | 12.0% | 9.0% |\n| ROOK-LM (124M) | rook-5m | 34,932  (3) | 13.4% | 11.5% |\n| ROOK-LM (124M) | rook-40m | 278,154 (3) | 22.2% | 24.4% |\n| RookWorld-LM (124M) | rookworld-7m | 47,203 (3) | 16.6% | 13.7% |\n| **RookWorld-LM (124M)** | **rookworld-46m** | **529,400 (5)** | **26.2%** | **32.1%** |\n| Feng et al., 2023 ChessGPT-Base (3B, w/o suffix, MC) | 28.1M documents |  |  | 13.6% |\n| Feng et al., 2023 ChessGPT-Base (3B, w/o suffix, ESM) | 28.1M documents |  |  | 26.5% |\n\n**Table 2**: Results overview and dataset scaling\n\n## Related Work\n\nDirect Inspirations:\n\n* 2022 [YoloChess \\- a Hugging Face Space by jrahn](https://huggingface.co/spaces/jrahn/yolochess) behavior cloning from human experts with DeBERTa v2\n* 2023 [Strategic Game Datasets for Enhancing AI Planning: An Invitation for Collaborative Research | LAION](https://laion.ai/blog/strategic-game-dataset/) LAION & Qi Sun  \n* 2024 [GitHub \\- karpathy/llm.c: LLM training in simple, raw C/CUDA](https://github.com/karpathy/llm.c)  \n* 2024 [Physics of Language Models \\- Part 2.1, Hidden Reasoning Process](https://physics.allen-zhu.com/part-2-grade-school-math/part-2-1) & [\\[2407.20311\\] Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process](https://arxiv.org/abs/2407.20311) \\- CoT training data\n\nRelated Work:\n\n* Chess-playing AI systems  \n  * [\\[1712.01815\\] Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815)  \n  * [official-stockfish/Stockfish](https://github.com/official-stockfish/Stockfish/blob/master/CITATION.cff)\n  * [\\[2402.04494\\] Grandmaster-Level Chess Without Search](https://arxiv.org/abs/2402.04494)  \n  * [A Very Unlikely Chess Game | Slate Star Codex](https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/)  \n  * [\\[2306.09200\\] ChessGPT: Bridging Policy Learning and Language Modeling](https://arxiv.org/abs/2306.09200)  \n  * [\\[2403.15498\\] Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models](https://arxiv.org/abs/2403.15498)  \n  * [\\[2008.04057\\] The Chess Transformer: Mastering Play using Generative Language Models](https://arxiv.org/abs/2008.04057)  \n* Language models in game environments  \n  * [\\[2406.06485\\] Can Language Models Serve as Text-Based World Simulators?](https://arxiv.org/abs/2406.06485)  \n  * [\\[2402.08078\\] Large Language Models as Agents in Two-Player Games](https://arxiv.org/abs/2402.08078)\n  * [\\[2404.02039\\] A Survey on Large Language Model-Based Game Agents](https://arxiv.org/abs/2404.02039)  \n  * [\\[2403.10249\\] A Survey on Game Playing Agents and Large Models: Methods, Applications, and Challenges](https://arxiv.org/abs/2403.10249)  \n* World models and environment simulation  \n  * [\\[1803.10122\\] World Models](https://arxiv.org/abs/1803.10122)  \n  * [\\[2406.03689\\] Evaluating the World Model Implicit in a Generative Model](https://arxiv.org/abs/2406.03689)  \n  * [\\[2408.14837\\] Diffusion Models Are Real-Time Game Engines](https://arxiv.org/abs/2408.14837)  \n  * [\\[2407.02466\\] PWM: Policy Learning with Large World Models](https://arxiv.org/abs/2407.02466)\n\n## Public Code & Datasets\n\n* Data generation  \n  * [https://github.com/jorahn/rook/tree/main/dev/data](https://github.com/jorahn/rook/tree/main/dev/data)\n* Training  \n  * [https://github.com/jorahn/rook/tree/main/scripts](https://github.com/jorahn/rook/tree/main/scripts)\n* Evaluation  \n  * [https://github.com/jorahn/rook/tree/main/dev/eval](https://github.com/jorahn/rook/tree/main/dev/eval)\n* Datasets  \n  * [lfsm/rook-5m · Datasets at Hugging Face](https://huggingface.co/datasets/lfsm/rook-5m)\n  * [jrahn/arbiter\\_2m · Datasets at Hugging Face](https://huggingface.co/datasets/jrahn/arbiter_2m)\n  * [jrahn/rookworld\\_7m · Datasets at Hugging Face](https://huggingface.co/datasets/jrahn/rookworld_7m)\n* Models  \n  * [https://huggingface.co/jrahn/rook\\_5m\\_3e\\_gpt2\\_124M\\_hf](https://huggingface.co/jrahn/rook_5m_3e_gpt2_124M_hf)\n  * [https://huggingface.co/jrahn/arbitersim\\_2m\\_3e\\_gpt2\\_124M\\_hf](https://huggingface.co/jrahn/arbitersim_2m_3e_gpt2_124M_hf)\n  * [https://huggingface.co/jrahn/rookworld\\_7m\\_3e\\_gpt2\\_124M\\_hf](https://huggingface.co/jrahn/rookworld_7m_3e_gpt2_124M_hf)\n* Space  \n  * Selfplay [https://huggingface.co/spaces/jrahn/RookWorld](https://huggingface.co/spaces/jrahn/RookWorld)\n* YouTube Presentation (23 mins) [https://www.youtube.com/watch?v=5SkJQBrYY\\_g](https://www.youtube.com/watch?v=5SkJQBrYY_g)\n","date":1737331200000},{"slug":"laion-debate","frontmatter":{"title":"LAION-Debate: dataset of competitive debates and discussions","author":"LAION","date":"June 28, 2024","previewImg":"/images/blog/laion-debate.png"},"content":"\nWe’re pleased to announce the World's first Large Competitive Debate Dataset: LAION-Debate. LAION-Debate is a large Competitive debate dataset providing links to Competitive Debate Championships, discussions and prominent speakers intake and conversations posted on YouTube by University of Cambridge and University of Oxford through their Cambridge and Oxford Union Debate clubs on their affiliated channels.\n\nCompetitive Debate datasets are scarce and hard to find in the public domain. Because these datasets are either gated by individuals and institutions who generate them or not archived properly enough to form them into a dataset. Hindering the ability to use them for Artificial Intelligence research.\n\nIn an era, where datasets are being scarce and the large AI models are exhausting entire human knowledge and depleting known data sources, Debate 2B encourages to use alternative credible sources and other forms of knowledge corpus that provides a unique outlook and understanding than the mainstream.\n\nToday, a community member of LAION (tawsif) released this novel dataset on Competitive Debate in the field of Natural Language Processing.\n\n## What’s Competitive Debate?\n\nCompetitive Debate is a sport where speakers of widely different backgrounds engage in discussions on relevant motions (subject matter). Subject matters include but are not limited to Philosophy, Politics, Historical Debate, Logical fallacy, morality and ethics, Science and Technology.\n\nSpeakers engage into these discussions from two sides, one in support of the subject matter and another against the subject matter and use speculative language, tone, logical traps, well-constructed sentences to reflect their intent and other strategies to convince the judge and audience for their school of thought.\n\nBoth sides of the spectrum include knowledgeable speakers well-versed in the subject matter and eloquent in their words and then engage into these discussions to convince the judges and audience their school of thought to be justified. In this sport, most knowledgeable and convincing speakers end up winning rather than those stating facts.\n\nIt’s a sport where logic and art of speech meet together in perfect harmony.\n\n## Characteristic of Debate 2B\n\nDebate 2B is largely a collection of YouTube links pointing towards the championship and discussion videos posted by University of Oxford and University of Cambridge on their official affiliated channels. Most of these speeches are either British Parliamentary speeches or interviews taken by aforementioned universities’ students of prominent and significant characters.\n\nAlthough these interviews conducted at both the Oxford Union and Cambridge Union are widely different from what we public view on Sky News and CNN. Because these interviews are conducted by individuals well-versed in the art of speech while having a neutral opinion whilst conducting the interviews. Making sure relevant questions are being addressed and most truest opinions are extracted from the interviewee without any intent of sensationalising the opinions expressed by the interviewee.\n\n## Intent fields and research routes\n\nDebate 2B is intended to represent Natural language processing as the primary field. Although, we understand it can be used in the context of Computer Vision and Reinforcement learning too.\n\nDebate 2B provides two datasets captured into one. Audio and textual form datasets. Audio datasets can be used to fine-tune large pretrained audio generation models to generate audio that sounds logical and emotional. Because these speakers used emotions and logical tone to convey their message and convince their audience of their school of thought.\n\nSimilarly, textual datasets provide an in-depth outlook into a new form of text generation datasets. That is backed by facts and how these facts and sentences should be structured to provide logical reasoning. We believe Debate 2B is the first dataset able to provide logical reasoning built-in within the dataset.\n\n**Note**: We don’t provide the textual form of this dataset yet.\n\n## Metadata and info of Debate 2B\n\nWe provide links to 2,700 hours of audio recordings; which accounts for 130GB for highest bitrate and 40GB for lowest possible bitrate for these recordings.\n\nCambridge Union links dates between 19th May, 2011 - 2nd of June, 2024\nOxford Union links between 6th of September - 12th of July, 2024\n\n## Licence\n\nIt is hosted under Apache 2.0.\n\n## Downloading the dataset\n\nDebate 2B links can be found on Hugging Face. Its access is gated and only academic and work emails are being allowed at the moment to ensure safety. Audio recordings of Debate 2B can be found on Kaggle.\n\n<https://huggingface.co/datasets/sleeping-ai/LAION-Debate>\n<https://www.kaggle.com/datasets/sleepingcat4/cambridge-2b>\n<https://www.kaggle.com/datasets/sleepingcat4/oxford-2b>\n\n## Acknowledgement\n\nWe acknowledge our LAION community member tawsif who created the dataset and made its audio recordings and links to the audio recordings public.\n\n<https://github.com/sleepingcat4>\nEmail: <tawsif.ahmed@science.ru.nl>\n","date":1719532800000},{"slug":"open-gpt-4-o","frontmatter":{"title":"Call to Build Open Multi-Modal Models for Personal Assistants","author":"Christoph Schuhmann","date":"May 29, 2024","previewImg":"/images/blog/gpt-4-omni.webp"},"content":"\nTechnologies like the recently introduced GPT-4-OMNI from OpenAI show again the potential which strong multi-modal models might have to positively transform many aspects of our lives. A particularly impressive example of this is in the field of education. Imagine every person in the world having their own personal learning assistant that acts like a attentive, caring, patient, and empathetic tutor. The demo from OpenAI last Monday showed that such a vision of the future is not too far off and is within reach.\n\n## The Path to Open Multi-Modal Models\n\nAn important milestone on this path could be training an open-source model with capabilities similar to GPT-4-OMNI. The first step would be to fine-tune an existing large language model so that it can natively understand and process audio in the same way large language models currently handle text. Simultaneously, this model should be able to generate audio natively, just as it can currently output and manipulate text.\n\nThis approach had been shown to work in the [AudioPalm paper](https://arxiv.org/abs/2306.12925):\n\n![Audio Palm Pipeline](/images/blog/gpt-4-omni-1.png)\n\nA promising approach to achieving this is converting audio signals into discrete tokens using codecs like SNAC. SNAC allows audio signals to be converted into about 80 tokens per second, enabling the language to be reconstructed in very high quality. For music, sound effects, and other general-purpose audio, other versions of SNAC demand around 200 tokens per second, enabling detailed understanding and generation of these domains. As a proof of concept, the initial goal would be to tune a large language model to process both text and audio tokens, with the 24kHz version of SNAC optimized for speech being a good starting point.\n\nSNAC (Multi-Scale Neural Audio Codec) compresses audio into discrete codes at a low bitrate, setting itself apart from other codecs like SoundStream, EnCodec, and DAC through its hierarchical token structure. This structure samples coarse tokens less frequently, covering a broader time span, which saves on bitrate and is particularly useful for language modeling approaches to audio generation.\n\n![Audio Palm Pipeline](/images/blog/gpt-4-omni-2.png)\n\n For instance, with coarse tokens of ~10 Hz and a context window of 2048, SNAC can effectively model the consistent structure of an audio track for up to three minutes. SNAC offers different types of codecs optimized for specific use cases: the 24 kHz version is tailored for speech, while the 32 kHz and 44 kHz versions are designed for general-purpose audio, including music and sound effects. This versatility and efficiency make SNAC an advantageous choice for integrating audio processing capabilities into large language models.\n\nAdditionally, SNAC can flatten its hierarchical structure segment-wise for each coarse token, allowing segments of approximately ~100 ms to be decoded individually and later reassembled. This depth-first flattening method facilitates low-latency streaming, making it possible to stream high-quality audio in near real-time ( [Tutorial](https://youtu.be/NwZufAJxmMA?si=WVA2H05m3xypRncc) ).\n\n![Audio Palm Pipeline](/images/blog/gpt-4-omni-3.png)\n\nNotebooks about how to use SNAC:\n\n| SNAC Tokenization |\n| --- |\n| [24kHz Speech Version](https://colab.research.google.com/drive/11qUfQLdH8JBKwkZIJ3KWUsBKtZAiSnhm?usp=sharing) |\n| [32kHz General Purpose Version](https://colab.research.google.com/drive/1g1H0bBWRhKzHutCJZNxtavpRamw1uaXr#scrollTo=pBiT7Jx6rxmm) |\n\nTo advance research in this area, we have converted the [parler-tts/mls-eng-10k-tags_tagged_10k_generated dataset](https://huggingface.co/datasets/blanchon/snac_llm_parler_tts) into 24kHz SNAC tokens.\n\n## SNAC Tokenized Dataset\n\nWe call upon the community to experiment with pretraining large language models using these tokens. The first step would be to get an existing open-weights model like Llama, Mistral, Dbrx, Qwen, StableLM 2 or Phi-3  to generate SNAC tokens from text transcriptions and descriptions, functioning like a text-to-speech model. Once this works well, the next step would be training the model to see various text data simultaneously, retaining its text generation and understanding capabilities while acquiring the ability to generate audio tokens in response to questions or instructions.\n\nThis way, the model could be asked a question in text and provide an answer in SNAC tokens, which could then be directly decoded into spoken language. It would also be interesting to see how well even a small scale  LLM, such as Phi-3 or Qwen-1.8B, could transcribe speech by feeding it SNAC tokens and generating a transcription text. The next step would be to train a chat model that understands SNAC tokens as input and responds with text, or directly responds with SNAC tokens to text inputs.\n\nOnce we can reliably perform functions like transcribing audio segments and generating speech in response to user queries or text inputs while maintaining the LLMs' ability to generate and understand text, we can consider extended pretraining. This involves training language models on a mixture of high-quality texts and SNAC tokens from complete, longer audio recordings. There are many publicly available sources of high-quality audio data that could impart more nuances and linguistic subtleties to the LLM than currently possible with existing ASR and TTS datasets. After extended pretraining with both text and audio data, we need instruction fine-tuning with audio-to-audio instruction datasets, where both the instruction and fulfillment are provided in audio tokens.\n\n## Audio-to-Audio Instruction Tuning Datasets\n\nAs potential sources for extended pre training of LLMs, we collected video links from sources like common crawl.\n\n[High quality podcasts, lectures & shows (330657)](https://huggingface.co/datasets/laion/links_to_pocasts_lecture_and_shows_for_tts)\n\nFor initial tests, it would be beneficial to generate both the instruction and its execution  through the chatbot using TTS systems. First, we create a conventional instruction tuning dataset with a text-based LLM and then generate audio files for both the user's and the chatbot's roles with different voices. These are then converted into SNAC tokens or other audio tokens.\n\nIf this type of instruction tuning proves successful, a theoretically feasible but limited approach could be to generate an instruction tuning dataset with volunteers where one person acts as the user and another as the chatbot.\n\nAnother possibility is to perform transcription with speaker separation on podcasts, and then use an LLM like LLAMA to identify transitions where speaker 1 appears to issue a request and speaker 2 helpfully responds. These parts from speaker 1 and speaker 2 could be components in an audio-to-audio instruction tuning dataset.\n\nAdditional ideas for audio text tuning datasets are:\n\n- Integrated Audio-Text Datasets: Create datasets where text segments are partially replaced with speech segments generated using Text-to-Speech (TTS) systems. This method helps the model learn to handle interleaved audio and text seamlessly.\n- Cross-Modal Translation Tasks: Use models like Meta's SeamlessM4T to generate speech translations from one language to another. For instance, translate English audio clips to German, creating paired datasets to enhance the model’s multilingual audio capabilities.\n- Music and Sound Effects Generation: Develop datasets containing music and sound effects with corresponding textual descriptions or generation instructions. This trains the model to understand and generate diverse audio outputs based on text or audio inputs.\n\n## Conclusion\n\nAs a community of volunteers and hobbyists, we cannot conduct all these experiments simultaneously. Therefore, we officially call on the open-source community to start experimenting with the datasets we have converted and share their results with us. Once we achieve promising small-scale results and eventually derive scaling laws based on the small scale experiments predicting behavior on larger scales, we can discuss how to provide computing resources for larger-scale experiments.\n\nWe look forward to your feedback and experiments. Together, we can create a future where advanced language models are accessible to all and have a positive impact on many lives.\n\n\n[Join our discord server](https://discord.com/invite/WugQF4YeT6)\n","date":1716940800000},{"slug":"laion-maintenance","frontmatter":{"title":"Safety Review for LAION 5B","author":"LAION.ai","date":"December 19 2023","previewImg":"/images/blog/laion-blue.png"},"content":"\nThere have been reports in the press about the results of a research project at Stanford University, according to which the LAION training set 5B contains potentially illegal content in the form of CSAM. We would like to comment on this as follows:\n\nLAION is a non-profit organization that provides datasets, tools and models for the advancement of machine learning research. We are committed to open public education and the environmentally safe use of resources through the reuse of existing datasets and models.\n\nLAION datasets (more than 5.85 billion entries) are sourced from the freely available Common Crawl web index and offer only links to content on the public web, with no images. We developed and published our own rigorous filters to detect and remove illegal content from LAION datasets before releasing them. [See our original announcement from 20.08.2021](https://laion.ai/blog/laion-400-open-dataset/#filtering-out-unsuitable-image-text-pairs), where points 6-9 describe the specific measures we took for filtering CSAM related material.\n\nLAION collaborates with universities, researchers and NGOs to improve these filters and are currently working with the [Internet Watch Foundation (IWF)](https://www.iwf.org.uk/) to identify and remove content suspected of violating laws. LAION invites the Stanford researchers to join its Community to improve our datasets and to develop efficient filters for detecting harmful content.\n\nLAION has a zero tolerance policy for illegal content and in an abundance of caution, we are temporarily taking down the LAION datasets to ensure they are safe before republishing them.\n\nFollowing a discussion with the Hamburg State Data Protection Commissioner, we would also like to point out that the CSAM data is data that must be deleted immediately for data protection reasons in accordance with Art. 17 GDPR.\n","date":1702944000000},{"slug":"cpretrain","frontmatter":{"title":"Conditional Pretraining of Large Language Models","author":"Rallio","date":"May 16 2023","previewImg":"/images/blog/wolf-round.jpg"},"content":"\n\n## **Introduction**\n\nLarge language models (LLMs), such as OpenAI's ChatGPT and similar chatbot products from other organizations, have recently gained widespread adoption. These models can extend text or respond to instructions in a natural and helpful manner. Despite the core technologies behind LLMs, namely the transformer architecture and the GPT decoder-only causal language model, remaining relatively unchanged for over five years, the surge in popularity of ChatGPT can be largely attributed to recent approaches that better align the output of LLMs with users' and service providers' intentions.\n\n\nTwo primary approaches have been employed to better align large language models with human expectations. The first is known as supervised finetuning (SFT) on natural instructions, while the second is called reinforcement learning from human feedback (RLHF). Both methods aim to improve the performance and usability of LLMs, but they differ in their implementation. SFT involves training the model using labeled datasets that contain natural instructions, which helps the model understand and respond more accurately to user queries. RLHF, on the other hand, is a technique that uses human preferences as a reward signal to fine-tune models. It involves collecting a dataset of human-written demonstrations on prompts, training supervised learning baselines, and then gathering a dataset of human-labeled comparisons between two model outputs on a larger set of prompts. A reward model (RM) is trained on this dataset to predict which output labelers would prefer, and this RM is used as a reward function to fine-tune the LLM using the PPO algorithm. However, there is an \"alignment tax\" associated with this approach, which can result in worse performance in some situations.\n\n![cond_pretrain_im1](https://github.com/LAION-AI/laion.ai/assets/22318853/77ce9e7d-4bdb-4fd4-b0fe-0a8d8498cea8)\n\n**Figure 1.** An example of document tagging on a popular user generated content website. The tags inform potential readers what kind of content will be in the text without spoiling the story.\n\n\nA third approach to align language models with human expectations in a more transparent and end-user controllable manner is called Conditional Pretraining. In this method, a large number of pretraining examples are tagged with labels that describe the content using human-understandable classifiers. Content tagging is used in nearly all human generated online information-sharing environments as a way to organize content, and help users find information most relevant to their interests. This labeling can be performed in a mostly unsupervised fashion, utilizing encoder-only or encoder-decoder natural language understanding (NLU) machine learning models.\n\nThere are many widely used tags online that help categorize and filter content based on user preferences. \"Suitable for work\" (SFW) and \"not suitable for work\" (NSFW) tags are commonly found on sites like Reddit, Imgur, and various online forums. Additionally, book and movie reviews often utilize the \"Spoilers\" tag to indicate if the review contains information that may negatively impact the enjoyment of the content. User-generated story sites, such as Archive of Our Own (AO3) and FanFiction.net, employ diverse tags to provide clear indications of the content readers can expect within the stories (Figure 1). Furthermore, labels like G, PG, PG-13, and R, have been utilized for decades to inform users about television and movie content.\n\nBy leveraging conditional pretraining, language models could be better adapted to users' interests and preferences, resulting in a more aligned and enjoyable experience.\n\n\n## **Converting Existing Pretraining Data into Conditional Pretraining Data**\n\nThe prevailing method for training LLMs involves collecting vast quantities of text from the internet and feeding this minimally processed text into the LLM. The pretraining objective is to predict the subsequent word given all prior words in the training example. Often, the text is divided in a manner that allows documents to be fragmented at any point, such as in the middle of a paragraph. These fragments are then randomly incorporated into larger batches of training examples, typically ranging from 2 to 4 million examples per training step. Although this approach has proven effective, it may not be the most optimal way to train these models.\n\n![cond_pretrain_im2](https://github.com/LAION-AI/laion.ai/assets/22318853/4e3adab4-b20c-4c91-9b2f-e2140a8902b0)\n\n**Figure 2.** Comparison of existing LLM training strategies and the conditional pretraining approach. Theoretically every example used to train the model could be tagged.\n\nIn contrast, conditional pretraining aims to prepend each training example with a set of descriptive tags and a brief synopsis that accurately represents the text in the training example (Figure 2). These tags and synopses can be efficiently generated using fine tuned NLU models such as BERT or T5. Although there is considerable computational cost associated with processing all the training examples, once the conditional pretraining examples are generated, they become reusable and easily understandable by humans. This approach enhances the training process, resulting in more accurate and user-friendly language models.\n\n\n## **Transparency and Accountability**\n\nAnother significant advantage of conditional pretraining is the transparency of the tags used on documents, which can be easily understood by auditors or end users of the models. At present, the instructions and reward models employed in most LLMs are proprietary and not available for public review. This lack of transparency makes it challenging to comprehend how and why models respond to culturally or politically sensitive topics. Even when there are disagreements among people about how these models should be aligned and what values they should uphold, it is difficult to engage in meaningful discussions or debates on these sensitive topics as long as the values of the organizations developing the LLMs remain concealed or obscured by carefully crafted press releases and position papers.\n\n\n## **How to Prepare a Conditional Pretraining Dataset**\n\n![cond_pretrain_im4a](https://github.com/LAION-AI/laion.ai/assets/22318853/741f09aa-37b8-4aa3-a2f2-365c57299137)\n\nWe have developed a fine tuned LoRA model based on the open source FLAN-UL2 that takes as input about 2000 words of text and outputs the conditional pretraining labels for the document. An example output from this conditional tagging model for a recent news article about LAION in [Forbes](https://www.forbes.com/sites/hessiejones/2023/04/19/amid-growing-call-to-pause-ai-research-laion-petitions-governments-to-keep-agi-research-open-active-and-responsible/) is below. To generate these document tags only text from the body of the article was used.\n\n## **Example Outputs from a New Conditional Pretrained Model**\n\nBelow you can find a toy example of how to control the behavior of the conditional language model. In this example, the conditional labels are used to create a very unhelpful chatbot or one that is helpful. These outputs are from the base conditional pretrained model, without any explicit instruction tuning or examples of chatbots in the training data.\n\n**<center>Adorable baby chatbot</center>**\n![image](https://github.com/LAION-AI/laion.ai/assets/22318853/85aca1d8-2243-467b-a5b1-d2abc7ffad09)\n\n**<center>Unhelpful chatbot</center>**\n\n![cond_pretrain_im3a](https://github.com/LAION-AI/laion.ai/assets/22318853/5b0a226a-04e0-49c3-9018-c4bb678e052c)\n\n\n**<center>Helpful chatbot</center>**\n![cond_pretrain_im3b](https://github.com/LAION-AI/laion.ai/assets/22318853/4e3878ea-3faa-4349-9b74-8d09d960516e)\n\n\n## **How to Use The Models and Contribute to This Project**\n\nThe initial code and models are available on Github and Huggingface. Conditional pretrained models can be used exactly the same way as any other large language model, just remember to prepend your conditionals to the start of your input and spend some time experimenting with what tags suit your use case. \n\nWe are in the process of converting very large pretraining datasets from the internet to conditional pretraining datasets and if you are someone that gets excited about building large datasets we would welcome your help on this effort. On the more experimental side of things, we are interested in developing reward models that efficiently calculate how well the outputs from conditional pretrained models conform with their conditionals. Please checkout the LAION discord or github if you are interested in contributing.\n\n\nIf you are interested, please check out the following links:\n- [Demo-Colab-Notebook](https://colab.research.google.com/drive/1fbXOqeEkqygnWKSPKddQtaMiZEc0KYFY?usp=sharing) - Colab for playing with our models.\n- [7B-redpajama-conditional-alpha](https://huggingface.co/Rallio67/7B-redpajama-conditional-alpha) - Redpajama base 7B model finetuned on ~2 million 2048 context conditional pretraining examples.\n- [3B-redpajama-conditional-alpha](https://huggingface.co/Rallio67/3B-redpajama-conditional-alpha) - Redpajama base 3B model finetuned on ~2 million 2048 context conditional pretraining examples.\n- [neox-20b-conditional-alpha](https://huggingface.co/Rallio67/neox-20b-conditional-alpha) - gpt-neox-20B base model finetuned on ~600 thousand 2048 context conditional pretraining examples.\n- [flan-ul2-20b-condlabeler-alpha](https://huggingface.co/Rallio67/condlabeler-alpha) - LoRA finetuned flan-ul2-20b model that you can use to create conditional labels for your own text. Please verify that the labels you are generating match your expectations with some texts you are already personally familiar with.\n- [LAION GitHub Repository](https://github.com/LAION-AI/)\n- 💬 [LAION Discord](https://discord.gg/HzJU2kuC)\n\n## **Acknowledgements**\n- [StabilityAI](https://stability.ai/) for pre-emptible compute resources.\n- [EleutherAI](https://github.com/EleutherAI/gpt-neox) for opensource GPT-Neox.\n- [huggingface](https://huggingface.co/) for open source model hosting and code base.\n- [RedPajama-INCITE](https://www.together.xyz/blog/redpajama-models-v1) for training and releasing opensource base models.\n- [google-research](https://github.com/google-research/t5x) for training and releasing opensource T5 models which we used to create conditional labels.\n\n## **References**\nConditional pretraining is very straightforward conceptually and does not require any complex mathematical arguments for it's justification. If you want to read a recent academic text discussing the concept in more detail please check out the paper by Anthropic. Conditional Pretraining was also used by Google to create Palm 2.\n- [Pretraining Language Models with Human Preferences](https://arxiv.org/abs/2302.08582) by Anthropic.\n- [PALM-2 Technical Report](https://ai.google/static/documents/palm2techreport.pdf) by Google AI. Search for \"control tokens\" to find relevant information.\n","date":1684195200000},{"slug":"letter-to-the-eu-parliament","frontmatter":{"title":"A Call to Protect Open-Source AI in Europe","author":"LAION.ai","date":"April 28, 2023","previewImg":"/images/blog/laion-blue.png"},"content":"\n**An Open Letter to the European Parliament: Protecting Open-Source AI for a Safe, Secure, and Sovereign Digital Future**\n\nLAION, alongside prominent research institutions and developers, has penned an [open letter to the European Parliament](/documents/open-letter-to-eu-parliament.pdf) to express concerns about the draft AI Act's potential impact on open-source research and development (R&D) in artificial intelligence (AI). The letter highlights the importance of open-source R&D for ensuring the safety, security, and competitiveness of AI in Europe and warns against the consequences of stifling such innovation.\n\n## The Importance of Open-Source AI\n\nThe letter outlines three main reasons why open-source AI is worth protecting:\n\n1. **Safety through transparency:** Open-source AI promotes safety by enabling researchers and authorities to audit model performance, identify risks, and establish mitigations or countermeasures.\n2. **Competition:** Open-source AI allows small to medium enterprises to build on existing models and drive productivity, rather than relying on a few large firms for essential technology.\n3. **Security:** Public and private organizations can adapt open-source models for specialized applications without sharing sensitive data with proprietary firms.\n\n## Concerns with the Draft AI Act\n\nThe draft AI Act may introduce new requirements for foundation models, which could negatively impact open-source R&D in AI. The letter argues that \"one size fits all\" rules will stifle open-source R&D and could:\n\n- Entrench proprietary gatekeepers, often large firms, to the detriment of open-source researchers and developers\n- Limit academic freedom and prevent the European research community from studying models of public significance\n- Reduce competition between model providers and drive investment in AI overseas\n\n## Recommendations for the European Parliament\n\nThe open letter makes three key recommendations:\n\n1. **Ensure open-source R&D can comply with the AI Act:** The Act should promote open-source R&D and recognize the distinctions between closed-source AI models offered as a service and AI models released as open-source code. Where appropriate, the Act should exempt open-source models from regulations intended for closed-source models.\n2. **Impose requirements proportional to risk:** The Act should impose rules for foundation models that are proportional to their actual risk. A \"one size fits all\" framework could make it impossible to field low-risk and open-source models in Europe.\n3. **Establish public research facilities for compute resources:** The EU should establish large-scale supercomputing facilities for AI research, enabling the European research community to study open-source foundation models under controlled conditions with public oversight.\n\n## The Future of AI in Europe\n\nThe letter concludes with a call to action for the European Parliament to consider the points raised and foster a legislative environment that supports open-source R&D. This approach will promote safety through transparency, drive innovation and competition, and accelerate the development of a sovereign AI capability in Europe.\n\nWith numerous esteemed supporters, including the European Laboratory for Learning and Intelligent Systems (ELLIS), the Pan-European AI Network of Excellence, and the German AI Association (KI-Bundesverband), the letter serves as a powerful reminder of the importance of protecting open-source AI for the future of Europe.\n\n## Supporters\n\n\n- European Laboratory for Learning and Intelligent Systems (ELLIS) - Pan-European AI Network of Excellence\n- German AI Association (KI-Bundesverband) - With more than 400 companies, the largest AI network in Germany\n- **Prof. Jürgen Schmidhuber**: Scientific Director of the Swiss AI Lab IDSIA (USI & SUPSI), Co-Founder & Chief Scientist of NNAISENSE, Inventor of LSTM Networks\n- **Prof. Sepp Hochreiter**: JKU Linz, Inventor of LSTM Networks\n- **Prof. Bernhard Schölkopf**: Director, Max Planck Institute for Intelligent Systems and ELLIS Institute, Tübingen, Germany\n- **Prof. Serge Belongie**: University of Copenhagen; Director, Pioneer Centre for AI\n- **Prof. Andreas Geiger**: University of Tübingen and Tübingen AI Center\n- **Prof. Irina Rish**: Full Professor at Université de Montréal, Canada Excellence Research Chair (CERC) in Autonomous AI and Canada CIFAR AI Chair, core member of Mila - Quebec AI Institute.\n- **Prof. Antonio Krüger**: CEO of the German Research Center for AI (DFKI) and Professor at the Saarland University\n- **Prof. Kristian Kersting**: Full Professor at Technical University of Darmstadt and Co-Director, Hessian Center for AI (hessian.AI)\n- **Jörg Bienert**: CEO of German AI Association, CPO of Alexander Thamm GmbH\n- **Patrick Schramowski**: Researcher at German Center for Artificial Intelligence (DFKI) and Hessian Center for AI (hessian.AI)\n- **Dr. Jenia Jitsev**: Lab Leader at Juelich Supercomputing Center, Research Center Juelich, Helmholtz Association, ELLIS member\n- **Dr. Sampo Pyysalo**: Research Fellow at the University of Turku, Finland\n- **Robin Rombach**: Co-Developer of Stable Diffusion, PhD Candidate at LMU Munich\n- **Prof. Michael Granitzer**: Chair of Data Science University of Passau, Germany and Coordinator of OpenWebSearch.eu\n- **Prof. Dr. Jens Meiler**: Leipzig University, ScaDS.AI Center for Scalable Data Analytics and Artificial Intelligence\n- **Prof. Dr. Martin Potthast**: Leipzig University, ScaDS.AI Center for Scalable Data Analytics and Artificial Intelligence, and OpenWebSearch.EU\n- **Prof. Dr. Holger Hoos**: Alexander von Humboldt Professor in AI at RWTH Aachen University (Germany) and Professor of Machine Learning at Universiteit Leiden (Netherlands)\n- **Prof. Dr. Henning Wachsmuth**: Chair of Natural Language Processing at the Institute of Artificial Intelligence, Leibniz University Hannover\n- **Prof. Dr. Wil van der Aalst**: Alexander von Humboldt Professor in Process and Data Science at RWTH Aachen University and Chief Scientist at Celonis\n- **Prof. Dr. Bastian Leibe**: Chair of Computer Vision at RWTH Aachen University (Germany)\n- **Prof. Dr. Martin Grohe**: Chair for Logic and the Theory of Discrete Systems, RWTH University\n- **Prof. Ludwig Schmidt**: Paul G. Allen School of Computer Science & Engineering, University of Washington\n- **Dr Morten Irgens**: Vice Rector, Kristiania, Co-founder and board member of CLAIRE (the Confederation of Laboratories of AI Research in Europe), Adra (the AI, Data and Robotics Association) and NORA (the Norwegian AI Research Consortium)\n- **Prof. Dr. Hector Geffner**: Alexander von Humboldt Professor in AI at RWTH Aachen University (Germany), and Wallenberg Guest Professor in AI at Linköping University, Sweden\n- **Prof. Dr. Hilde Kuehne**: Goethe University Frankfurt (Germany), MIT-IBM Watson AI Lab (USA)\n- **Prof. Gerhard Lakemeyer, Ph.D.**: Head of the Knowledge-based Systems Group and Chair of the Computer Science Department, RWTH Aachen University, Germany\n- **Sebastian Nagel**: Crawl Engineer, Common Crawl, Konstanz, Germany","date":1682640000000},{"slug":"realfake","frontmatter":{"title":"Training a Binary Classifier to Distinguish Images Generated with Stable Diffusion (v1.4) from Real Ones","author":"Christoph Schuhmann, Ilia Zaitsev","date":"Apr 12 2023","previewImg":"https://raw.githubusercontent.com/LAION-AI/laion.ai/e095bb080a77443cc6a7e07d97b412af53beebc0/public/images/blog/realfake-classifier-artifacts.png"},"content":"\nWe present the development and assessment of a binary classifier designed to distinguish between authentic images and images generated \nusing Stable Diffusion (SD) v1.4. We will discuss the dataset employed, describe the model architecture, outline the training process, \nand present the results obtained. Furthermore, we will explore potential future work aimed at enhancing the classifier's performance. \nThe source code, training parameters, and model weights are [available in this repository](https://huggingface.co/realfakerepo/realfake).\n\n### Dataset\n\nThe training dataset was assembled in two steps. First, four image datasets were merged:\n\n1. [`imagenet-1k`](https://huggingface.co/datasets/imagenet-1k): A widely used subset of ImageNet spanning 1,000 object classes.\n2. [`laion2B-en-aesthetic`](https://huggingface.co/datasets/laion/laion2B-en-aesthetic) (parts 400 to 699): A subset of images from the LAION-5B dataset, estimated to be [aesthetic](https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md) by a model trained on top of CLIP embeddings.\n3. [`imagenet-1k-SD-1.4`](https://huggingface.co/datasets/ChristophSchuhmann/Imagenet-1k-SD-1.4): A newly-created dataset that serves as a \"twin\" to the \"real\" `imagenet-1k`, containing the same 1,000 classes but generated using Stable Diffusion v1.4 with a variety of prompts per class.\n4. [`DiffusionDB 2M`](https://huggingface.co/datasets/poloclub/diffusiondb): The first large-scale text-to-image prompt dataset.\n\nSecond, two million images were sampled from the merged data, ensuring an equal distribution of real and SD-generated images. Around 10% of that data \nis put aside as a validation subset to track the prediction quality during the training process. The following table shows the number of records \nassigned to each subset. This diverse and balanced dataset provided a solid foundation for training the model.\n\n| Label \\ Subset | Training | Validation |\n|----------------|----------|------------|\n|      fake      |  898785  |   101215   |\n|      real      |  899986  |   100014   |\n\nThe specific list of samples used in training is stored in the [`metadata/prepared.2000k.jsonl`](https://huggingface.co/realfakerepo/realfake/tree/main/metadata) file available in the repository. Each record includes information about its subset and path to the sample stored on a local disk. \nThis allows for flexible selection of images for training and validation. Additionally, the folder contains smaller prepared subsets used for debugging purposes. Note that for the `imagenet-1k` dataset, the training and validation subsets were prepared such that the classes of images do not overlap.\n\n### Model Architecture and Training Process\n\nWe selected a straightforward model architecture utilizing a fine-tuned [ConvNext Large](https://pytorch.org/vision/main/models/generated/torchvision.models.convnext_large.html) model with approximately 200 million parameters. This choice was made to obtain quick results using 8x A100 GPUs on the Stability AI cluster.\n\nThe training process employed a One-Cycle learning rate scheduler, AdamW optimizer, and basic augmentations such as affine transformations, crops, and cutouts. The model was trained for five epochs starting from pre-trained weights (imagenet-1k) with all layers unfrozen from the beginning. Investigating more sophisticated training strategies is beyond the scope of this work but may be interesting for future research.\n\n### Results\n\nThe trained classifier achieved close to 99% accuracy on the validation dataset described in the #Dataset section. Further testing of the model's generalization capability in distinguishing between real and SD-generated images was performed by creating _an additional, out-of-sample test set_. \nIt comprised 2,500 images generated with SDv1.4 using a set of prompts proposed by LLM, with each prompt generating 100 different images. In addition,\nthe test set included 2,500 images from the `imagenet-1k` validation set. Therefore, none of the test set images is seen during the training process.\n\nThe following plots illustrate the model's confidence levels. Analyzing the results, several interesting conclusions can be drawn:\n* Views of nature, construction works, and furniture often cause confusion.\n* Real images with visual noise or uncommon objects are mistakenly classified as generated images.\n* Images with visually distinguishable generative artifacts (incorrectly rendered humans, wheels, airplanes, unrealistic lines) are classified as fakes with high confidence.\n\n![](/images/blog/realfake-classifier-real-least-confident.png)\n![](/images/blog/realfake-classifier-real-most-confident.png)\n![](/images/blog/realfake-classifier-fake-least-confident.png)\n![](/images/blog/realfake-classifier-fake-most-confident.png)\n\nAs expected, cases with obvious generative model-produced artifacts are easily classified that . For instance, images with humans often include clear artifacts such as unnatural postures or impossible positions. Another interesting class of images pertains to natural landscapes. In some instances, they are easily recognized as fakes, while others confuse the model. This also holds true for construction works and some furniture images.\n\nThe inference notebook is available on [Google's Colab](https://colab.research.google.com/drive/1zZR55CpHdKaVQXhZ3yxvOu55jCDkADam).\n\n### Limitations\n\nIt is important to note that the current model is still a work in progress. The classifier only saw images produced with Stable Diffusion V1.4, \nwith all possible image artifacts that it produces. (See the example below.)\n\n![](/images/blog/realfake-classifier-artifacts.png)\n\nTherefore, it might be the case that the classifier pays attention to those SD-specific artifacts, and wouldn't perform that well on the output \nof other generative models.\n\nAnother possible limitation is low image resolution. The classifier resizes images to 256px per side, and further crops it to 224px. It might be difficult to effectively classify high-resolution examples.\n\nFinally, the classifier's quality isn't compared against human's performance. As was mentioned before, some fakes have easily recognized artifacts, while others aren't distinguishable by the human eye because of low resolution. Building a testing dataset assets by humans should give a baseline to better estimate model's performance.\n\n### Future Work\n\nBuilding on this work, there are several avenues for further exploration:\n\n1. Using various kinds of generative models for building a more challenging dataset to ensure that the classifier works well across \nvarious generative techniques.\n1. Increasing input resolution to ensure that the model can capture fine details.\n1. Creating a test set classified by volunteers to establish a quality baseline for better assessing model's performance.\n1. Investigating whether the classifier can be used to guide SD models (akin to GANs) to steer them towards generating more realistic images. By providing feedback on the realism of generated images, the classifier might help improve the quality of synthesized images.\n\n### Acknowledgements and Contributions\n\n* Christoph Schuhmann conceived the initial idea of building a binary classifier to distinguish real vs. generated images, prepared the `imagenet-1k-SD` dataset, and guided the development process.\n* [Stability AI](https://stability.ai/) provided us with compute resources to store the data and train the classifier.\n* The [fast.ai](https://docs.fast.ai/) library was used for quick prototyping of the initial model.\n* Scalable training was done via [PyTorch-Lightning](https://lightning.ai/docs/pytorch/stable/).\n* Numerous other open-source tools, models, and datasets made this work possible.\n","date":1681257600000},{"slug":"general-gpt","frontmatter":{"title":"General-GPT: Breaking the Modality Constraint","author":"Shivaen Ramshetty and Christoph Schuhmann","date":"March 28 2023","previewImg":"/images/blog/general-gpt-logo.png"},"content":"## Introduction\n\nWith the rapid explosion of large language models and utilization of their encompassing applications, most notably [ChatGPT](https://openai.com/blog/chatgpt), there is a clear promise of more capable and useful AI models/systems. Often, such models are compared to us as humans using the Turing test or their performance on tasks relative to humans. As of recent, these models have even achieved incredible success on tests designed for humans such as the LSAT. However, the limited means by which one can interact with such systems  elucidates a variety of opportunities for exploration and possibly discovery. We ask whether modalities can be mixed and learnt alongside one another, and whether that environment of learning offers new avenues for understanding.\n\nWith this in mind, we are excited to introduce a relatively new project at [LAION](https://laion.ai/) called General-GPT.\n\n\n## Goals\n\nIn an effort to keep this concise, we enumerate our goals as follows:\n\n1. Explore the ability to directly intertwine any modality into large language models (LLMs), such that expression of ideas and responses can be more natural and informative.\n2. Allow longer contexts by inputting embedded sequences rather than operating directly on the sequences themselves. Though we may lose fine-grained details of the original sequences, it may prove useful for higher-level tasks.\n3. Provide open-source tools, methods, and models that we hope extend our bigger picture goal of \"democratizing AI.\"\n\n\n## Experiments\n\n### Text-Image Expression\nCurrently, our efforts have been primarily centered around experimenting with whether or not we can format our first goal into a trainable and functioning model. In order to do so, we first simplified the problem in a three ways. First, we choose to focus on tackling only the text-image domain rather than the full gamut that we hope to include. Secondly, we format the problem as a straightforward mapping from $x \\rightarrow y$ or $y \\rightarrow x$. Where $x$ represents an image embedding and $y$ represents the accompanying text. Finally, we tune on just the [MS-COCO](https://cocodataset.org/#home) [1] 2017 training set of 591753 image-caption pairs.\n\nTo construct $x$ we utilize [CLIP](https://openai.com/research/clip) [2], specifically CLIP *ViT-L/14*, to encode the images. On the other hand, we utilize [GPT-2](https://huggingface.co/gpt2) [3] as our LLM that receives mixed inputs and grounds for multimodal understanding or expression. The choice of these two models as baselines comes from their relatively reasonable scale, existing work and research, and the common dimensionality of their encodings. \n\n#### Image Captioning: $x \\rightarrow y$ \nFor this task, we introduce two specific tokens into the vocab so that the model may recognize when an embedding is being input and what that embedding is. Intuitively, the first token (\"[CLIP IN]\") should signal that there is an image embedding before the second token (\"[\\CLIP IN]\"). Therefore, the training data for this task is structured as follows:\n\n*<center>[CLIP IN] **embedding** [\\CLIP IN] Caption: [MS-COCO caption ...].</center>*\n\nIn regards to training itself, we follow [CLIP prefix captioning](https://github.com/rmokady/CLIP_prefix_caption) [4] and simply insert the image embedding as a new token in between our two new tokens. Then, we introduce a dummy token as our target token at the same inserted position. Lastly, the loss for this task is just cross-entropy between shifted-by-1 logits and the original target indices with the dummy token being ignored.\n\n\n| Encoded Image | Generated Caption | Original Caption|\n|  :----: | :----: | :----: |\n| ![Catch Example](/images/blog/general-gpt_captioning_example-1.png) | A man and a child playing baseball. | A man and a boy are playing catch in a yard. |\n| ![Sleeping Dog](/images/blog/general-gpt_captioning_example-2.png) | A dog laying on a sidewalk next to a bike. | a white dog is sleeping on a street and a bicycle |\n\nTable 1: Results of image captioning with CLIP embeddings as input into GPT-2.\n\n\n#### Image Retrieval: $y \\rightarrow x$\nSimilar to the first task, we also introduce two additional tokens: \"[CLIP OUT]\" and \"[\\CLIP OUT].\" As there text suggests, they represent the position and container for the CLIP image embedding. The training data for task is formatted as such:\n\n*<center>Caption: [MS-COCO caption ...]. [CLIP OUT][\\CLIP OUT] </center>*\n\nAn interesting difference between the two task arises in the training procedure. Here, we must enforce GPT-2 to learn image representations that are as close to the original CLIP image embeddings as possible. In order to do this, we compute the mean squared error between the last hidden state at the position of the \"[\\CLIP OUT]\" token and the original CLIP embedding. Finally, we perform the same cross-entropy loss for language modeling.\n\n| Caption      | MS-COCO | LAION-5B\n| :---: | :---: | :---: |\n| Birds flying over the beach. | ![Beach Birds](/images/blog/general-gpt_coco-retrieval_example-1.png)| <img src=\"/images/blog/general-gpt_laion-retrieval_example-1.jpg\" width=600></src> |\n| A nightstand with a collection of books. |  ![Room with Books](/images/blog/general-gpt_coco-retrieval_example-2.png) | <img src=\"/images/blog/general-gpt_laion-retrieval_example-2.jpg\" width=300></src> |\n\nTable 2: Nearest neighbors of GPT-2 image embedding prediction within MS-COCO and LAION-5B [5].\n\n\n### Sentence Reconstruction\nOne significant limitation of current open-source LLMs is the constraint on context length. This constraint prevents models from effectively comprehending and reasoning over extensive background knowledge spanning thousands of sentences. To address this challenge, we propose an innovative approach that enables GPT models with a context length of 2048 or 4096, for example, to process and understand vast amounts of background information more efficiently.\n\nAs a preliminary experiment we evaluated how reasonable our second goal was by reconstructing the original text with GPT-2 from an input of its embedded representation. In other words, we hoped to see whether we could embed sentences into some shared dimensional space and then generate the same tokens from those sentences? If so, we may be able to shrink longer contexts into a series of sequence embeddings which would be useful across diverse sets of inputs.\n\nTo model this behavior, we followed a method similar to how we performed the aforementioned image captioning. However, we avoid adding any new tokens or structuring our training data. Instead, a simple encoding of each sentence using the sentence transformer [*all-mpnet-base-v2*](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) [6] is followed by the sentence itself. Then, we compute the cross-entropy loss as previously described with the output logits and target token indices.\n\n| Original Caption | Reconstructed Caption |\n| :---: | :---: |\n| A man riding a motorcycle down the street. | A man riding a motorcycle down the street. |\n| Two animals chasing each other in a barn. | Two animals chasing each other in a barn. |\n| Two animals chasing each other in a farmhouse. | Two animals chase after a flock of farm animals in a barn. |\n\nTable 3: Results of sentence reconstruction with *all-mpnet-base-v2* and GPT-2.\n\n\n## Next Steps\n\nUltimately, our aim is to train GPT models to handle texts and sequences of other modalities entirely in semantic embeddings, such as sequences of CLIP embeddings for videos, where each CLIP embedding represents the image embedding of one image frame, or where one embedding could be the audio clip (CLAP) [7] embedding of 5 or 10 seconds of audio. By predicting sequences in these semantic spaces or streams of ideas, truly multimodal sequence learning could be realized, capable of learning robust and sophisticated world models by pretraining on data from various modalities.\n\nAdditionally, embeddings could be decoded by specialized decoders into different outputs, such as text, images, audio, and video, similar to what DALL-E (Ramesh et al., 2021) does with CLIP embeddings that get decoded into images. Coalescing modalities could open the door to more \n\n### Scale\nIn terms of scale, there are a few dimensions of the experimental setup that we will modify. Three such dimensions include larger models, larger datasets, and more complex data, which we expect will improve the generalization across inputs. In order to tune these larger models on richer data we also need to expand our computational resources, possibly in a distributed setting. \n\nWe plan on introducing greater complexity to the current data by utilizing truly interleaved datasets and large context inputs. For the latter, we convert the background text into a series of sentence embeddings using a pre-trained sentence embedding model, CLIP, or the recently proposed SGPT [8]. Then, create a sequence of these sentence embeddings, effectively compressing the original lengthy text into a condensed representation that captures high-level semantic information. Next, the sequence of embeddings is provided to the GPT model with the more recent context in the form of text tokens. This additional input serves to inform the model about the specific grammar, syntax, and style of the text. The model is then tasked with generating a continuation of the text based on the thousands of sentence embeddings and the few hundred words of the most recent context.\n\nBy representing longer contexts as a series of sequence embeddings, we enable the GPT model to reason over the entire text at once, leading to more coherent and contextually informed outputs. This method could be especially useful for tasks requiring a deep understanding of vast amounts of background information, such as generating summaries of novels, long articles, or comprehensive research papers.\n\nCurrent trends suggest that these modifications will improve our results, but greater complexity may lead to instability. If that is the case, additional modifications or redesigns will be necessary; all of which will be shared as they arise.\n\n### New Tasks\nSome obvious directions we plan to investigate include the extrapolation of the current design into other modalities such as audio and video. Additionally, we wish to understand whether a LLM can generate both text and images that play off one another. In such a case, the LLM wouldn't necessarily generate the images directly, but rather condition an image generation model. If we are able to show that image generation can be guided in an interleaved manner, then other modalities will again be an extension. \n\nAlthough our research in this direction is still preliminary and incomplete, it is highly promising, and we encourage everyone interested in this topic to join our server and contribute to our research. Part of what makes us excited for this project is all the ideas that the open-source community may come up with and even implement. For that reason, we would love any suggestions, feedback, and help!\n\n## Notes\n\nIt is quite clear from the results that inputs that are out-of-distribution in both experiments leads to poor results. Though this isn't unexpected for the scale and goals of our experiments, it does hint at poor generalization in such a configuration. Further experiments will be essential in diagnosing the impacts of richer data and scale.\n\nIf you wish to contribute, stay updated, or learn a bit more about the current work, please check out the following links:\n- 🧑‍💻 [GitHub Repository](https://github.com/LAION-AI/General-GPT)\n- 💬 [LAION Discord](https://discord.gg/HzJU2kuC)\n- 🎥 [Introduction Video](https://www.youtube.com/watch?v=LA3AC8gM6hw)\n\n\n## Acknowledgements\nWe further thank the authors and contributors of the following works/repositories:\n- [HuggingFace](https://github.com/huggingface/transformers)\n- [CLIP Retrieval](https://github.com/rom1504/clip-retrieval)\n\nLogo generated with [Craiyon](https://www.craiyon.com/)\n\n\n## References\n\n[1] Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., ... & Zitnick, C. L. (2014). Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 (pp. 740-755). Springer International Publishing.\n\n[2] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). PMLR.\n\n[3] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9.\n\n[4] Mokady, R., Hertz, A., & Bermano, A. H. (2021). Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734.\n\n[5] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., & Jitsev, J. (2022). LAION-5B: An open large-scale dataset for training next generation image-text models. _ArXiv, abs/2210.08402_.\n\n[6] Reimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.\n\n[7] Elizalde, B., Deshmukh, S., Ismail, M. A., & Wang, H. (2022). Clap: Learning audio concepts from natural language supervision. arXiv preprint arXiv:2206.04769.\n\n[8] Muennighoff, N. (2022). Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904.\n","date":1679961600000}]},"__N_SSG":true}