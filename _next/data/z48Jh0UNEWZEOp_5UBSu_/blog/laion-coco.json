{"pageProps":{"frontmatter":{"title":"Laion coco: 600M synthetic captions from Laion2B-en","author":"Christoph Schuhmann, Andreas Köpf, Richard Vencu, Theo Coombes, Romain Beaumont","date":"Sep 15, 2022","previewImg":"/images/blog/aerial.png"},"content":"\nAuthor: [Christoph Schuhmann](https://github.com/christophschuhmann), [Andreas Köpf](https://github.com/andreaskoepf) , [Theo Coombes](https://github.com/TheoCoombes), [Richard Vencu](https://github.com/rvencu/), [Benjamin Trom](https://github.com/limiteinductive) , [Romain Beaumont](https://github.com/rom1504) \n\n**We present LAION-COCO, the world’s largest dataset of 600M generated high-quality captions for publicly available web-images**\n\nLaion5B has five billion natural captions. They provide a lot of information, but could synthetic captions complement them ?\n\nTo answer this question, we use a combination of existing, publicly available models to produce high quality captions for images in the style of [MS COCO](https://paperswithcode.com/dataset/coco).\n\nWe captioned 600M images from the english subset of Laion-5B with an ensemble of [BLIP](https://github.com/salesforce/BLIP) L/14 and 2 CLIP versions (L/14 and RN50x64).  \n\nWith this post we release them openly today.\n\nThis will make it possible to investigate the value of generated captions to train models. We’re curious on how these synthetic captions could impact models trained on them!\n\n\n## Download it\n\nThe 600M samples are provided in parquet files. Columns include the original caption, the url, the top caption and a list of alternative captions with lower CLIP-similarity scores.\n\n[https://huggingface.co/datasets/laion/laion-coco](https://huggingface.co/datasets/laion/laion-coco) \n\n\n## Samples\n\n\n\n<img src=\"/images/blog/ring.png\" width=\"200px\" />\n\n\n**Original: **LGSY 925 Sterling Silver Double Heart Rings Infinity Love Thin Rings Wedding Engagement Promise Engraved Love Rings for Women for Dainty Gift\n\n**Generated: **An open ring with two hearts on it.\n\n\n\n<img src=\"/images/blog/boot.png\" width=\"200px\" />\n         \n\n**Original:** Female Thick with Pointy Head High Heel Chelsea Ankle Boots\n\n**Generated:** Red leather ankle boots with gold buckles.\n\n\n\n<img src=\"/images/blog/laion_coco_beach.png\" width=\"200px\" />\n\n\n**Original:** a group of people on horses on a beach\n\n**Generated:** Several people riding horses down the beach on a cloudy day.\n\n\n\n\n<img src=\"/images/blog/laion_coco_tags.png\" width=\"200px\" />\n\n\n**Original:** a wall with a bunch of graffiti on it\n\n**Generated:** The parking meter is near a graffiti covered building.\n\n                                                                                                                                        \n\n\n\n<img src=\"/images/blog/sheeple.png\" width=\"200px\" />\n\n\n**Original:** sheeple family\n\n**Generated:** A cartoon drawing of sheep watching TV with their babies.\n\n\n## More samples of images with their generated captions can be found here: \n\n(no cherry picking)\n\n[http://captions.christoph-schuhmann.de/eval_laion/eval.html](http://captions.christoph-schuhmann.de/eval_laion/eval.html) \n\n\n## Method\n\nThe method we used to generate these captions was to\n\n\n\n1. We use Blip L/14 to generate 40 captions\n2. Rank them using openai Clip Open AI L/14 ; selected the best 5 captions\n3. Rank using Open AI RN50x64 Clip model to select the best one\n4. Use a small, fine-tuned T0 model to roughly repair grammar and punctuation of the texts\n\nThe hyperparameters were chosen through a [grid search](https://wandb.ai/andreaskoepf/blip_coco_val_sample_sweep_bayes_02/sweeps/1bsha6b0) ([settings](https://github.com/andreaskoepf/CLIP-Image-Captioning/blob/blip_test/blip_coco_val_sample_sweep_bayes_02.yaml)) by Andreas Köpf to best match the style ( ROUGE scores ) of MS COCO texts.\n\n\n## Evaluation\n\nWe evaluated these generated captions by asking human evaluators to guess whether a caption is coming from a human or an AI model. We also asked them to rate the quality on a scale from 0(bad) to 5 (good). \n\nIn a first round we presented the evaluators each 200 samples, that contained 100 AI generated and 100 human written MS COCO captions.\n\n### Observations\n\n \n\n<img src=\"/images/blog/eval_laion_coco.png\" width=\"200px\" />\n\n\nGT: Y-Axis\n\nAnnotation: X-Axis\n\nMean rating & standard deviation of samples, that were written by a human:\n\nMean: 3.98\n\nStdev: 0.99\n\nMean rating & standard deviation of samples, that were written by an AI \n\nMean: 3.89\n\nStdev: 1.12\n\nMean rating & standard deviation of samples, where the annotator believed they were written by a human:\n\nMean: 4.44\n\nStdev: 0.61\n\nMean rating & standard deviation of samples, where the annotator believed they were generated by an AI \n\nMean: 3.50\n\nStdev: 1.15\n\n### Interpretation\n\nIt is very interesting that the mean scores of the samples generated by humans and generated by the model are very similar. We also notice that the standard deviation of the generated captions is a little bit higher.\n\nWe hypothesize that most in most cases the quality of the generated captions is perceived as as good as the quality of the human written captions.\n\nBut sometimes the captioning model obviously fails and the quality of the results is pretty low because the model doesn't relevant understand concepts about what is going on in the picture, because it's knowledge is not grounded in a sufficiently sophisticated world model.\n\n### Failure cases\n\n\n\n<img src=\"/images/blog/laion_coco_umbrella.png\" width=\"200px\" />\n\n\n_“Two people posing for the camera in their wedding attire, one with an umbrella over his head and another with long red hair.”_\n\n\n\n<img src=\"/images/blog/laion_coco_man.png\" width=\"200px\" />\n\n\n_“An older man having a heart attack, with his hand on the chest.”_\n\nWhen we remove all samples from the evaluations that have ratings of either 0 or 1, we Observe that the mean ratings and standard deviations move closer together. \n\n### Scores without ratings of 0 and 1\n\nMean rating & standard deviation of samples, that were written by a human:\n\nMean: 4.07\n\nStdev: 0.81\n\nMean rating & standard deviation of samples, that were written by an AI \n\nMean: 4.02\n\nStdev: 0.94\n\nThe mean ratings of the generated captions are still a little bit lower and the standard deviation is still a little bit higher, but the trend is pretty clear. By removing samples with rating 2, the gap between the qualities would probably decrease even further. \n\nPresentation only generated captions:\n\nIn a next step, we presented the human evaluators 400 captions that were only generated by the model (no human written captions in between):\n\nMean rating of all samples \n\n3.81\n\nStandard deviation of all samples \n\n0.94\n\n% rated as human\n\n47.5\n\n% rated as AI\n\n52.5\n\nWe observe that the human evaluators thought in 47.5% of all cases, that the captions were written by a human. This makes us confident that our captains are on average pretty good. When we told the evaluators later that all captions were generated by the model they told us that it was very hard for them to judge whether a caption was written by a model or a human, and that it only was easy for them in obvious failure cases.\n\n### Conclusions\n\nWe conclude that Our ensemble of BLIP and CLIP is already pretty good and capable of generating captions with a quality that is on average pretty close to the human written captions of MS Coco. \n\nIt would be very interesting for future work to let people rate our generated captions at larger scale and then filter out the samples with low rating values. These results could be used to train models to rate the quality of captions and to predict whether a caption looks like a generated or a human written caption.\n\nAnd even without further automated filtering, an ensemble of our captions and human evaluators would be a pretty good workflow to curate high quality captions at much lower costs than if we would ask humans to write them from scratch.\n\n\n## Credit assignments\n\n\n\n* [Christoph Schuhmann](https://github.com/christophschuhmann) lead the project, implemented a first version of the code, ran most of the generations & conducted the human evaluations\n* [Andreas Köpf](https://github.com/andreaskoepf) conducted the hyperparameter search & wrote the code to execute BLIP + CLIP filtering at scale\n* [Theo Coombes](https://github.com/TheoCoombes) managed the server that coordinated which GPU worker got which part of LAION to work on\n* [Romain Beaumont](https://github.com/rom1504) packaged the .json into parquet files, sent to HF and wrote the first draft of this post\n* [Richard Vencu](https://github.com/rvencu/) provided the infra structure to use the idle compute for this project\n* [Benjamin Trom](https://github.com/limiteinductive) wrote code that help us to convert the .json files to parquet\n\nWe thank [stability.ai](https://stability.ai/) for providing the compute used to generate the captions in the dataset.","slug":"laion-coco"},"__N_SSG":true}