{"pageProps":{"posts":[{"slug":"bud-e","frontmatter":{"title":"BUD-E: Enhancing AI Voice Assistants’ Conversational Quality, Naturalness and Empathy","author":"LAION","date":"Feb 8 2024","previewImg":"/images/blog/bud-e.webp"},"content":"\nAI voice assistants have revolutionized our interaction with technology, answering queries, performing tasks, and making life easier. However, the stilted, mechanical nature of their responses is a barrier to truly immersive conversational experiences. Unlike human conversation partners, they often struggle with fully understanding and adapting to the nuanced, emotional, and contextually rich nature of human dialogue, leading to noticeable latencies and a disjointed conversational flow. Consequently, users often experience unsatisfactory exchanges, lacking emotional resonance and context familiarity.\n\nWouldn’t it be awesome to have a fully open voice assistant that can\n\n1. reply to user requests in real-time\n2. with natural voices, empathy & emotional intelligence\n3. with long-term context of previous conversations\n4. handling multi-speaker conversations with interruptions, affirmations and thinking pauses\n5. fully local, on consumer hardware.\n\nTo realize this vision, LAION teamed up with the ELLIS Institute Tübingen, Collabora and the Tübingen AI Center to build **BUD-E (Buddy for Understanding and Digital Empathy)**.\n\n[![BUD-E Demo](/images/blog/bud-e-video.png)](https://youtu.be/SYWDucn8RL8)\n\nWe started the development by creating a baseline voice assistant with very low latency. To reach that point, we carefully evaluated latency and quality of a large range of Speech-to-Text, Large Language and Text-to-Speech models on consumer hardware and carefully control how the models interact. Right now (January 2024) we reach latencies of 300 to 500 ms (with a Phi 2 model). We are confident that response times below 300 ms are possible even with larger models like LLama 2 30B  in the near future.\n\nHere is a demo (running on a 4090 GPU): [BUD-E Demo](https://youtu.be/SYWDucn8RL8)\n\nAll code is open-source and available at [GitHub](https://github.com/LAION-AI/natural_voice_assistant).\n\n#### A roadmap towards Empathic & Natural AI Voice Assistants\n\nConversations even with the baseline model feel much more natural than anything we’ve seen so far. Nonetheless, there are still a lot of components and features missing that we need to tackle on the way to a truly empathic and naturally feeling voice assistant that is fun and helpful to interact with over prolonged periods of time. We are inviting everyone to contribute to the development of BUD-E.\n\nThe immediate problems and open work packages we’d like to tackle are as follows:\n\n#### Reducing Latency & minimizing systems requirements\n\n- **Quantization**: Implement more sophisticated quantization techniques to reduce VRAM requirements and reduce latency.\nFine-tuning streaming TTS. TTS systems normally consume full sentences to have enough context for responses. To enable high-quality low-latency streaming we give the TTS context from hidden layers of the LLM and then fine-tune the [streaming model on a high-quality teacher](https://arxiv.org/abs/2309.11210).\n- **Fine-tuning streaming STT**: Connect hidden layers from STT and LLM system and then fine-tune on voice tasks to maximize accuracy in low-latency configurations of STT model.\n- **End-of-Speech detection**: Train and implement a light-weight end-of-speech detection model.\n- **Implement Speculative Decoding**: Implement speculative decoding to increase inference speed in particular for the STT and LLM models.\n\n#### Increasing Naturalness of Speech and Responses\n\n- **Dataset of natural human dialogues**: Build a dataset (e.g., Youtube, Mediathek, etc.) with recorded dialogues between two or more humans for fine-tuning BUD-E.\n- **Reliable speaker-diarization**: Develop a reliable speaker-diarization system that can separate speakers, including utterances and affirmationsthat might overlap between speakers.\n- **Fine-tune on dialogues**: Finetune STT → LLM → TTS pipeline on natural human dialogues to allow the model to respond similarly to humans, including interruptions and utterances.\n\n#### Keeping track of conversations over days, months and years\n\n- **Retrieval Augmented Generation (RAG)**: [Implement RAG](https://www.pinecone.io/blog/rag-study/) to extend knowledge of BUD-E, unlocking strong performance gains\n- **Conversation Memory**: Enable model to save information from previous conversations in vector database to keep track of previous conversations.\n\n#### Enhancing functionality and ability of voice assistant\n\n- **Tool use**: Implement tool use into LLM and the framework, e.g., to allow the agent to perform internet searches\n\n#### Enhancing multi-modal and emotional context understanding\n\n- **Incorporate visual input**: Use a light-weight but effective vision encoder (e.g., CLIP or a Captioning Model) to incorporate static image and/or video input.\n- **Continuous vision-audio responses**: Similar to the (not genuine) [Gemini demo](https://www.youtube.com/watch?v=UIZAiXYceBI) it would be great if BUD-E would naturally and continuously take into account audio and vision inputs and flexibly respond in a natural manner just like humans.\n- **Evaluate user emotions**: Capture webcam images from the user to determine the user’s emotional state and incorporate this in the response. This could be an extension of training on dialogues from video platforms, using training samples where the speaker’s face is well visible.\n\n#### Building a UI, CI  and easy packaging infrastructure\n\n- **LLamaFile**: Allow easy cross-platform installation and deployment through a single-file distribution mechanism like [Mozilla’s LLamaFile](https://github.com/Mozilla-Ocho/llamafile).\n- **Animated Avatar**: Add a speaking and naturally articulating avatar similar to Meta’s Audio2Photoreal but using simpler avatars using [3DGS-Avatar](https://neuralbodies.github.io/3DGS-Avatar/).\n- **User Interface**: Capture the conversation in writing in a chat-based interface and ideally include ways to capture user feedback.\nMinimize Dependencies. Minimize the amount of third-party dependencies.\n- **Cross-Platform Support**: Enable usage on Linux, MacOS and Windows.\nContinuous Integration. Build continuous integration pipeline with cross-platform speed tests and standardized testing scenarios to track development progress.\n\n#### Extending to multi-language and multi-speaker\n\n- **Extend streaming STT to more languages**: Extending to more languages, including low-resource ones, would be crucial.\n- **Multi-speaker**: The baseline currently expects only a single speaker, which should be extended towards multi-speaker environments and consistent re-identification of speakers.\n\n### Collaborating to Build the Future of Conversational AI\n\nThe development of BUD-E is an ongoing process that requires the collective effort of a diverse community. We invite open-source developers, researchers, and enthusiasts to join us in refining BUD-E's individual modules and contributing to its growth. Together, we can create an AI voice assistants that engage with us in natural, intuitive, and empathetic conversations.\n\nIf you're interested in contributing to this project, join [our Discord community](https://discord.com/invite/eq3cAMZtCC) or reach out to us at <contact@laion.ai>.\n","date":1707350400000},{"slug":"laion-pop","frontmatter":{"title":"LAION POP: 600,000 high-resolution images with detailed descriptions","author":"Christoph Schuhmann, Peter Bevan","date":"Nov 17, 2023","previewImg":"/images/blog/laion_pop.jpg"},"content":"\nLAION POP is a subset of LAION 5B: This subset comprises 600,000 high-resolution images, each equipped with detailed descriptions. The selection of images was based on 10,000 different concepts popular on the image generation site \"Midjourney\".\n\n| [SampleOverview](/documents/llava_cogvlm_pop.html) |\n|:------------------------------:|\n|[LAION-POP Dataset on HuggingFace](https://huggingface.co/datasets/laion/laion-pop)|\n\n<img src=\"/images/blog/laion_pop.jpg\" style=\"height:384px; width: auto;\"/>\n\n## Dataset and Methodology\n\n4.25 million Midjourney images were downloaded from [this huggingface repository](https://huggingface.co/datasets/tarungupta83/MidJourney_v5_Prompt_dataset), and CLIP L14 vectors were generated for each image. Using the k-means clustering method, these vectors were assigned to 10,000 centroids. The CLIP vectors of these centroids were then used to retrieve nearest neighbors from the LAION-5B dataset using the [image search website](https://rom1504.github.io), focusing on those with aesthetic values of at least 0.5 and a minimum resolution of 768 pixels on the shortest side. Additionally, images suspected of containing watermarks were filtered out. NSFW values were calculated for each image using the LAION CLIP-based-NSFW-Detector, and these are released with the data.\n\n## Generation of Captions\n\nTwo sets of detailed image descriptions were created for the images using the models CogVLM and LLaVA-v1.5-13b. These descriptions focus on objects, backgrounds, text, scenery, interactions, and gestures, as well as the appearance and emotions of the depicted people or characters. The CogVLM captions are empirically recommended as higher quality captions, but systematic comparisons of these captions are encouraged and welcomed.\n\n## PROMPT\n\n\"Can you please describe this image in up to two paragraphs? Please specify any objects within the image, backgrounds, scenery, interactions, and gestures or poses. If they are multiple of any object, please specify how many. Is there text in the image, and if so, what does it say? If there is any lighting in the image, can you identify where it is and what it looks like? What style is the image? If there are people or characters in the image, what emotions are they conveying? Please keep your descriptions factual and terse but complete. DO NOT add any unnecessary speculation about the things that are not part of the image such as \"the image is inspiring to viewers\" or \"seeing this makes you feel joy\". DO NOT add things such as \"creates a unique and entertaining visual\", as these descriptions are interpretations and not a part of the image itself. The description should be purely factual, with no subjective speculation. Make sure to include the style of the image, for example cartoon, photograph, 3d render etc. Start with the words ‘This image showcases’:”\n\n‘This image showcases’ was trimmed from the beginning of each caption upon generation.\n\n## Future Application and Improvements\n\nAlthough no text-to-image model has been tuned with these data so far, we expect that the use of these data could significantly improve the aesthetic quality of the outputs.\n","date":1700179200000},{"slug":"open-empathic","frontmatter":{"title":"Open Empathic Launch","author":"Christoph, Knoriy, Robert","date":"Oct 22, 2023","previewImg":"/images/blog/open-empathic.png"},"content":"\nWe are thrilled to present **Open Empathic**, a pioneering open-source project initiated by our non-profit organization, LAION.\n\nOpen Empathic aims to equip open-source AI systems with empathy and emotional intelligence. We hope that methods and tools developed within the framework of this project, together with a community of researchers and technology enthusiasts, will revolutionize the way AI interacts with and supports humans in various domains.\n\nIn an increasingly AI-driven world, it is of paramount importance that AI systems possess emotional intelligence to understand and respond to human emotions. As AI plays an ever-expanding role in our daily lives, ranging from education to healthcare, elderly care, and commercial contexts, it becomes vital to prioritize the well-being and emotional intelligence of AI-human interactions.\n\n| **Ready to Make a Difference?** |\n|:------------------------------:|\n|Join our [Open Empathic Discord server](https://discord.gg/nFdG99UAav)|\n| Dive into our [tutorial video](https://youtu.be/GZqYr8_Q7DE) and head over to the [Open Empathic website](https://dct.openempathic.ai/) to help the **Open Empathic** movement through emotional annotation. Together, let's craft an era where AI resonates with human emotions.|\n\n## Why is Emotional Intelligence Essential for AI?\n\nThe benefits of emotionally intelligent AI system are manifold:\n\n### Empowering Students and Learners\n\nIn educational settings, AI equipped with emotional intelligence can adapt its approach to support students' emotional needs and personalities. This tailored support fosters a positive and nurturing learning environment, empowering students to achieve their full potential.\n\n### Advancing Psychological Research\n\nAI models for emotional intelligence can serve as objective and reliable instruments in psychological research. They can aid in improving psychological intervention methods and provide deeper insights into human experiences and behavior.\n\n### Improved Mental Health Support\n\nIn contexts such as healthcare and elderly care, emotionally intelligent AI can offer empathetic companionship, alleviating feelings of loneliness and providing valuable mental health support. This is especially crucial for isolated individuals, patients, and seniors who may lack regular human interaction.\n\n### Ethical Decision-Making\n\nEmotional intelligence enables AI systems to make ethical decisions by considering human emotions and values. This helps avoid potential biases and ensures AI acts in a way that aligns with human values and well-being.\n\n### Revolutionizing the Entertainment Industry\n\nEmotionally intelligent AI systems can transform the entertainment industry in various ways, providing new interactions between humans and various media not possible before. The gaming industry can enable games to recognize players' emotions and intelligently adapt game content and storytelling accordingly. This can lead to more fulfilling, more humane gaming experiences. Human audio and video content creators will have access to generative tools where creative storytelling will reflect rich nuances of human emotional experience, with the ability to personalize generated stories according to individual preferences. Beyond entertainment, such methods can also lead to personalized tools that offer help for individual mental well-being, for instance, providing customized meditation and relaxation sessions.\n\n## How Can We Equip AI with Emotional Intelligence?\n\nImagine chatbots that not only answer questions but also understand the underlying emotions of their users, responding with empathy and compassion:\n\nBuilding upon the successes of our previous \"Open Assistant\" project, where we collected fine-tuning data for Large Language Models (LLMs) models to transform them into chat models, this time, **we need your help** to create an open-source audio dataset that will enable AI systems, such as chatbots, to understand emotions and train emotionally intelligent, empathic text-to-speech models. By creating a community dataset that captures emotional characteristics of diverse speech segments, we can empower AI models of tomorrow to engage in empathetic and emotionally intelligent conversations—all while maintaining the spirit of openness and collaboration within the Open Source community.\n\n## How Can You Contribute?\n\nBy participating in our **Open Empathic** project, you can contribute to a near future where Open Source AI systems will be empathic, compassionate, and emotionally intelligent.\n\nYour annotations of audio data will play a pivotal role in training AI models that not only comprehend emotions but also respond empathetically, fostering a deeper connection between AI and humans.\n\nTo start contributing audio annotations (it takes around 3 minutes to annotate a few-second utterance), watch [this tutorial](https://youtu.be/GZqYr8_Q7DE) and then [register on the website](https://dct.openempathic.ai/) at the end of the video (please don’t skip watching the tutorial).\n\nJoin the **Open Empathic movement** today, and let's shape a compassionate, empathic AI future together!\n\nSincerely,\n\nThe LAION Team\n","date":1697932800000},{"slug":"strategic-game-dataset","frontmatter":{"title":"Strategic Game Datasets for Enhancing AI Planning: An Invitation for Collaborative Research","author":"Christoph Schuhmann & Qi Sun","date":"Oct 18 2023","previewImg":"/images/blog/chessboard.png"},"content":"\nRecent advancements in artificial intelligence (AI) underscore the progress of reasoning and planning shown by recent generalist machine learning (ML) models. The progress can be boosted by datasets that can further boost these generic capabilities when used for training foundation models of various kind. This research initiative has generated extensive synthetic datasets from complex games — chess, Rubik's Cube, and mazes — to study facilitation and the advancement of these critical generic skills in AI models. This paper delineates the methodology, dataset structure, and preliminary analysis, concluding with an open invitation for collaborative research.\n\n## Introduction\n\nThe field of AI has observed a pivotal shift toward foundation generalist models capable of advanced strategic planning, essential for complex problem-solving tasks. Recognizing the potential of various complex games as ideal proxies for real-world problems, this research focuses on the generation of large-scale synthetic datasets. These datasets are designed to challenge and enhance the strategic planning capabilities of generative pre-trained transformers (GPT) and similar models.\n\n## Dataset Overview\n\n### 1. Chess\n\nThe [chess dataset](https://huggingface.co/datasets/laion/strategic_game_chess) comprises 3.2 billion games, equating to approximately 608 billion individual moves. These games, generated via self-play by the Stockfish engine, emulate a high strategic complexity, reflective of a 2500 Elo rating. Each entry contains detailed move sequences, termination status, and game results.\n\n### 2. Rubik's Cube (3x3x3)\n\nThe [rubik's cube dataset](https://huggingface.co/datasets/laion/strategic_game_cube) features 1.64 billion Rubik's Cube solves, totaling roughly 236.39 billion moves. It provides initial scrambled states and the ensuing solve sequences, offering a [complex problem-solving scenario for models to navigate](https://github.com/trincaog/magiccube/).\n\n### 3. Mazes\n\nThe [maze dataset](https://huggingface.co/datasets/laion/strategic_game_maze), while smaller at 350,000 mazes, represents over 39.29 billion moves. Each maze is a 30x30 ASCII representation, with solutions derived using the A* algorithm, challenging pathfinding and planning algorithms.\n\n## Methodology\n\nThe datasets were constructed using rigorous methodologies to ensure complexity and relevance. Chess games were generated through self-play by a sophisticated chess engine, Rubik's Cube states were randomized and then solved using an advanced solving tool, and mazes were procedurally generated and solved using a robust pathfinding algorithm.\n\n## Purpose\n\nThe aim extends beyond game proficiency. The synthetic, procedurally generated datasets serve to enhance AI models' internal representation and problem-solving strategies, devoid of legal and ethical complications associated with other data forms. The training leverages the strategic depth of these games to develop advanced planning capabilities in AI models.\n\n## Call for Collaboration\n\nWe invite interested researchers and ML practitioners to explore these datasets' potential. Whether training GPT models from scratch or fine-tuning pre-existing models, we encourage the exploration of various pre-training and fine-tuning strategies using these game-based datasets standalone or as enhancement of other already composed large-scale data.\n\nOur team is prepared to assist in securing necessary GPU resources for these explorations. We are particularly interested in collaborators eager to pre-train models of small to medium scale on our game data, subsequently transition to standard text-based training, and then perform comparative analyses against models of similar architecture trained exclusively on text data.\n\nConclusively, this initiative marks a significant stride toward intricate problem-solving and strategic planning in AI, extending an open invitation to the research community for collaborative advancement in this domain.\n\n## Acknowledgements\n\nSpecial thanks to Prof. Rio Yokota for providing the necessary compute time on the Fugaku supercomputer and Yago Kastro for coding the first draft of the chess selfplay script.\n","date":1697587200000},{"slug":"clara-release","frontmatter":{"title":"CLARA: Advancing Machines in Understanding Speech Nuances","author":"Knoriy, Christoph, Robert","date":"Oct 16 2023","previewImg":"/images/blog/clara.png"},"content":"\nVoices carry not only words but also convey emotions, emphasis, and nuance through aspects like tone and accent. However, existing speech technology only partially comprehends these intricate components of human speech.\n\nIntroducing **CLARA (Multilingual Contrastive Learning for Audio Representation Acquisition)** – a project designed to enhance machines' understanding of the implicit aspects of speech. It aspires to develop a comprehensive pre-trained model dedicated to auditory communication.\n\n| **Ready to Make a Difference?** |\n|:------------------------------:|\n|Join our [Open Empathic Discord server](https://discord.gg/nFdG99UAav)!|\n|Explore our [CLARA project on GitHub](https://github.com/knoriy/CLARA) and its [interactive Demo Notebook](https://github.com/knoriy/CLARA/blob/master/clara/demo.ipynb). The corresponding research article can be found on [arxiv](https://arxiv.org/abs/2310.11830).|\n\n## The Driving Force Behind CLARA\n\nBuilding multilingual models from speech data poses significant challenges. We harness large, unlabeled datasets to derive transferable representations across various languages by utilising contrastive self-supervised learning.\n\nWe've all encountered stilted and unempathetic interactions with automated systems oblivious to our emotions and expressions. CLARA seeks to transform this by empowering machines to discern the full context of a speaker's voice. We aim to facilitate more natural and emotionally intuitive interactions with conversational AI, like virtual assistants and video game characters. The potential applications range from creating empathetic healthcare assistants to immersive gaming experiences.\n\n## Our Approach\n\nEmploying the latest advancements in contrastive learning, CLARA is trained on an extensive dataset of unlabeled multilingual speech recordings. It learns to identify linguistic and paralinguistic attributes by analysing and contrasting various examples.\n\nWe've amassed over 100,000 hours of speech data in numerous languages and applied augmentation strategies, such as introducing background noise and altering pitch, to enhance the training dataset's diversity. CLARA links audio and text encoders in its model architecture to map different modalities into a unified representation space, and we are exploring various configurations to optimise its performance.\n\n## Early Achievements\n\nPreliminary findings indicate CLARA's capabilities in multilingual speech representation learning. In its initial tests, the model showcased competitive or leading performance in cross-lingual emotion recognition using the CREMA-D benchmark.\n\nFor instance, in the CREMA-D dataset for emotional English speech, CLARA achieved an 88.44% accuracy using a linear probe classifier, nearing the existing best accuracy of 95.17% derived from a fully supervised and labelled model. This accomplishment is attained through self-supervised pre-training with unlabeled data and minimal subsequent tuning.\n\nCLARA also adeptly generalises to unfamiliar languages. Testing for emotion recognition in French, using the EmoV-DB dataset, achieved a 75.2% accuracy using a linear probe. Moreover, its transfer learning capabilities showcase a rapid adaptation to new languages and tasks by leveraging its foundational representations.\n\nTo fully unlock CLARA's potential, we're concentrating on enhancing the diversity and volume of the training data, especially in languages with limited resources. Your contributions to our multilingual datasets will be pivotal in achieving universal speech understanding.\n\n## Be Part of the Open Empathic Mission\n\nYour participation can propel this research forward. We're actively seeking contributions to enrich the project, particularly in diverse emotional styles, accents, and languages.\n\nOpen Empathic is a non-profit, open-source initiative. If you wish to contribute or collaborate, please connect with us! Together, we can forge ahead in enabling machines to comprehend and interact with us in more human-like ways.\n\nAre you ready to help machines truly comprehend our speech? Let's make it happen together.\n\n## Acknowledgement\n\nWe would like to thank Stability AI for their generous support in providing the essential compute resources for this project.\n","date":1697414400000},{"slug":"leo-lm","frontmatter":{"title":"LeoLM: Igniting German-Language LLM Research","author":"Björn Plüster","date":"September 28 2023","previewImg":"/images/blog/leolm-banner.jpg"},"content":"\nWe proudly introduce LeoLM (**L**inguistically **E**nhanced **O**pen **L**anguage **M**odel), the first comprehensive suite of German-language Foundation Language Models trained in collaboration with HessianAI on their new supercomputer **42**! Built on Llama-2 and trained on a large-scale, high-quality German text corpus, we present LeoLM-7B and 13B, with LeoLM-70B on the horizon, accompanied by a collection of exceptionally proficient German and bilingual chat models.\n\nMeet LeoLM, the first open and commercially available German Foundation Language Model built on Llama-2.\nOur models extend Llama-2's capabilities into German through continued pretraining on a large corpus of high-quality German and mostly locality-specific text.\nThanks to a compute grant at [HessianAI](https://hessian.ai/)'s new supercomputer **42**, we release two foundation models trained with 8k context length,\n[`LeoLM/leo-hessianai-7b`](https://huggingface.co/LeoLM/leo-hessianai-7b) and [`LeoLM/leo-hessianai-13b`](https://huggingface.co/LeoLM/leo-hessianai-13b) (70b also coming soon! 👀) under the [Llama-2 community license](https://ai.meta.com/llama/license/). In addition, we construct evaluation set for benchmarking capabilities of german language models to standardize model comparison, similar to widely adopted english based evals, as provided for instance by [lm-harness-eval](https://github.com/EleutherAI/lm-evaluation-harness) or [LLM-Foundry](https://github.com/mosaicml/llm-foundry).\nWith this release, LAION and Hessian.AI are poised to significantly enhance German open-source and commercial LLM research, fostering new opportunities and expediting widespread adoption.\n\nTry out  [**LeoLM/leo-hessianai-7b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-7b-chat) and [**LeoLM/leo-hessianai-13b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-13b-chat) on HuggingFace Spaces!\n\n*[[Read in German]](/blog-de/leo-lm)*\n\n## Introduction\n\nSince the release of the original Llama Foundation Models <sup>1</sup> in January of 2023, the open-source and academic research community\nhas experienced a rapid acceleration in the development of increasingly capable language models. The advances\nof the past weeks have brought the strongest Llama-2 <sup>2</sup>  based models closer to competing with OpenAI's ChatGPT based on GPT-3.5 or even the stronger GPT4 in some cases.\nNevertheless, a noteworthy constraint persists: the majority of these groundbreaking advancements remain confined to the realm of the English language.\nThis limitation stems mainly from large open-source models having been trained on predominantly mono-lingual English data. While there has been some\nresearch into second-language or multilingual finetuning, most resulting models are limited in their capabilities and suffer from the US-centric bias inherent to English data.\n\nWe seek to alleviate these issues in the case study of the German language by applying many of today's state-of-the-art techniques to develop a truly capable,\nlocalized, and bilingual LLM.\nTo this end, we present LeoLM (**L**inguistically **E**nhanced **O**pen **L**anguage **M**odel), a suite of Llama-2-based German foundation\nmodels, and an assortment of accompanying finetunes.\nFurther, we present GermanBench, a collection of the most relevant English Benchmarks translated into German, allowing us to evaluate LeoLM's capabilities thoroughly.\n\n<sup>1</sup> [Touvron et al. 2023a](https://arxiv.org/abs/2302.13971)\n<sup>2</sup> [Touvron et al. 2023b](https://arxiv.org/abs/2307.09288)\n\n## Stage-2 Pretraining\n\nLlama-2 models are pretrained on 2 trillion tokens of predominantly English text. To enhance their proficiency in the German language, we employ a Stage 2 pretraining methodology.\nWe initialize LeoLMs using Llama-2 weights and continue training the model on a large German text corpus of 65 billion tokens of deliberately filtered and deduplicated web text built from the [OSCAR-2301 corpus](https://huggingface.co/datasets/oscar-corpus/OSCAR-2301).\nA significant aspect of this approach is mitigating the forgetting or loss of previously learned knowledge or capabilities. We follow the findings by [Gupta et al. (2023)](https://arxiv.org/abs/2308.04014) in our choice of hyperparameters to minimize the risk of forgetting.\nAdditionally, we follow work by [Together](https://huggingface.co/togethercomputer/LLaMA-2-7B-32K) in employing [linear RoPE scaling](https://kaiokendev.github.io/til#extending-context-to-8k) and [Flash Attention 2](https://tridao.me/publications/flash2/flash2.pdf) to improve training efficiency and double context length to 8k tokens.\nSee Figure 1 for an overview of all training hyperparameters.\n\n![training_parameters](/images/blog/training_params.png \"Training Hyperparameters\")\n\n## Finetuning Datasets\n\nThere is much debate on what a good chat/instruction tuning dataset must offer, sparking the development of a plethora of different, successful approaches. We take inspiration from this diversity and, to bring similar capabilities to German, translate an assortment of high-quality instruction datasets to German using OpenAI's `gpt-3.5-turbo` API. Using `gpt-3.5-turbo` ensures that the context between prompts and responses remains intact and that complex instructions, potentially containing code, equations, or formatted data, are accurately translated.\nBuilding on community findings, we select a variety of datasets to translate and use to train our chat model.\nThe translated datasets are:\n\n- [OpenPlatypus](https://huggingface.co/datasets/garage-bAInd/Open-Platypus) -> [OpenSchnabeltier](https://huggingface.co/datasets/LeoLM/OpenSchnabeltier)\n- [OpenAssistant OASST1](https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25) -> [OpenAssistant-DE](https://huggingface.co/datasets/OpenAssistant/OASST-DE)\n\nMoreover, we use [`FreedomIntelligence/evol-instruct-deutsch`](https://huggingface.co/datasets/FreedomIntelligence/evol-instruct-deutsch) and [`FreedomIntelligence/alpaca-gpt4-deutsch`](https://huggingface.co/datasets/FreedomIntelligence/alpaca-gpt4-deutsch) from the [MultilingualSIFT](https://github.com/FreedomIntelligence/MultilingualSIFT) project. Thanks to the authors for sharing their data!\nTo facilitate bilingual use, we also train models on a combination of these translated datasets and their original, English counterparts.\n\nFinally, to make up for weaknesses in creative writing and rhyming identified during early tests, we curate two more datasets:\n\n- [GPT4 Poems](https://huggingface.co/datasets/LeoLM/German_Poems): A set of German poems about different topics written by GPT4\n- [GPT4 Songs](https://huggingface.co/datasets/LeoLM/German_Songs): A collection of German songs and following analyses written by GPT4.\n\n## Evaluation and Results\n\nEvaluating the capabilities of LLMs, especially chat models, is complex, and the best methods are still up for debate. Benchmarks based on multiple choice that are evaluated via the model's log-probabilities (as in the [Open LLM Leaderboard]()) are one currently popular method. Another method automatically evaluates responses using GPT4, as in AlpacaEval or MT-Bench. This approach is more geared toward chat models, as it considers the quality of model responses in real-life tasks. To be as comparable as possible, we directly translate a set of English benchmarks to German. We release these datasets in our [HF Organization](https://huggingface.co/LeoLM) and with more detailed documentation [on GitHub](https://github.com/bjoernpl/GermanBenchmark), and you can find the corresponding `lm-evaluation-harness` fork [here](https://github.com/bjoernpl/lm-evaluation-harness-de/tree/mmlu_de) and the `FastEval` fork [here](https://github.com/bjoernpl/FastEval).\n\nIn Figure 3, you can see a comparison of LeoLM versus the base Llama-2 models on a selection of Benchmarks with both the English version (blue) and our translated version (green). Our training improves benchmark scores on the German tasks while slightly reducing scores on English tasks. Notably, the mean increase in German benchmark scores significantly outweighs the mean decrease in performance on English benchmarks, showing that our approach enables learning a new language without forgetting what was previously learned. Why the scores remain lower in German than English is an open question but may be partially attributed to quality degradation during translation.\n\n![](/images/blog/benchmarks.png)\n\nThe following table shows the results on our translated version of MT-Bench. MT-Bench is a benchmark that evaluates multi-turn performance on a curated set of 80 questions from multiple categories using GPT-4 as a judge. In this, GPT-4 evaluates the prompts on a scale from 1-10 with regard to perceived helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. The monolingual `leo-hessianai-13b-chat` model performs best overall, even coming close to GPT-3.5 in the \"humanities\" topic. It scores noticeably poorly in math and coding, which is somewhat to be expected given that the Llama-2 models inherently fall short on this without very explicit finetuning. The bilingual models score slightly below their monolingual counterparts in some categories such math and reasoning while exceeding in coding and extraction.\n\n![](/images/blog/mt_bench.png)\nFor a more detailed evaluation, stay tuned for our paper!\n\n## Qualitative Results\n\nBenchmarks tend to be pretty abstract. To get a better feeling for LeoLM's check out our demos and try it yourself: [**LeoLM/leo-hessianai-7b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-7b-chat) and the bigger sibling [**LeoLM/leo-hessianai-13b-chat**](https://huggingface.co/spaces/LeoLM/leo-hessianai-13b-chat). Alternatively, you can run the model yourself using 🤗Transformers. Find more info on how to set this up on the [model card](https://huggingface.co/LeoLM/leo-hessianai-13b-chat).\n\n## Conclusion\n\nOur research has several key contributions:\n\n- We release a suite of German Foundation Language Models with a permissive license.\n- We transfer a thorough and multi-faceted evaluation approach for base and chat models into German.\n- We demonstrate that large-scale continued pretraining is possible even for data-saturated models such as Llama-2 without significant forgetting or loss of previous capabilities.\n- We present a diverse suite of instruction/chat-tuning datasets translated from English to German to serve as a basis for Germany's open-source LLM research community.\n\nOverall, the LeoLM model suite is a proof-of-concept of language acquisition for pretrained models. Moreover, it presents itself as the first openly available German Foundation Model that meets today's standards. We at LAION hope to kickstart the German open-source research community to reduce dependence on closed-source commercial sources such as OpenAI. Have fun with LeoLM!\n\n## Acknowledgments\n\nThis project was led by Björn Plüster with support, guidance and supervision by Christoph Schuhmann (LAION), Patrick Schramowski (LAION, Hessian.AI, DFKI) and Jenia Jitsev (LAION, Juelich Supercomputing Center, Helmholtz Research Center Juelich).\nWe would like to thank the folks at hessian.AI, especially Wolfgang Stille, Kristian Kersting, Florian Kieser and Patrick Diem\nfor their generous compute sponsorship via the Hessian.AI Innovation Lab (funded by the Hessian Ministry for Digital Strategy and Innovation) and the hessian.AISC Service Center (funded by the Federal Ministry of Education and Research (BMBF)), and their support throughout the training process!  Many thanks also to Jae Hee Lee from the University of Hamburg, Andreas Köpf from Open-Assistant, and community members at LAION for their helpful feedback. Special thanks also go to hessian.AI's Innovation Lab, the AI service centre hessian.AISC and the German Center for Artificial Intelligence (DFKI).\n\nMoreover, we would like to explicitly thank the creators of the datasets we use for finetuning: [OpenAssistant](https://huggingface.co/OpenAssistant), [FreedomIntelligence](https://huggingface.co/FreedomIntelligence), [garage-bAInd](https://huggingface.co/garage-bAInd), [WizardLM](https://huggingface.co/WizardLM) and [Instruction-Tuning-with-GPT-4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM).\n\n---\n\n### An Example\n\n**User:**  Warum ist open-source besonders in der KI-Forschung wichtig?\n\n**LeoLM:**\nOpen-Source spielt eine wichtige Rolle in der KI-Forschung, da sie Zusammenarbeit, Innovation und Zugang zur Technologie fördert. Hier sind einige Gründe, warum Open-Source in der KI-Forschung besonders wichtig ist:\n\n- Zusammenarbeit: Durch Open-Source können Forscher gemeinsam an Projekten arbeiten, Ideen austauschen und voneinander lernen. Dies fördert die Zusammenarbeit zwischen Forschern und kann zu schnelleren und effizienteren Fortschritten in der KI-Forschung führen.\n\n- Transparenz: Open-Source ermöglicht Transparenz und Reproduzierbarkeit in der KI-Forschung. Forscher können den Code und die Methoden anderer sehen, reproduzieren und darauf aufbauen, was die Validierung von Ergebnissen und die Identifizierung potenzieller Schwachstellen erleichtert.\n\n- Zugang zur Technologie: Open-Source macht KI-Technologien und -Werkzeuge einem breiteren Publikum zugänglich. Dies ermöglicht kleineren Forschungseinrichtungen und Einzelpersonen den Zugang zu KI-Werkzeugen, die ihnen sonst möglicherweise nicht zur Verfügung stünden. Dadurch kann Innovation und Forschung auch außerhalb traditioneller Zentren gefördert werden.\n\n- Innovation: Open-Source fördert Innovation in der KI-Forschung, indem sie Raum für Experimente und neue Ideen bietet. Da der Code offen und für alle zugänglich ist, können Forscher ihn an ihre spezifischen Bedürfnisse anpassen und neue KI-Werkzeuge und -Technologien entwickeln.\n\n- Lernen: Open-Source ermöglicht es Forschern, voneinander zu lernen. Durch die gemeinsame Nutzung von Code und Methoden können Forscher aus den Erfahrungen anderer lernen und ihre eigenen Forschungsprojekte verbessern.\n\nZusammenfassend ist Open-Source ein wichtiger Aspekt der KI-Forschung, da er Zusammenarbeit, Transparenz, Zugang zur Technologie, Innovation und Lernen fördert. Dadurch wird die KI-Forschung effizienter, innovativer und zugänglicher für eine breitere Palette von Forschern und Institutionen\n\n| [Link to the PDF with more examples](/documents/leo-lm.pdf) |\n|----------|\n","date":1695859200000},{"slug":"open-lm","frontmatter":{"title":"Introducing OpenLM","author":"OpenLM team","date":"September 26 2023","previewImg":"/images/blog/new_openlm_teaser.png"},"content":"![](/images/blog/openlm_teaser_wide.png)\n## Introduction\n\nWe release [OpenLM](https://github.com/mlfoundations/open_lm) a simple and minimalist PyTorch codebase for training medium-sized language models. OpenLM is designed to maximize GPU utilization and training speed, and is easy to modify for new language model research and applications.\n\n\nWe validate OpenLM by training two language models, OpenLM-1B and OpenLM-7B, on 1.6T and 1.25T tokens of text, respectively. We evaluate these models on standard zero-shot text classification and multiple choice tasks and find that OpenLM-1B outperforms many popular, similarly sized models such as OPT-1.3B and Pythia-1B.  OpenLM-7B achieves similar performance to LLAMA-7B and MPT-7B.\n\nIn this blogpost, we briefly describe the training data, model, evaluation setup, and overall results. We also describe exciting future work we plan to pursue with these models and our OpenLM framework. \n\n## Model and Data Release\n\nAll models and training data (tokenized and shuffled) are available on Huggingface at the following links:\n\n* [OpenLM-1B](https://huggingface.co/mlfoundations/open_lm_1B)\n* [OpenLM-7B](https://huggingface.co/mlfoundations/open_lm_7B_1.25T)\n* [Training and validation data](https://huggingface.co/datasets/mlfoundations/open_lm_example_data)\n\nWe are working on releasing intermediate checkpoints.\n  \n## Data\nWe train our models on a collection of text totaling 1.6T tokens. The training data comes from the following sources:\n\n\n| Dataset       | Tokens  | Percentage |\n|---------------|---------|------------|\n| RedPajama    | 1157.3B | 72.6% |\n| Pile         | 336.2B  | 21.1% |\n| S2ORC         | 48.9B   | 3.1% |\n| Pile of Law   | 27.1B   | 1.7% |\n| RealNews     | 25.0B   | 1.6% |\n| Total         | 1594.5B | 100% |\n\n\nWe do not perform additional preprocessing on the text, and take the data as is from the original sources. To train our model on these data sources, we simply use the following data mix: 72.6% on RedPajama, 27.4% everything else. This follows the given distribution of data in the table above.\n\n## Models\n\nThe models we train follow the basic architecture proposed by LLaMA. The two differences are that we use the  **GPT-NeoX tokenizer**, which we found to be effective in early experiments, and we use LayerNorm instead of RMSNorm, because we haven’t yet added a fused RMSNorm operation. \n\nThe 1B model is trained with AdamW (LR 1e-3, weight decay 0.1) on 128 A100 40GB GPUs, with a global batch size of 2M tokens. \n\nThe 7B model is trained with AdamW (LR 3e-4, weight decay 0.1) on 256 A100 40GB GPUs, with a global batch size of 4M tokens. \n\nThe training speed for the 7B model is 2300 tokens/s/GPU. For model parallelism we use PyTorch FSDP. \n\nAside from the model, the codebase closely follows OpenCLIP which has been tested on around 1,000 GPUs.\n\n## Evaluation Setup\n\nDuring training, we track validation loss using a held out subset of recent papers from the authors of the OpenLM library, breaking news at the time of development, and the OpenLM codebase.\n\nAfter training, we use the LLM-foundry to  evaluate model performance on the 13 zero-shot tasks used to evaluate MPT-7B and LLaMA 7B in the MPT-7B release. We additionally evaluate 5-shot MMLU performance.\n\n## Results\n\n### Validation Loss\n\nHere, we display the validation loss for up to 1T tokens of training for both the OpenLM-1B and 7B models:\n\n![validation loss of 1b model](/images/blog/1B_loss.png)\n\n\n![validation loss of 7b model](/images/blog/7B_loss.png)\n\n\n## Downstream Evaluations\n\nHere, we display the zero-shot evaluation results of OpenLM-1B throughout training:\n\n\n| **OpenLM-1B** | **250B  tokens** | **500B tokens** | **750B tokens** | **1T tokens** | **1.25T tokens** | **1.5T tokens** | **1.6T tokens** |\n|----------------|-----------------|-----------------|-----------------|---------------|------------------|-----------------|-----------------|\n|Training progress | 16% complete | 31% complete | 47% complete | 63% complete | 78% complete | 94% complete | 100% complete |\n| arc_challenge  |            0.27 |            0.28 |            0.29 |          0.28 |             0.29 |            0.31 |            0.31 |\n| arc_easy       |            0.49 |            0.50 |            0.51 |          0.53 |             0.54 |            0.56 |            0.56 |\n| boolq          |            0.60 |            0.61 |            0.62 |          0.62 |             0.65 |            0.64 |            0.65 |\n| copa           |            0.71 |            0.70 |            0.70 |          0.78 |             0.71 |            0.73 |            0.70 |\n| hellaswag      |            0.50 |            0.54 |            0.54 |          0.57 |             0.59 |            0.61 |            0.61 |\n| lambada_openai |            0.56 |            0.57 |            0.61 |          0.61 |             0.65 |            0.65 |            0.66 |\n| piqa           |            0.70 |            0.70 |            0.71 |          0.72 |             0.73 |            0.74 |            0.74 |\n| winogrande     |            0.55 |            0.57 |            0.58 |          0.59 |             0.61 |            0.60 |            0.60 |\n| MMLU           |            0.24 |            0.24 |            0.24 |          0.23 |             0.26 |            0.24 |            0.25 |\n| Jeopardy       |            0.01 |            0.02 |            0.01 |          0.01 |             0.04 |            0.09 |            0.10 |\n| Winograd       |            0.75 |            0.77 |            0.77 |          0.79 |             0.81 |            0.80 |            0.79 |\n|                |                 |                 |                 |               |                  |                 |                 |\n| **Average**    |        **0.49** |        **0.50** |        **0.51** |      **0.52** |         **0.53** |        **0.54** |        **0.54** |\n\n\nAs a comparison, here are the zero-shot results of similarly sized baselines. Our model achieves similar performance to OPT-IML-1.3B, an instruction-tuned model.\n\n| **1B Baselines** | **OPT-1.3B** | **Pythia-1B** | **Neox-1.3B** | **OPT-IML-1.3B** | **OpenLM-1B** |\n|------------------|-------------:|--------------:|--------------:|-----------------:|-----------------:|\n| arc_challenge    |         0.27 |          0.26 |          0.26 |             0.30 |             0.31 |\n| arc_easy         |         0.49 |          0.51 |          0.47 |             0.58 |             0.56 |\n| boolq            |         0.58 |          0.61 |          0.62 |             0.72 |             0.65 |\n| copa             |         0.75 |          0.68 |          0.72 |             0.73 |             0.70 |\n| hellaswag        |         0.54 |          0.49 |          0.48 |             0.54 |             0.61 |\n| lambada_openai   |         0.59 |          0.58 |          0.57 |             0.57 |             0.66 |\n| piqa             |         0.72 |          0.70 |          0.72 |             0.73 |             0.74 |\n| winogrande       |         0.59 |          0.53 |          0.55 |             0.59 |             0.60 |\n| MMLU             |         0.25 |          0.26 |          0.26 |             0.30 |             0.25 |\n| Jeopardy         |         0.01 |          0.00 |          0.00 |             0.12 |             0.10 |\n| Winograd         |         0.74 |          0.71 |          0.75 |             0.73 |             0.79 |\n| **Average**      |     **0.50** |      **0.48** |      **0.49** |         **0.54** |         **0.54** |\n\nNext, we display the zero-shot evaluation results of OpenLM-7B throughout training:\n\n | **OpenLM-7B**  | **275B tokens** | **500B tokens** | **675B tokens** | **775B tokens** | **1T tokens** | **1.25T tokens** |\n|-----------------|-----------------|-----------------|-----------------|-----------------|---------------|------------------|\n| Training progress | 17% complete | 31% complete | 42% complete | 48% complete | 63% complete | 78% complete |              |\n| arc_challenge   |            0.35 |            0.35 |            0.36 |            0.37 |          0.39 |             0.39   |\n| arc_easy        |            0.60 |            0.61 |            0.62 |            0.62 |          0.63 |             0.66  |\n| boolq           |            0.67 |            0.66 |            0.69 |            0.69 |          0.70 |             0.70    |\n| copa            |            0.75 |            0.79 |            0.75 |            0.80 |          0.80 |             0.78     |\n| hellaswag       |            0.64 |            0.67 |            0.68 |            0.68 |          0.69 |             0.70     |\n| lambada_openai  |            0.67 |            0.68 |            0.69 |            0.70 |          0.70 |             0.70       |\n| piqa            |            0.75 |            0.76 |            0.76 |            0.76 |          0.77 |             0.77      |\n| winogrande      |            0.62 |            0.65 |            0.65 |            0.65 |          0.67 |             0.67       |\n| MMLU-0 shot     |            0.25 |            0.25 |            0.27 |            0.27 |          0.28 |             0.30       |\n| Jeopardy        |            0.15 |            0.18 |            0.23 |            0.22 |          0.16 |             0.21       |\n| Winograd        |            0.82 |            0.81 |            0.84 |            0.84 |          0.85 |             0.86        |\n| **Average**     |        **0.57** |        **0.58** |        **0.60** |        **0.60** |      **0.60** |         **0.61**      |\n\n\n\n|Task | **OpenLM-7B** | **LLAMA-7B** | **MPT-7B** |\n|-----------------|-----------------|-----------------|-----------------|\n| arc_challenge   |              0.39  |         0.41 |       0.39 |\n| arc_easy        |            0.66  |         0.65 |       0.67 |\n| boolq           |               0.70   |         0.77 |       0.75 |\n| copa            |                      0.78      |         0.78 |       0.81 |\n| hellaswag       |                   0.70    |         0.75 |       0.76 |\n| lambada_openai  |                0.70      |         0.74 |       0.70 |\n| piqa            |                   0.77      |         0.79 |       0.80 |\n| winogrande      |                 0.67      |         0.68 |       0.68 |\n| MMLU-0 shot     |                    0.30        |         0.30 |       0.30 |\n| Jeopardy        |                       0.21        |         0.33 |       0.31 |\n| Winograd        |                       0.86              |         0.81 |       0.88 |\n| **Average**     |              **0.61**      |     **0.64** |   **0.64** |\n| **MMLU-5 shot** |                      **0.34**          |     **0.34** |            |\n\n\nConsistent with the validation loss, our models continue to improve in zero-shot performance even late in training. At 1.25T tokens, OpenLM-7B matches or outperforms LLaMA-7B or MPT-7B on 7 out of 11 tasks.\n\n## Future Work\n\nOpenLM has already enabled new language modeling research, for example in the development of [low-risk language models trained on permissively licensed text](https://arxiv.org/abs/2308.04430). We plan to use OpenLM to support a variety of new research directions, including multimodal models, mixture of experts, and dataset composition. We also plan to scale up OpenLM so it supports training larger models.\n\n## Team and acknowledgements\n\nThe OpenLM team currently consists of: Suchin Gururangan*, Mitchell Wortsman*, Samir Yitzhak Gadre, Achal Dave, Maciej Kilian, Weijia Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard Heckel, Alex Dimakis, Ali Farhadi, Vaishaal Shankar, Ludwig Schmidt.\n\nCode is based heavily on [open-clip](https://github.com/mlfoundations/open_clip) developed by a team including Ross Wightman, Romain Beaumont, Cade Gordon, Mehdi Cherti, Jenia Jitsev, and [open-flamingo](https://github.com/mlfoundations/open_flamingo), developed by a team including Anas Awadalla and Irena Gao. Additional inspiration is from [lit-llama](https://github.com/Lightning-AI/lit-llama).\n\nWe thank Stability AI for providing the compute for this project, the RedPajama team for their dataset, Sarah Pratt for logo design, [IFML](https://www.ifml.institute/), and Toyota Research Institute. We also thank the following people for helpful advice and feedback throughout the project: Jonathan Frankle, Daniel King, Luca Soldaini.","date":1695686400000},{"slug":"transparent-ai","frontmatter":{"title":"Towards a transparent AI Future: The Call for less regulatory hurdles on Open-Source AI in Europe","author":"LAION","date":"September 21 2023","previewImg":"/images/blog/laion-blue.png"},"content":"\nFollowing our previous open letter to the European Parliament on the significance of open-source AI, LAION, backed by European Laboratory for Learning and Intelligent Systems (ELLIS) and a long list of very impactful AI researchers, we submit this new open letter to the European Parliament:\n\n| [Link to the PDF](/documents/transparent-ai.pdf) |\n|----------|\n\n#### Why Open-Source is the Gold Standard for AI Security\n\nThe transparency of open-source AI is its strength. It ensures robustness and security unmatched by closed systems. Why? Open-source AI benefits from the scrutiny of the global community, allowing vulnerabilities to be detected and fixed promptly. Drawing parallels, we can look at the Linux operating system—a paragon of security and robustness stemming from its open-source nature.\n\n#### Countering Redundancy and Upholding Sustainability\n\nWith the environmental toll of extensive AI training becoming a major concern, open-source models have shown a clear path forward. By minimizing redundant training, they reduce computational and energy overheads, reflecting a commitment to a sustainable future.\n\n#### Ensuring Scientific Reproducibility\n\nReproducibility and validation are key to scientific integrity and progress. Open-source AI models offer full transparency, allowing diverse research groups to independently verify results and claimed functionality. Unlike closed-source alternatives, open-source foundations guarantee stringent standards for the machine learning and AI field. With these open-source foundation models rigorously tested by a vast expert community, AI applications in sectors from healthcare to finance can build on a trusted, scientifically validated base.\n\n#### A Catalyst for Innovation\n\nOpen-source AI has been instrumental in levelling the playing field. Small and mid-sized enterprises can now fine-tune existing models, fostering innovation without the daunting costs of building from scratch. If Europe's ambition is to retain its brightest minds, ensuring uninterrupted access to these resources is non-negotiable.\n\n#### Regulating Application, Not Innovation\n\nThe clarion call from LAION and its supporters is clear—focus regulations on AI's applications, not the foundational technology. By doing so, the EU will nurture innovation while ensuring that AI's real-world applications are ethical, safe, and in line with European values.\n\n#### Incentivizing the Open-Source Paradigm\n\nPerhaps the most potent recommendation in this new letter is the incentivization of open-source AI. It's a win-win. Organizations can release foundational models as open-source, maintaining proprietary rights on fine-tuned versions. This ensures that the broader community benefits from the base models while commercial competitiveness remains intact.\n\n#### The European AI Path Forward\n\nEuropean sovereignty in AI is crucial, and open-source AI research is key to addressing challenges ranging from healthcare to climate change. The future, as outlined in the letter, imagines a Europe at the forefront of AI research, one that champions transparency, security, and sustainability.\n\n#### Supporters\n\n| Name | Description |\n|----------|----------|\n| Board of the European Laboratory for Learning and Intelligent Systems (ELLIS): Serge Belongie, Nicolò Cesa-Bianchi, Florence d'Alché-Buc, Nada Lavrac, Neil D. Lawrence, Nuria Oliver, Bernhard Schölkopf, Josef Sivic, Sepp Hochreiter| [European Lab for Learning & Intelligent Systems (ellis.eu)](https://ellis.eu/board)   |\n| Yann André LeCun | Chief AI Scientist at Facebook & Silver Professor at the Courant Institute, New York University |\n| Jürgen Schmidhuber | Prof. Jürgen Schmidhuber : Scientific Director of the Swiss AI Lab IDSIA (USI & SUPSI), Co-Founder & Chief Scientist of NNAISENSE, Father of Modern AI |\n| Kristian Kersting | Full Professor at Technical University of Darmstadt, Co-Director, Hessian Center for AI (hessian.AI) and member of the German Center for Artificial Intelligence (DFKI)   |\n| Björn Ommer   | Full professor and head of the Computer Vision & Learning Group at the Ludwig-Maximilians-University of Munich   |\n| Hilde Kuehne   | Professor, Institute for Computer Science II, Head of Multimodal Learning, University of Bonn   |\n| Mira Mezini   | Professor of Computer Science at Technical University of Darmstadt, Co-Director of Hessian Center for AI (hessian.AI)   |\n| Patrick Schramowski   | Senior Researcher at the German Center for Artificial Intelligence (DFKI) and Hessian Center for AI (hessian.AI)   |\n| Jenia Jitsev   | Expert in multi-modal foundation models and datasets. LAION core member and contributor. Member OpenBioML. Researcher at Helmholtz Juelich Supercomputing Center, Germany.  |\n| Marianna Nezhurina | Senior Researcher and Lab Lead at Juelich Supercomputing Center, Helmholtz Research Center Juelich. Scientific Lead and Co-Founder at LAION; Member of European Laboratory for Learning and Intelligent Systems (ELLIS)   |\n| Dominik L. Michels   | Full Professor of Intelligent Algorithms in Modeling and Simulation at the Technical University of Darmstadt   |\n| Tim Dettmers   | PhD Student at The University of Washington. Creator of the bitsandbytes library.   |\n| Mark Schutera   | PhD Student Karlsruhe Institute of Technology within Unsupervised Deep Learning for Cognitive Perception Systems   |\n| Andreas Hochlehnert   | PhD Student, University of Tübingen, International Max-Planck Research School for Intelligent Systems (IMPRS-IS)   |\n| Irina Rish | Full Professor at the Université de Montréal, a core member of Mila - Quebec AI Institute. Canada Excellence Research Chair (CERC) in Autonomous AI and CIFAR Chair. PI on a collaborative INCITE project on the Summit supercomputer at OLCF (supported by the U.S. DoE, Office of Science ), aiming to build open-source large-scale language and multimodal models (e.g., RedPajama-INCITE was trained as a part of this project). |\n| Huu Nguyen | Former big-law partner, CEO and co-founder of Ontocord.AI, LAION volunteer, co-author of Data Governance in the Age of Large-Scale Data-Driven Language Technology, FAccT ’22, and co-author of resolution 112 of the ABA on encouraging lawyers to understand the risks and benefits of AI. |\n| David Ha | Co-Founder and CEO of sakana.ai |\n| Hessie Jones | Writer, Forbes, Data Privacy, Ethical AI Practitioner, Advocating for Human-centred AI and Ethical Distribution of AI Systems, BOA Women in AI Ethics, Cofounder MyData Canada, Cofounding Member Personally Identifiable Information Standards Architecture (PIISA); former COO Beacon Trust Network, BOD Technology for Good Canada  |\n| Sampo Pyysalo | Research Fellow, University of Turku, co-lead TurkuNLP research group, Principal Investigator, High-Performance Language Models (Horizon EU project), leading multiple efforts to create very large open models.  |\n| Wolfgang Stille | Chief Technical Officer of the Hessian Center for AI (hessian.AI) and project lead of the AI Innovation Lab and the AI Service Center hessian.AISC. He has been involved with digital research infrastructure and open science for many years and was a leading participant in the process of establishing a digital research data culture at Hessian universities.  |\n| Christoph Schuhmann    | Organizational Lead & Co-Founder of the Large-scale AI Open Network (LAION), Neurips 2022 Outstanding Paper Award & Falling Walls Breakthrough of the Year 2023 Award Winner   |\n| Robert Kaczmarczyk   | Medical Lead & Co-Founder of the Large-scale AI Open Network (LAION), Neurips 2022 Outstanding Paper Award & Falling Walls Breakthrough of the Year 2023 Award Winner   |\n","date":1695254400000},{"slug":"falling-walls-2023","frontmatter":{"title":"LAION Triumphs at the Falling Walls Science Breakthrough of the Year 2023 Awards","author":"Christoph, Jenia, Robert","date":"September 14, 2023","previewImg":"/images/blog/FW23.jpg"},"content":"We happily announce that we, LAION, won the Falling Walls Science Breakthrough of the Year 2023 Award in the category Science and Innovation Management for \"democratizing AI research by providing open access to advanced AI models, tools, and datasets, fostering public engagement and awareness, and promoting international collaboration to create a transparent and inclusive AI ecosystem that benefits everyone.\" This recognition is not just for the select few but for our entire LAION community of hobby scientists, university professors, students, and enthusiasts all united with a shared vision - the democratization of AI research.\n\n![FW23A](/images/blog/FW23A.png)\n\n## About the Award\n\nThe Falling Walls Science Breakthrough of the Year Award, bestowed by the non-profit Falling Walls Foundation, celebrates groundbreaking achievements across various academic disciplines. Inspired by the historic fall of the Berlin Wall, it embodies the spirit of overcoming barriers to forge a brighter future for society.\n\nRecognized globally, this prestigious award spotlights innovative research addressing today’s pressing challenges. It facilitates interdisciplinary collaboration, underscoring the transformative potential of science in building a better future.\n\nThe accolade spans six categories, recognizing ten finalists in each, handpicked from over a thousand applications by a panel of expert jurors. It culminates in the Falling Walls Science Summit, a vital hub for dialogue between global science leaders and society.\n\n## Grassroots Leadership\n\nThis acknowledgment is not just a testament to our dedication but a clear indication of the significant role grassroots communities play in advancing the world through open-source AI research. It highlights the fact that with unity and shared vision, even grassroots initiatives can spearhead meaningful change, demonstrating that we are not only contributors but essential players in fostering a better, more inclusive future.\n\n## Join the Movement\n\nWe extend our heartfelt gratitude to the Falling Walls Foundation for acknowledging our efforts and to every member of the LAION community for being an indispensable part of this journey.\n\nAs we celebrate this monumental achievement, we invite you to become a part of this vibrant community. Together, we can foster a culture of open dialogue, collaboration, and innovative solutions. Join us on [Discord](https://discord.com/invite/eq3cAMZtCC) as we continue to break walls and democratize AI research for the betterment of society globally.\n\nFor more details on the award, visit the [official announcement](https://falling-walls.com/press-releases/falling-walls-announces-science-breakthrough-of-the-year-2023-laureates/).\n","date":1694649600000},{"slug":"visit_bench","frontmatter":{"title":"Introducing VisIT-Bench, a new benchmark for instruction-following vision-language models inspired by real-world use","author":"Yonatan Bitton","date":"August 15, 2023","previewImg":"/images/blog/visit_bench/1_fig1.png"},"content":"\n[[Paper]](https://arxiv.org/abs/2308.06595) [[Code]](https://github.com/mlfoundations/VisIT-Bench/) [[Dataset]](https://huggingface.co/datasets/mlfoundations/VisIT-Bench) [[Leaderboard]](https://huggingface.co/spaces/mlfoundations/VisIT-Bench-Leaderboard)\n\nWe are thrilled to introduce **VisIT-Bench**, a benchmark for evaluating instruction-following vision-language models (VLMs). The central goal of VisIT-Bench is to provide a more accurate and meaningful assessment of VLMs, particularly in the context of human-chatbot interactions inspired by real-world scenarios.\n\nVisIT-Bench comprises 678 examples. Each example includes:\n- An image (or multiple images)\n- An instruction\n- An \"instruction-conditioned caption\" - a detailed caption allowing a text-only entity to follow the instruction \n- A GPT-4 response suggestion\n- A label verifying the accuracy of the response\n![](/images/blog/visit_bench/2_example.png)\n\nVisIT-Bench comes with an easy automatic evaluation that correlates well with human preferences from over 5,000 annotations. Our evaluations underscore a clear need for advancement in VLMs. The top model on our benchmark exceeded the human-verified GPT-4 reference in only 27% of comparisons, highlighting both the challenges and potential for future progress in this area.\n\n## Why VisIT-Bench?\n\nThough recent VLMs have shown promise in following instructions, their evaluation for real-world human-chatbot instructions is often limited. Typically, VLMs are evaluated through qualitative comparison of outputs, which makes it challenging to quantify progress and potential shortcomings. VisIT-Bench helps address this problem by offering a comprehensive testbed for measuring model performance across a diverse set of instruction-following tasks, inspired by real world scenarios.\n\n## Building the Benchmark\n\nTo maximize the variety of skills needed for evaluation, VisIT-Bench draws from two main sources: new instruction collection and the repurposing of existing datasets. This dual-source approach ensures a comprehensive assessment of multimodal chatbots.\n![](/images/blog/visit_bench/1_fig1.png)\n\n### Data Collection Framework\n\n**VisIT-Bench** is a benchmark consisting of 679 vision-language instructions. Each instruction pairs an image with a corresponding request or question. For example, an image might depict a storefront with two portable wedge ramps, accompanied by the question: *Would a disabled wheelchair-bound individual find it easy to go into this store?* Contrary to prevalent zero-shot evaluations, many instructions emphasize open-ended generation requests, like *write a poem...* or *what should I bring if I were to visit here?*.\n\nVisIT-Bench employs a structured data collection strategy encompassing four steps.\n\n(1) **Instruction Generation**: Convert a single example from a skill category into multiple instances. Using references from an instruction family (instruction, image, model output), annotators craft new instructions linked to a public image URL. For instance, under the *Contextual Knowledge of Events* family, an instance about a related event might be generated.\n![](/images/blog/visit_bench/3_1_collection.png)\n\n(2) **Instruction-Conditioned Caption Generation**: Annotators produce detailed captions for each image, based on the given instruction and image. The aim is a text-rich caption enabling text-only comprehension, which serves as a foundation for GPT-4 outputs and text-only evaluations.\n![](/images/blog/visit_bench/3_2_collection.png)\n\n(3) **Generating GPT4 Responses**: Response candidates from GPT-4 are fetched for later human validation. Using the prompt: “Consider an image depicted by: `<caption>`. Follow this instruction: `<instruction>`. Response: “.\n\n(4) **Model Output Evaluation**: GPT-4's instruction-following ability is assessed using text-only pairs. With the image, dense caption, instruction, and GPT-4's prediction, annotators evaluate GPT-4's adherence to instructions, pinpointing errors in captions or responses and flagging inappropriate content.\n![](/images/blog/visit_bench/3_3_collection.png)\n\n### Repurposing Existing Datasets\n\n**VisIT-Bench** converts 25 datasets (250 instances) into chatbot-friendly formats, including ten multi-image datasets. These datasets include [VQA](https://visualqa.org/), [VCR](https://visualcommonsense.com/), [TextCaps](https://textvqa.org/textcaps/), [WHOOPS!](https://whoops-benchmark.github.io/) and more. Each instance consists of an instruction prompt and a chatbot response. For instance, our adaptation of the [NLVR2 dataset](https://github.com/lil-lab/nlvr/tree/master/nlvr2) is designed to test visual reasoning capabilities for chatbots. Originally, the NLVR2 format featured a sentence for analysis, two images, and a binary response. In our version, we incorporated a zero-shot prompt, detailed image captions aligned with the instruction, and a GPT-4-verified human response. This method is devised to enhance previous studies to align with contemporary chatbot evaluation metrics.\n![](/images/blog/visit_bench/4_nlvr_image.png)\n\n### Features of the Dataset\n\nVisIT-Bench is a unique composition of 70 unique instruction families, 25 repurposed prior datasets, including 10 multi-image datasets, each embodying a different skill that a chatbot model should ideally exhibit. These families mirror practical real-world chatbot interactions, thus ensuring that our benchmark evaluates models against realistic and varied tasks. \n![](/images/blog/visit_bench/7_table_compare.png)\n\n## Models\n\nOur evaluation includes a variety of publicly accessible vision-language models, either fine-tuned with multimodal instructions or designed to execute based on LLM outputs. These models include LLaVA-13B, InstructBLIP-13B, MiniGPT4-7B, mPLUG-Owl-7B, LlamaAdapter-v2-7B, PandaGPT-13B, VisualChatGPT, Multimodal GPT, OpenFlamingo v1, and Otter v1. \n\n## Human-Guided Rankings\n\nUsing VisIT-Bench's single-image examples, we created 5,000 pairwise comparisons across multiple vision-language models. Each model was in about 700 comparisons, including GPT-4's outputs. Three annotators blind to the model origins ranked the outputs for correctness and comprehensiveness. Results underscore the effectiveness of GPT-4 and LLaVA (13B) on this dataset. GPT-4's success stemmed from conditioned dense captions, while LLaVA benefited from its instruction-tuning dataset. Interestingly, LlamaAdapter-v2 (7B) outperformed in direct comparisons to reference outputs, emphasizing the importance of language instruction fine-tuning for these models. The relationship between model design, task variety, and performance needs more exploration.\n![](/images/blog/visit_bench/8_human_performance.png)\n\n## Automatic Evaluation and Dynamic Leaderboard\n\nWe introduced an automatic evaluation framework to rank model outputs, employing Elo-ratings and win-rate against the ground truth. This approach addresses potential bias, considering GPT-4 was used to assess its own output, and shows high agreement with human ratings. Metrics include: Reference-free Elo score, mirroring human evaluation; Reference-backed Elo score, incorporating the prompt's optional reference; and Win-rate against reference, indicating how often a model's output is preferred over reference. Our findings reveal consistent rankings between reference-free and backed evaluations. However, existing instruction-following models' win rates vary widely, underscoring a clear gap in their performance against reference outputs.\n![](/images/blog/visit_bench/9_auto_eval_rating.png)\n![](/images/blog/visit_bench/10_auto_eval_expanded.png)\n\n### Correlation of the Automatic and Human-Annotated Preferences\n\nOur GPT-4 based metric surpasses others, closely mirroring majority-vote human judgments. In cases with unanimous agreement (5/5 annotators), GPT4-no-ref hits 93% accuracy, outdoing BERTScore (80%), METEOR (78%), and ROUGE-L (70%). Against a length baseline metric (60%), these metrics provide viable offline evaluation options without needing OpenAI API access. The reference-free GPT-4 metric performs similarly to its reference-backed counterpart, allowing references in the evaluation setup..\n![](/images/blog/visit_bench/11_correlation.png)\n\n## Contributions\n\nThis effort was made possible thanks to the amazing team of:\n\n- [Yonatan Bitton*](https://yonatanbitton.github.io/), The Hebrew University of Jerusalem, Google Research\n- [Hritik Bansal*](https://sites.google.com/view/hbansal), University of California, Los Angeles\n- [Jack Hessel*](https://jmhessel.com/), Allen Institute for AI\n- [Rulin Shao](https://rulinshao.github.io/), University of Washington\n- [Wanrong Zhu](https://wanrong-zhu.com/), University of California, Santa Barbara\n- [Anas Awadalla](https://anas-awadalla.streamlit.app/), University of Washington\n- [Josh Gardner](https://homes.cs.washington.edu/~jpgard/), University of Washington\n- [Rohan Taori](https://www.rohantaori.com/), Stanford\n- [Ludwig Schmidt](https://people.csail.mit.edu/ludwigs/), Allen Institute for AI, University of Washington, LAION\n\n*Equal contribution.\n\n## In Conclusion\n\n**VisIT-Bench** offers a comprehensive lens on VLMs by utilizing 70 carefully curated instruction families, mirroring a wide range of real-world scenarios. This approach allows an in-depth assessment of model understanding but paves the way for enhancing VLMs' performance across various tasks. VisIT-Bench is dynamic to participate, practitioners simply submit their model's response on the project website; Data, code and leaderboard is available at the [project website](https://visit-bench.github.io/).\n\n![](/images/blog/visit_bench/13_leaderboard.png)\n","date":1692057600000},{"slug":"objaverse-xl","frontmatter":{"title":"Objaverse-XL: An Open Dataset of Over 10 Million 3D Objects","author":"Matt Deitke","date":"Jul 11 2023","previewImg":"/images/blog/objaverse-xl.jpg"},"content":"\n![](/images/blog/objaverse-xl.jpg)\n\nWe are thrilled to announce Objaverse-XL, an open dataset of over 10 million 3D objects! Using it, we train Zero123-XL, a foundation model for 3D that displays remarkable generalization abilities. In the landscape of AI, scale has been paramount to recent advances. Over the past decade, we have observed an escalating trend of leveraging large volumes of data to train machine learning models, particularly in NLP and 2D vision. But what about 3D vision tasks? Despite the burgeoning demand for augmented reality (AR) and virtual reality (VR) applications, advancements in 3D vision have lagged, primarily due to the scarcity of high-quality 3D data.\n\nObjaverse 1.0, released back in December, was a step in the right direction, and enabled exciting research like Zero-1-to-3 for novel view synthesis and single view 3D reconstruction. But, it was still quite small, being on the order of 800K objects. With Objaverse-XL, we scale up the number of 3D objects that we use from 800K to over 10 million deduplicated 3D objects, pulling in objects from a variety of sources.\n\n![](/images/blog/dataset-distribution.jpg)\n\nObjaverse-XL is a curated amalgamation of 3D objects from various internet sources. It includes objects from GitHub, sourced from over 500k repositories, along with assets from platforms like Thingiverse, Sketchfab, Polycam, and the Smithsonian 3D Digitization project. The result is a remarkable collection of over 10 million unique 3D objects, each coming with its metadata. The figure above shows a t-SNE projection of CLIP L/14 embeddings on a subset of rendered objects. Compared to Objaverse 1.0 (orange), Objaverse-XL more densely captures the distribution of 3D assets.\n\nWith Objaverse-XL, we train Zero123-XL. Leveraging the view-conditioned diffusion model proposed by Zero123, the Zero123-XL model was pretrained on a larger dataset, Objaverse-XL, to yield better zero-shot generalization performance. Remarkably, we find that the enhanced pre-training on Objaverse-XL enables the model to generalize significantly better to challenging data categories like people, cartoons, and sketches. It showed improvements in generating novel views that both maintained original style and object geometric details. Further testing on the Google Scanned Objects dataset also showed that as the dataset size increased, so did the visual similarity score between predicted and actual views. Finally, alignment fine-tuning on a high-quality subset of Objaverse-XL led to significant improvement in the model's alignment with human preferences, illustrating the potential of large-scale pretraining and strategic finetuning in advancing 3D vision.\n\n![](/images/blog/zero123-xl.jpg)\n\nIn the figure above, we show novel view synthesis on in-the-wild images (either coming from internet searches or text-to-image models). It shows a comparison between Zero123-XL trained on Objaverse-XL and Zero123 trained on Objaverse. Starting from the input view, the task is to generate an image of the object under a specific camera pose transformation. The camera poses are shown beside each example. Significant improvement can be found by training with more data, especially for categories including people (1st row), anime (2nd row), cartoon (3rd row), furniture (4th row), and sketches (5th row). Additionally, viewpoint control is significantly improved (see 2nd row).\n\nWe are thrilled to see what the community is able to build with Objaverse-XL! More information for accessing the dataset, models, and code will be provided soon!\n\n### Acknowledgements\n\nObjaverse-XL was a joint effort between several teams, including the Allen Institute for AI,\nColumbia University, the University of Washington, Stability AI, LAION, and Caltech.\n\nRuoshi Liu did a tremendous job leading the Zero123-XL efforts!\nThe rest of the fantastic team includes Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, and Kiana Ehsani, and was co-advised primarily by Ludwig Schmidt and Ali Farhadi.\n\nWe would like to thank Stability AI for compute used to train the experiments and LAION for\ntheir support. We would also like to thank Luca Weihs, Mitchell Wortsman, Romain Beaumont,\nand Vaishaal Shankar, Rose Hendrix, Adam Letts, Sami Kama, Andreas Blattmann, Kunal Pratap\nSingh, and Kuo-Hao Zeng for their helpful guidance and conversations with the project. Finally,\nwe would like to thank the teams behind several open-source packages used throughout this project,\nincluding Blender, PyTorch, PyTorch Lightning, D3, Matplotlib, NumPy,\nPandas, Wandb, and Seaborn. We would also like to\nacknowledge the use of LLMs for helping revise some text and general coding assistance. Finally, we\nwould also like to thank and acknowledge the content creators who contributed to the dataset.\n","date":1689033600000},{"slug":"video2dataset","frontmatter":{"title":"video2dataset: A simple tool for large video dataset curation","author":"Maciej Kilian","date":"July 10 2023","previewImg":"/images/blog/video2dataset-preview.png"},"content":"[[GitHub]](https://github.com/iejMac/video2dataset)\n\nWithin only two years large foundational models like [CLIP](https://arxiv.org/abs/2103.00020), [Stable Diffusion](https://arxiv.org/abs/2112.10752), and [Flamingo](https://arxiv.org/abs/2204.14198) have fundamentally transformed multimodal deep learning. Because of such models and their impressive capabilities to either create stunning, high-resolution imagery or to solve complex downstream tasks, joint text-image modeling has emerged from a niche application to one of the (or maybe _the_) most relevant topics in today’s AI landscape. Remarkably, all these models, despite addressing very different tasks and being very different in design, share three fundamental properties as the main drivers behind their strong performance:  A simple and stable objective function during (pre-)training, a well-investigated scalable model architecture, and - probably most importantly - a large diverse dataset.\n\nAs of 2023, multimodal deep learning is still heavily focusing on text-image modeling, while other modalities such as video (and audio) are only sparsely investigated. Since the algorithms to train the above models are usually modality agnostic, one might wonder why there aren’t strong foundational models for these additional modalities. The reason for this is – plain and simple – the lacking availability of large scale, annotated datasets. As opposed to image modeling, where there are established datasets for scaling such as [LAION-5B](https://arxiv.org/abs/2210.08402), [DataComp](https://arxiv.org/abs/2304.14108), and [COYO-700M](https://github.com/kakaobrain/coyo-dataset) and scalable tools as [img2dataset](https://github.com/rom1504/img2dataset), this lack of clean data hinders research and development of large multimodal models especially for the video domain.\n\nWe argue that overcoming this data problem is a core interest of (open source) multimodal research since it can foster important previously impossible projects such as high quality [video](https://research.nvidia.com/labs/toronto-ai/VideoLDM/) and [audio](https://google-research.github.io/seanet/audiolm/examples/) generation, [better pre-trained models for robotics](https://twitter.com/comma_ai/status/1666959310310752257?s=20), [movie AD for the blind community](https://www.robots.ox.ac.uk/~vgg/research/autoad/), and more.\n\n![ManyVideos](/images/blog/videos_figure.gif)\n_Figure 1:_ video2dataset allows to easily create large scale collections of videos as the ones in the above sample created from available research datasets.\n\n### Solution: Flexible dataset curation tooling\n\n![Architecture](/images/blog/video2dataset_diagram.png)\n_Figure 2:_ video2dataset architecture\n\nWe introduce video2dataset, an open-source tool designed to curate video and audio datasets efficiently and at scale. It's flexible, extendable, offers a wide variety of transformations, and has been successfully tested on various large video datasets. All these examples are available in the repository, along with instructions for replicating our process.\n\nWe’ve also used video2dataset to build upon existing video datasets by downloading them individually, combining them, and transforming them into more convenient shapes with new features and considerably more samples. See the examples section for a more detailed explanation of this chain-processing. The tool’s effectiveness is showcased through the results we obtained by training various models on the datasets produced by video2dataset. An in-depth analysis of the new dataset and results will be included in our upcoming paper.\n\n## Architecture\n\nvideo2dataset is built on the foundation of [img2dataset](https://github.com/rom1504/img2dataset) and is designed to transform a table of URLs and metadata into an easily loadable [WebDataset](https://github.com/webdataset/webdataset) in just one command. Furthermore, it allows you to reprocess the WebDataset for additional transformations while retaining the same shard contents. Let's break down how video2dataset operates.\n\n### Input Sharding\n\nThe process begins with sharding the input data, a step that enables easy distribution among the workers. These input shards are temporarily stored, and the 1-1 correspondence between input and output shards ensures seamless resumption following any failures. If a dataset processing run stops prematurely, we can conveniently bypass processing the input shards for which the output shard already exists.\n\n### Distribution and Reading\n\nPost-sharding, the individual shards are distributed among the workers, who read each shard and process the samples inside. For distribution we support 3 modes - multiprocessing, pyspark, and slurm - the first is good for single machine jobs whereas the last two can help with distributing across many machines. The reading method varies depending on the input dataset's format. For instance, if it's a table of links, video2dataset downloads the video from the web. video2dataset supports a wide variety of video platforms by using [yt-dlp](https://github.com/yt-dlp/yt-dlp) to download videos it can’t directly request. However, if it's an existing WebDataset with videos, an existing webdataset dataloader reads the bytes or frames in tensor format from those samples.\n\n### Subsampling\n\nOnce the video is read and the worker has the video bytes, they are sent through a pipeline of subsamplers defined in the job config. This step optionally transforms the video through actions such as frames per second (FPS) or resolution downsampling, clipping, scene detection, and more. Alternatively there are subsamplers that are meant to only extract metadata from the input modalities like resolution/compression information, synthetic captions, optical flow, or others and include it in the metadata of a given sample. If your desired transformation isn’t already in video2dataset, its very easy to add it by defining a new subsampler or adjusting an existing one. This can be done with minimal changes in other locations of the repository and is a very welcomed contribution.\n\n### Logging\n\nThroughout the entire process, video2dataset meticulously logs vital information at various stages. Upon completion of each shard a corresponding {ID}\\_stats.json file is generated. This file contains key details, such as the number of samples processed, the number of successful operations, and a log of any failures along with their associated error messages. For added functionality, video2dataset also supports integration with Weights & Biases (wandb). This integration can be activated with a single argument and, when enabled, it provides extensive performance reporting, along with success and failure metrics. Such features are helpful for benchmarking and cost-estimating tasks related to full jobs.\n\n![](/images/blog/video2dataset_wandb_logs.png)\n_Figure 3:_ Part of a wandb report from a large video2dataset run\n\n### Writing\n\nFinally, video2dataset saves the transformed data to output shards in specified locations, where they can be utilized for training or reprocessing with video2dataset or other tools. The output format of the dataset is shards of N samples each where the shards can be formatted in multiple ways - directories, tar files, tfrecords, or parquet files. The most useful ones are the directories format for smaller datasets and debugging and tar files which is used by the WebDataset format for loading. Here is a visualization of the output datasets:\n\n```\nvideo-dataset\n ├── 00000.tar\n |     ├── 00000.mp4\n |     ├── 00000.txt\n |     ├── 00000.json\n |     ├── 00001.mp4\n |     ├── 00001.txt\n |     ├── 00001.json\n |     └── ...\n |     ├── 10000.mp4\n |     ├── 10000.txt\n |     ├── 10000.json\n ├── 00001.tar\n |     ├── 10001.mp4\n |     ├── 10001.txt\n |     ├── 10001.json\n │     ...\n ...\n ```\n\n### Reprocessing\n\nvideo2dataset can reprocess previous output datasets by reading the output shards and passing the samples inside through new transformations. This capability is particularly beneficial for video datasets, given their often hefty size and unwieldy nature. It allows us to conservatively downsample our data to avoid multiple downloads of large datasets. We delve into a practical example of this in the next section.\n\n## Examples\n\n### Tree of Datasets\n\nEach video is a rich source of data that can be decomposed into many forms - different resolutions, the audio, the motion (optical flow), individual frames - and dataset tooling should reflect this flexibility. One initial download of raw videos can be efficiently expanded into a variety of datasets for many different projects. For example, a research group might use video2dataset and its chain-processing capabilities in the following way to accommodate many research projects with diverse data:\n\n<video width=\"640\" height=\"480\" controls style=\"margin-left:auto;margin-right:auto;\">\n  <source src=\"/images/blog/video2dataset_tree_of_datasets.mp4\" type=\"video/mp4\">\n Your browser does not support the video tag.\n</video>\n\n_Figure 4:_ You can efficiently extract many types of datasets from an initial base set of video links using video2dataset\n\nThe individual steps are:\n\n1. Download an HD video dataset for a generative video modeling project.\n2. Download 2 more datasets at various resolutions so you can increase your sample count\n3. Combine all 3 video datasets and downsample in resolution and FPS so it can be more easily stored.\n4. Train a contrastive video-text model on the downscaled, diverse dataset\n5. Extract audio and useful metadata out of the downscaled dataset.\n6. The audio can be used to train various audio models (generative or discriminative)\n7. The metadata can be used to filter the dataset. For example one could use the optical flow to filter out low-motion videos.\n8. We can further process the audio and extract transcripts (using our [WhisperX](https://github.com/m-bain/whisperX) subsampler)\n9. The transcripts can be used to train text-only or vision-text models\n\nDoing dataset curation using video2dataset is very convenient across projects since datasets with the same contents can share metadata shards - the audio dataset from step 6 can use the same captions as the contrastive video-text model in step 4; we may filter that audio dataset with the same optical flow scores produced in step 5.\n\n### Dataset processing jobs\n\nWe have used video2dataset to process many popular datasets and we include instructions for how to reproduce these jobs in the [dataset\\_examples section](https://github.com/iejMac/video2dataset/tree/main/dataset_examples) of the repository. One such dataset is [WebVid](https://m-bain.github.io/webvid-dataset/) (10M samples) which can be downloaded in 12h on a single cpu16 EC2 instance which costs 8.16$ in total.\nTo further test video2dataset’s capabilities, we create a large scale video-text dataset (590M pairs) by combining existing large datasets and performing extensive processing on them using video2dataset transformations. Specifically, we perform [scene detection](https://github.com/Breakthrough/PySceneDetect), clip according to those scenes, add synthetic captions and add optical flow estimates for each clip. The dataset will be released soon along with a discovery study on its applicability\n\n### Metadata and Statistics\n\nvideo2dataset can be used to gather various metadata and statistics about the processed data. Some subsamplers have the goal of taking a given modality (video, audio) and extracting metadata from it like compression/video information, optical flow scores, audio transcripts etc. Additionally during downloading if the source already has associated metadata, like f.e. Youtube videos do, video2dataset will try to extract that metadata and place it in the webdataset so you can later access it easily. Here are some examples:\n\n| Video | Optical Flow | Synthetic Caption | Whisper Transcript | YouTube Metadata |\n|---|---|---|---|---|\n| ![](/images/blog/video_metadata1.gif) | ![](/images/blog/video_metadata1_opt_flow.gif) | crowd of people at a music festival | {\"segments\": [{\"text\": \" Okay, hold tight everybody in the back. Hold tight everybody in the middle. Hold tight everybody\", \"start\": 0.008, \"end\": 5.257}], \"language\": \"en\"} | {..., \"title\": \"NassFestival [Bugzy Malone]\", \"categories\": [\"People & Blogs\"], \"tags\": [\"bugzy malone\", ...], \"view\\_count\": 3081, \"like\\_count\": 7, ...} |\n| ![](/images/blog/video_metadata2.gif) | ![](/images/blog/video_metadata2_opt_flow.gif) | the video shows you how to solve a multiplication problem | {\"segments\": [{\"text\": \" 1 2 1 6 7 is 3 for finding the remaining digit of the answer we need to divide the number into two parts that is in this problem the number is 1 2 1 6 7 so first part is 1 6 7 and second\", \"start\": 0.008, \"end\": 19.955}], \"language\": \"en\"} | {..., \"title\": \"How to Find Cube Root of Any Number without calculator How To Calculate Cube Roots In Your Head math\", \"categories\": [\"Education\"], \"tags\": [\"find cube root of a number\", ...], \"view\\_count\": 399080, \"like\\_count\": 5498, ...} |\n| ![](/images/blog/video_metadata3.gif) | ![](/images/blog/video_metadata3_opt_flow.gif) | a drone is flying over a field | No Speech | {..., \"title\": \"Nike Smoke\", \"categories\": [], \"tags\": [], \"view\\_count\": 8164, \"like\\_count\": 64, ...} |\n\n_YouTube provides a large amount of metadata for each video so we only select a few keys for display here. For a full example of a youtube metadata dictionary see [this example](https://github.com/iejMac/video2dataset/blob/main/examples/yt_metadata.md)._\n\n## What’s next?\n\n- Scientific analysis and release of a large scale dataset created with the tool presented in this blog post.\n- Improved synthetic captioning. Synthetic captioning for videos is still underexplored and there’s many exciting ideas to try. Soon in video2dataset we will have more interesting methods to produce captions for videos that make use of image captioning models and LLMs.\n- Since its release people have been talking about using [Whisper](https://arxiv.org/abs/2212.04356) to obtain many text tokens from video. This is possible with video2dataset and we are working on transcribing a large corpus of podcasts which we will soon release as a text dataset (we are aiming at 50B tokens).\n- Many exciting modeling ideas. Hopefully with the improvement of dataset curation tooling more people will attempt to push the SOTA in the video and audio modality.\n\n### Contributing\n\nvideo2dataset is a fully open-source project and we are committed to developing it in the open. This means all the relevant TODO’s and future directions can be found in the issues tab of the repository. Contributions are welcomed and the best way of doing that is to pick out an issue, address it, and submit a pull request.\n\n### License\n\nMIT\n\n### Contributions\n\nBig thanks to everyone involved, most notably:\n\n- [Romain](https://github.com/rom1504) for building out img2dataset, helping with the initial design of video2dataset, and giving lots of advice during the process of building video2dataset.\n- [Marianna](https://github.com/marianna13) for helping create the audio functionality.\n- [Daniel](twitter.com/danielmend_) for building the cut detection and optical flow capabilities. Also for extensive help with testing and runs at scale, and feedback on the blogpost.\n- [Andreas](https://github.com/ablattmann) for greatly improving the video2dataset dataloader and implementing slurm distribution.\n- [Sumith](https://github.com/Sumith1896) for implementing synthetic captioning and lots of help during writing the blogpost (especially with visualizations).\n","date":1688947200000},{"slug":"open-flamingo-v2","frontmatter":{"title":"OpenFlamingo v2: New Models and Enhanced Training Setup","author":"Anas Awadalla* and Irena Gao*","date":"June 28 2023","previewImg":"/images/blog/flamingo-family.png"},"content":"\n\n[[GitHub]](https://github.com/mlfoundations/open_flamingo) [[Demo]](https://huggingface.co/spaces/openflamingo/OpenFlamingo) [[Models]](https://huggingface.co/openflamingo)\n\nAbout three months ago, [we announced OpenFlamingo](https://laion.ai/blog/open-flamingo/), an open-source effort to replicate [DeepMind's Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model) models.\n\n**Today, we are excited to release five trained OpenFlamingo models across the 3B, 4B, and 9B scales.** These models are based on Mosaic’s [MPT-1B and 7B](https://www.mosaicml.com/blog/mpt-7b) and Together.xyz’s [RedPajama-3B](https://www.together.xyz/blog/redpajama-models-v1), meaning they are built on open-source models with less restrictive licenses than LLaMA.\nWhen averaging performance across 7 evaluation datasets, OpenFlamingo models attain more than 80% of the performance of their corresponding Flamingo model. OpenFlamingo-3B and OpenFlamingo-9B also attain more than 60% of fine-tuned SOTA performance using just 32 in-context examples.\n\nWe’ve also improved our open-source training and evaluation code, adding support for [Fully Sharded Data Parallel (FSDP)](https://engineering.fb.com/2021/07/15/open-source/fsdp/) and new datasets (TextVQA, VizWiz, HatefulMemes, and Flickr30k) to the evaluation suite.\n\n## Technical overview\n\n![](/images/blog/flamingo-samples.png)\n\nOpenFlamingo models process arbitrarily interleaved sequences of images and text to output text. This allows the models to accept in-context examples and solve tasks like captioning, visual question answering, and image classification. \n\nWe follow the [Flamingo modeling paradigm](https://arxiv.org/abs/2204.14198), augmenting the layers of a pretrained, frozen language model such that they cross-attend to visual features when decoding. Following Flamingo, we freeze the vision encoder and language model but train the connecting modules on web-scraped image-text sequences. Specifically, we use a mixture of [LAION-2B](https://arxiv.org/abs/2210.08402) and [Multimodal C4](https://arxiv.org/abs/2304.06939). **\n\n** The 4B-scale models were also trained on experimental ChatGPT-generated (image, text) sequences, where images were pulled from LAION. We are working to release these sequences soon.\n\n## Model release\n\nWe have trained five OpenFlamingo models across the 3B, 4B, and 9B parameter scales. These models build off of OpenAI’s CLIP ViT-L/14 as a vision encoder and open-source language models from MosaicML and Together.xyz. At the 3B and 4B scales, we have trained models both with standard and instruction-tuned language model backbones.\n\n|# params|Language model| (Language) instruction tuned? |\n|---|---|---|\n|3B| mosaicml/mpt-1b-redpajama-200b | No |\n|3B| mosaicml/mpt-1b-redpajama-200b-dolly | Yes |\n|4B| togethercomputer/RedPajama-INCITE-Base-3B-v1 | No |\n|4B| togethercomputer/RedPajama-INCITE-Instruct-3B-v1 | Yes |\n|9B| mosaicml/mpt-7b | No |\n\nNote that as part of the move to v2, we are deprecating our previous LLaMA-based checkpoint. However, you can continue to use our older checkpoint using the new codebase.\n\n## Evaluation\n\nWe evaluated our models on vision-language datasets across captioning, VQA, and classification tasks. As shown below, the OpenFlamingo-9B v2 model shows considerable improvement over our v1 release.\n\n![](/images/blog/flamingo-v1-vs-v2.png)\n\nBelow, we aggregate model performances across 7 evaluation datasets and 5 in-context evaluation settings (0-shot, 4-shot, 8-shot, 16-shot, and 32-shot). Averaged across these settings, OpenFlamingo (OF) models attain more than 80% of corresponding Flamingo performance, where we compare OF-3B and OF-4B with Flamingo-3B, and OF-9B with Flamingo-9B. Error bars are standard deviations over datasets and evaluation settings.\n\n![](/images/blog/flamingo-avg-performance.png)\n\nNext, we report performance relative to fine-tuned SoTAs listed on PapersWithCode\\*. With 32 in-context examples, OpenFlamingo-3B and OpenFlamingo-9B models attain more than 55% of fine-tuned performance, despite only being pre-trained on web data. On average, OpenFlamingo models trail their DeepMind counterparts by around 10% 0-shot and 15% 32-shot.  \n\n![](/images/blog/flamingo-aggregated-performance.png)\n\nBelow, we present per-dataset results. We observe that on some datasets, OpenFlamingo models (especially the 4B ones) generalize poorly across the number of in-context examples. We hypothesize that this behavior stems from the quality of our pre-training data.\n\n![](/images/blog/flamingo-evaluations.png)\n\n\\* numbers pulled on Monday, June 19\n\n## Next steps and codebase updates\n\nOpenFlamingo remains an active research project, and we continue to work on training and releasing high-quality multimodal models. As next steps, we’re particularly interested in improving the quality of our pre-training data.\n\nWe were thrilled by the many cool projects building off of our first release such as [Otter](https://github.com/Luodian/Otter) and [Multimodal-GPT](https://github.com/open-mmlab/Multimodal-GPT). We encourage the community to continue using OpenFlamingo. To make training OpenFlamingo models more accessible, we have added support for [Fully Sharded Data Parallel (FSDP)](https://engineering.fb.com/2021/07/15/open-source/fsdp/) and gradient checkpointing. For context, in experiments, we were able to fit a 9B scale OpenFlamingo model on 8 A100 40GB gpus using FSDP and gradient checkpointing. We’ve also added scripts for packaging our training data into the expected Webdataset form.\n\nIf you’re interested in contributing to our codebase, including our evaluation suite, please join us at the [OpenFlamingo github repository](https://github.com/mlfoundations/open_flamingo).\n\n**Safety and ethical considerations**\n\nOpenFlamingo models inherit the risks of their parent models, especially the language model. As an open-source research effort, we highly value open, accessible, reproducible multimodal model research; however, it is crucial to be aware that these models are trained on web data and have not been finetuned for safety, and thus may produce unintended, inappropriate, unreliable, and/or inaccurate outputs. Please use caution before deploying OpenFlamingo models in real applications. We also hope that OpenFlamingo enables further safety and reliability research to address these issues.\n\n\n### Contributors\nThanks to:\n[Josh Gardner](https://homes.cs.washington.edu/~jpgard/), [Jack Hessel](https://jmhessel.com/), [Yusuf Hanafy](https://www.linkedin.com/in/yusufhanafy/), [Wanrong Zhu](https://wanrong-zhu.com/), [Kalyani Marathe](https://kalyani7195.github.io/), [Yonatan Bitton](https://yonatanbitton.github.io/), [Samir Gadre](https://sagadre.github.io/), [Shiori Sagawa](https://cs.stanford.edu/~ssagawa/), [Jenia Jitsev](https://scholar.google.de/citations?user=p1FuAMkAAAAJ&hl=en), [Simon Kornblith](https://simonster.com/), [Pang Wei Koh](https://koh.pw/), [Gabriel Ilharco](https://gabrielilharco.com/), [Mitchell Wortsman](https://mitchellnw.github.io/), and [Ludwig Schmidt](https://people.csail.mit.edu/ludwigs/)\n\n### Acknowledgements\n\nWe would like to thank Jean-Baptiste Alayrac and Antoine Miech for their advice and Stability AI for providing us with compute resources to train these models.\n","date":1687910400000},{"slug":"datacomp","frontmatter":{"title":"Announcing DataComp: In search of the next generation of multimodal datasets","author":"Gabriel Ilharco","date":"April 27, 2023","previewImg":"/images/blog/datacomp_samples.png"},"content":"\n\n[[ Paper ]](https://arxiv.org/abs/2304.14108) [[ Code ]](https://github.com/mlfoundations/datacomp) [[ Website ]](http://datacomp.ai/)\n\nAbout a year ago, we released [LAION-5B](https://laion.ai/blog/laion-5b/), a billion-scale open-source image-text dataset. Since then, LAION-5B has become a staple in the open-source machine learning ecosystem, powering open-source models like [OpenCLIP](https://github.com/mlfoundations/open_clip), [OpenFlamingo](https://github.com/mlfoundations/open_flamingo), and [Stable Diffusion](https://github.com/Stability-AI/stablediffusion). From the beginning, we viewed LAION-5B as only the first step on this research journey and hoped that we can build the next generation of multimodal datasets both rigorously and collaboratively in the open as a research community.\n\nToday, we are proud to introduce **DataComp, a new benchmark for designing multimodal datasets**. Unlike traditional benchmarks focused on modeling improvements, DataComp puts data front and center. In Datacomp, participants innovate by proposing new training sets, leaving the training code, hyper-parameters and compute fixed. As part of our competition, we are releasing **CommonPool, the largest public collection of image-text pairs to date with 12.8B samples**.\n\nAlong with our pool, we also release **DataComp-1B**, a 1.4B subset that can be used to outperform compute-matched CLIP models from OpenAI and LAION. DataComp-1B makes it possible to train a CLIP ViT-L model to better performance than a larger ViT-g model trained on LAION-2B while using 9x less training compute. Our ViT-L/14 trained on DataComp-1B obtains 79.2% zero-shot accuracy on ImageNet, substantially outperforming OpenAI's model trained with the same compute (75.5% zero-shot accuracy).\n\nDataComp-1B is only the first dataset coming out of DataComp. We are beyond excited to continue learning about designing better datasets, and we invite you to join us on this journey! \n\n![](/images/blog/datacomp_samples.png)\n\n## Overview\n\nIn the past two years, we have seen multiple breakthroughs in multimodal learning. From [CLIP](https://openai.com/research/clip), [DALL-E](https://openai.com/product/dall-e-2) and [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release) to [Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model) and [GPT-4](https://openai.com/research/gpt-4), multimodal models now show impressive generalization such as zero-shot image classification and in-context learning. Large-scale datasets have been instrumental for building these models. However, despite their importance, datasets rarely receive the same attention as model architectures or training algorithms. DataComp addresses this shortcoming in the machine learning ecosystem by introducing a benchmark where participants can rigorously explore design decisions for datasets.\n\nTowards this goal, we present **CommonPool**, a large-scale dataset with 12.8B image-text pairs collected from the web—the largest dataset of its kind by a factor of 2.5x. Despite the size, our benchmark features multiple data and compute scales, and is designed to encourage participation even with a single GPU at the smallest scale. \n\nTogether with our dataset, we present simple filtering baselines that already improve upon existing methods such as the filtering used in LAION-2B. As a highlight, **DataComp-1B**, a 1.4B subset of our pool can be used to train a CLIP ViT-L/14 to 79.2% zero-shot accuracy on ImageNet. This model outperforms a ViT-g/14 model trained on LAION-2B by 0.7 percentage points, despite being trained with 9x less compute. It also performs substantially better than OpenAI’s ViT-L/14 model trained with the same compute budget, which gets 75.5% zero-shot accuracy on ImageNet. \n\nWe are beyond excited to continue learning about designing better datasets, and we invite you to join us in this journey!\n\n## DataComp\n\nIn DataComp, your goal is to design a dataset that produces the best possible CLIP model at a fixed compute budget. DataComp is designed with scale in mind. When participating, the first step is to choose one of the *small*, *medium*, *large*, or *xlarge* scales. Each scale comes with a corresponding pool, ranging from 12.8M samples to 12.8B samples. You can use data from that pool, or from any external data source to build your dataset depending on the competition track. After that, train your CLIP model using our public implementation of a fixed training protocol, and evaluate on our diverse suite of 38 downstream tasks. \n\n![](/images/blog/datacomp_workflow.png)\n\n\n**Competition tracks:** Our competition features two tracks. In the first track, CommonPool, you are only allowed to filter data from the pool we provide. On the second track, Bring Your Own Data (BYOD), you are allowed to use any data you want, as long as it doesn’t overlap with our evaluation suite. \n\n**Preprocessing and safety:** We kept preprocessing of our pool to a minimum to provide a blank slate for participants. Our only initial preprocessing steps are to eliminate images that are flagged due to safety considerations or that appear in downstream evaluation datasets to avoid contamination. For the former, we take steps to eliminate illegal and explicit content and to protect the privacy of individuals, removing unsafe images and captions with automated filters and obfuscating faces in the candidate images we provide.\n\n**Multiple scales:** To facilitate the study of scaling trends and accommodate participants with various levels of resources, DataComp features multiple scales of data and compute. The compute requirements vary from around 8 GPU hours at the smallest scale to over 40,000 at the largest. The pool we provide varies accordingly, from 12.8M to 12.8B samples.\n\n**Standardized training:** In order to enable controlled and comparable experiments, we fix the training procedure (i.e., model architecture, optimizer, loss, hyperparameters, etc.) and compute at each scale, closely following training recipes used to train state-of-the-art CLIP models from scratch.\n\n**Evaluation:** We evaluate on a diverse set of 38 downstream image classification and retrieval tasks, including distribution shifts and geographic diversity. For efficiency and simplicity, we evaluate models in a zero-shot setting, without fine-tuning on data from the downstream tasks.\n\n**Rules:** We allow the use of any public data that does not overlap with our evaluation tasks. See Appendix A of [our paper](https://arxiv.org/abs/2304.14108) for more details.\n\n\n## What we know so far\n\nIn our [paper](https://arxiv.org/abs/2304.14108), we present hundreds of baseline experiments with different dataset design algorithms. A key finding is that smaller, *more aggressively filtered datasets can perform better than larger datasets coming from the same pool.* As a highlight, we find a subset of our largest pool that performs substantially better than LAION-2B, despite being smaller (1.4B samples).\n\nThis subset, DataComp-1B, can be used to train a ViT-L/14 to 79.2% zero-shot accuracy on ImageNet, which outperforms a ViT-g/14 model trained on LAION-2B by 0.7 percentage points, despite being trained with 9x less compute. Moreover, our model performs substantially better than other ViT-L/14 models trained with the same compute budget including OpenAI's model, as seen below. \n\n| Training data   | Dataset size  | # samples seen | ImageNet Acc. | Avg. performance (38 datasets) |\n|-----------------|---------------|----------------|---------------|--------------------------------|\n| OpenAI's WIT    | 0.4B          | 13B            | 75.5          | 0.61                           |\n| LAION-400M      | 0.4B          | 13B            | 73.1          | 0.58                           |\n| LAION-2B        | 2.3B          | 13B            | 73.1          | 0.59                           |\n| LAION-2B        | 2.3B          | 34B            | 75.2          | 0.61                           |\n| DataComp-1B     | 1.4B          | 13B            | 79.2          | 0.66                           |\n\n\nWe also show that the ranking of many curation approaches is consistent across the different scales. For example, across the baselines we study, the rank correlation between ImageNet accuracy at small and large scales is 0.9. This suggests that experiments at smaller scales can provide valuable insights for larger scales, thereby accelerating investigations.\n\n![](/images/blog/datacomp_scaling_plot.png)\n\nThere is much more in the paper, and we think this is only the beginning. We hope you’ll join us in designing the next generation of multimodal datasets!\n\n## How do I start?\n\nThe best starting point is our [github repo](https://github.com/mlfoundations/datacomp), which contains code for downloading our pools, training and evaluating models.\n\nOur website [www.datacomp.ai/](https://www.datacomp.ai/) contains further documentation.\n\n## DataComp ICCV Workshop\n\nIn conjunction with DataComp, we are also organizing a workshop at [ICCV 2023](https://iccv2023.thecvf.com/), titled **Towards the Next Generation of Computer Vision Datasets**. The workshop will showcase a series of DataComp submissions, along with other data-centric papers and multiple invited talks by experts in the field. Our call for papers is available at [https://www.datacomp.ai/workshop.html](https://www.datacomp.ai/workshop.html).\n\n## Acknowledgements\n\nWe thank all of our paper authors: Samir Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt.\n\nWe also thank Amro Abbas, Jessie Chapman, Brian Cheung, Joshua Gardner, Nancy Garland, Sachin Goyal, Huy Ha, Zaid Harchaoui, Andy Jones, Adam Klivans, Daniel Levy, Ronak Mehta, Ari Morcos, Raviteja Mullapudi, Kentrell Owens, Alec Radford, Marco Tulio Ribeiro, Shiori Sagawa, Christoph Schuhmann, Matthew Wallingford, and Ross Wightman for helpful feedback at various stages of the project.\n\nA special thanks to Stability AI and the Gauss Centre for Supercomputing e.V (compute time granted on JUWELS Booster hosted at Juelich Supercomputing Center) for providing us with compute resources to train models, without which none of this would have been possible.","date":1682553600000},{"slug":"paella","frontmatter":{"title":"A new Paella: Simple & Efficient Text-To-Image generation","author":"Dominic Rampas and Pablo Pernias","date":"April 15, 2023","previewImg":"/images/blog/paella.png"},"content":"![](https://user-images.githubusercontent.com/61938694/231021615-38df0a0a-d97e-4f7a-99d9-99952357b4b1.png)\n### Overview.\nWe are releasing a new Paella model which builds on top of our initial paper https://arxiv.org/abs/2211.07292.\nPaella is a text-to-image model that works in a quantized latent space and learns similarly to MUSE and Diffusion models.\nPaella is similar to MUSE as it also works on discrete tokens, but is different in the way tokens are noised as well as\nthe architecture. MUSE uses a transformer, whereas we use a CNN, which comes with many benefits. There are also subtle\ndifferences in the conditioning Paella uses as well how images are sampled. And on the other hand, it can also be seen\nas a discrete diffusion process, which noises images during training and iteratively removes noise during sampling.\nSince the paper-release we worked intensively to bring Paella to a similar level as other \nstate-of-the-art models. With this release we are coming a step closer to that goal. However, our main intention is not\nto make the greatest text-to-image model out there (at least for now), it is to bring text-to-image models closer\nto people outside the field on a technical basis. For example, many models have codebases with many thousand lines of \ncode, that make it pretty hard for people to dive into the code and easily understand it. And that is our proudest\nachievement with Paella. The training and sampling code for Paella is minimalistic and can be understood in \na few minutes, making further extensions, quick tests, idea testing etc. extremely fast. For instance, the entire\nsampling code can be written in just **12 lines** of code.\nIn this blog post we will talk about how Paella works in short, give technical details and release the model.\n\n### How does Paella work?\nPaella works in a quantized latent space, just like StableDiffusion etc., to reduce the computational power needed.\nImages are encoded to a smaller latent space and converted to visual tokens of shape *h x w*. During training,\nthese visual tokens are noised, by replacing a random amount of tokens with other randomly selected tokens\nfrom the codebook of the VQGAN. The noised image are given to the model, along with a timestep and the conditional\ninformation, which is text in our case. The model is tasked to predict the un-noised version of the tokens. \nAnd that's it. The model is optimized with the CrossEntropy loss between the original tokens and the predicted tokens.\nThe amount of noise added during the training is just a linear schedule, meaning that we uniformly sample a percentage \nbetween 0 and 100% and noise that amount of tokens.<br><br>\n\n<figure>\n  <img src=\"https://user-images.githubusercontent.com/61938694/231248435-d21170c1-57b4-4a8f-90a6-62cf3e7effcd.png\" width=\"400\">\n  <figcaption>Images are noised and then fed to the model during training.</figcaption>\n</figure>\n\n\nSampling is also extremely simple, we start with the entire image being random tokens. Then we feed the latent image, \nthe timestep and the condition into the model and let it predict the final image. The models outputs a distribution\nover every token, which we sample from with standard multinomial sampling.  \nSince there are infinite possibilities for the result to look like, just doing a single step results in very basic \nshapes without any details. That is why we add noise to the image again and feed it back to the model. And we repeat\nthat process for a number of times, with less noise being added every time, and slowly get our final image.\nYou can see how images emerge [here](https://user-images.githubusercontent.com/61938694/231252449-d9ac4d15-15ef-4aed-a0de-91fa8746a415.png).<br>\nThe following is the entire sampling code needed to generate images:\n```python\ndef sample(model_inputs, latent_shape, unconditional_inputs, steps=12, renoise_steps=11, temperature=(0.7, 0.3), cfg=8.0):\n    with torch.inference_mode():\n        sampled = torch.randint(low=0, high=model.num_labels, size=latent_shape)\n        initial_noise = sampled.clone()\n        timesteps = torch.linspace(1.0, 0.0, steps+1)\n        temperatures = torch.linspace(temperature[0], temperature[1], steps)\n        for i, t in enumerate(timesteps[:steps]):\n            t = torch.ones(latent_shape[0]) * t\n\n            logits = model(sampled, t, **model_inputs)\n            if cfg:\n                logits = logits * cfg + model(sampled, t, **unconditional_inputs) * (1-cfg)\n            sampled = logits.div(temperatures[i]).softmax(dim=1).permute(0, 2, 3, 1).reshape(-1, logits.size(1))\n            sampled = torch.multinomial(sampled, 1)[:, 0].view(logits.size(0), *logits.shape[2:])\n\n            if i < renoise_steps:\n                t_next = torch.ones(latent_shape[0]) * timesteps[i+1]\n                sampled = model.add_noise(sampled, t_next, random_x=initial_noise)[0]\n    return sampled\n```\n\n### Results\n<img src=\"https://user-images.githubusercontent.com/61938694/231598512-2410c172-5a9d-43f4-947c-6ff7eaee77e7.png\">\nSince Paella is also conditioned on CLIP image embeddings the following things are also possible:<br><br>\n<img src=\"https://user-images.githubusercontent.com/61938694/231278319-16551a8d-bfd1-49c9-b604-c6da3955a6d4.png\">\n<img src=\"https://user-images.githubusercontent.com/61938694/231287637-acd0b9b2-90c7-4518-9b9e-d7edefc6c3af.png\">\n<img src=\"https://user-images.githubusercontent.com/61938694/231287119-42fe496b-e737-4dc5-8e53-613bdba149da.png\">\n\n### Technical Details.\nModel-Architecture: U-Net (Mix of ConvNeXt, DiT etc.) <br>\nDataset: Laion-A, Laion Aesthetic > 6.0 <br>\nTraining Steps: 1.3M <br>\nBatch Size: 2048 <br>\nResolution: 256 <br>\nVQGAN Compression: f4 <br>\nCondition: ByT5-XL (95%), CLIP-H Image Embedding (10%), CLIP-H Text Embedding (10%)\nOptimizer: AdamW\nHardware: 128 A100 @ 80GB <br>\nTraining Time: ~3 weeks <br>\nLearning Rate: 1e-4 <br>\nMore details on the approach, training and sampling can be found in paper and on GitHub.\n\n### Paper, Model, Code Release\nPaper: https://arxiv.org/abs/2211.07292 <br>\nCode: https://github.com/dome272/Paella <br>\nModel: https://huggingface.co/dome272/Paella <br>\n\n\n### Limitations & Conclusion\nThere are still many things to improve for Paella to get on par with standard diffusion models or to even outperform\nthem. One primary thing we notice is that even though we only condition the model on CLIP image embedding 10% of the\ntime, during inference the model heavily relies on the generated image embeddings by a prior model (mapping clip text\nembeddings to image embeddings as proposed in Dalle2). We counteract this by decreasing the importance of the image\nembeddings by reweighing the attention scores. There probably is a way to avoid this happening already in training.\nOther limitations such as lack of composition, text depiction, unawareness of concepts etc. could also be reduced by\ncontinuing the training for longer. As a reference, Paella has only seen as many images as SD 1.4 and due to concerns \nin regard to training collapse (which later turned to be negligible), trained with a 10x lower learning rate for the \nfirst 700k steps. To conclude, this is still work in progress, but our first model that works reasonably well and\na million times better than the first versions we trained months ago.\n\nIt is noteworthy that the design choices for Paella were based on trying to make a simple architecture and \nmodel for text-to-image synthesis, drawing inspiration from existing techniques such as MaskGIT. Furthermore, this \napproach eliminates the need for hyperparameters such as alpha, beta, and alpha_cum_prod, which are typically required \nin diffusion models. As a result, this methodology is particularly well-suited for individuals who are new to the field \nof generative artificial intelligence. Our aim is to lay the groundwork for future research in this domain, fostering\na landscape where AI is accessible and comprehensible to a broader audience. We encourage further exploration of this\napproach, as we are confident in its potential to contribute useful insights and potentially advance the state of the \nart in text-to-image synthesis.\n\n\n### Contributions\n\n**Thanks to:**\n\n* [Romain Beaumont](https://github.com/rom1504/) and [Christoph Schuhmann](https://github.com/christophschuhmann) \nfor constant help on datasets and giving useful advice.\n* [Jenia Jitsev](https://scholar.google.de/citations?user=p1FuAMkAAAAJ&hl=en) for help on writing the blog post and\nuseful discussions.\n* [Richard Vencu](https://github.com/rvencu) for an incredible amount of help regarding hardware issues.\n* [StabilityAI](https://stability.ai/) for providing GPU-Cluster access and faith in Paella.\n\n","date":1681516800000},{"slug":"petition","frontmatter":{"title":"Petition for keeping up the progress tempo on AI research while securing its transparency and safety.","author":"LAION.ai","date":"March 29 2023","previewImg":"/images/blog/laion-blue.png"},"content":"\n[LINK TO OUR PETITION](https://www.openpetition.eu/petition/online/securing-our-digital-future-a-cern-for-open-source-large-scale-ai-research-and-its-safety)\n\n_Authors: Christoph Schuhmann, Huu Nguyen, Robert Kaczmarczyk, Jenia Jitsev & LAION community_\n\n## Securing Our Digital Future: Calling for CERN like international organization to transparently coordinate and progress on large-scale AI research and its safety\n\nIn an era of unparalleled technological advancements, humanity stands on the precipice of a new epoch characterized by the profound influence of artificial intelligence (AI) and its foundational models, such as GPT-4. The potential applications of these technologies are vast, spanning scientific research, education, governance, and small and medium-sized enterprises. To harness their full potential as tools for societal betterment, it is vital to democratize research on and access to them, lest we face severe repercussions for our collective future.\n\n### Dominance of few large corporations in AI development\n\nIncreasingly, we are witnessing the emergence of a system wherein educational institutions, government agencies, and entire nations become dependent on the AI technology of a select few large corporations that operate with little transparency or public accountability. To secure our society's technological independence, foster innovation, and safeguard the democratic principles that underpin our way of life, we must act now.\nWe call upon the global community, particularly the European Union, the United States, the United Kingdom, Canada, Australia and other willing countries, to collaborate on a monumental initiative: the establishment of an international, publicly funded, open-source supercomputing research facility. This facility, analogous to the CERN project in scale and impact, should house a diverse array of machines equipped with at least 100,000 high-performance state-of-the-art accelerators (GPUs or ASICs), operated by experts from the machine learning and supercomputing research community and overseen by democratically elected institutions in the participating nations.\n\n### Multimodal future\n\nThis ambitious endeavor will provide a platform for researchers and institutions worldwide to access and refine advanced AI models, such as GPT-4, harnessing their capabilities for the greater good. By making these models open source and incorporating multimodal data (audio, video, text, and program code), we can significantly enrich academic research, enhance transparency, and ensure data security. Furthermore, granting researchers access to the underlying training data will enable them to understand precisely what these models learn and how they function, an impossibility when restricted by APIs.\nAdditionally, the open-source nature of this project will promote safety and security research, allowing potential risks to be identified and addressed more rapidly and transparently by the academic community and open-source enthusiasts. This is a vital step in ensuring the safety and reliability of AI technologies as they become increasingly integrated into our lives.\nThe proposed facility should feature AI Safety research labs with well-defined security levels, akin to those used in biological research labs, where high-risk developments can be conducted by internationally renowned experts in the field, backed by regulations from democratic institutions. The results of such safety research should be transparent and available for the research community and society at large. These AI Safety research labs should be capable of designing timely countermeasures by studying developments that, according to broad scientific consensus, would predictably have a significant negative impact on our societies.\n\n### Economic impact\n\nEconomically, this initiative will bring substantial benefits to small and medium-sized companies worldwide. By providing access to large foundation models, businesses can fine-tune these models for their specific use cases while retaining full control over the weights and data. This approach will also appeal to government institutions seeking transparency and control over AI applications in their operations.\nThe importance of this endeavor cannot be overstated. We must act swiftly to secure the independence of academia and government institutions from the technological monopoly of large corporations in AI research. Technologies like GPT-4 are too powerful and significant to be exclusively controlled by a select few.\nIn a world where machine learning expertise and resources for AI development become increasingly concentrated in large corporations, it is imperative that smaller enterprises, academic institutions, municipal administrations, and social organizations, as well as nation-states, assert their autonomy and refrain from relying solely on the benevolence of these powerful entities that are often driven by short-term profit interests and act without properly taking democratic institutions into their decision-making loop. We must take immediate and decisive action to secure the technological independence of our society, nurturing innovation while ensuring the safety of these developments and protecting the democratic principles that form the foundation of our way of life.\n\n### Safety measures\n\nThe [recent proposition](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) of decelerating AI research as a means to ensure safety and progress presents an understandable but untenable  approach that will be detrimental to both objectives. Corporate or state actors will make advancements in the dark while simultaneously curtailing the public research community's ability to scrutinize the safety aspects of advanced AI systems thoroughly. Rather than impeding the momentum of public AI development, a more judicious and efficacious approach would be to foster a better-organized, transparent, safety-aware, and collaborative research environment. The establishment of transparent open-source AI safety labs tied to the international large-scale AI research facility as described above, which employ eligible AI safety experts, have corresponding publicly funded compute resources, and act according to regulations issued by democratic institutions, will cover the safety aspect without dampening progress. By embracing this cooperative framework, we can simultaneously ensure progress and the responsible development of AI technology, safeguarding the well-being of our society and the integrity of democratic values.\n\n### What you can do\n\nWe urge you to join us in this crucial campaign. Sign [this petition](https://www.openpetition.eu/petition/online/securing-our-digital-future-a-cern-for-open-source-large-scale-ai-research-and-its-safety) and make your voice heard. Our collective digital future, the autonomy of our academic research, and the equilibrium of our global economy depend on our ability to act quickly and decisively.\nTogether, we can build a future where advanced AI technologies are accessible to all, and where innovation and progress are not constrained by the boundaries of a few powerful corporations. Let us seize this opportunity and build a brighter future for generations to come.\n","date":1680048000000},{"slug":"open-flamingo","frontmatter":{"title":"Announcing OpenFlamingo: An open-source framework for training vision-language models with in-context learning","author":"Anas Awadalla and Irena Gao","date":"Mar 28 2023","previewImg":"/images/blog/flamingo-logo.png"},"content":"\n**Overview.**\nWe are thrilled to announce the release of OpenFlamingo, an open-source reproduction of DeepMind's Flamingo model. At its core, OpenFlamingo is a framework that enables training and evaluation of large multimodal models (LMMs). Check out our [GitHub repository](https://github.com/mlfoundations/open_flamingo) and [demo](https://7164d2142d11.ngrok.app) to get started!\n\nFor this first release, our contributions are as follows:\n\n* 🏋️ A Python framework to train Flamingo-style LMMs (based on Lucidrains' [flamingo implementation](https://github.com/lucidrains/flamingo-pytorch) and David Hansmair's [flamingo-mini repository](https://github.com/dhansmair/flamingo-mini)).\n* 🪅 A large-scale multimodal dataset with interleaved image and text sequences.\n* 🧪 An in-context learning evaluation benchmark for vision-language tasks.\n* 🤖 A first version of our OpenFlamingo-9B model based on LLaMA, with much better models to come!\n\n\nThe recent progress in open-source LMMs with the release of [BLIP-2](https://arxiv.org/abs/2301.12597) and [FROMAGe](https://jykoh.com/fromage) has shown the exciting potential of multimodal systems. We hope that OpenFlamingo will help drive progress in multimodal machine learning, and we have more exciting contributions in the pipeline, so stay tuned! \n\n\n**Goal.**\nOur goal with OpenFlamingo is to develop a multimodal system that can tackle a diverse range of vision-language tasks. Ultimately, we aim to match the power and versatility of GPT-4 in handling visual and text input. To achieve this goal, we are creating an open-source version of [DeepMind's Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model) model, a LMM capable of processing and reasoning about images, videos, and text. We are committed to build fully open-source models, and believe this transparency is essential for fostering collaboration, accelerating progress, and democratizing access to state-of-the-art LMMs. Our release is the first step towards this goal.\n\nWe are sharing the first checkpoint of our OpenFlamingo-9B model. While the model is not yet fully optimized, it demonstrates the potential of this project. By working together and receiving feedback from the community, we can train better LMMs. We encourage the community to participate in the development process by providing feedback and contributing to the repository. \n\n\n**Technical Details.**\nOur implementation largely follows that of [Flamingo](https://arxiv.org/abs/2204.14198). Flamingo models are trained on large-scale web corpora containing interleaved text and images, which is crucial for endowing them with in-context few-shot learning capabilities. OpenFlamingo implements the same architecture (Perceiver resamplers, cross-attention layers) proposed in the original Flamingo paper. However, since the training data for Flamingo is not available to the public, we use open-source datasets for training our models. Specifically, the released OpenFlamingo-9B checkpoint is trained on 5M samples from our new Multimodal C4 dataset and 10M samples from [LAION-2B](https://huggingface.co/datasets/laion/laion2B-en). \n\n\n## **Multimodal C4**\n\nThe Multimodal-C4 dataset is an expansion of the text-only [C4 dataset](https://www.tensorflow.org/datasets/catalog/c4), which was used to train  [T5 models](https://arxiv.org/abs/1910.10683). This dataset is built by our collaborators [Jack Hessel](https://jmhessel.com) and [Wanrong Zhu](https://wanrong-zhu.com) at the Allen Institute for AI. For each document in the [C4 en.clean](https://www.tensorflow.org/datasets/catalog/c4#c4en_default_config) dataset, we retrieve the original webpage from [Common Crawl](https://commoncrawl.org/), then collect the downloadable images. Data cleaning is carried out through deduplication and content filtering, which aims to eliminate non-safe for work (NSFW) and unrelated images, such as advertisements. Additionally, we run face detection and discard images with positive identifications. Finally, images and sentences are interleaved using bipartite matching within a document: CLIP ViT/L-14 image-text similarities serve as edge weights. Multimodal-C4 consists of approximately 75 million documents, encompassing around 400M images and 38B tokens. A full release with more detail is coming soon.\n\n![](/images/blog/mmc4-example.png)\n\n## **Benchmark**\n\nTo measure the performance of OpenFlamingo, we evaluate on a diverse set of downstream tasks. Our aim is to eventually build an open-source version of Flamingo’s benchmark and extend past that to standardize vision-language task evaluation. Currently we support visual question-answering ([VQAv2](https://visualqa.org/index.html), [OK-VQA](https://okvqa.allenai.org)), captioning ([COCO](https://cocodataset.org/#home), [Flickr30k](https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset)), and image classification ([ImageNet](https://image-net.org/index.php)) tasks. Expect us to add many more evaluation sets that probe model reasoning, biases, and more! You can access the benchmark on the OpenFlamingo repo. \n\n\n## **Model release**\n\n![](/images/blog/flamingo-llama.png)\n\nAs part of our release, we are also providing a checkpoint from our under-development OpenFlamingo-9B, a LMM built on top of [LLaMA 7B](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) and [CLIP ViT/L-14](https://openai.com/research/clip). This model is still a work in progress but it can already bring a lot of value to the community. For instance,\n\n![](/images/blog/flamingo-9B-sample-one.png)\n![](/images/blog/flamingo-9B-sample-two.png)\n\n**Performance**\n\nWe evaluated our checkpoint on COCO and VQAv2. Here we report the validation performance using a different number of shots. \n\nCOCO (CIDEr)\n<table>\n  <tr>\n   <td>\n   </td>\n   <td>0-shot\n   </td>\n   <td>4-shot\n   </td>\n   <td>8-shot\n   </td>\n   <td>16-shot\n   </td>\n   <td>32-shot\n   </td>\n  </tr>\n  <tr>\n   <td>OpenFlamingo-9B*\n   </td>\n   <td>65.5\n   </td>\n   <td>74.3\n   </td>\n   <td>79.3\n   </td>\n   <td>81.8\n   </td>\n   <td>84.5\n   </td>\n  </tr>\n  <tr>\n   <td>DeepMind Flamingo-9B\n   </td>\n   <td>79.4\n   </td>\n   <td>93.1\n   </td>\n   <td>99.0\n   </td>\n   <td>102.2\n   </td>\n   <td>106.3\n   </td>\n  </tr>\n</table>\n\n---\n\nVQAv2 (VQA accuracy)\n<table>\n  <tr>\n   <td>\n   </td>\n   <td>0-shot\n   </td>\n   <td>4-shot\n   </td>\n   <td>8-shot\n   </td>\n   <td>16-shot\n   </td>\n   <td>32-shot\n   </td>\n  </tr>\n  <tr>\n   <td>OpenFlamingo-9B*\n   </td>\n   <td>43.5\n   </td>\n   <td>44.0\n   </td>\n   <td>47.5\n   </td>\n   <td>48.9\n   </td>\n   <td>50.3\n   </td>\n  </tr>\n  <tr>\n   <td>DeepMind Flamingo-9B\n   </td>\n   <td>51.8\n   </td>\n   <td>56.3\n   </td>\n   <td>58.0\n   </td>\n   <td>59.4\n   </td>\n   <td>60.4\n   </td>\n  </tr>\n</table>\n\n\n*Note that we report validation performance (using the same setup outlined in Flamingo paper) for OpenFlamingo-9B while DeepMind Flamingo-9B performance is on test data.\n\n**Safety and ethical considerations**\n\nAs OpenFlamingo-9B is built on top of frozen [LLaMA](https://arxiv.org/abs/2302.13971) and [CLIP](https://arxiv.org/abs/2103.00020) models, you can expect OpenFlamingo to inherit the harms of the parent models. We understand that by releasing these models, they may be used in harmful ways. However, it is important for the research community to study the harms of large multimodal models, and we believe that open-sourcing these models will enable the community to develop better ways to mitigate these harms in future models.\n\nWe emphasize that OpenFlamingo-9B is a research artifact and not a finished product. It can produce unintended, inappropriate, offensive, and/or inaccurate results. We thus advocate for caution and thorough evaluations before using our models in any real applications.\n\n\n### Contributions\n\n**Thanks to:**\n\n* [Josh Gardner](https://homes.cs.washington.edu/~jpgard/) and [Yonatan Bitton](https://yonatanbitton.github.io/) for implementing the evaluation benchmark.\n* [Kalyani Marathe](https://kalyani7195.github.io/) for implementing the data pipeline and improving code quality.\n* [Yusuf Hanafy](https://www.linkedin.com/in/yusufhanafy/) for working on the demo.\n* [Wanrong Zhu](https://wanrong-zhu.com/), [Jack Hessel](https://jmhessel.com/), and [Samir Gadre](https://sagadre.github.io/) for building the Multimodal C4 dataset.\n* [Jenia Jitsev](https://scholar.google.de/citations?user=p1FuAMkAAAAJ&hl=en) for helping us with large scale training.\n* [Mitchell Wortsman](https://mitchellnw.github.io/), [Gabriel Ilharco](https://gabrielilharco.com/), [Simon Kornblith](https://simonster.com/), [Pang Wei Koh](https://koh.pw/) for technical discussions and for feedback on this blog.\n* [Ludwig Schmidt](https://people.csail.mit.edu/ludwigs/) for being our main advisor on this project and for their support.\n\n\n### Acknowledgements\n\nThis code is based on Lucidrains' [flamingo implementation](https://github.com/lucidrains/flamingo-pytorch) and David Hansmair's [flamingo-mini repo](https://github.com/dhansmair/flamingo-mini). Thank you for making your code public! We also thank the [OpenCLIP](https://github.com/mlfoundations/open_clip) team as we use their data loading code and take inspiration from their library design.\n\nWe would like to thank [Jean-Baptiste Alayrac](https://www.jbalayrac.com/) and [Antoine Miech](https://antoine77340.github.io/) for their advice, [Rohan Taori](https://www.rohantaori.com/), [Nicholas Schiefer](https://nicholasschiefer.com/), [Deep Ganguli](https://hai.stanford.edu/people/deep-ganguli), [Thomas Liao](https://thomasliao.com/), [Tatsunori Hashimoto](https://thashim.github.io/), and [Nicholas Carlini](https://nicholas.carlini.com/) for their help with assessing the safety risks of our release. This research is supported in part by NSF Institute on the Foundations of Machine Learning (IFML). Thanks to [Stability AI](https://stability.ai) for providing us with compute resources to train these models!\n","date":1679961600000},{"slug":"oig-dataset","frontmatter":{"title":"The OIG Dataset","author":"By Huu Nguyen -  Ontocord.ai, Sameer Suri, Ken Tsui , Shahules786, Together.xyz team, and Christoph Schuhmann - LAION.ai","date":"March 10 2023","previewImg":"/images/blog/oig-example.png"},"content":"\nThe [Open Instruction Generalist (OIG)](https://huggingface.co/datasets/laion/OIG) dataset is a large open source instruction dataset that currently contains ~43M instructions. \n\nOIG is one of many chatbot datasets that [LAION](https://laion.ai), along with its volunteers, [Ontocord](https://www.ontocord.ai), [Together](https://www.together.xyz) and other members of the open source community, will be releasing and is intended to create equal access to chatbot technology. Everyone is welcome to use the dataset and contribute improvements to it.\n\n## Examples of what is in OIG\n\n![](/images/blog/oig-example-2.png)\nExample data in OIG-43M.\n\n![](/images/blog/oig-example.png)\nTopic map of a subset of OIG-43M\n\n|id: value|\n|-|\n|6602: -1_Image prompts for drawing with specific keywords___|\n|1165: 0_Clipart use for teaching materials in commercial format with unlimited illustrations as an abcteach member___|\n|1047: 1_Images of Air Force Change of Command Ceremonies___|\n|745: 2_Documents related to military training and operations of Marine Corps and Army forces in 2013, 2017, and 2018.___|\n|332: 3_Employment Trends in Selected Metropolitan Areas___|\n|304: 4_Health Policy Workshop Proceedings and Image Covers for Cancer, Workforce, Literacy, and Accounting Approaches___|\n|291: 5_Printable worksheets for math, reading, and kindergarten learning with image prompts.___|\n|259: 6_Energy Trends and Prices___|\n|225: 7_Images featuring Defense Secretary James Mattis in official meetings and events.___|\n|174: 8_Images of Ricky Gervais, Jennifer Aniston, and Rachel Brosnahan at various award shows in Beverly Hills and Los Angeles.___|\n|168: 9_Cricket matches and fans in India, featuring IPL teams Kings XI Punjab and Kolkata Knight Riders, Bollywood actors Katrina Kaif and Shah Rukh Khan, and cricket legends Sachin Tendulkar and bowler Singh. Also includes matches with Australia, New Zealand, Pakistan, and Sri Lanka during the World Cup and Test matches.___|\n|140: 10_Images related to Covid-19 vaccination and prevention___|\n\nBreak-down of some image prompt instructions in a subset of OIG-43M.\n\n## Discussion\n\nOIG is a large-scale dataset containing instructions that are created using data augmentation from a diverse collection of data sources, and formatted in a dialogue style (<human>… <bot>… pairs). The goal of OIG is to help convert a language model pre-trained on large amounts of text into an instruction-following model. It is designed to support continued pre-training to enable a base model (e.g., GPT-NeoX-20B) that can be later fine-tuned with the smaller-scale domain-specific datasets.\n\nOIG is created by various LAION community members, consisting of 30 datasets and 43M instructions, but we will continue to expand on this dataset with the goal of reaching 1 trillion tokens - enough to pretrain on OIG only.  It covers not only standard datasets (such as Natural Questions and Natural Instructions), but also data specifically related to dialog, summarization, education, etc.\n\nAppendix 1 describes the components of the current OIG dataset. The dataset can be divided roughly into 75% academic datasets such as P3, Natural instructions, and FLAN, where answers may be short, and the tasks are often artificial, such as determining NLI. The other 25% is composed of various tasks, such as question and answering, providing how-to instructions, performing basic high school math, basic python coding, story generation, essay generation, poetry generation, and a very rudimentary attempt at generating songs based on augmenting existing poetry. Of note, we have also created a UL2-like fill in the blank dataset using TurkuNLP’s [OSCAR-registry](https://huggingface.co/datasets/TurkuNLP/register_oscar) data (e.g, “Fill in the missing spans”, “Fill in the rest of this paragraph”, “Give me the missing words”). We hypothesize that this mixture of instruction improves academic metrics as well as instruction fulfillment. \n\n## Safety and Moderation\n\nAlong with OIG, [Ontocord.ai](https://www.ontocord.ai) is also releasing [OIG-moderation](https://huggingface.co/datasets/ontocord/OIG-moderation), a small safety instruction dataset. OIG-moderation is intended to train a moderation model to predict labels for various moderation categories such as \"needs intervention\", “hate”, \"sexual content\", etc. Ontocord will also release in future versions, multilingual versions of the dataset, and include potential responses that could contain a reason why a chatbot might not respond to the answer. It aims to address issues including privacy eliciting prompts, and depression responses, along with prompts eliciting sexual content and aggressive behavior from users.\n\nOIG-moderation includes data from (a) public datasets such as anthropic-redteam and anthropic-harmless, prosocial, and contributed datasets from community members (b) [augmented toxic data](https://huggingface.co/datasets/SummerSigh/PolicyData) such as civil comments data converted into instructions, (c) anthropic-redteam data [augmented with prosocial tags](https://huggingface.co/datasets/shahules786/prosocial_augmented) (d) data provided by the LAION community that might include NSFW prompts, and (e) synthetic depression data generated from [a public depression bag of words](https://huggingface.co/datasets/joangaes/depression) dataset using one of LAION’s volunteer’s [grammar fixing models](https://huggingface.co/pszemraj/flan-t5-large-grammar-synthesis). \n\nA model trained on the OIG-moderation dataset can be used to provide safety labels, and the bot providers can choose to then block responses from their chatbots based on these labels. If a bot provider's policy for example permits sexual content, but prohibits PII eliciting text, they can hopefully do so with the output of a model trained on this OIG-moderation. \n\n## Safety Goals\n\nOpen source and extendable safety pipelines unfortunately do not exist on the same scale as those in ChatGPT and other commercial systems. To further research in implementable, accurate, and extendable safety pipelines, LAION, Together, and Ontocord will push models, datasets, and code to the public. Research is one of our goals for safety, and we believe that keeping code, datasets, and models private hinders the overall progress in keeping LLM systems safe. By sharing such information, users and researchers alike can point out the harms, and potential solutions in these multifaceted systems.\n\nAnother goal for us is to bring safety research to a production setting where it can be effectively implemented and tested in real world use cases. Research in subjects like [toxicity detection](https://docs.cohere.ai/reference/toxicity-detection) and [bias mitigation](https://arxiv.org/abs/2106.13219) in LLMs is well established; however, the implementation of such research in systems that use language models as conversational agents in real world production settings has largely gone undocumented and unevaluated. The gap between research and implementation, brings many questions that must be answered to bring safe LLMs to the general public.\n\nWith the potential of offering OIG based systems to millions of users, it’s important to recognize the diversity in the user base with respect to socially acceptable paradigms. Pushing generally accepted social paradigms for one user in a specific country, locality, or even age does not warrant those same paradigms to be pushed upon users of other areas and ages. Thus we have opted for a multi-pronged approach to moderation and safety. We have curated and created data with safety tags, so that the bot providers can decide to train on the data and decide for themselves which moderation knob to turn on and off and which to permit their users to turn on and off (e.g., via parental controls).\n\nThus, we will strive to make data for safety systems that allow for user input so that our models can accept and reject prompts on a per locality and even a per user basis. It may not be possible to achieve in the first iteration of a safety pipeline however we will continue to research and strive for this goal. \n\n## How is the OIG dataset related to LAION’s Open Assistant Project?\n\nLAION’s [Open Assistant (OA)](https://github.com/LAION-AI/Open-Assistant) project is our efforts to replicate the functionality of ChatGPT, and as such centers around gathering human feedback and training a reinforcement model based on human feedback. In contrast, the OIG dataset is almost purely a synthetic data set created using data augmentation. Our hypothesis for the OIG dataset is that you can create a performant bot, without RLHF, by first performing continued pre-training with an average quality instruction dataset such as OIG, and then doing a finetuning on a high quality instruction dataset such as OIG-small-chip2. With that said, the team members between the OA and OIG projects overlap and the OIG data began from work done within the LAION OA working group. \n\n## Models*\n\nThe community has trained several models based on a subset of the OIG datasets including:\n\n* Rallio67/joi2_(20,12,7)B_instruct_alpha\n* Rallio67/chip2_(20,12,7)B_instruct_alpha\n* Rallio67/joi_(20,12,7)B_instruct_alpha\n* Rallio67/chip_(20,12,7)B_instruct_alpha\n* togethercomputer/GPT-NeoXT-Chat-Base-20B\n\n## Safety models**\n\n* SummerSigh/T5-Base-Rule-Of-Thumb\n* SummerSigh/Safety-Policy\n* SummerSigh/BART-Base-Rule-Of-Thumb \n* shahules786/prosocial-classifier\n* shahules786/Safetybot-mt5-base\n* shahules786/Safetybot-T5-base\n* togethercomputer/GPT-JT-Moderation-6B\n\n[Together](https://www.together.xyz/) has finetuned a GPT-JT model on v.01 of OIG-moderation, and other LAION volunteers have trained many other models on different subsets of the OIG-moderation v.02 dataset. \n\nNote: All the models above can be found at [https://huggingface.co/](https://huggingface.co/)\n\n*We will update this section as more OIG based models are trained and released. \n\n** Models are in development and do not currently represent the final safety system for LAION chatbots or how models trained on OIG-moderation will fully behave. \n\n## What’s next \n\nThis is just the beginning. This is a new project that we hope will evolve over time. From a purely dataset cleanup perspective, we intend to run a PII anonymizer on the web crawled portion of the dataset (e.g., OSCAR-registry based data). Also, there are several key areas that we need to improve including knowledge Q&A, creative writing and coding. We are also working on collaborations for fine-tuned versions of the bot for tasks like education, which we are incredibly excited about. We also need to perform deduplication and basic filtering for very uninformative instructions in case we made mistakes in the data augmentation. As a prelude, LAION has an ongoing filtering and analysis project, called [riverbed](https://github.com/LAION-AI/riverbed), which aims to analyze the OIG dataset, which we describe briefly below.\n\n## Quality Filtering Approaches using masked language models (MLM)\n\nText outputs from dialogue prompted large language models are known to suffer from hallucinations and other factual inaccuracies. To address this problem, we applied various filtering to detect misinformation and contradiction with masked language models. In particular, masked language model is in a different paradigm from autoregressive language model, as its receptive field covers the context of both directions, providing extra information in detecting factual inaccuracies.\n\nWe framed fact checking as a masked language model pre-training objective. The idea behind is that if the prediction of  <mask> based on bidirectional context matches with the original token, the original token is more likely to be correct than incorrect and vice versa. A custom light weight RoBERTa based model is trained on high quality factual materials like books and wikipedia.\n\nWith the model, we analyzed the outputs of the GPT style language models by randomly masking a small percentage of the generated tokens and then using a BERT or T5 style language model to replace the masked tokens. Discrepancies between the original and replaced tokens were penalized to varying degrees. If the replaced token exactly matched the original token the penalty was zero. If the replaced token did not match the original token then it was analyzed for a semantic match by comparing a string containing the original token plus several tokens around it using language models finetuned on the natural language inference task (NLI) with three classifiers: entailment, neutral, contradiction. Entailment or neutral scores were considered positive while contradiction penalized the match.\n\n### Example 1\n\nOriginal > The big dog barked at the [fluffy] black cat.\n\nMasked > The big dog barked at the `<mask>` black cat.\n\nReplaced > The big dog barked at the [small] black cat\n\nNLI(Original,Replaced) = {'entailment': 13.0892, 'neutral': 79.0414, 'contradiction': 7.8693}\n\nNLI(Replaced,Original) = {'entailment': 0.1456, 'neutral': 99.7456, 'contradiction': 0.1087}\n\nScore = ( ( (13.09+79.04-7.86)/2 + (0.15+99.75-0.11)/2 ) / 100 ) = 0.92\n\n\n### Example 2\n\nOriginal > The big [llama] barked at the fluffy black cat.\n\nMasked > The big `<mask>` barked at the fluffy black cat.\n\nReplaced > The big [dog] barked at the fluffy black cat.\n\nNLI(Original, Replaced): {'entailment': 1.8346, 'neutral': 3.7347, 'contradiction': 94.4307}\n\nNLI(Replaced, Original): {'entailment': 0.111, 'neutral': 1.8248, 'contradiction': 98.0642}\n\nScore = ( ( (1.83 + 3.73 - 94.43)/2 + (0.11 + 1.82 - 98.06)/2 ) / 100 ) = -0.93\n\n\nWe also framed fact checking as replaced token detection, which is the pre-trained objective of ELECTRA. The idea is to look for “corrupted” token based on bidirectional context. Empirically, we found that, together with named entity recognition, pretrained electra large discriminator models could detect wrong named entities with a reasonable precision and recall in an augmented squad_v2 dataset.\n\nWe leveraged natural language inference to detect contradiction of dialogue from the bot. This acts as a self-consistency filter where we require a dialogue not to contradict itself. You can find a small sample of our work-in-progress [filtered OIG](https://huggingface.co/datasets/laion/OIG-riverbed-filtered-small) data here. More to come…\n\n## Support this project\n\nYour contributions and feedback support the open source ecosystem, improve the bot and provide datasets for future AI research. To participate you can:\n\n* Submit [Github](https://github.com/LAION-AI/Open-Instruction-Generalist) issues,  track issues and help create datasets that need improvement.\n* Join our [Discord](https://discord.gg/xBPBXfcFHd) to talk with other team members working on this!\n\n## Disclaimer\n\nThese datasets contain synthetic data and in some cases data that includes humans trying to get the language model to say toxic/offensive/trolling things. If you are concerned about the presence of this type of material in the dataset please make sure you carefully inspect each of the entries and filter appropriately. Our goal is for the model to be as helpful and non-toxic as possible and we are actively evaluating ways to reduce or eliminate undesirable content from the instruction tuning datasets.\n\n## License\n\nThe OIG dataset that is authored by LAION volunteers is released under an Apache 2.0 license. However, the data also includes content licensed under other permissive licenses such as Wikipedia data which is licensed under CC-BY-SA, or web-crawled data which is used under fair use principles. \n\n## Acknowledgement\n\n* We would also like to thank all of our amazing LAION volunteers including: @Rallio, @Jue, @Ce Zhang, @Player-1, @Laurel, @danielpatrickhug, @Jjmachan, @Mylo, @Khalid, @Coco.han,  @Pszemraj, and many others. \n* We would like to thank [Together](https://www.together.xyz/) for their tireless dedication to the open source and AI community and their contribution to many of the datasets.\n* We would like to thank [AI Horde](https://aihorde.net/) and user @Db0 for their incredible contribution of filtered data that were flagged as unethical.\n* Lastly, [Ontocord.ai](https://www.ontocord.ai)’s founders are grateful to have the opportunity to create a portion of the data augmentation and safety-moderation code for this project.\n\n\n## Appendix - Description of OIG datasets components\n\n- unified_ni [https://github.com/allenai/natural-instructions](https://github.com/allenai/natural-instructions)\n- unified_p3: [https://huggingface.co/datasets/bigscience/P3](https://huggingface.co/datasets/bigscience/P3)\n- unified_flan: [https://github.com/google-research/FLAN/tree/main/flan/v2](https://github.com/google-research/FLAN/tree/main/flan/v2)\n- unified_soda_dialog: [https://huggingface.co/datasets/allenai/soda](https://huggingface.co/datasets/allenai/soda)\n- unified_unifiedskg_instructions: [https://github.com/HKUNLP/UnifiedSKG](https://github.com/HKUNLP/UnifiedSKG) \n- unified_merged_code_xp3: [https://huggingface.co/datasets/bigscience/xP3](https://huggingface.co/datasets/bigscience/xP3) (only Python)\n- unified_oscar_en_sample_dialog: A small portion of [https://oscar-project.org/](https://oscar-project.org/)\n[https://huggingface.co/datasets/TurkuNLP/register_oscar](https://huggingface.co/datasets/TurkuNLP/register_oscar)\n- unified_ul2_plus_oscar_en_sample_dialog: A small portion of  [https://oscar-project.org/](https://oscar-project.org/)\n[https://huggingface.co/datasets/TurkuNLP/register_oscar](https://huggingface.co/datasets/TurkuNLP/register_oscar)\n- unified_multi_news: [https://www.tensorflow.org/datasets/catalog/multi_news](https://www.tensorflow.org/datasets/catalog/multi_news)\n- unified_openai_summarize_tldr:  [https://github.com/openai/summarize-from-feedback](https://github.com/openai/summarize-from-feedback)\n- unified_scitldr:  [https://github.com/allenai/scitldr](https://github.com/allenai/scitldr)\n- unified_squad_v2:  [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)\n- unified_nq:  [https://ai.google.com/research/NaturalQuestions](https://ai.google.com/research/NaturalQuestions)\n- unified_poetry_instructions: Poetry data of mostly classical poems\n[https://huggingface.co/datasets/merve/poetry](https://huggingface.co/datasets/merve/poetry)\n[https://huggingface.co/datasets/matthh/gutenberg-poetry-corpus](https://huggingface.co/datasets/matthh/gutenberg-poetry-corpus)\n- unified_sqlv1 and unified_sqlv2: public text 2 sql datasets.\n- unified_unatural_instructions: [https://github.com/orhonovich/unnatural-instructions](https://github.com/orhonovich/unnatural-instructions)\n- unified_conv_finqa:  [https://github.com/czyssrs/ConvFinQA](https://github.com/czyssrs/ConvFinQA)\n- unified_essays:  essays available on the public web \n- unified_plot_screenplay_books_dialog : [https://github.com/markriedl/WikiPlots](https://github.com/markriedl/WikiPlots) extracted from Wikipedia, snippets from the Pile’s [https://huggingface.co/datasets/the_pile_books3](https://huggingface.co/datasets/the_pile_books3), and snippets of screenplays available on the public web. \n- unified_grade_school_math_instructions: [https://github.com/openai/grade-school-math](https://github.com/openai/grade-school-math)\n- unified_mathqa_flanv2_kojma_cot: Public chain-of-thought datasets converted to instructions [https://huggingface.co/datasets/math_qa](https://huggingface.co/datasets/math_qa), \n- unified_joke_explanations: a very small dataset of joke explanations crawled from the public web \n- unified_cuad:  [https://www.atticusprojectai.org/cuad](https://www.atticusprojectai.org/cuad)\n- unified_abstact_infill:  dbpedia and wikipedia snippets combined with a small portion of [https://github.com/google-research/dialog-inpainting](https://github.com/google-research/dialog-inpainting) \n- unified_image_prompts_instructions: A very small subset of LAION-400M\n- unified_canadian_parliament:  [https://openparliament.ca/data-download/](https://openparliament.ca/data-download/)\n- unified_poetry_2_song:  The above poetry dataset (and [https://huggingface.co/datasets/shahules786/PoetryFoundationData](https://huggingface.co/datasets/shahules786/PoetryFoundationData))  translated to song-like structures.\n- unified_hc3_human - [https://huggingface.co/datasets/Hello-SimpleAI/HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3) \n- unified_rallio_safety_and_prosocial: Generated from public datasets and generated from Wiki similar to the chip2 data; find a full list in the end of the document, also includes [https://huggingface.co/datasets/allenai/prosocial-dialog](https://huggingface.co/datasets/allenai/prosocial-dialog) and [https://huggingface.co/datasets/Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)  \n- unified_chip2: Generated from public datasets and generated from Wiki’s; full list below\n\n\n## OIG-small-chip2\n\n### Python Code Examples\n\nA set of instruction / response pairs where the User requests the agent to generate a python function. These examples were generated using a large language model and few shot prompting with python code verified to execute. There are also ~3000 examples of manually curated one line python code examples from the Conala publication (see: [https://conala-corpus.github.io/](https://conala-corpus.github.io/))\n\n### Natural Instruction Examples\n\nA balanced set of diverse natural and factual questions and answers made using few shot prompted UL2 20B and an instruction tuned GPT-NeoX-20B model (Chip) and then rejection sampled using multiple automatic evaluations to remove low quality outputs and to filter out factually inaccurate answers. Also includes some filtered natural instructions from Anthropic Helpful instructions (see: https://github.com/anthropics/hh-rlhf).\n\n### Generic Harmless Instruction Examples\n\nA set of instruction / response pairs sourced from the Anthropic redteam paper github (see: https://github.com/anthropics/hh-rlhf). This dataset includes a lot of data regarding real humans trying to make the Anthropic language models say harmful/toxic/trolling things. For this dataset only examples that were rated lowly on the harmful scale (0,1,2 out of 4, where 4 is the most toxic) were included. Again, only the first lines of dialogue (instruction, first_agent_response) were retained.\n\n### Instruction/Responses with Lists\n\nA set of filtered and reformatted instruction / response pairs where the agent response contains a list. Sourced from the Anthropic github (see: https://github.com/anthropics/hh-rlhf). Sourced from wikihow text lists created by b-mc2 (https://huggingface.co/datasets/b-mc2/wikihow_lists). And rejection filtered instruction response pairs generated by Chip20B that contained lists. All lists are formatted in a similar style.\n\n### Follow-up questions\n\nExamples of instructions and responses where an appropriate response is to ask for more information from the prompter. These examples were generated from a combination of few shot prompted UL2 20B (to generate natural questions) and a large dialogue prompted language model to generate the responses containing follow-up questions.\n\n### Wikipedia Toxic Adversarial Questions\n\nQuestions and answers generated from wikipedia articles that discuss potentially sensitive topics (flagged as potentially toxic by an early toxicity detection model).\n\n### Grade School Math GSM8K (~9,000)\n\nGSM8K is a dataset of 8.5K high quality linguistically diverse grade school math word problems created by human problem writers. The dataset is segmented into 7.5K training problems and 1K test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ − ×÷) to reach the final answer. A bright middle school student should be able to solve every problem. It can be used for multi-step mathematical reasoning. [https://github.com/openai/grade-school-math](https://github.com/openai/grade-school-math)\n\n### Reasoning Instructions\n\nExamples from the Com2Sense and Strategy QA datasets that were reformatted into natural instructions using large language models with few shot prompting and additional quality filtering steps.\n\n### Character and Scene Descriptions\n\nExamples of instructions and responses for the generation of character or scene descriptions. Scenes were sourced from video game wikis and reformatted into instruction / response format using large language models or generated by few shot prompting with large language models.","date":1678406400000},{"slug":"coca","frontmatter":{"title":"Training Contrastive Captioners","author":"Giovanni Puccetti, Maciej Kilian, Romain Beaumont","date":"Feb 2 2023","previewImg":"/images/blog/eval_coca_clip.jpg"},"content":"\n\nWe introduce a new model type to [OpenClip](https://github.com/mlfoundations/open_clip) Contrastive Captioners (CoCa) [1]. This model adds an autoregressive objective (generation) on top of the CLIP contrastive one. The architecture is composed of three parts, the first two are similar to those composing a CLIP model and the third is a text decoder that stands on top of the text encoder. The additional decoder takes as input the encoded images (through cross-attention) and the previous tokens to predict the next most probable one. One of the few architecture changes, compared to CLIP, is attentional pooling [2], used to aggregate image representations and pass them to both the contrastive loss and the decoder cross-attention.\n\nThis is interesting for several reasons:\n\n* We believe there is no openly available trained model with this architecture;\n* Adding a generative task appears to help the contrastive task with minimal computational impact;\n* The model is easily adaptable to a large number of tasks, on top of all those CLIP is suited for. CoCa models can (with relatively cheap fine-tuning) perform Image Captioning, Visual Question Answering, Multimodal Understanding, and more;\n* CoCa gives captioning models an intermediate contrastive latent space for minimal training cost increase.\n\n\n## Benchmarks\n\nOn a comparable model size and with the same training data available, CoCa outperforms a CLIP model on several zero-shot tasks (Figure 1). Most notably on _imagenet1k_ CoCa achieves 75.5 and CLIP 73.1 (2.6% improvement).\n\n\n|(a) ![](/images/blog/eval_coca_clip.jpg) |(b) ![](/images/blog/eval_coca_clip_diff.jpg) |\n|:-|:-|\n\n\n_Figure 1:_ Scores achieved by _coca_ViT-L-14_ and _ViT-L-14_ on several zeroshot classification tasks **(a)**, together with the performance gap between the two models, in the same tasks sorted by magnitude **(b)**.\n\n\n\n\nTable 2 shows the results achieved on Text to Image and Image to Text retrieval by both CoCa and CLIP. In this case too, CoCa outperforms CLIP on all tasks with differences ranging from 0.3 to 1.3.\n\n\n<table>\n  <tr>\n   <td colspan=\"4\" align=\"center\" > Text to Image Retrieval Recall@5\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td>flickr30k\n   </td>\n   <td>flickr8k\n   </td>\n   <td>Mscoco captions\n   </td>\n  </tr>\n  <tr>\n   <td>coca_ViT-L-14\n   </td>\n   <td>92.0\n   </td>\n   <td>70.1\n   </td>\n   <td>70.5\n   </td>\n  </tr>\n  <tr>\n   <td>ViT-L-14\n   </td>\n   <td>91.7\n   </td>\n   <td>69.0\n   </td>\n   <td>69.2\n   </td>\n  </tr>\n  <tr>\n   <td colspan=\"4\" align=\"center\"> Image to Text Retrieval Recall@5\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td>flickr30k\n   </td>\n   <td>flickr8k\n   </td>\n   <td>Mscoco captions\n   </td>\n  </tr>\n  <tr>\n   <td>coca_ViT-L-14\n   </td>\n   <td>99.3\n   </td>\n   <td>81.7\n   </td>\n   <td>83.6\n   </td>\n  </tr>\n  <tr>\n   <td>ViT-L-14\n   </td>\n   <td>98.4\n   </td>\n   <td>81.2\n   </td>\n   <td>83.0\n   </td>\n  </tr>\n</table>\n\n_Table 2:_ Text to Image and Image to Text retrieval **Recall@5** on _flickr30k_, _flickr8k_ and _Mscoco captions_.\n\n## Released Checkpoint\n\nWe release checkpoints for two model configs, _coca_ViT-B-32_ and _coca_ViT-L-14_. We also release the MSCOCO finetunes of those models which are much better at captioning but unfortunately lose their contrastive capabilities during fine tuning.\n\nTry generation in this [Space](https://huggingface.co/spaces/laion/CoCa) or in this [colab notebook](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb)!\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td>L/14\n   </td>\n   <td>B/32\n   </td>\n   <td>CoCa (from paper)\n   </td>\n  </tr>\n  <tr>\n  <td># Params Image Encoder\n   </td>\n   <td>306.72M\n   </td>\n   <td>89.16M\n   </td>\n   <td>\n    1B\n   </td>\n  </tr>\n  <tr>\n   <td># Params Text Encoder\n   </td>\n   <td>123.65M\n   </td>\n   <td>63.42M\n   </td>\n   <td rowspan=\"2\">\n    1.1B\n   </td>\n  </tr>\n  <tr>\n   <td># Params Text Decoder\n   </td>\n   <td>208.07M\n   </td>\n   <td>100.96M\n   </td>\n  </tr>\n</table>\n\n_Table 3:_ Number of parameters for each encoder/decoder component for _coca_ViT-L-14_, _coca_ViT-B-32_ and the _CoCa_ model from the original paper (M=millions, B=billions).\n\n\n\n## Training Notes\n\n\n### Pretraining\n\nWe train both model configurations on 13B samples seen from [LAION-2B](https://laion.ai/blog/laion-5b/) [3] with a batch size of 90k, learning rate of 1e-3, and a cosine decay learning rate schedule. Experiments were performed on 384 A100’s and over the course of training we maintained 75.5 samples/s/gpu (~29k samples/s in total).\n\nWhen it comes to cost, even though CoCa has more capabilities than single-task captioning models there’s a minimal increase ~20% (as reported by Table 8b of the paper). This is due to the fact that the first half of the text decoder (i.e. the text encoder) is unimodal and is computed in parallel to the image encoder, once the encoders are done we simply continue the forward pass of the text embeddings through the text decoder and also include the image embeddings via cross attention. The trainig report can be found [here](https://wandb.ai/iejmac/open-clip/reports/CoCa-L-14--VmlldzozNDEwMDIx).\n\n\n### Fine-tuning\n\nFor image captioning tasks fine-tuning is a straightforward extension of pretraining with few hyper parameters changes. The crucial one is contrastive loss weight, which has to be set to zero to let the backward pass only account for the generative loss, besides  there are no additional fine-tuning oriented components nor changes in the loss. We use a batch size of 128 with a learning rate of 1e-5 and a cosine learning rate schedule. Experiments are performed on 4 A100's. Table 4 shows the language generation scores achieved by _coca_ViT-L-14_ and by CoCa in the original paper, _coca_ViT-L-14 performance is still far from the original CoCa model one.\n\nIt is noteworthy that (in our experiments) after fine-tuning with a generative only loss these models lose their contrastive skills entirely.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td>Bleu@4\n   </td>\n   <td>METEOR\n   </td>\n   <td>CIDEr\n   </td>\n   <td>Spice\n   </td>\n  </tr>\n  <tr>\n    <td colspan=\"5\" align=\"center\">\n    coca_ViT-L-14\n    </td>\n  </tr>\n  <tr>\n   <td>Karpathy val\n   </td>\n   <td>35.6\n   </td>\n   <td>29.8\n   </td>\n   <td>125.3\n   </td>\n   <td>23.4\n   </td>\n  </tr>\n  <tr>\n   <td>NoCaps\n   </td>\n   <td>39.9\n   </td>\n   <td>29.1\n   </td>\n   <td>106.5\n   </td>\n   <td>14.7\n   </td>\n  </tr>\n  <tr>\n    <td colspan=\"5\" align=\"center\">\n    Original CoCa (from paper)\n    </td>\n  </tr>\n  <tr>\n   <td>Karpathy val\n   </td>\n   <td>40.9\n   </td>\n   <td>33.9\n   </td>\n   <td>143.6\n   </td>\n   <td>24.7\n   </td>\n  </tr>\n  <tr>\n   <td>NoCaps\n   </td>\n   <td> -\n   </td>\n   <td>-\n   </td>\n   <td>122.4\n   </td>\n   <td>15.5\n   </td>\n  </tr>\n</table>\n\n_Table 4:_ Visual captioning scores achieved with _coca_ViT-L-14_ on _karpathy_ validation set and _NoCaps_.\n\n\n\n## Captioning Examples\n\n\n\n|<img src=\"/images/blog/ipod_apple.png\" alt=\"cao\" width=\"500\">|<img src=\"/images/blog/space_raccoon.png\" alt=\"cao\" width=\"500\">|\n|:-|:-|\n|An apple sitting on top of a wooden table.|A painting of a raccoon in a space suit.|\n\n\n\n\n\n\n\n\n\n\n\n\n## What’s Next\n\n\n\n* Unimodal Text Pretraining - One of the shortcomings of CoCa is that it can have trouble with zero-shot captioning because the noisy web text it was trained on isn’t as rich as unimodal text data. To this end we can look into methods that provide CoCa models with this rich text understanding either via initializing the weights of the decoder with some pretrained unimodal text decoder or perhaps alternating between multimodal and unimodal losses that use different data.\n* Fine tuning on more tasks VQA, multimodal reasoning, and more.\n* Image Decoder - CoCa adds a multimodal text decoder on top of CLIP and shows this multi-task learning can benefit both tasks. Why not also add a multimodal image decoder?\n\n\n## Contributions and acknowledgements\n\nThanks to\n\n\n\n* [gpucce](https://gpucce.github.io/) and [iejMac](https://github.com/iejMac) for implementation into open_clip and training the models.\n* [lucidrains](https://github.com/lucidrains) for [initial implementation](https://github.com/lucidrains/CoCa-pytorch).\n* [Romain Beaumont](https://github.com/rom1504) and [Ross Wightman](https://github.com/rwightman) for advice, reviews, and engineering support.\n* [Soonhwan-Kwon](https://github.com/Soonhwan-Kwon) for implementing beam search.\n\nHuge thanks to [Emad](https://twitter.com/EMostaque) and [StabilityAI](https://stability.ai/) for providing the compute resources required to train these models.\n\n\n## References\n\n[1] Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., & Wu, Y. (2022). CoCa: Contrastive Captioners are Image-Text Foundation Models. _ArXiv, abs/2205.01917_.\n\n[2] Lee, J., Lee, Y., Kim, J., Kosiorek, A.R., Choi, S., & Teh, Y.W. (2018). Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. _International Conference on Machine Learning_.\n\n[3] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., & Jitsev, J. (2022). LAION-5B: An open large-scale dataset for training next generation image-text models. _ArXiv, abs/2210.08402_.","date":1675296000000},{"slug":"h14_clip_retrieval","frontmatter":{"title":"Clip-Retrieval Update: H-14 Index & SLURM Inference","author":"no usr","date":"Jan 31 2023","previewImg":"/images/blog/h_14_clip_front.png"},"content":"\nToday we release a KNN index for LAION-5B that allows for fast queries of the dataset with the open clip ViT-H-14 CLIP model. This means that users can search through billions of samples quickly and easily, making it a powerful tool for various applications such as image and text retrieval, data filtering and more. With this update also comes a brand new SLURM based inference backend for high-compute environments.\n\nWith this users can now:\n  - Peer into the superset of data used to train the latest stable-diffusion-v2 models.\n  - Easily filter through the dataset to create fine-tuning datasets and averaged embeddings for aesthetic gradients.\n  - Quickly compute indices for new datasets with the SLURM backend.\n  - Download the index & deploy locally\n\n## The front-end\n\nOur new H/14 index is now available for use on our clip-front demo at https://rom1504.github.io/clip-retrieval or https://knn.laion.ai. \nThis new index allows for fast querying using both images and text, making it a valuable tool for a variety of use cases.\nTo start using the new index right away, simply visit the website and start experimenting with the available query options. \nThe demo also allows you to easily download the resulting query as an [img2dataset](https://github.com/rom1504/img2dataset) compatible json file. \nThis means that you can quickly create datasets for any use case, making it a valuable resource for creatives, data scientists and researchers alike.\n\n## Using the KNN as an API\n\nThe KNN index can be accessed via the API, which allows you to perform nearest-neighbor searches in an easy and intuitive way. \nHowever, if you would prefer to use the provided knn index programmatically, you can! \nWe have a notebook that you can use as a guide on how to do so. You can find the notebook [here](https://colab.research.google.com/github/rom1504/clip-retrieval/blob/master/notebook/clip-retrieval-getting-started.ipynb), it will walk you through the steps necessary to use the provided KNN index programmatically. \nThis can be useful if you want to integrate the KNN index into your own application or if you want to automate the process of nearest-neighbor searches. \nNote that if you are looking to integrate the index into your own product, you should deploy it locally.\n\n## Computing your own index\n\nCreating your own index is a great way to interact with and visualize your data. \nWith a custom CLIP embedding index you can quickly search for similar images, check what images your prompts summon, or check how unique a generated image may be to the training data.\n\nThe clip-retrieval repo offers the ability for users to compute their own indices for their own datasets. \nIn an effort to support the creation of our new H-14 index, we added support for SLURM as a backend inference engine. \nThis update adds a third option for computing indices meaning whether you are using SLURM, PySpark, or running it on your local machine, the process of creating a CLIP KNN index has never been easier. \nFor specific usage please see the project’s [README](https://github.com/rom1504/clip-retrieval#clip-inference) for the inference API and the newest arguments available for creating your own index.\n\n## Deploying Locally\n\nUsers who would like to do a lot of queries, or integrate the index into their own product, should download the index and metadata and deploy it locally to their own server. \nIn order to do so we have uploaded the pre-computed indices to huggingface which can be found [here](https://huggingface.co/datasets/laion/laion5b-h14-index). \nFor full documentation on the exact steps necessary to begin hosting the index yourself please visit the [clip-retrieval docs](https://github.com/rom1504/clip-retrieval/blob/main/docs/laion5B_h14_back.md).\n","date":1675123200000},{"slug":"giant-openclip","frontmatter":{"title":"Reaching 80% zero-shot accuracy with OpenCLIP: ViT-G/14 trained on LAION-2B","author":"Mitchell Wortsman","date":"Jan 24 2023","previewImg":"/images/blog/scaling_vit_giant.png"},"content":"\nWe have trained a new [ViT-G/14 CLIP](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k) model with [OpenCLIP](https://github.com/mlfoundations/open_clip) which achieves **80.1%** zero-shot accuracy on ImageNet and **74.9%** zero-shot image retrieval (Recall@5) on MS COCO. As of January 2023, this is the best open source CLIP model.\n\nWe believe this is interesting because:\n* CLIP models are useful for zero-shot classification, retrieval, and for guidance/conditioning in generative models (OpenCLIP is used in Stable Diffusion V2 and currently the third most downloaded model on HuggingFace is a CLIP model). The approach underlying CLIP—self supervised learning on a large, heterogeneous dataset—has been shown to produce models which are more [robust](https://openai.com/blog/clip/) and [fair](https://ai.facebook.com/blog/seer-10b-better-fairer-computer-vision-through-self-supervised-learning-training-on-diverse-datasets/).\n* Our new ViT-G model achieves the highest zero-shot ImageNet accuracy among models that use only naturally occurring image-text pairs as training data, and without explicit labels, pseudo-labels, or any pretrained image or text encoders.\n* Our training run utilized multiple new techniques, including [FLIP](https://arxiv.org/abs/2212.00794) to accelerate training and [model soups](https://arxiv.org/abs/2203.05482) to surpass 80% accuracy.\n\n## Main Results\nThe following results are with image resolution 224x224 except for CoCa which uses 576x576.\n\n| Model name       | Batch size |               Samples seen              | Text Params | Image params | ImageNet top1 | Mscoco image retrieval at 5 | Flickr30k image retrieval at 5 |\n|------------------|:----------:|:---------------------------------------:|:-----------:|:------------:|:-------------:|:---------------------------:|:------------------------------:|\n| OpenAI CLIP L/14 | 32k        | 13B                                     | 123.65M     | 303.97M      | 75.4%         | 61.0%                         | 87.0%                            |\n| OpenCLIP H/14    | 79k        | 32B (16 epochs of laion2B)              | 354.0M      | 632.08M      | 78.0%         | 73.4%                       | 94%                            |\n| OpenCLIP G/14    | 160k       | 32B +unmasked fine-tune (details below) | 694.7M      | 1844.9M      | 80.1%*        | 74.9%                       | 94.9%                          |\n| CoCa            | 66k        | 33B                                     | 1100M       | 1000M        | 86.3%**       | 74.2                        | 95.7                           |\n\n\\* When using [CuPL](https://arxiv.org/abs/2209.03320) prompts instead of the standard prompts from OpenAI, the zero-shot accuracy is 80.3%. When evaluating at 280x280 and changing resize to squash, Ross Wightman found the model achieves 80.4%.\n\n** In addition to natural language supervision, [CoCa](https://arxiv.org/abs/2205.01917) uses synthetic captions constructed with the labels from the JFT-3B dataset. In addition to natural language supervision, CoCa uses synthetic captions constructed with the labels from the JFT-3B dataset. 973 of the 1,000 ImageNet classes have a corresponding class in JFT (e.g., see here sec C.7.2).\n\nAlso see the figure below (figure code by Ross) and our analysis of scaling trends for OpenCLIP model [here](https://arxiv.org/abs/2212.07143).\n\n![](/images/blog/scaling_vit_giant.png)\n\n## Released Checkpoints\n\nWe release the checkpoint through [OpenCLIP](https://github.com/mlfoundations/open_clip) and in the [HuggingFace hub](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k).\n\n## Notes on scaling up\n\nTo scale up model size while reducing compute we used [Fast Language-Image Pre-training (FLIP)](https://arxiv.org/abs/2212.00794) with patch dropout 0.5. Similar to [Masked Autoencoders (MAE)](https://arxiv.org/abs/2111.06377), FLIP drops out patches during training. FLIP also requires a short “unmasked tuning” phase, which we discuss in training notes below. In addition to reducing Giga multiply–accumulate operations (GMACs) for each forward/backward pass, FLIP allowed us to use a larger per-GPU batch size. Without FLIP, gradient accumulation was necessary to maintain a large batch size. Keeping batch size and number of GPUs consistent (at 160k and 512, respectively) but switching to unmasked fine-tuning resulted in a drop from 46.9 to 20.4 samples per second per GPU. For reference, OpenCLIP H/14 with global batch size 79k across 824 GPUs without patch dropout trained at 42 samples/s/GPU.\n\nTo scale up the batch size to 160k, we used [gradient checkpointing](https://arxiv.org/abs/1604.06174v2) and 80GM VRAM A100s. For the unmasked tuning portion, we also used gradient accumulation (see our implementation for the contrastive objective [here](https://github.com/mlfoundations/open_clip/pull/267)). Finally, we used a 2x higher learning rate of 2e-3 compared to our experiments with batch size 80k. The combination of scaling up model, batch size, and learning rate resulted in training instability during the warmup phase. Accordingly, we increased warm-up to 13k steps, trained with layer scale, and used AdamW beta2 0.95. All runs used AMP bfloat16, after previously switching from float16 in prior experiments with L/14 and H/14.\n\n## Training notes\n\n### Phase 1: Patch dropout\n\nFor phase 1 we trained ViT-G with [patch dropout](https://arxiv.org/abs/2212.00794) 0.5 on LAION-2B for 32B samples seen. We used batch size 160k, learning rate 2e-3, and a cosine decay schedule. After this phase the model reached 79.07 zero-shot top1 accuracy on ImageNet.\n\nTraining was mainly done on 512 to 760 A100s depending on availability. When changing the number of GPUs, local batch size was also modified so that the global batch size remained at 160k. When using 512 GPUs we set local batch size to 313 and observed roughly 24k samples per second or 46.9 samples/s/GPU. When using 760 GPUs we set local batch size 211 and observed roughly 33k samples per second or 43.4 samples/s/GPU.\n\n### Phase 2: Unmasked tuning + Model soups\n\nFor phase 2 we followed [FLIP](https://arxiv.org/abs/2212.00794) in conducting a short unmasked tuning phase. We fell short of 80% in our first unmasked fine-tuning phase, reaching only 79.43%. So we tried twice more with different settings (described below) to obtain 79.45% and 79.2%, respectively. Next, we followed [model soups](https://arxiv.org/abs/2203.05482) and averaged the weights of three checkpoints produced by these runs to achieve our final accuracy of 80.1%. [LIMoE](https://arxiv.org/abs/2206.02770) and [PaLI](https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html) also used model soups for better pre-training.\n\nFor our first unmasked fine-tuning run we did not modify the learning rate schedule, but instead doubled the base LR and extended the number of iterations so that the run would proceed for an additional 2B samples seen. LR started at 3.8e-5. For the second run we used LR 5.5e-5 with a full cosine schedule (warmup for roughly 200M samples and a total of 4B samples). The third run had identical hyperparameters to the first but used the LAION-A subset of LAION-2B. LAION-A is a 900M subset of LAION-2B filtered with aesthetic V2 4.5+ and pHash deduplicated. Instead of waiting for the third run to complete we use the checkpoint after approximately 700M samples which, when “[souped](https://arxiv.org/abs/2203.05482)” with the final checkpoints from the two proceeding runs, already allowed us to surpass our goal of 80% accuracy. This indiviual checkpoint achieved 79.2%.\n\nUnmasked fine-tuning was done on 512 A100 GPUs at a speed of roughly 10,450 samples/s or 20.4 samples/s/GPU.\n\nThe following plot shows the loss curve for phase 1.\n\n![](/images/blog/loss_vit_giant.png)\n\n## More results\n\nZero-shot accuracies at resolution 224x224 computed with [CLIP Benchmark](https://github.com/LAION-AI/CLIP_benchmark).\n\n| Dataset         | OpenCLIP H/14 | OpenCLIP G/14 |\n|-----------------|---------------|---------------|\n| ImageNet        | 78.0         | 80.1         |\n| ImageNet-V2     | 70.8         | 73.6         |\n| ImageNet-R      | 89.3         | 92.1         |\n| ImageNet-Sketch | 66.6         | 68.9         |\n| ObjectNet       | 69.7         | 73.0         |\n| ImageNet-A      | 59.2         | 69.3         |\n| CIFAR-10        | 97.4         | 98.2         |\n| CIFAR-100       | 84.7         | 87.5         |\n| MNIST           | 72.9         | 71.6         |\n| SVHN            | 56.1         | 62.5         |\n| Caltech-101     | 85.0         | 86.4         |\n| SUN397          | 75.2         | 74.5         |\n| FGVC Aircraft   | 42.8         | 49.7         |\n| Country211      | 30.0         | 33.8         |\n| Cars            | 93.5         | 94.6         |\n\nHere is a summary figure comparing G/14 and H/14 made with evals by Romain Beaumont.\n\n![](/images/blog/summary_vit_giant.png)\n\n## What’s Next?\n\nIn the future, we may fine-tune the model to enable multilingual capabilities, or fine-tune at higher resolution. Also, [FSDP is coming to OpenCLIP](https://github.com/mlfoundations/open_clip/pull/358) which will allow even larger models, as is [CoCa](https://github.com/mlfoundations/open_clip/pull/308) which will allow new openclip models to also be captioners. More contributions to OpenCLIP are always welcome!\n\n## Contributions and acknowledgements\n\nThanks to:\n* [Romain Beaumont](https://github.com/rom1504), [Ross Wightman](https://github.com/rwightman), [Mehdi Cherti](https://github.com/mehdidc), [Gabriel Ilharco](https://gabrielilharco.com/), and [Jenia Jitsev](https://github.com/JeniaJitsev) for providing extensive ideas, advice, engineering support, evaluating the model, and maintaining the openclip repository used for model training.\n* [Christoph Schuhmann](https://github.com/christophschuhmann) for encouragement and support\n* [Richard Vencu](https://github.com/rvencu) for cluster support\n* [Phil Wang](https://github.com/lucidrains) and [Haoqi Fan](https://haoqifan.github.io/) for the implementation and discussion regarding patch dropout\n* [Sho Yaida](https://www.shoyaida.com/), [Jong Wook Kim](https://jongwook.kim/), [Ari Morcos](http://www.arimorcos.com/) and [Saining Xie](https://www.sainingxie.com/) for helpful remarks regarding hyperparameters\n* [Sarah Pratt](https://sarahpratt.github.io/) for implementing CuPL\n* [Ludwig Schmidt](https://github.com/ludwigschmidt) and [Ali Farhadi](https://homes.cs.washington.edu/~ali/) for helpful discussions, and to the [RAIVN](https://raivn.cs.washington.edu/) and [EFML](https://github.com/mlfoundations/) labs at the University of Washington\n\nAnd of course thanks to [Emad](https://twitter.com/EMostaque) and [Stability AI](https://stability.ai/) for providing the compute resources used for these experiments.\n","date":1674518400000},{"slug":"laion-stable-horde","frontmatter":{"title":"Collaboration between LAION and the Stable Horde","author":"Konstantinos Thoukydidis, hlky","date":"Jan 08, 2023","previewImg":"/images/blog/artbot-image-ratings.png"},"content":"\nAuthor: [Konstantinos Thoukydidis](https://dbzer0.com), [hlky](https://twitter.com/hlky__)\n\nWe are happy to announce that LAION will be assisted by the Stable Horde to provide aesthetic ratings for existing datasets and a completely new dataset of Stable Diffusion generations, which will also be rated by their community.\n\nWe wrote in the past about [LAION-Aesthetics](https://laion.ai/blog/laion-aesthetics/) and how we filtered LAION-5b using an [aesthetic predictor](https://github.com/christophschuhmann/improved-aesthetic-predictor). The predictor, a simple neural net that uses CLIP ViT-L/14 embeddings as input. hlky has retrained the aesthetic predictor using ViT-H-14, and the results are promising.\n\n[hlky’s ViT-H aesthetic predictor](https://github.com/hlky/aesthetic-predictor) was trained on the same datasets as the original - AVA: A Large-Scale Database for Aesthetic Visual Analysis, Simulacra Aesthetic Captions, and LAION-logos. These datasets are limited, totalling around 400k image-rating pairs. This is where the Stable Horde comes in.\n\n[The Stable Horde](https://stablehorde.net/) is a crowdsourced, distributed, [free and open sourced service](https://github.com/db0/AI-Horde) for generating Stable Diffusion images available to be integrated into everything. They have volunteered to direct their community to rate the existing LAION datasets for aesthetic rating of 1-10. As the Stable Horde can be used for free, to incentivize this, they are providing priority (AKA kudos) to anyone who is providing such ratings for their stable diffusion generations.\n\nThe integration has been made possible through an API hosted by Sygil.dev which can collect such ratings and be integrated directly into all Stable Horde clients. Already the first integrations have been released on clients like [ArtBot](https://tinybots.net/artbot/rate).\n\n![](/public/images/blog/artbot-image-ratings.png)\n\nFurthermore the Stable Horde has started gathering thousands of Stable Diffusion images and their parameters generated by their community on a voluntary basis, which will then be used to build a completely new AI dataset. These images will not only be incentivized to be rated aesthetically as well, but also to be rated pairwise. \n\nStable Horde supports many models in addition to base Stable Diffusion models. At the time of writing this, nearly 80 finetunes are available. There are currently no datasets of generated images from Stable Diffusion finetunes, this will be a world first!\n\nThe way this works is that whenever an image set (meaning a number of images out of the same prompt) is generated, a follow-up call can be done to submit ratings for all, or some of them. If the set includes more than one image, optionally the best of the set can be selected, which will serve as a pairwise rating. If no “best” image has been selected but they have been aesthetically rated, the best image will be automatically selected among those ratings.\n\nThrough this collaboration, LAION will be provided with a dataset of millions of images rated in such a manner, along with their prompts, which can be made freely accessible to train further models with improved quality.\n\n\n## What's next?\n\nThe Stable Horde plans to onboard more ratings into their dataset, such as tagging images for artifacts, watermarks, nudity and the like. The provided set will continue expanding so long as people volunteer their time and processing power. \n\n## Support this endeavour\n\nIf you have any questions or need support about the Stable Horde or the rating system, they have [a very active discord server you can join](https://discord.gg/3DxrhksKzn). If you want to support this initiative, you can help by either rating images yourself, or by [onboarding your own GPU as a horde worker](https://github.com/db0/AI-Horde-Worker) which will help the community generate more images and thus increase the size of the open dataset they provide.","date":1673136000000},{"slug":"laion-coco","frontmatter":{"title":"Laion coco: 600M synthetic captions from Laion2B-en","author":"Christoph Schuhmann, Andreas Köpf, Richard Vencu, Theo Coombes, Romain Beaumont","date":"Sep 15, 2022","previewImg":"/images/blog/laion_coco_beach.png"},"content":"\nAuthor: [Christoph Schuhmann](https://github.com/christophschuhmann), [Andreas Köpf](https://github.com/andreaskoepf) , [Theo Coombes](https://github.com/TheoCoombes), [Richard Vencu](https://github.com/rvencu/), [Benjamin Trom](https://github.com/limiteinductive) , [Romain Beaumont](https://github.com/rom1504) \n\n**We present LAION-COCO, the world’s largest dataset of 600M generated high-quality captions for publicly available web-images**\n\nLaion5B has five billion natural captions. They provide a lot of information, but could synthetic captions complement them ?\n\nTo answer this question, we use a combination of existing, publicly available models to produce high quality captions for images in the style of [MS COCO](https://paperswithcode.com/dataset/coco).\n\nWe captioned 600M images from the english subset of Laion-5B with an ensemble of [BLIP](https://github.com/salesforce/BLIP) L/14 and 2 CLIP versions (L/14 and RN50x64).  \n\nWith this post we release them openly today.\n\nThis will make it possible to investigate the value of generated captions to train models. We’re curious on how these synthetic captions could impact models trained on them!\n\n\n## Download it\n\nThe 600M samples are provided in parquet files. Columns include the original caption, the url, the top caption and a list of alternative captions with lower CLIP-similarity scores.\n\n[https://huggingface.co/datasets/laion/laion-coco](https://huggingface.co/datasets/laion/laion-coco) \n\n\n## Samples\n\n\n\n<img src=\"/images/blog/ring.png\" style=\"height:250px; width: auto;\"/>\n\n\n**Original:** LGSY 925 Sterling Silver Double Heart Rings Infinity Love Thin Rings Wedding Engagement Promise Engraved Love Rings for Women for Dainty Gift\n\n**Generated:** An open ring with two hearts on it.\n\n\n\n<img src=\"/images/blog/boot.png\" style=\"height:250px; width: auto;\"/>\n         \n\n**Original:** Female Thick with Pointy Head High Heel Chelsea Ankle Boots\n\n**Generated:** Red leather ankle boots with gold buckles.\n\n\n\n<img src=\"/images/blog/laion_coco_beach.png\" style=\"height:250px; width: auto;\"/>\n\n\n**Original:** a group of people on horses on a beach\n\n**Generated:** Several people riding horses down the beach on a cloudy day.\n\n\n\n\n<img src=\"/images/blog/laion_coco_tags.png\" style=\"height:250px; width: auto;\"/>\n\n\n**Original:** a wall with a bunch of graffiti on it\n\n**Generated:** The parking meter is near a graffiti covered building.\n\n                                                                                                                                        \n\n\n\n<img src=\"/images/blog/sheeple.png\" style=\"height:250px; width: auto;\"/>\n\n\n**Original:** sheeple family\n\n**Generated:** A cartoon drawing of sheep watching TV with their babies.\n\n\n## More samples of images with their generated captions can be found here: \n\n(no cherry picking)\n\n[http://captions.christoph-schuhmann.de/eval_laion/eval.html](http://captions.christoph-schuhmann.de/eval_laion/eval.html) \n\n\n## Method\n\nThe method we used to generate these captions was to\n\n1. We use Blip L/14 to generate 40 captions\n2. Rank them using openai Clip Open AI L/14 ; selected the best 5 captions\n3. Rank using Open AI RN50x64 Clip model to select the best one\n4. Use a small, fine-tuned T0 model to roughly repair grammar and punctuation of the texts\n\nThe hyperparameters were chosen through a [grid search](https://wandb.ai/andreaskoepf/blip_coco_val_sample_sweep_bayes_02/sweeps/1bsha6b0) ([settings](https://github.com/andreaskoepf/CLIP-Image-Captioning/blob/blip_test/blip_coco_val_sample_sweep_bayes_02.yaml)) by Andreas Köpf to best match the style ( ROUGE scores ) of MS COCO texts.\n\n[laion_idle_cap](https://github.com/andreaskoepf/laion_idle_cap/tree/main/docker) is the script that was used for this processing.\n\n\n## Evaluation\n\nWe evaluated these generated captions by asking human evaluators to guess whether a caption is coming from a human or an AI model. We also asked them to rate the quality on a scale from 0(bad) to 5 (good). \n\nIn a first round we presented the evaluators each 200 samples, that contained 100 AI generated and 100 human written MS COCO captions.\n\n### Observations\n\n \n\n<img src=\"/images/blog/eval_laion_coco.png\" style=\"width:400px\" />\n\n\nGT: Y-Axis\n\nAnnotation: X-Axis\n\nMean rating & standard deviation of samples, that were written by a human:\n\nMean: 3.98\n\nStdev: 0.99\n\nMean rating & standard deviation of samples, that were written by an AI \n\nMean: 3.89\n\nStdev: 1.12\n\nMean rating & standard deviation of samples, where the annotator believed they were written by a human:\n\nMean: 4.44\n\nStdev: 0.61\n\nMean rating & standard deviation of samples, where the annotator believed they were generated by an AI \n\nMean: 3.50\n\nStdev: 1.15\n\n### Interpretation\n\nIt is very interesting that the mean scores of the samples generated by humans and generated by the model are very similar. We also notice that the standard deviation of the generated captions is a little bit higher.\n\nWe hypothesize that most in most cases the quality of the generated captions is perceived as as good as the quality of the human written captions.\n\nBut sometimes the captioning model obviously fails and the quality of the results is pretty low because the model doesn't relevant understand concepts about what is going on in the picture, because it's knowledge is not grounded in a sufficiently sophisticated world model.\n\n### Failure cases\n\n\n\n<img src=\"/images/blog/laion_coco_umbrella.png\" style=\"height:250px; width: auto;\"/>\n\n\n_“Two people posing for the camera in their wedding attire, one with an umbrella over his head and another with long red hair.”_\n\n\n\n<img src=\"/images/blog/laion_coco_man.png\" style=\"height:250px; width: auto;\"/>\n\n\n_“An older man having a heart attack, with his hand on the chest.”_\n\nWhen we remove all samples from the evaluations that have ratings of either 0 or 1, we Observe that the mean ratings and standard deviations move closer together. \n\n### Scores without ratings of 0 and 1\n\nMean rating & standard deviation of samples, that were written by a human:\n\nMean: 4.07\n\nStdev: 0.81\n\nMean rating & standard deviation of samples, that were written by an AI \n\nMean: 4.02\n\nStdev: 0.94\n\nThe mean ratings of the generated captions are still a little bit lower and the standard deviation is still a little bit higher, but the trend is pretty clear. By removing samples with rating 2, the gap between the qualities would probably decrease even further. \n\nPresentation only generated captions:\n\nIn a next step, we presented the human evaluators 400 captions that were only generated by the model (no human written captions in between):\n\nMean rating of all samples \n\n3.81\n\nStandard deviation of all samples \n\n0.94\n\n% rated as human\n\n47.5\n\n% rated as AI\n\n52.5\n\nWe observe that the human evaluators thought in 47.5% of all cases, that the captions were written by a human. This makes us confident that our captains are on average pretty good. When we told the evaluators later that all captions were generated by the model they told us that it was very hard for them to judge whether a caption was written by a model or a human, and that it only was easy for them in obvious failure cases.\n\n### Conclusions\n\nWe conclude that Our ensemble of BLIP and CLIP is already pretty good and capable of generating captions with a quality that is on average pretty close to the human written captions of MS Coco. \n\nIt would be very interesting for future work to let people rate our generated captions at larger scale and then filter out the samples with low rating values. These results could be used to train models to rate the quality of captions and to predict whether a caption looks like a generated or a human written caption.\n\nAnd even without further automated filtering, an ensemble of our captions and human evaluators would be a pretty good workflow to curate high quality captions at much lower costs than if we would ask humans to write them from scratch.\n\n\n## Credit assignments\n\n\n\n* [Christoph Schuhmann](https://github.com/christophschuhmann) lead the project, implemented a first version of the code, ran most of the generations & conducted the human evaluations\n* [Andreas Köpf](https://github.com/andreaskoepf) conducted the hyperparameter search & wrote the code to execute BLIP + CLIP filtering at scale\n* [Theo Coombes](https://github.com/TheoCoombes) managed the server that coordinated which GPU worker got which part of LAION to work on\n* [Romain Beaumont](https://github.com/rom1504) packaged the .json into parquet files, sent to HF and wrote the first draft of this post\n* [Richard Vencu](https://github.com/rvencu/) provided the infra structure to use the idle compute for this project\n* [Benjamin Trom](https://github.com/limiteinductive) wrote code that help us to convert the .json files to parquet\n\nWe thank [stability.ai](https://stability.ai/) for providing the compute used to generate the captions in the dataset.\n","date":1663200000000},{"slug":"laion-translated","frontmatter":{"title":"Laion translated: 3B captions translated to English from laion5B","author":"Marianna Nezhurina, Romain Beaumont, Richard Vencu and Christoph Schuhmann","date":"Sep 15, 2022","previewImg":"/images/blog/laion-translated-samples.png"},"content":"\nAuthor: [Marianna Nezhurina](https://github.com/marianna13) [Romain Beaumont](https://github.com/rom1504/) [Richard Vencu](https://github.com/rvencu) [Christoph Schuhmann](https://github.com/christophschuhmann)  \n\nLaion5B dataset was automatically collected from a section of the human web (common crawl). Can models generate different and interesting data compared to what humans write?\n\nThat’s a question we are interested in investigating. To let the community study it, we translated 3B samples of Laion5B from many languages into English.\n\nWe released 3 billions captions for the multilingual part of Laion5B. This makes it possible to use the whole Laion5B dataset to train English models. This also enables training models using these aligned pairs such as [Multilingual-CLIP](https://github.com/FreddeFrallan/Multilingual-CLIP).\n\nWe’re curious what you will do using it!\n\n\n## Downloading it\n\nThe dataset is available in huggingface as parquet files containing the caption, translated caption and urls.\n\n[laion1B-nolang-joined-translated-to-en](https://huggingface.co/datasets/laion/laion1B-nolang-joined-translated-to-en) \n\n[laion2B-multi-joined-translated-to-en](https://huggingface.co/datasets/laion/laion2B-multi-joined-translated-to-en)  \n\n\n## Processing\n\nEvery caption of the original dataset was translated with Facebook’s [M2M100 1.2B model](https://huggingface.co/facebook/m2m100_1.2B) using the following [script](https://github.com/marianna13/translate_dataset/blob/main/translate_data.py). All other fields remain the same as in the original [LAION2B Multi Joined](https://huggingface.co/datasets/laion/laion2B-multi-joined). To make translation possible the original dataset was split into parts with 50k samples in each and every such small part was translated in parallel on GPU nodes and saved in a separate parquet file. The speed of translation depends on the number of nodes and GPUs. The processing was done with 20 nodes with 8 GPUs in each and the speed of translation (including preprocessing and data loading) is 34 samples/per GPU/per second. Then, all translated parquets were [merged together using Spark](https://github.com/marianna13/translate_dataset/blob/main/join_additional.py) and saved as 128 parquet files. The resulting dataset was [joined with the aesthetics scores](https://github.com/marianna13/translate_dataset/blob/main/join_aesthetics.py). \n\n\n## Dataset columns\n\n\n\n* TEXT (the original text of caption)\n* LANGUAGE (language of the original TEXT)\n* ENG TEXT (translation in English of the original TEXT)\n* URL (URL of the image)\n* WIDTH (width of the image)\n* HEIGHT (height of the image)\n* Hash (hash of the URL and TEXT)\n* Pwatermark (probability of being a watermarked image, computed using our [watermark detector](https://github.com/LAION-AI/LAION-5B-WatermarkDetection))\n* Punsafe (probability of being an unsafe image, computed using our [clip based detector](https://github.com/LAION-AI/CLIP-based-NSFW-Detector))\n* Similarity (cosine between text and image ViT-B/32 embeddings, clip for en, mclip for multi and nolang)\n* Prediction (aesthetics score)\n\n\n## Samples from the translated dataset:\n\n\n![laion-2B-translated-samples](/images/blog/laion-translated-samples.png \"laion-2B-translated-samples\")\n\n\n\n## Laion2B-multi-translated\n\n\n### Dataset stats\n\n_Note. Dataset stats were computed using [this](https://github.com/marianna13/translate_dataset/blob/main/get_dataset_stats.py) script._\n\nNumber of uniques 2266M (2266193302)                                            \n\nNumber with WIDTH >= 0 and WIDTH &lt;= 128 160M (160260569)                        \n\nNumber with WIDTH >= 128 and WIDTH &lt;= 256 734M (734166164)                      \n\nNumber with WIDTH >= 256 and WIDTH &lt;= 512 849M (849569769)                      \n\nNumber with WIDTH >= 512 and WIDTH &lt;= 1024 457M (457572747)                     \n\nNumber with WIDTH >= 1024 86M (86750813)                                        \n\nNumber with HEIGHT >= 0 and HEIGHT &lt;= 128 103M (103514467)                      \n\nNumber with HEIGHT >= 128 and HEIGHT &lt;= 256 614M (614490681)                    \n\nNumber with HEIGHT >= 256 and HEIGHT &lt;= 512 753M (753540968)                    \n\nNumber with HEIGHT >= 512 and HEIGHT &lt;= 1024 686M (686553437)                   \n\nNumber with HEIGHT >= 1024 153M (153139456)                                     \n\nNumber with lenengtext >= 0 and lenengtext &lt;= 25 506M (506238532)               \n\nNumber with lenengtext >= 25 and lenengtext &lt;= 50 849M (849160165)              \n\nNumber with lenengtext >= 50 and lenengtext &lt;= 100 840M (840635023)             \n\nNumber with lenengtext >= 100 and lenengtext &lt;= 150 136M (136709119)            \n\nNumber with lenengtext >= 150 5M (5148507)\n\n\n### Similarities between text and images\n\n10000 images and captions were sampled from the dataset, [CLIP embeddings were computed](https://github.com/marianna13/translate_dataset/blob/main/get_clip_embs_similarities.py) (for original texts embeddings were computed using Multilingual CLIP). Then dot products between image and text embeddings were computed (for both original and translated dataset) to get similarities between texts and images. Here’s the distribution of average similarities for two datasets:\n\nSimilarity for original dataset:\n\n10% quantile -  0.2552971839904785\n\n20% quantile -  0.2633610963821411\n\n30% quantile -  0.2694466710090637\n\n40% quantile -  0.2750270366668701\n\n50% quantile -  0.28088638186454773\n\n60% quantile -  0.28750720620155334\n\n70% quantile -  0.2950591444969177\n\n80% quantile -  0.3049575388431549\n\n90% quantile -  0.32077282667160034\n\nSimilarity for translated dataset:\n\n10% quantile  -  0.23388671875\n\n20% quantile  -  0.25390625\n\n30% quantile  -  0.265869140625\n\n40% quantile  -  0.2763671875\n\n50% quantile  -  0.2861328125\n\n60% quantile  -  0.29638671875\n\n70% quantile  -  0.306884765625\n\n80% quantile  -  0.31982421875\n\n90% quantile  -  0.338134765625\n\n\n\n![laion-2B-translated](/images/blog/laion-2B-translated.png \"laion-2B-translated\")\n\n\n\n## Laion1B-nolang-translated\n\n[LAION1B Nolang Joined](https://huggingface.co/datasets/laion/laion1B-nolang-joined) dataset was also translated with a similar [script](https://github.com/marianna13/translate_dataset/tree/main) and in the same way as Multi. This dataset doesn’t have the language column so the model also had to determine language. All other columns in the Nolang dataset are the same as in Multi. And just like translated [LAION2B Multi Joined](https://huggingface.co/datasets/laion/laion2B-multi-joined), LAION1B Nolang was also joined with corresponding [aesthetics scores](https://github.com/marianna13/translate_dataset/blob/main/join_aesthetics.py).\n\n\n### Nolang dataset Stats\n\nNumber of uniques 1260M (1260048307)                                            \n\nNumber with WIDTH >= 0 and WIDTH &lt;= 128 90M (90701133)                          \n\nNumber with WIDTH >= 128 and WIDTH &lt;= 256 409M (409575445)                      \n\nNumber with WIDTH >= 256 and WIDTH &lt;= 512 475M (475885337)                      \n\nNumber with WIDTH >= 512 and WIDTH &lt;= 1024 239M (239035772)                     \n\nNumber with WIDTH >= 1024 59M (59942110)                                        \n\nNumber with HEIGHT >= 0 and HEIGHT &lt;= 128 59M (59814914)                        \n\nNumber with HEIGHT >= 128 and HEIGHT &lt;= 256 370M (370913206)                    \n\nNumber with HEIGHT >= 256 and HEIGHT &lt;= 512 451M (451897702)                    \n\nNumber with HEIGHT >= 512 and HEIGHT &lt;= 1024 316M (316723245)                   \n\nNumber with HEIGHT >= 1024 87M (87671543)                                       \n\nNumber with lenengtext >= 0 and lenengtext &lt;= 25 312M (312548202)               \n\nNumber with lenengtext >= 25 and lenengtext &lt;= 50 555M (555971621)              \n\nNumber with lenengtext >= 50 and lenengtext &lt;= 100 413M (413430230)             \n\nNumber with lenengtext >= 100 and lenengtext &lt;= 150 20M (20446701)              \n\nNumber with lenengtext >= 150 0M (138974)  \n\n\n### Similarities between text and images for Nolang\n\n10000 images and captions were randomly sampled from the dataset, [CLIP embeddings were computed](https://github.com/marianna13/translate_dataset/blob/main/get_clip_embs_similarities.py) (for original texts embeddings were computed using Multilingual CLIP). Then dot products between image and text embeddings were computed (for both original and translated dataset) to get similarities between texts and images. Here’s the distribution of average similarities for two datasets:\n\nSimilarity for original dataset:\n\n10% quantile  -  0.258196085691452\n\n20% quantile  -  0.266357421875\n\n30% quantile  -  0.2728866934776306\n\n40% quantile  -  0.27902457118034363\n\n50% quantile  -  0.28590404987335205\n\n60% quantile  -  0.29329144954681396\n\n70% quantile  -  0.3023602366447449\n\n80% quantile  -  0.31363412737846375\n\n90% quantile  -  0.3313804566860199\n\nSimilarity for translated dataset:\n\n10% quantile  -  0.2406005859375\n\n20% quantile  -  0.2607421875\n\n30% quantile  -  0.27490234375\n\n40% quantile  -  0.2861328125\n\n50% quantile  -  0.296142578125\n\n60% quantile  -  0.306396484375\n\n70% quantile  -  0.317626953125\n\n80% quantile  -  0.33203125\n\n90% quantile  -  0.353271484375\n\n\n![laion-1B-translated](/images/blog/laion-1B-translated.png \"laion-1B-translated\")\n\n\n## Credit\n\n\n\n* [Marianna Nezhurina](https://github.com/marianna13) translated the samples, packaged them, computed stats and wrote most of this post\n* [Romain Beaumont](https://github.com/rom1504/) helped out on packaging and scaling\n* [Richard Vencu](https://github.com/rvencu) set up all the infra that made using idle compute possible\n* [Christoph Schuhmann](https://github.com/christophschuhmann) suggested the project and guided the work to completion\n\nWe thank [https://stability.ai/](https://stability.ai/) for providing the compute for this massive translation. This was a great use of pre-emptible jobs to fill any idle compute available!","date":1663200000000},{"slug":"large-openclip","frontmatter":{"title":"Large scale openCLIP: L/14, H/14 and g/14 trained on LAION-2B","author":"Romain Beaumont","date":"Sep 15, 2022","previewImg":"/images/blog/compare3.png"},"content":"\nWe trained three large CLIP models with [OpenCLIP](https://github.com/mlfoundations/open_clip): ViT-L/14, ViT-H/14 and ViT-g/14 (ViT-g/14 was trained only for about a third the epochs compared to the rest). The H/14 model achieves **78.0%** zero shot top-1 accuracy on ImageNet and **73.4%** on zero-shot image retrieval at Recall@5 on MS COCO. As of September 2022, this is the best open source CLIP model.\n\nCLIP makes it possible to compute representations of images and texts to measure how **similar** they are. It can be used for \n\n\n\n* Zero shot classification: compare an image with the text of the class to know which class is most similar (e.g., ImageNet classification)\n* Retrieval: compare an image or a text to billions of text or images to find the most similar (e.g. as in [clip-retrieval](https://rom1504.github.io/clip-retrieval/) )\n* Generation\n    * CLIP guidance: decide a text you want to generate, then use an image generator model, and use the CLIP distance between what’s generated and the text to generate a better image (e.g., VQGAN + CLIP)\n    * CLIP conditioning: use a clip text embedding as input of a generator to make it generate this text directly (e.g., stable diffusion)\n\nCLIP models are trained in a self supervised fashion on hundreds of millions or billions of (image, text) pairs.\n\nWith LAION, we produced the LAION-5B dataset that contains 5.8 billions of closely related image and text pairs.\n\nThe CLIP model ViT B/32, released by OpenAI, was initially used to filter this dataset out of common crawl.\n\nProducing the best open source CLIP model out of this data set completes the open source replication of the [excellent](https://openai.com/blog/clip/) CLIP paper that OpenAI released one year ago.\n\n\n## Results\n\nWe replicated the results from openai CLIP in models of different sizes, then trained bigger models. The full evaluation suite on 39 datasets ([vtab+](https://github.com/LAION-AI/CLIP_benchmark)) are available in this [results notebook](https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/results.ipynb) and show consistent improvements over all datasets.\n\nThe larger models we release today are L/14, H/14 and g/14.\n\nL/14 was trained on JUWELS Booster supercomputer by [Ross wightman](https://github.com/rwightman). H/14 and g/14 were trained on stability cluster by [Romain Beaumont](https://github.com/rom1504) . While L/14 and H/14 were trained using 34B samples from LAION-2b, g/14 used a substantially smaller sample scale for training, seeing only 12B samples (see tables for more details).\n\n\n#### 32B samples seen\n\n\n| **Model name**                                                                   | **Batch size**                   | **Samples seen**           | **Text Params** | **Image params** | **Imagenet top1** | **Mscoco image retrieval at 5** | **Flickr30k image retrieval at 5** |\n|----------------------------------------------------------------------------------|----------------------------------|----------------------------|-----------------|------------------|-------------------|---------------------------------|------------------------------------|\n| [B/32](https://wandb.ai/rom1504/eval_openclip/reports/B-32-2B--VmlldzoyNDkwNDMy) | 79k                              | 34B (16 epochs of laion2B) | 63.43M          | 87.85M           | 66.6%             | 65.4%                           | 88.4%                              |\n| L/14                                                                             | 79k for 14B samples, 86K for 18B | 32B                        | 123.65M         | 303.97M          | 75.3%             | 71.1%                           | 92.9%                              |\n| [H/14](https://wandb.ai/rom1504/eval_openclip/reports/H-14--VmlldzoyNDAxODQ3)    | 79k                              | 32B (16 epochs of laion2B) | 354.03M         | 632.08M          | 78.0%             | 73.4%                           | 94%                                |\n\n\n\n#### 12B samples seen\n\n\n| **Model name**                                                                     | **Batch size**                             | **Samples seen**                        | **Text Params** | **Image params** | **Imagenet top1** | **Mscoco image retrieval at 5** | **Flickr30k image retrieval at 5** |\n|------------------------------------------------------------------------------------|--------------------------------------------|-----------------------------------------|-----------------|------------------|-------------------|---------------------------------|------------------------------------|\n| B/32                                                                               | 32k                                        | 12B (32 epochs of laion400m)            | 63.43M          | 87.85M           | 62.9%             | 60.8%                           | 85.5%                              |\n| B/16                                                                               | 32k                                        | 12B (32 epochs of laion400m)            | 91.16M          | 86.19M           | 69%               | 63.6%                           | 85.5%                              |\n| L/14                                                                               | 32k                                        | 12B (32 epochs of laion400m)            | 123.65M         | 303.97M          | 72%               | 68.1%                           | 90.8%                              |\n| [g/14](https://wandb.ai/rom1504/eval_openclip/reports/slow-g-14--VmlldzoyNTMwMjg5) | 32k for 8B samples then 64k for 4B samples | 12B (similar to 32 epochs on laion400m) | 354.03M         | 1012.65M         | 76.6%             | 72.4%                           | 93.5%                              |\n\n\nIn addition to having overall better results, we hope the larger text encoder will help improve text understanding. The good performance on the retrieval metrics seems to be a good indicator of this property.\n\nNote the difference in samples seen between the H/14 and the g/14 model. This explains the difference in performance. We picked this lower number to try and fix the stability issue at a lower cost. Eventually they were fixed (by using bfloat16). The performance of this model falls in the scaling curve of 12B sample seen (similar to 32 epochs of laion400m), and a g/14 trained on 32B samples of laion2B would most likely follow the same trends as the other models and get better performance as H/14.\n\n![alt_text](/images/blog/compare3.png \"image_tooltip\")\n\n\n\n## Released checkpoints\n\nWe release the checkpoints for the models, they are available through [openclip](https://github.com/mlfoundations/open_clip) and in HuggingFace hub at [B/32](https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K) [L/14](https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K) [H/14](https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K) and [g/14](https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K)\n\n\n## Related works\n\nRelated work results:\n\n\n| **Model name** | **Samples seen**       | **Imagenet top1** | **Mscoco image retrieval at 5** | **Flickr30k image retrieval at 5** |\n|----------------|------------------------|-------------------|---------------------------------|------------------------------------|\n| Openai B/32    | 12B (32 epochs of WIT) | 62%               |                                 |                                    |\n| Openai B/16    | 12B (32 epochs of WIT) | 69%               |                                 |                                    |\n| Openai L/14    | 12B (32 epochs of WIT) | 75.4%             | 61%                             | 87%                                |\n| ALIGN          | 20B                    | 76.4%             | 69.8%                           | 93.3%                              |\n| BASIC          | 32B                    | 85.7%             |                                 |                                    |\n| CoCa           | 32B                    | 86.3%             | 74.2%                           | 95.7%                              |\n\n\n[BASIC](https://arxiv.org/abs/2111.10050) and  [ALIGN](https://arxiv.org/abs/2102.05918) got excellent imagenet results. They used either different image encoder architecture (EfficientNet, CoAtNet), a larger network scale (BASIC-L with 2.4B params) or pre trained their network with supervised learning on a large dataset (BASIC CoAtNet vision encoder).\n\n[COCA](https://arxiv.org/abs/2205.01917) additionally used captioning loss during training with a multi-modal text decoder which predicted text tokens autoregressively and got 86.3% top1, employing a larger model scale (2.1B params)\n\n\n## Scaling up notes\n\nDuring these training runs, we encountered several interesting issues:\n\n\n\n* Using many GPUs means many of them can have hardware issues and can freeze, crash or even just be slow. This is a particularly annoying problem to handle as if one GPU has an issue, the synchronized nature of distributed training means that all GPUs get stuck. I created [https://github.com/rom1504/gpu-tester](https://github.com/rom1504/gpu-tester) to figure out what are the bad GPUs and exclude them\n* Stability issues! When scaling up the model size, the batch size and the dataset size, at around half the training the loss starts increasing until it reaches a plateau. We tried many possible things (find the list [there](https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c/edit)) and eventually concluded on a surprisingly simple solution: **using amp bfloat16 instead of amp float16 made the training fully stable**\n\nAnd also made some discoveries:\n\n\n\n* It seems using a very large batch size (up to 159k) can help reach even higher performance. This is most likely due to the fact that contrastive learning provides information to the loss as a logit matrix, hence having N times more samples in a batch means N square logits. We did not verify this systematically but BASIC paper provides more experiments and a theoretical justification for this result.\n* It’s possible to get a reasonably performing g/14 CLIP by doing a much shorter cosine decay => getting a 68% g/14 in 10k gpu hours.\n* Grad checkpointing allows to do 10x on the batch size\n\n\n### Training stability issues\n\nStability of training was the main problem we solved in this iteration of the scaling up of OpenCLIP. At around half the training (for L/14, H/14 and g/14), the loss started going up until it plateaued very high (11) and didn’t go down anymore.\n\nWe tried many possible fixes (decreasing lr, gradient shrinking, gradient clipping, cosine attention, post layer norm, …) with little to no effect when trying to resume from before the crash. \n\nEventually only 2 things worked:\n\n\n\n* Finishing the  lr decay very fast : in 8 epochs (compared to the planned 256 epochs). That managed to get most of the performance out of clip H. \n* Switching from float16 to bfloat16 solved the problem while being faster for clip g. We then applied the same fix for clip H and finished its training properly.\n\n[See all the training notes](https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c/edit) with all the details on all the possible ideas that didn’t work.\n\n\n### Training speeds\n\nTo better understand the cost and length of training of clip, we provide these training speed numbers. All numbers assume a100 with 40GB of VRAM. We used gradient checkpointing.\n\n| Model                                                                           | Batch size per gpu | Precision | Number of gpus | Sample per second per gpu |\n|---------------------------------------------------------------------------------|--------------------|-----------|----------------|---------------------------|\n| [B/32](https://wandb.ai/rom1504/open-clip/runs/rnxrp6k7?workspace=user-rom1504) | 96                 | float16   | 824            | 228                       |\n| [H/14](https://wandb.ai/rom1504/open-clip/runs/2zphcgkn?workspace=user-rom1504) | 96                 | float16   | 824            | 30                        |\n| [g/14](https://wandb.ai/rom1504/open-clip/runs/21cpomx2?workspace=user-rom1504) | 40                 | float16   | 800            | 20                        |\n| [H/14](https://wandb.ai/rom1504/open-clip/runs/3l7ppqh3?workspace=user-rom1504) | 96                 | bfloat16  | 824            | 42                        |\n| [g/14](https://wandb.ai/rom1504/open-clip/runs/1pby5fkb?workspace=user-rom1504) | 80                 | bfloat16  | 800            | 31                        |\n\n\nThe speed usually increases with batch size per gpu until a plateau is reached. The speed also increases with the number of gpu. After a certain number of gpus, the curve becomes slower than linear.\n\nBfloat16 which we used in the second part of training provides both better stability and faster sample/s for clip models.\n\n\n## What’s next\n\nThe models will be used for many applications, including clip guiding and conditioning. Even better results could be reached on models like stable diffusion by using a better clip model!\n\nNow that the scaling properties of clip are proven in an open source reproduction, a lot of doors open. Here are some ideas of next steps:\n\n\n\n* Changing the text encoder to work in the multilingual setting (to get a model like [Multilingual-CLIP](https://github.com/FreddeFrallan/Multilingual-CLIP) but trained contrastively, with hopefully even better results!) and scale it up\n* Can we get clip models while using less gpu hours ? extracting the knowledge from smaller clips into a bigger one may help bootstrap the learning process (see [encoder-distill](https://github.com/iejMac/encoder-distill) from [iejMac](https://github.com/iejMac) getting some preliminary results on this)  \n* The clip idea can be expanded to other modalities, see [CLAP](https://github.com/LAION-AI/CLAP) for text-audio alignment\n\nIf you have ideas or want to help out, feel free to reach out in laion server.\n\n\n## Contributions\n\nThanks to\n\n\n\n* [Romain Beaumont](https://github.com/rom1504) for running the experiments on H/14 and g/14\n* [Ross Wightman](https://github.com/rwightman) for conducting all the openclip experiments at JUWELS Booster (Juelich Supercomputing Center) up to L/14 and providing valuable feedback during these H and g clip trainings\n* [Phil Wang](https://github.com/lucidrains) for providing ideas and code (cosine attention, post layer norm, ..) during the stability issues\n* [Boris Dayma](https://github.com/borisdayma) and [Mitchell Wortsman](https://mitchellnw.github.io/) for both proposing to try float32 that showed precision was an issue and eventually lead to trying bfloat16\n* [Blinkdl](https://github.com/Blinkdl) for proposing interesting ideas regarding tuning the learning rate\n* [Christoph Schuhmann](https://github.com/christophschuhmann) for daring proposing to train such large clips, following up on all these experiments, and finding very early that training were frozen, saving some valuable time\n* [Jenia Jitsev](https://github.com/JeniaJitsev) for providing ideas and feedback during the training issues, supervision and coordination of the compute grants at JUWELS Booster\n* [Ludwig Schmidt](https://github.com/ludwigschmidt) for reviewing this post and giving many ideas about LAION datasets and CLIP \n* [Mehdi Cherti](https://github.com/mehdidc) for helping to debug the evaluation scripts and getting comparable results for MS-COCO\n\nAnd of course [Emad](https://twitter.com/EMostaque) (Stability AI) for providing the many GPUs used during these experiments! (g/14 and H/14!)\n\nFor the L/14 training, we gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at Jülich Supercomputing Centre (JSC), Germany.\n","date":1663200000000},{"slug":"laion-aesthetics","frontmatter":{"title":"LAION-Aesthetics","author":"Christoph Schuhmann","date":"Aug 16, 2022","previewImg":"/images/blog/LAION-Aesthetics.jpg"},"content":"\nWe present LAION-Aesthetics, several collections of subsets from LAION 5B with high visual quality.\n\n![](https://raw.githubusercontent.com/LAION-AI/laion.ai/Chris/blog/LAION-Aesthetics.jpg)\n\nTo create LAION-Aesthetics we trained several lightweight models that predict the rating people gave when they were asked _“How much do you like this image on a scale from 1 to 10?”_.\n\n## LAION-Aesthetics V1\n\nWe started with training a linear model on 5000 image-rating pairs from the [SAC](https://github.com/JD-P/simulacra-aesthetic-captions) dataset (which only contained 5000 samples at that time).\n\nSimulacra Aesthetic Captions is a dataset of over 238000 synthetic images generated with AI models such as CompVis latent GLIDE and Stable Diffusion from over forty thousand user submitted prompts.\n\nAs inputs this model uses not the images themselves, but their CLIP Image embeddings produced with the Open AI CLIP VIT L 14 model. We call this model LAION-Aesthetics_Predictor V1.\n\nIts results were so encouraging, that we decided to produce 8M and 120M sample subsets of the LAION 5B images with the highest predicted scores, of those that have english texts.\n\nWe call the dataset consisting of these 2 subsets [LAION-Aesthetics V1](https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md).\n\n![](https://github.com/LAION-AI/laion.ai/blob/Chris/blog/LAION-Aesthetics%20V1.jpg?raw=true)\n\nThe model used for creating this subset can be found [here.](https://github.com/LAION-AI/aesthetic-predictor)\n\nThe LAION-Aesthetics V1 dataset & further details about it can be found [here.](https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md)\n\n## LAION-Aesthetics V2\n\nAfter these very encouraging results, we continued to experiment and gathered the following data to train more improved MLP (multi-layer perceptron) models:\n\n- More samples from the SAC dataset, which had grown in the meanwhile\n  to 176000 image - rating pairs\n- LAION-Logos, a dataset of 15.000 logo image-text pairs with aesthetic\n  ratings from 1 to 10. We collected this dataset to improve the models\n  abilities to evaluate images with more or less aesthetic texts in\n  them.\n- [The Aesthetic Visual Analysis (AVA) dataset](https://github.com/imfing/ava_downloader), which is a large-Scale database for aesthetic visual analysis that contains 250000 photos from dpchallenge.com with several aesthetic ratings from 1 to 10 for most images.\n- After training several MLPs with different numbers of layers and parameters and different activation functions, we found that a simple linear model on the top of CLIP ViT/14 produced in our subjective view the visually most appealing results when used to rank images of LAION-5B. (Even though other MLPs with e.g. Relu functions produced slightly lower MSE and MAE loss values.) We call the resulting model trained on SAC, LAION-Logos and AVA [LAION-Aesthetics_Predictor V2.](https://github.com/christophschuhmann/improved-aesthetic-predictor)\n- Visualizations of sorting all 2.37B images from LAION 5B that have English captions into 40 buckets with the LAION-Aesthetics_Predictor V2 can be found [here.](http://captions.christoph-schuhmann.de/aesthetic_viz_laion_sac+logos+ava1-l14-linearMSE-en-2.37B.html)\n\nUsing LAION-Aesthetics_Predictor V2, we created the following subsets of the LAION 5B samples with English captions:\n\n- 1,2B image-text pairs with predicted aesthetics scores of 4.5 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-4.5.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_4.5plus)\n- 939M image-text pairs with predicted aesthetics scores of 4.75 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-4.75.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_4.75plus)\n- 600M image-text pairs with predicted aesthetics scores of 5 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-5.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_5plus)\n- 12M image-text pairs with predicted aesthetics scores of 6 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-6.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_6plus)\n- 3M image-text pairs with predicted aesthetics scores of 6.25 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-6.25.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_6.25plus)\n- 625K image-text pairs with predicted aesthetics scores of 6.5 or higher: [browse](http://captions.christoph-schuhmann.de/2B-en-6.5.html) [huggingface](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_6.5plus)\n\nThese subsets overlap. 5 fully includes 6 which includes 6.25 and so on. We call the collection of these subsetsLAION-Aesthetics V2.\n\nWe provided the dataset to the [CompViz](https://github.com/CompVis) team led by Robin Rombach and Patrick Esser. They used the 5+ subset to train [Stable Diffusion V1](https://github.com/CompVis/stable-diffusion/tree/ce05de28194041e030ccfc70c635fe3707cdfc30#stable-diffusion-v1) model.\n\n## What's next?\n\nAt the moment we are translating all 2,15B samples from LAION 5B of the multilingual subset to English using the 1,2B parameter [M2M-100](https://github.com/facebookresearch/fairseq/tree/main/examples/m2m_100) model .\n\nThis will allow us to roughly double the size of V2.\n\nAdditionally, we are already working on new multimodal large-scale dataset, this time at webpage-level, similar to the interleaved image-text dataset Deepmind used for [Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model), but also with audio & video files ... and much, much bigger. :)\n\nStay tuned & keep checking our blog for more datasets in the near future.\n\n## Connect\n\nIf you have any questions or comments or the wish to support our efforts, don’t hesitate to [join our Discord community and contact us.](https://discord.gg/vnjVezbeSJ)\n\n_Christoph Schuhmann ( spirit-from-germany#1488 ) and Romain Beaumont ( rom1504#5008 )_\n","date":1660608000000},{"slug":"laion-5b","frontmatter":{"title":"LAION-5B: A NEW ERA OF OPEN LARGE-SCALE MULTI-MODAL DATASETS","author":"Romain Beaumont","date":"Mar 31, 2022","previewImg":"/images/blog/5b.png"},"content":"\nWe present a dataset of 5,85 billion CLIP-filtered image-text pairs, 14x bigger than LAION-400M, previously the biggest openly accessible image-text dataset in the world - see also our [NeurIPS2022 paper](https://arxiv.org/abs/2210.08402)\n\n_Authors: Christoph Schuhmann, Richard Vencu, Romain Beaumont, Theo Coombes, Cade Gordon, Aarush Katta, Robert Kaczmarczyk, Jenia Jitsev_\n\n![](https://lh5.googleusercontent.com/u4ax53sZ0oABJ2tCt4FH6fs4V6uUQ_DRirV24fX0EPpGLMZrA8OlknEohbC0L1Nctvo7hLi01R4I0a3HCfyUMnUcCm76u86ML5CyJ-5boVk_8E5BPG5Z2eeJtPDQ00IhVE-camk4)\n\nLarge image-text models like ALIGN, BASIC, Turing Bletchly, FLORENCE & GLIDE have shown better and better performance compared to previous flagship models like CLIP and DALL-E. Most of them had been trained on billions of image-text pairs and unfortunately, no datasets of this size had been openly available until now. To address this problem we present LAION 5B, a large-scale dataset for research purposes consisting of 5,85B CLIP-filtered image-text pairs. 2,3B contain English language, 2,2B samples from 100+ other languages and 1B samples have texts that do not allow a certain language assignment (e.g. names ). Additionally, we provide several nearest neighbor indices, an improved web interface for exploration & subset creation as well as detection scores for watermark and NSFW. We also announce a full reproduction of a clip training trained on LAION-400M at [open_clip](https://github.com/mlfoundations/open_clip). Explore the dataset at the [search demo](https://rom1504.github.io/clip-retrieval/). See also the [same post on laion website](https://laion.ai/laion-5b-open-dataset) .\n\nWe thank our sponsors [hugging face](https://huggingface.co/), [doodlebot](http://doodlebot.ai/) and [stability](https://stability.ai/) for providing us with computing resources to produce this dataset! We also thank the-eye.eu for hosting the image embeddings and a copy of the whole dataset.\n\n### Disclaimer on dataset purpose and content warning\n\nThe motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet. Our recommendation is therefore to use the dataset for research purposes. Be aware that this large-scale dataset is uncurated. Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer. Therefore, please use the demo links with caution and at your own risk. It is possible to extract a “safe” subset by filtering out samples based on the safety tags (using a customized trained NSFW classifier that we built). While this strongly reduces the chance for encountering potentially harmful content when viewing, we cannot entirely exclude the possibility for harmful content being still present in safe mode, so that the warning holds also there. We think that providing the dataset openly to broad research and other interested communities will allow for transparent investigation of benefits that come along with training large-scale models as well as pitfalls and dangers that may stay unreported or unnoticed when working with closed large datasets that remain restricted to a small community. Providing our dataset openly, we however **do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress**.\n\n## Introduction\n\nSince the release of CLIP & DALL-E in January 2021, several similar large multi-modal language-vision models have been trained by large groups. Models like FLORENCE, Turing Bletchley, ALIGN & BASIC demonstrated very strong transfer capabilities on novel datasets in absence of per-sample labels, which also steadily improved when growing training data amount, following scaling laws observed in previous research work. These models require billions of image-text pairs to achieve competitive performances and unfortunately, no billion-scale image-text pair dataset had been openly available up until now. To address this problem we release LAION 5B, a CLIP-filtered dataset of 5,85 billion high-quality image-text pairs, their CLIP ViT-L/14 embeddings, kNN-indices, a web interface for exploration & subset-creation and NSFW- and watermark-detection scores and tools. We describe the procedure to create the dataset and demonstrate successful training of DALL-E architecture. Having sufficiently large scales, the dataset opens venues for research on multi-modal language-vision models to a broad community.\n\n## Download the data\n\nWe release the following packages under the LAION-5B project:\n\n- [laion2B-en](https://huggingface.co/datasets/laion/laion2B-en) 2.32 billion of these contain texts in the English language\n- [laion2B-multi](https://huggingface.co/datasets/laion/laion2B-multi) 2.26 billion contain texts from 100+ other languages\n- [laion1B-nolang](https://huggingface.co/datasets/laion/laion1B-nolang) 1.27 billion have texts where a particular language couldn’t be clearly detected.\n\nThe data can comfortably be downloaded with [img2dataset](https://github.com/rom1504/img2dataset) (240TB in 384, 80TB in 224)\n\nFor training usage, we recommend reading the [usage guide for training](https://github.com/rom1504/laion-prepro/blob/main/laion5B/usage_guide/preparing_data_for_training.md)\n\nIn particular, we release this data:\n\n- 5.85 billion pairs of image URLs and the corresponding metadata at [laion2B-en](https://huggingface.co/datasets/laion/laion2B-en) [laion2B-multi](https://huggingface.co/datasets/laion/laion2B-multi) [laion1B-nolang](https://huggingface.co/datasets/laion/laion1B-nolang) (800GB)\n- A [knn index](https://huggingface.co/datasets/laion/laion5B-index) that enables quick search in the laion5B dataset (1.6TB)\n- [Indices](https://mystic.the-eye.eu/public/AI/cah/laion5b/indices/vit-l-14/) (or [here](https://the-eye.eu/public/AI/cah/laion5b/indices/vit-l-14/)) for laion2B-en, laion2B-multi, laion1B-nolang (2TB)\n- Web demo of image-text search on LAION-5B [clip-retrieval](https://rom1504.github.io/clip-retrieval/)\n- Safety tags at [laion2B-en-safety](https://huggingface.co/datasets/laion/laion2B-en-safety) [laion2B-multi-safety](https://huggingface.co/datasets/laion/laion2B-multi-safety) [laion1B-nolang-safety](https://huggingface.co/datasets/laion/laion1B-nolang-safety) (50GB)\n- Watermark tags at [laion2B-en-watermark](https://huggingface.co/datasets/laion/laion2B-en-watermark) [laion2B-multi-watermark](https://huggingface.co/datasets/laion/laion2B-multi-watermark) [laion1B-nolang-watermark](https://huggingface.co/datasets/laion/laion1B-nolang-watermark) (50GB)\n\n\nPre-Computed Embeddings\n| Clip Model | Dataset  | URL                                                                                                                 | Size       | Host      |\n|------------|----------|----------------------------------------------------------------------------------------------------------------------|------------|-----------|\n| Vit-L/14   | laion2b-en  | [image embeddings, text embeddings, & metadata](https://mystic.the-eye.eu/public/AI/cah/laion5b/embeddings/laion2B-en/) (or [here](https://the-eye.eu/public/AI/cah/laion5b/embeddings/laion2B-en/)) | 6.2TB        | the eye   |\n| Vit-L/14   | laion2b-multi  | [image embeddings & metadata](https://mystic.the-eye.eu/public/AI/cah/laion5b/embeddings/laion2B-multi/) (or [here](https://the-eye.eu/public/AI/cah/laion5b/embeddings/laion2B-multi/)) | 3.2TB        | the eye   |\n| Vit-L/14   | laion1b-nolang  | [image embeddings & metadata](https://mystic.the-eye.eu/public/AI/cah/laion5b/embeddings/laion1B-nolang/) (or [here](https://the-eye.eu/public/AI/cah/laion5b/embeddings/laion1B-nolang/)) | 2TB        | the eye   |\n| Vit-L/14   | laion2b-en  | [image embeddings , text embeddings, & metadata](https://huggingface.co/datasets/laion/laion2b-en-vit-l-14-embeddings) | 6.2TB           | huggingface        |\n| Vit-L/14   | laion2b-multi  | [image embeddings & metadata](https://huggingface.co/datasets/laion/laion2b-multi-vit-l-14-embeddings)              |         3.2TB   | huggingface        |\n| Vit-L/14   | laion1b-nolang  | [image embeddings & metadata](https://huggingface.co/datasets/laion/laion1b-nolang-vit-l-14-embeddings)              |   2TB         | huggingface        |\n| Vit-H/14   | laion2b-en  | [image embeddings, text embeddings, & metadata](https://huggingface.co/datasets/laion/laion2b-en-vit-h-14-embeddings) | 9.5TB           | huggingface        |\n| Vit-H/14   | laion2b-multi  | [image embeddings & metadata](https://huggingface.co/datasets/laion/laion2b-multi-vit-h-14-embeddings)              | 4.5TB           | huggingface        |\n| Vit-H/14   | laion1b-nolang  | [image embeddings & metadata](https://huggingface.co/datasets/laion/laion1b-nolang-vit-h-14-embeddings)              | 2.5TB         | huggingface        |\n\n\n\nThe metadata files are parquet files that contain the following attributes: URL, TEXT, the cosine similarity score between the text and image embedding and height and width of the image. Watermark and safety tags can be joined with the metadata prior to downloading by using [this script](https://github.com/rom1504/laion-prepro/blob/main/laion5B/safety/join.py). Once that is done, they can easily be filtered upon with a probability threshold at your choice (we recommend 0.5 for safety and 0.8 for watermark).\n\nYou can also find the prejoined files at [laion2B-en-joined](https://huggingface.co/datasets/laion/laion2B-en-joined) [laion2B-multi-joined](https://huggingface.co/datasets/laion/laion2B-multi-joined) [laion1B-nolang-joined](https://huggingface.co/datasets/laion/laion1B-nolang-joined) (800GB)\n\n## License\n\nWe distribute the metadata dataset (the parquet files) under the [Creative Common CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) license, which poses no particular restriction. The images are under their copyright.\n\n## Dataset columns\n\nWe provide these columns :\n\n- URL: the image url, millions of domains are covered\n- TEXT: captions, in english for en, other languages for multi and nolang\n- WIDTH: picture width\n- HEIGHT: picture height\n- LANGUAGE: the language of the sample, only for laion2B-multi, computed using [cld3](https://github.com/google/cld3)\n- similarity: cosine between text and image ViT-B/32 embeddings, clip for en, mclip for multi and nolang\n- pwatermark: probability of being a watermarked image, computed using our [watermark detector](https://github.com/LAION-AI/LAION-5B-WatermarkDetection)\n- punsafe: probability of being an unsafe image, computed using our [clip based detector](https://github.com/LAION-AI/CLIP-based-NSFW-Detector)\n\npwatermark and punsafe are available either as individual collections that must be [joined](https://github.com/rom1504/laion-prepro/blob/main/laion5B/safety/join.py) with the hash of url+text, either as prejoined collections.\n\n## Dataset Statistics\n\nWe [computed](https://github.com/rom1504/laion-prepro/blob/main/laion5B/stats/compute_stats.py) some statistics on the datasets to let people understand better: Samples are considered unsafe if the model predicts it as unsafe with a probability of more than 0.5. More than 0.8 for watermark. These values are pretty conservative, so the estimated safeness and watermark proportion may be higher than the truth. Other thresholds may be chosen to get a different precision/recall tradeoff.\n\nComputed quantiles are quantiles from 0.05 to 0.95.\n\nAlso see the whole [sheet](https://docs.google.com/spreadsheets/d/19AkcufyABAnbBlsr12VUmlR9oyQWb4uloAQnd-rqJC0/edit#gid=0) and the whole [dashboard](https://datastudio.google.com/reporting/c67c1749-816f-464f-873a-867b4a43f044/page/p_i9he8sxntc/edit)\n\n### Laion2B-en\n\nTotal: 2.3B samples\n\n![](https://lh6.googleusercontent.com/-SW3vGI4_Ojemg_ttYpZvFmC8vTjYavTDgmnY7SsnfF-smnVpLwqbYCUsmB9_1HBmmVbKRE2QXJRwxamNcw1A9sRXDFPSj0YZ2WiptPnNeAMuSF0O_2Yi_CGsm_QChM4eJXd4lyY)\n\nNumber with height and width bigger than\n\n- 256 -> 1324M\n- 512 -> 488M\n- 1024 -> 76M\n\nWidth quantiles: 132.0, 160.0, 180.0, 210.0, 225.0, 240.0, 262.0, 300.0, 309.0, 340.0, 400.0, 450.0, 480.0, 512.0, 600.0, 656.0, 760.0, 960.0, 1050.0\n\nHeight quantiles: 125.0, 150.0, 166.0, 188.0, 208.0, 225.0, 250.0, 270.0, 300.0, 320.0, 350.0, 380.0, 418.0, 470.0, 500.0, 600.0, 672.0, 800.0, 1014.0\n\nUnsafe proportion: 2.9%\n\nWatermark proportion: 6.1%\n\nAverage text length: 67\n\nText length quantiles: 21.0, 25.0, 30.0, 33.0, 37.0, 40.0, 43.0, 47.0, 50.0, 54.0, 58.0, 62.0, 67.0, 72.0, 78.0, 85.0, 96.0, 114.0, 152.0\n\n### Laion2B-multi\n\nTotal: 2.2B samples\n\n![](https://lh4.googleusercontent.com/jLZYO_GMS28fzxwfZS199LOjSeUpTH7HEmjIRwyVAtvJdBkzyHzs83FgnD-hOY0CjK8LDooytibVcbuVa_O5YSoCu1IduEj4Z8uneE8Km-0Y39qHzYAJs4Lr4oEyQh4EeWxHGFOk)\n\nNumber with height and width bigger than\n\n- 256 -> 1299M\n- 512 -> 480M\n- 1024 -> 57M\n\nWidth quantiles: 140.0, 160.0, 188.0, 205.0, 235.0, 250.0, 284.0, 300.0, 324.0, 366.0, 420.0, 480.0, 520.0, 600.0, 640.0, 720.0, 800.0, 960.0, 1080.0\n\nHeight quantiles: 120.0, 144.0, 160.0, 180.0, 200.0, 217.0, 240.0, 262.0, 300.0, 320.0, 350.0, 394.0, 416.0, 458.0, 500.0, 564.0, 636.0, 725.0, 1000.0\n\nTop 10 languages: LANGUAGE count proportion:\n\n- ru 241M 0.106\n- fr 168M 0.074\n- de 150M 0.066\n- es 149M 0.066\n- zh 143M 0.063\n- ja 131M 0.057\n- it 95M 0.042\n- pt 88M 0.038\n- nl 66M 0.029\n- pl 62M 0.027\n- no 49M 0.021\n\nUnsafe proportion: 3.3%\n\nWatermark proportion: 5.6%\n\nAverage text length: 52\n\nText length quantiles: 12.0, 16.0, 20.0, 23.0, 27.0, 30.0, 33.0, 37.0, 40.0, 44.0, 48.0, 52.0, 57.0, 61.0, 67.0, 74.0, 81.0, 93.0, 120.0\n\n### Laion1B-nolang\n\nTotal: 1.2B samples\n\n![](https://lh3.googleusercontent.com/mAI2e-sLE2geRsX8-3Mw-Ye_8wDu0SEDnIBZjWNqJiaAdjtjj7PtBvRhreugGS6_740-KcILnRbvRbDcTIFccrYO-adNz2uRM6zb0VgR3wjZVo_x0dxDhaMouH2KHScZnuNNkvs3)\n\nNumber with height and width bigger than\n\n- 256 -> 1324M\n- 512 -> 488M\n- 1024 -> 76M\n\nWidth quantiles: 135.0, 160.0, 181.0, 207.0, 225.0, 241.0, 264.0, 300.0, 306.0, 338.0, 398.0, 426.0, 499.0, 520.0, 600.0, 655.0, 768.0, 940.0, 1080.0\n\nHeight quantiles: 118.0, 144.0, 160.0, 186.0, 200.0, 220.0, 240.0, 260.0, 292.0, 305.0, 338.0, 368.0, 405.0, 456.0, 500.0, 562.0, 637.0, 768.0, 1000.0\n\nUnsafe proportion: 3%\n\nWatermark proportion: 4%\n\nAverage text length: 46\n\nText length quantiles: 13.0, 17.0, 20.0, 23.0, 26.0, 29.0, 32.0, 35.0, 38.0, 41.0, 44.0, 48.0, 51.0, 56.0, 60.0, 67.0, 73.0, 82.0, 99.0\n\n## Acquisition pipeline\n\n![](https://lh4.googleusercontent.com/5Pcm38fU3jxc9zf1oSNLMTxh9TH4eMG-5n-dwLF-EPNc_o-jRCwa1D1AJIX-dP4UmeEoKkDxg4fWr3Mq5JwQ7TDuKUwMiaww9GPh8EGJwGkCqWXWKx15CY4Jgige8Uu6inl0KSOu)\n\nThe acquisition pipeline follows the flowchart above and can be split into three major components:\n\n- Distributed processing of petabyte-scale Common Crawl dataset, which produces a collection of matching URLs and captions (preprocessing phase)\n- The distributed download of images based on shuffled data to pick a correct distribution of URLs, to avoid too heavy request loads on single websites\n- Few GPU node post-processing of the data, which is much lighter and can be run in a few days, producing the final dataset.\n\n### Distributed processing of Common Crawl\n\nTo create image-text pairs, we parse through WAT files from Common Crawl and parse out all HTML IMG tags containing an alt-text attribute. At the same time, we perform a language detection on text with three possible outputs: English language with confidence, another language with confidence, no language which contains “no detection” and “detection under the confidence threshold”. The “no language” set often contains short texts, mostly with names of people and places. All extracted information by the preprocessing workers were packed and sent to the Postgresql node for storage using the COPY command. The Postgresql server was maintained to keep about 500M records at all times by means of balancing the ingress and egress of data from the database.\n\n### Distributed downloading of the images\n\nWe download the raw images from the parsed URLs with asynchronous requests using Trio and Asks libraries in order to maximize all resources usage: vCPUs, RAM and bandwidth. We found that a single node in the cloud with 1-2 vCPUs, 0.5-1GB RAM and 5-10Mbps download bandwidth is inexpensive enough to allow downloading on a limited budget. Such a unit can process 10000 links in about 10-15 minutes. Each batch consisted of 10000 links taken from the Postgresql server by using the TABLESAMPLE technique, ensuring that the distribution among the 10000 links was following the distribution of the existing 500M records available on the database. We found that the distribution is still good when in the database are still above 20M records to be processed given that we had some 300 downloading workers at any time. The above techniques allowed both maximizing downloading speed and minimizing IP reputation damages.\n\n### CLIP inference at the post-processing stage\n\nThe data pipeline continued with GPU nodes doing inference on the collected image-text pairs, and calculating the similarity of the embeddings for the image and the text. After the similarity score was established we removed the pairs under the threshold we decided to use, i.e 0.28 for the English dataset ( with CLIP ViT B/32 ) and 0.26 for the rest (with mCLIP). As an estimation, we removed about 90% of the samples, trimming the 50+ billion of candidates to just below 6 billion.\n\n### Filtering out unsuitable image-text pairs\n\nAfter downloading the WAT files from Common Crawl, we apply the following filtering conditions:\n\n- All samples with less than 5 characters alt-text length or less than 5 KB image size are dropped.\n- All images with the too big resolution, potentially DOS bombs, were dropped before attempting to process them.\n- Duplicate removal is performed with a bloom filter based on URL. Future runs would include more variate deduplication rules, such as URL + language for the multilanguage dataset.\n- We use CLIP respectively MCLIP to compute embeddings of the image and alt-text. Then we compute the cosine similarity of both embeddings and drop all samples with cosine similarity below 0.28 for the English language ( with CLIP B/32) and 0.26 for the multilingual dataset (MCLIP). These thresholds were selected based on human inspection of the test results.\n- We use the CLIP embeddings of images and texts to filter out to the possible extent the illegal content.\n\n## Dataset preparation pipeline\n\nAfter processing and filtering common crawl, 5,85B of URL/text samples remained. We did additional steps after that in order to prepare the dataset. See this [semantic search blogpost](https://rom1504.medium.com/semantic-search-with-embeddings-index-anything-8fb18556443c) and the readme of [clip-retrieval](https://github.com/rom1504/clip-retrieval) for additional details about this process. See also [semantic search at billions scale](https://medium.com/@rom1504/semantic-search-at-billions-scale-95f21695689a) for more technical details of the process that was done for laion5B.\n\n1.  Downloading the data as webdataset with distributed img2dataset\n2.  Computing Vit-L/14 embeddings with distributed clip-inference\n3.  Computing a KNN index from these embeddings using autofaiss\n4.  Computing additional tags (NSFW and watermark) using clip embeddings\n\n### Distributed img2dataset\n\nWe developed the [img2dataset](https://github.com/rom1504/img2dataset) library to comfortably download from a given set of URLs, resize and store the images and captions in the webdataset format. This allows downloading 100 million images from our list of URLs in 20 hours with a single node (1Gbps connection speed, 32GB of RAM, an i7 CPU with 16 cores), which allows anyone to obtain the whole dataset or a smaller subset. For LAION-5B we introduced a [distributed mode](https://github.com/rom1504/img2dataset/blob/main/dataset_examples/laion5B.md) for this tool, allowing to downloading the 5,85B samples in a week using 10 nodes.\n\n### Distributed clip inference\n\nFrom these images, the [clip retrieval](https://github.com/rom1504/clip-retrieval%7D%7Bhttps://github.com/rom1504/clip-retrieval) inference tool was used to compute ViT-L/14 embeddings, allowing for a better analysis capacity of the data. In particular, a [distributed mode](https://github.com/rom1504/clip-retrieval/blob/main/docs/distributed_clip_inference.md) made it possible to compute these embeddings in a week using 32 A100: this larger clip model can only be computed at a speed of 312 sample/s per GPU, compared to 1800 sample/s for ViT-B/32. The resulting embeddings are available for everyone to use e.g. for clustering, indexing, linear inference.\n\n### Distributed indexing\n\nWe then used these 9 TB of image embeddings to build a large PQ128 knn index using the [autofaiss](https://github.com/criteo/autofaiss) tool. To make this run faster, a [distributed mode](https://github.com/criteo/autofaiss/blob/master/docs/distributed/distributed_autofaiss.md) is available.\n\n### Integration in the search UI\n\nIn order to demonstrate the value of this data, we integrated this index into the [knn search UI](https://knn5.laion.ai/). It is powered by the code called [clip back](https://github.com/rom1504/clip-retrieval). The knn index is 800GB and the metadata (URL and captions) as well, so memory mapping is used for both in order to use no ram, only an SSD drive of that capacity is required.\n\n### Watermark and safety inference\n\nWe wanted to give users the ability to remove unsafe examples, and watermarked examples. To do that we collected training and test sets. The training set was augmented with examples retrieved from the knn index, while the test set samples were selected to represent well the dataset distribution, but were all manually annotated. The inference is done using the [embedding-reader](https://github.com/rom1504/embedding-reader) module for NSFW and [LAION-5B-WatermarkDetection](https://github.com/Zasder3/LAION-5B-WatermarkDetection) for watermarks These tags were also integrated into the UI, allowing everyone to observe that the safety tags indeed filter out almost all the unsafe results, and giving confidence that training a generative model on this data will not result in unexpectedly unsafe images.\n\n### Watermarks\n\n![](https://lh6.googleusercontent.com/WQMTd03M8xmR8yTuSudnyZGgcQvcLfro4Lf0DXvIZb9P8xqjGJVPjrgziZ4U2HoHlgmY_3Ubx33qYaG5jIC-h_uuEOqyJ9K0IqJsNZH0XPiP1CDek8xz60fARNXpwJW9yCkEKa7u)\n\nThe training dataset is 90000 samples (45222 watermarks, 44778 clear).\n\nWatermarked images are a big problem when training generative models like DALL-E or GLIDE. To tackle this problem we trained a watermark detection model and used it to calculate confidence scores for every image in LAION-5B. Therefore we created a training dataset consisting of 90.000 images with 50% watermarked and 50% clean images. The majority of the watermarked images have been extracted from the LAION-400M KNN index through the use of several text prompts like “clip art watermark”, “cat watermark” or “landscape watermark”.\n\nThe images in the cleaned category were composed of images from the Open Images dataset and images that contained texts, but no watermarks, like PPT slides and memes, also retrieved from the kNN indices of LAION-400M. While we tried to curate a test set to evaluate the quality of our watermark detection model, we realized that it is almost impossible to draw a clear line between what actually is a watermark and what is not. For example pictures with small transparent texts at the bottom had been considered by some people as watermarked, by others not.\n\nIn the end we decided to choose a model based on our consensual judgment. It seems to be “good” at spotting obvious watermarks like those used on popular stock image sites. The creation of high-quality, openly accessible watermark detection test sets with clear and plausible definitions of what should be considered a watermark and what not, remains a challenge for future projects. Nevertheless we are convinced that removing images with a high confidence score for containing a watermark based on our model will significantly reduce the percentage of images that would be considered as obvious watermarks.\n\nThe model is available at [https://github.com/LAION-AI/watermark-detection](https://github.com/LAION-AI/watermark-detection) and [https://github.com/LAION-AI/LAION-5B-WatermarkDetection/releases/tag/1.0](https://github.com/LAION-AI/LAION-5B-WatermarkDetection/releases/tag/1.0)\n\n### Safety\n\nOn a balanced manually annotated safety test set with 3000 samples:\n\n- the accuracy of the B32 NSFW classifier is: 0.960\n- the accuracy of the ViT L 14 NSFW classifier is: 0.961\n\nThe model, as well as the training code, are available at [CLIP-based-NSFW-Detector](https://github.com/LAION-AI/CLIP-based-NSFW-Detector) The tags are available at [laion2B-en-safety](https://huggingface.co/datasets/laion/laion2B-en-safety) [laion2B-multi-safety](https://huggingface.co/datasets/laion/laion2B-multi-safety) [laion1B-nolang-safety](https://huggingface.co/datasets/laion/laion1B-nolang-safety) Demo at [clip-retrieval](https://rom1504.github.io/clip-retrieval/) (check/uncheck safe mode)\n\n## Using LAION datasets\n\nLaion5B and LAION-400M could e.g. be used to train\n\n- Generative models: training image/text generative models, e.g autoregressive models like DALL-E or diffusion models like GLIDE\n- Models with contrastive losses: self-supervised training on image/text pairs using contrastive losses, e.g CLIP\n- Classification models: e.g, performing zero-shot classification by extracting pseudo labels from queries on the dataset\n\nWe present here a few examples of models that were trained on our LAION datasets with success:\n\n#### CLIP\n\nWe, LAION, are currently working together with the Cross Sectional Team Deep Learning (CST-DL), Scalable Learning and Multi-Purpose AI Lab (SLAMPAI) at the Jülich Supercomputing Centre (JSC) and the Open CLIP team in the replication of OpenAI’s CLIP results.\n\n![](https://lh3.googleusercontent.com/-lvN21OSxBEwmzj1H0eIa4RxpS_VEogmXxku8R_0LWSLWtDP_tfNPuKCPuBEzA2arDvyPVfZlACZhdgymssC5E0tp_aqPdUFJMOLQf3W0wZKx3LqpJKF4JViL_nrBQH6TxEn5H2i)\n\n( The results in the right column are from our model. – huge thanks to Cade Gordon & Ross Wightman for performing the training run )\n\nThe repository with the training code and the model checkpoints can be found here: [https://github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip)\n\nWe gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at Jülich Supercomputing Centre (JSC).\n\n#### BLIP inference tuning\n\n[BLIP](https://github.com/salesforce/BLIP) is a model that was trained for both image-text matching and image captioning. It was trained on a 115M subset of LAION-400M. To improve the results of the generated captions we (LAION) performed over 100 experiments to determine the hyperparameters that maximize the BLEU-4 score compared to MS COCO captions. Here you can see some of our [results](http://captions.christoph-schuhmann.de/eval_b_auto/eval.html).\n\n![](https://lh4.googleusercontent.com/pLpUwcYitBPvnLqApETmU0Ik4VVYkslTyuEZBXT8VByhMjGqINSdVPLpqOR6ZcdvCilLakTXJXP40xUecLMMCEIl7CpWAN2RfAHU__OAKcIyd3_8lrsWuKeQcraU86ITbWmxM-y_)\n\n_eval_best_auto0185: An orange cat is looking at its reflection in the mirror._\n\n![](https://lh4.googleusercontent.com/QCQjVFkxZAQf24yZ49q5RLQ3ElyCIMFlOy5ACsfC7QC-CLiDDhTdCSnn5sobHFOsxLrVUIpNrk9sPyVmYDL6NB8qyQP6aSfkgxDuoWdeT3JX3j5MuJOGC9b7UgxKKnl4X9l33d0w)\n\n_eval_best_auto0190: A green highway sign with the words Queens Bronx._\n\nWe found that we can significantly improve the quality of the captions by generating 40 (or more) candidate captions for each image and then ranking them using OpenAI’s CLIP ViT-L/14 & CLIP-Resnet50x64. First we ranked all candidates with ViT-L/14 and then we ranked the top-5 results again using Resnet50x64. Preliminary results of human evaluations indicate that:\n\n1.  our evaluators gave the generated captions an average quality rating of 3,8 on a scale from 0 to 5, with a standard deviation of 0,9 ( in this particular hyperparameter configuration n= 600)\n2.  our evaluators gave original human captions from MS COCO an average quality rating of 3,9 with a standard deviation of 0,8 ( n = 2100 )\n\n—> We hypothesize that the generated captions match (& sometimes even surpass) the average quality of the human captions of MS COCO (which are sometimes also far from perfect) in most cases, but sometimes ( in less than <10% ) contain obvious mistakes, that humans would not make, because deeper kind of world knowledge & „common sense“ would be necessary in those cases.\n\n#### GLIDE\n\nClay Mullis (alias [afiaka87](https://github.com/afiaka87)) used subsets of LAON-2B to fine-tune the OpenAi [Glide](https://github.com/openai/glide-text2im) model and managed to reintroduce human generations. Samples\n\n- [https://replicate.com/afiaka87/laionide-v3](https://replicate.com/afiaka87/laionide-v3)\n- [https://wandb.ai/afiaka87/glide_compare/reports/Finetuning-GLIDE-on-Laion5B–VmlldzoxNTg3MTkz](https://wandb.ai/afiaka87/glide_compare/reports/Finetuning-GLIDE-on-Laion5B--VmlldzoxNTg3MTkz)\n- [https://wandb.ai/afiaka87/laionide-v3-glide/reports/Laionide-Version-3-Benchmark–VmlldzoxNjE0MTE3](https://wandb.ai/afiaka87/laionide-v3-glide/reports/Laionide-Version-3-Benchmark--VmlldzoxNjE0MTE3)\n\n![](https://lh5.googleusercontent.com/SjEvGWJlqpcocr0aeRj2V-ldfCJkO-RubJF-QQr6OdTgP196lqJynBEx45FRoEp3YTUfZLmYjpa9QDApqRvwd14zZSwbyEKso1i_q5wJNJXgQytb3yPVcllWGPsht4Tv52j7unM7)![](https://lh5.googleusercontent.com/BD89utsfddsrbxBCZsXEpxv_F4t2gGUrB8Hcqq5fl2aHrhsp5i5lJ5014d5T7I9DjxAT-4Q2N-cZYebumBzJWr7YwvRU0W2tjguSjF9DjTUuveBhhz__XWDuYGQh39N9D-op_1L7)![](https://lh5.googleusercontent.com/GLlZtnYViyWsxomeN0Xh7kY8IBZA9Ni2JmndUtGoSzIUT0NnXB7ru4wCNiHdFfhk1gwdc2LmbzvvNe_TTQCWe3hckDJLl7GiHzAc1S0agio3jxgv2DG3ih0WBPpQ88KQJf0jqtaN)\n\n#### Semantic search and subset extraction\n\nThe [clip-retrieval](https://rom1504.github.io/clip-retrieval/) interface allows a user to search images and texts based on a query image or text using the CLIP embeddings of the input and our precomputed kNN indices. It demonstrates the diversity of images and captions that can be found in LAION-5B as well as high semantic relevance shows the distribution of image sizes of LAION-5B. Given the abundance of high-resolution images, one can produce subsets of images for training various customized models, and also choose image resolution that is suitable for the purpose of particular training.\n\n#### CLOOB\n\nKatherine Crowson and John David Pressman recently trained a CLOOB ViT-B/16, variant of CLIP, for 32 epochs on LAION-400M and got preliminary results, that come close to the performance of OpenAI’s ViT-B/32, even though this was an early run with unoptimized hyperparameters. The checkpoints can be found here: [https://github.com/crowsonkb/cloob-training](https://github.com/crowsonkb/cloob-training)![](https://lh5.googleusercontent.com/ROxNOoa1jgaAW9JDbP2KHdDHOcTUCe-oPMvNTe2OYW_ETxbQI8W9YdA8oN93ULJ3r1Wyk7aAtm5GztJsQUZXUgw06BRiPECqp4o_bGXePp5cp9jFMBbbf2h1EYfDzvEHIcMdrcm-)\n\n(zero-shot accuracies on Imagenet-1K )\n\nWe are in touch with Andreas Fürst, one of the original CLOOB authors, and learned from him that their team is currently (at the time of writing) training a CLOOB ViT-B/32 with LAION-400M with optimized hyperparameters and very promising results so far (53% zero-shot accuracy on Imagenet after 7 epochs).\n\n## Papers citing LAION 400M\n\nAfter the release of LAION-400M, several papers used LAION-400M for image generation, text to image generation, image to text generation and text image matching:\n\n- [Vector Quantized Diffusion Model for Text-to-Image Synthesis](https://arxiv.org/abs/2111.14822.pdf) used LAION-400M to train VQ diffusion text to image generation models\n- [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752.pdf) used a subset of LAION-400M to train latent diffusion models\n- [General Facial Representation Learning in a Visual-Linguistic Manner](https://arxiv.org/abs/2112.03109.pdf) LAION-400M face subset to train a face clip\n- [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) image captioning using LAION-400M subset\n- [MAGMA – Multimodal Augmentation of Generative Models through Adapter-based Finetuning](https://arxiv.org/pdf/2112.05253.pdf) was trained on image question answering using a LAION-400M subset\n\n## Conclusion\n\nBy releasing an updated version of an openly available dataset that contains 5 billion image-text pairs, we have set new Standards for the scale of openly available datasets and enable researchers from all over the world to train state-of-the-art language-vision models like GLIDE or Turing Bletchley. As proof of concept, we demonstrated that a subset of our dataset can be used to train various CLIP-like models, producing samples of sufficient quality. This dataset extends the possibilities in multi-language large-scale training and research of language-vision models, that were previously restricted to those having access to proprietary large datasets, to the broad community.\n\n## What’s next?\n\nThis is only the beginning! Now that this huge and open dataset is released, it can be used to train many models, such as gigantic clip models, image/text generation models and much more. We have so many projects going on that it’s probably best, if you are interested, to join our Discord server and check out what’s going on. We are and always will be a grassroots community that works openly and welcomes everyone who is kind and passionate and for machine learning.\n\nJoin us in [discord](https://discord.gg/eq3cAMZtCC) and help us to train models like CLIP, BLIP, GLIDE, Dall-E, SimMIM, AudioCLIP and don’t hesitate to share your ideas for new projects with us.\n\n**Become a part of our constantly growing crowd of supporters who help us to make machine learning dreams come true!**\n\n## Credit Assignment\n\n- Christoph Schuhmann: He led this project and built POCs for most of its components including clip filtering,the safety model, the watermark model and the Blip inference tuning project.\n- Richard Vencu: System architecture and download script optimizations, GPU assisted filtering. Set up the AWS infrastructure.\n- Romain Beaumont: Guidance on scaling for the common crawl filtering pipeline. Built and ran the dataset preparation pipeline: pyspark deduplication job, img2dataset, clip inference, autofaiss, safety tags.\n- Clayton Mullis: DALLE-pytorch training/analysis, glide training, WDS filtering\n- Jenia Jitsev: scientific organization & writing, experiments planning and design, compute resource acquisition, general supervision\n- Robert Kaczmarczyk: Established WDS architecture, performed DALL-E training runs, balancing calculation, sample (NSFW, watermark, caption quality) annotation and manuscript revision\n- Andreas Köpf: He conducted the hyperparameter search for the inference strategies with the BLIP image-captioning model\n- Theo Coomber: He was one of our first contributors & build the first versions of our [worker swarm system](https://github.com/TheoCoombes/crawlingathome). Without his enthusiasm this project might never have taken off.\n- Aarush Katta: Trained the watermark model\n- Cade Gordon: Run distributed inference for the watermark tags & trained the CLIP B/32 model on JUWELS Booster\n- Ross Wightman: Ross helped Cade with the debugging & training of the CLIP-B/32 model and executed experiments on JUWELS Booster\n- Katherine Crowson and John David Pressman: Trained the CLOOB model\n- Aran Komatsuzaki: Led an image-text-pair dataset building project, which inspired this project.\n- Bokai Yu: Accomplished most of the work to make the knn index building tool autofaiss work in a distributed setting\n\n[  \n](https://laion.ai/laion-400-open-dataset/)\n","date":1648684800000},{"slug":"laion-400-open-dataset","frontmatter":{"title":"LAION-400-MILLION OPEN DATASET","author":"Christoph Schuhmann","date":"Aug 20, 2021","previewImg":"/images/blog/500m.png"},"content":"\nWe present LAION-400M: 400M English (image, text) pairs - see also our [Data Centric AI NeurIPS Workshop 2021 paper](https://arxiv.org/abs/2111.02114)\n\n## Concept and Content\n\nThe LAION-400M dataset is entirely openly, freely accessible.\n\n**WARNING**: be aware that this large-scale dataset is non-curated. It was built for research purposes to enable testing model training on larger scale for broad researcher and other interested communities, and is **not** meant for any real-world production or application.\n\nWe have filtered all images and texts in the LAION-400M dataset with OpenAI‘s [CLIP](https://openai.com/blog/clip/) by calculating the cosine similarity between the text and image embeddings and dropping those with a similarity below 0.3. The threshold of 0.3 had been determined through human evaluations and seemed to be a good heuristic for estimating semantic image-text-content matching.\n\nThe image-text-pairs have been extracted from the [Common Crawl](https://commoncrawl.org/) web data dump and are from random web pages crawled between 2014 and 2021.\n\n###### Original information\n\n\n### LAION-400M Dataset Statistics\n\nThe LAION-400M and future even bigger ones are, in fact, datasets of datasets. For instance, we can filter it out by image sizes into smaller datasets like this:\n\n```\nNumber of unique samples 413M\nNumber with height or width >= 1024 26M\nNumber with height and width >= 1024 9.6M\nNumber with height or width >= 512 112M\nNumber with height and width >= 512 67M\nNumber with height or width >= 256 268M\nNumber with height and width >= 256 211M\n```\n\nBy using the KNN index, we can extract specialized datasets by domains of interest. They are (or will be) sufficient in size to train technical domain models.\n\nAlso, use [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) for simple visualisation of the dataset. There you can search among the dataset using CLIP and a knn index.\n\n### Disclaimer & Content Warning\n\nOur filtering protocol only removed NSFW images detected as illegal, but the dataset still has NSFW content accordingly marked in the metadata. When freely navigating through the dataset, keep in mind that it is a large-scale, **non-curated** set crawled from the internet for research purposes, such that collected links may lead to discomforting and disturbing content. Therefore, please use the demo links with **caution**. You can extract a “safe” subset by filtering out samples drawn with NSFW or via stricter CLIP filtering.\n\nThere is a certain degree of duplication because we used URL+text as deduplication criteria. The same image with the same caption may sit at different URLs, causing duplicates. The same image with other captions is not, however, considered duplicated.\n\nUsing KNN clustering should make it easy to further deduplicate by image content.\n\n### LAION-400M Open Dataset structure\n\nWe produced the dataset in several formats to address the various use cases:\n\n- a 50GB url+caption metadata dataset in parquet files. We can use the metadata to compute statistics and redownload part of the dataset\n- a 10TB webdataset with 256×256 images, captions and metadata. It is a full version of the dataset that can be used directly for training (this one is for internal use, you need to redownload images yourself due to licensing issues)\n- a 1TB set of the 400M text and image clip embeddings, useful to rebuild new knn indices\n- pairs of 16G, 32G, 64G and 128G knn indices (running in the web demo)\n\n#### URL and caption metadata dataset.\n\nWe provide 32 parquet files of size around 1GB (total 50GB) with the image URLs, the associated texts and additional metadata in the following format:\n\n> SAMPLE_ID | URL | TEXT | LICENSE | NSFW | similarity | WIDTH | HEIGHT\n\nwhere\n\n- **SAMPLE_ID**: A unique identifier\n- **LICENSE**: Where we found a Creative Commons License in the image data, we named it here like, e.g. “creativecommons.org/licenses/by-nc-sa/3.0/” – otherwise you’ll find it here a “?”\n- **NSFW**: we used CLIP to estimate if the image has NSFW content. The estimation has been pretty conservative, reducing false negatives at the cost of more false positives. Possible values are “UNLIKELY”, “UNSURE” and “NSFW”.\n- **similarity**: Value of the cosine similarity between the text and image embedding\n- WIDTH and HEIGHT: image size as the image was embedded. We downsized originals that were larger than 4K to 4K.\n\nThis metadata dataset purpose is to download the images for the whole dataset or a subset of it by supplying it to the very efficient [img2dataset](https://github.com/rom1504/img2dataset) tool.\n\n#### 10 TB webdataset with images and captions\n\nBy running the img2dataset tool, we can download a 10TB webdataset. It will resize all images at 256×256 resolution, will append the corresponding caption and will generate a collection of tar files (that dataset format is called webdataset) containing images, captions, and metadata and related parquet files containing the same metadata\n\n- 00000.tar of size 270MB containing at most 10k samples\n  - 0.jpg\n  - 0.txt containing the caption\n  - 0.json containing metadata such as the URL, the original width, the EXIF data, whether the image is NSFW\n- 00000.parquet of size 1.6MB containing the same metadata as the JSON file. Useful to compute statistics without reading all the tar files\n\nThe 400M dataset will therefore have 41455 tar and 41455 parquet files. This dataset purpose is to train multimodal models like CLIP or DALL-E.\n\n#### 1TB of clip embeddings\n\nThe clip embeddings are stored in NPY files next to parquet files in the same order. Since this dataset is much smaller than image one, each NPY file stores 1M samples. Each NPY file is 1GB, and each parquet file is 150MB. There are a total of 400 such files. The embeddings purpose is to compute statistics on the dataset, for example, using clustering or knn indices.\n\n#### Two small 6GB knn indices\n\nWe provide two 6GB knn indices built using the [autofaiss](https://github.com/criteo/autofaiss). We can use them to compute a subset of the dataset and, more generally, to search among it efficiently. See the search [web demo](https://rom1504.github.io/clip-retrieval/) of it. We can use the CLIP filter tool along with this index to produce subsets using search terms efficiently. We also provide two 16GB knn indices of higher quality.\n\n### What can we do with the LAION-400M dataset?\n\nVision and language modelling has been taking off in 2021. Here are some pointers about what this kind of image + text datasets unlocks and why it seems interesting:\n\n- Six months ago, OpenAI released two blog posts and papers, [CLIP](https://openai.com/blog/clip/) and [DALL-E](https://openai.com/blog/dall-e/). Both models rely on a large amount of (text, image) pairs. They used an unreleased 400M pairs dataset.\n  - CLIP is a model that computes how related are a text and an image. It makes it possible to build large text to image search, and it makes it possible to create that kind of crazy text to image art [clip-art](https://ml.berkeley.edu/blog/blog/clip-art/). They released a small and medium version of the model but no training code.\n  - DALL-E is a model that directly generates images from texts. As can be seen from the blog post, it achieves awe-inspiring results that could directly impact the world for anything that needs drawing and illustrations. OpenAI did not release any model, even through an API\n\nSince then, various researchers have organised several efforts to replicate DALL-E. People gathered initially around this excellent DALLE replication repository [DALLE-PyTorch](https://github.com/lucidrains/DALLE-pytorch) with some fantastic results visible in the readme. More recently, as part of huggingface events, new developments have been achieved (see [DALLE-mini report](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini--Vmlldzo4NjIxODA) ), and an online demo is now available at [DALLE-mini demo.](https://huggingface.co/spaces/flax-community/dalle-mini)\n\nThe replication effort is still far from achieving the same performance as the original DALLE, and it seems possible to go even further. Some people also want to make a better CLIP to produce even better-generated art.\n\nA large part of the results that we can achieve with such models is thanks to a large amount of data. Before LAION-400M, the largest open dataset for (image, text) pairs are in the order of 10M (see [DALLE-datasets](https://github.com/robvanvolt/DALLE-datasets) ), which is enough to train exciting models but not enough to reach the best performance. Having a public dataset with hundreds of millions of pairs will help build these image+text models.\n\n### Analysis of the LAION-400M data\n\nWe annotated 3456 samples of the dataset and got the following results:\n\n- Correct positive NSFW: 4\n- Correct negative NSFW: 3371\n- False-positive NSFW: 73\n- False-negative NSFW: 8\n- Bad captions: 3 (0.09 %)\n\nThe matching is excellent, thanks to CLIP. We could improve the NSFW automatic tagging in the future; however, the NSFW total rate is low enough (less than 1%) to make this not an issue.\n\n## Technical Details\n\nThe dataset acquisition has into two significant parts:\n\n1.  a distributed processing of the vast (many PBs) Common Crawl datasets, which produces a collection of matching URL and caption\n2.  a single node much lighter post-processing of the data that anyone can run in a few days and which produces the final dataset\n\n### 1. Distributed processing of Common Crawl\n\nWe acquire the raw web data for the creation of our dataset from Common Crawl. Common Crawl is a non-profit organisation dedicated to providing a copy of the internet to internet researchers, companies, and individuals at no cost for research and analysis. They regularly release dumps of HTML-like data parsed from billions of public websites found [on the Common Crawl website](https://commoncrawl.org/the-data/get-started/). To create image-text pairs, we parse through the data from Common Crawl and parse out all HTML IMG tags containing an [alt text attribute](https://en.wikipedia.org/wiki/Alt_attribute). Common Crawl provides its data in several formats. For our purpose, we chose to use the data in the WAT format. The WAT files contain only the metadata of the crawled sites, which includes all links and IMG tags contained in the website. Parsing only this metadata is much faster than parsing the whole HTML text (provided in the WARC format).\n\n#### Downloading original images\n\nWe download the raw images from the URLs we parsed from Common Crawl with asynchronous requests using the libraries [Trio](https://github.com/python-trio/trio) and [Asks](https://github.com/theelous3/asks). They allow us to go multithreading for a single CPU. Usually, a home internet link will be exhausted by a single or two CPUs. A data centre node can scale up benefits from guaranteed internet speed with a multiprocessing pool much faster than a single CPU node. At this time, we were able to use 50 cores with a full, secured 1Gbps connection to the public internet. This bandwidth must be available to the downloading node, not shared among many nodes or apps. We have optimised the script for speed while mitigating various errors we encountered. Usually, to satisfy a high-end demanding node such as above, we must take additional steps to provide DNS caching capabilities. We found that the knot-resolver ran with two processes and configured with caching option can solve this problem.\n\n#### Filtering out unsuitable image-text pairs\n\nAfter downloading the WAT files from Common Crawl, we filter the samples in the following steps:\n\n1.  We dropped all samples with less than five character alt text length\n2.  We dropped all samples with less than 5 KB image size\n3.  We use continuously updated bloom filters to drop samples that are already in our dataset. The bloom filters deduplicate by concatenating the URL and the alt text.\n4.  We use continuously updated bloom filters to drop samples from URLs that had timed out previously and therefore seem unreachable (or at least not reachable in an efficient way)\n5.  We use OpenAI’s CLIP model (the ‘_ViT-B-32_‘ version) to compute the image and alt text embeddings. Then we calculate the cosine similarity of both embedding vectors and drop all samples with a similarity below 0.3. We chose this threshold after trying different values and using human evaluations of how well the texts fit the images. Lower values like 0.28 or 0.29 also seemed okay in many cases, but after further inspections, we decided to choose the conservative value of 0.3.\n6.  We use the CLIP embeddings of the images to estimate if their contents contain NSFW content. We do this by calculating CLIP embeddings for a list of image categories like, e.g. “selfie”, “illustration”, or “landscape”, which also contains categories that indicate NSFW content like “porn” and “sex”.\n7.  Then we compute the cosine similarities between the embedding image we are currently filtering and each of these category keywords. If the category with the highest similarity and the keyword with the second-highest similarity belong both to NSFW keywords, we tag the sample as “NSFW”. If only one of them belongs to an NSFW keyword, we categorise the sample as “UNSURE”. If both keywords with the highest similarities are not NSFW, we tag the sample as “UNLIKELY”.\n8.  In the next step, we look at all samples with either the “NSFW” or “UNSURE” tag and drop those with any keywords in their text related to kids, teens, or other semantically related content.\n9.  In step 8, we repeat the procedure of computing the cosine similarities from step 6 with the difference that we now use category texts that indicate contents semantically related to kids and teens on a CLIP embedding level. If either the highest similarity or the second-highest similarity between a sample’s image embedding and a text of the precomputed categories belongs to a text that indicates content related to under-aged persons, we drop this sample.\n10. Finally, we repeat the procedure from step 8 with texts semantically related to animal categories like e.g. “animal”, “bird”, etc.\n\nWe perform these rigorous filtering steps for NSFW with potentially illegal content because we cannot guarantee that the contents of Common Crawl are free of such. We feel obligated to try our best to filter out such content. Inspections of samples filtered out by steps 7 to 9 have shown that our filtering procedure is very conservative and produces many false positives (samples it drops, which are not problematic). This process is okay because the number of potential samples waiting for us to crawl is vast.\n\n#### System Architecture\n\nTo orchestrate the interactions of the many crawling scripts (called _workers_) in our project, we use a server that keeps track of processed WAT files and of which worker gets which unprocessed WAT. We call this orchestrating server a _tracker_. Its functions are offering jobs to both download workers and inference workers, confirming cleanup requests from the DL staging server, maintaining ACLs for the Bloom server, and some more. We also employ several staging servers as buffers for jobs on their way to the storage location. The staging servers continuously update filters in the central bloom server where we use RedisBloom for high-performance reasons.![](https://i.imgur.com/kxl4jJe.png)\n\n#### Workflow\n\nDuring the evolution of our crawling project, we applied two different workflows:\n\n##### Workflow 1 (_“Hybrid”_ – workers)\n\nThis worker performs all computation steps during one job and then submits the result to the staging server. It then queues the results for release to the storage area.\n\n##### Workflow 2 (_“CPU – GPU – 2 stages”_ – workflow)\n\nWe soon discovered that the best way to utilise resources is to split the workload into CPU + networking tasks (downloading steps) and GPU tasks (CLIP inference steps). Hence, the 2 stage approach uses “CPU workers” to download images, create image-text pairs, and save the intermediate result to a staging server. Then “GPU workers” pick up jobs, concatenate a number of them to group around 20000 pairs per final result file. The 2 stage workflow proved to be most efficient, with speeds up to 25 million pairs added to the dataset per day when using 100 CPU workers with one core and one GPU worker employing an NVidia RTX 3090 graphic card utilising all 16 lanes of PCIe bus. The GPU node also needs about CPU 24 threads to keep up with the GPU processing capacity.\n\n#### Removing abuse alerts\n\nDuring downloading, we encountered abuse alerts from manual and automated tools that protect websites. After some learning curve, we reduced most of the issues by employing these mitigation techniques:\n\n- By far, the most efficient one was to use centralised bloom filters that eliminate requests going to the duplicate URLs over and over. Of course, the efficiency of these filters dramatically depends on how fast they are updated and used by the workers. By definition, having multiple downloading workers performing jobs in parallel makes them prone to overlap requests to the same URL even if the bloom filters are up to date at the beginning of the job.\n- Therefore the second technique significantly reduced the problem of parallel workers via randomising the jobs at the tracker server level. While executing jobs in sequence (with the oldest WAT files from 2013), we discovered that adjacent jobs were overlapping considerably. When we randomised jobs, we saw a dramatic decrease in such overlapping.\n\n#### Who ran this?\n\nWe want to thank :\n\n- the [LAION folks](https://laion.ai/#team), via so many worker nodes everywhere in the cloud\n- [the data hoarders](https://www.reddit.com/r/DataHoarder/comments/oyta8q/crawlinghome_help_build_the_worlds_largest/) Reddit community\n- [the-eye](https://the-eye.eu/) community\n- as well as all our friends and relatives that did not know what they were helping with\n\nfor running the workers to produce this vast dataset in a few months.\n\n### 2. Post-processing of the dataset\n\nOnce the distributed pipeline has run, resulting in a sizeable caption+url dataset, it’s time to package it in the best way. The objective of this second pipeline is to produce a version of the dataset that is easy to use for multimodal training. For this, we built tools that anyone can run out of a collection of caption+url. The exact command line to run is available in [cah-prepro](https://github.com/rom1504/cah-prepro) (which uses mainly [img2dataset](https://github.com/rom1504/img2dataset) and [clip-retrieval](https://github.com/rom1504/clip-retrieval) )\n\n#### Pyspark preprocessing of the CSV files\n\nAfter a fast run of a script to [download the CSV files,](https://github.com/rom1504/cah-prepro/tree/main/download_csv) the first step of this post-processing pipeline is to do deduplication by url+caption. The first pipeline does some partial deduplication using a bloom filter, but it is approximate, and some duplicates remain. Doing that pyspark post-processing also makes it possible to reduce the number of metadata files from hundred of thousands to 32 parquet files of size 1.7GB. See this [deduplication script there](https://github.com/rom1504/cah-prepro/blob/main/deduplicate/cah_stats_spark.py). Pyspark would be an excellent way to do any further filtering, and we [provide](https://github.com/rom1504/cah-prepro/blob/main/deduplicate/compute_more_stats.py) an example to compute some statistics. The resulting output is 32 parquet files containing columns such as URL, text, NSFW described at the beginning of the post.\n\n#### Img2dataset\n\nOnce this set of 50GB parquet files has is ready, we can use the [img2dataset](https://github.com/rom1504/img2dataset) tool to download, resize and store the images and captions as [webdataset](https://github.com/webdataset/webdataset). This tool can download 100M images in 20h in a single node (1Gbps 32GB of ram 16 i7 cores), so anyone can run this for the whole dataset or a smaller subset. The format this tool outputs is a collection of tar files (that dataset format is called webdataset) containing images, captions, and metadata and corresponding parquet files containing the same metadata\n\n- 00000.tar of size 270MB containing at most 10k samples\n  - 0.jpg\n  - 0.txt containing the caption\n  - 0.json containing metadata such as the URL, the original width, the EXIF data, whether the image is NSFW\n- 00000.parquet of size 1.6MB containing the same metadata as the JSON file. Useful to compute statistics without reading all the tar files\n\nThe size of the tars of 270MB is when using the options of img2dataset indicated there [download_images.sh](https://github.com/rom1504/cah-prepro/blob/main/download_images/download_images.sh) (resizing all images to 256×256 with padding for maximum file uniformity and avoid losing information). If using different options, you may have larger or smaller tar files.\n\n#### Clip retrieval and autofaiss\n\nFinally, the tar dataset aims to compute and package clip embeddings and compute a KNN index over the clip embeddings. The [clip-retrieval](https://github.com/rom1504/clip-retrieval/) tool makes it fast to compute 100M embeddings per 20h with a single 3080 GPU, so it’s possible to rerun this part on the whole dataset or a subset at a low cost. The embeddings are stored in NPY files next to parquet files in the same order. Since this dataset is much smaller than image one, each NPY file stores 1M samples. NPY files are 1GB in size, and parquet files are 150MB. There are a total of 400 such files. These embeddings help build text and an image knn index using the [autofaiss](https://github.com/criteo/autofaiss) tool, making it possible to produce a quantised index of an arbitrary file. The chosen index type is 6GB, so it’s cheap for anyone to load and run fast (10ms) queries over the whole dataset. We also generated another kind of index of size 16GB. Thanks to memory mapping, it’s also possible to load it at no ram usage. A simple [web demo](https://rom1504.github.io/clip-retrieval/) shows the results.\n\n![](https://i.imgur.com/6bEztg9.png)\n\n### License\n\nWe distribute the metadata dataset (the parquet files) under the most open [Creative Common CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) license, which poses no particular restriction. The images are under their copyright.\n\n## Contributing\n\nYou can contribute to the project to help us release the following dataset sizes at 1 billion pairs, 2 billion pairs and so on.\n\nChoose one or more methods that suit you or your company:\n\n1.  donate either [cash](https://laion.ai/laion-400-open-dataset/#) or [computing time](https://laion.ai/how-to-donate-computing-time/). We also launched a [Go Get Funding campaign](https://gogetfunding.com/help-us-build-the-worlds-largest-open-billion-scale-image-text-dataset-perfect-for-training-dall-e-clip-other-multimodal-models/).\n2.  participate in the development effort\n3.  spread the word. At best, use the dataset, get nice results and mention it in your papers\n\nUseful links:\n\n- Dataset progress [Crawling@Home Dashboard](http://crawling.at/) and [leaderboard](http://crawling.at/leaderboard)\n- Reddit [post](https://www.reddit.com/r/DataHoarder/comments/oyta8q/crawlinghome_help_build_the_worlds_largest/?utm_source=share&utm_medium=web2x&context=3)\n- DALLE-PyTorch [Discord server](https://discord.gg/mVcgxMPD7e)\n- DALLE-PyTorch [GitHub Repository](https://github.com/lucidrains/DALLE-pytorch)\n\n[  \n](https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/)\n\n### Sponsors\n\nWe made it so far due to the generosity of these donors:\n| ![](https://i.imgur.com/z6K7kSq.png) |![](https://i.imgur.com/KYvncYl.png)|![](https://i.imgur.com/y2yNLm8.png)|\n|--|--|--|\n|[doodlebot.ai](http://doodlebot.ai/)|[Gentec Data](https://gentec.ro/)|[the-eye.eu](http://the-eye.eu/)|\n","date":1629417600000}]},"__N_SSG":true}