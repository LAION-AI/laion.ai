(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[327],{9182:function(e,t,a){(window.__NEXT_P=window.__NEXT_P||[]).push(["/projects",function(){return a(4746)}])},8396:function(e,t,a){"use strict";a.d(t,{Z:function(){return r}});var i=a(5893),n=a(9008),s=a.n(n),o=a(1163);function r(e){var t=(0,o.useRouter)(),a=e.title?e.title+" | LAION":"LAION",n=e.desc?e.desc:"LAION, Large-scale Artificial Intelligence Open Network, is a non-profit organization making machine learning resources available to the general public.",r=e.image?e.image:"/social.png",l=e.alt?e.alt:"The text: LAION. Large-scale Artificial Intelligence Open Network, TRULY OPEN AI. 100% NON-PROFIT. 100% FREE.",d=e.slug?e.slug:t.route;return(0,i.jsxs)(s(),{children:[(0,i.jsx)("title",{children:a}),(0,i.jsx)("meta",{name:"title",content:a}),(0,i.jsx)("meta",{property:"og:title",content:a}),(0,i.jsx)("meta",{property:"twitter:title",content:a}),(0,i.jsx)("meta",{name:"description",content:n}),(0,i.jsx)("meta",{property:"og:description",content:n}),(0,i.jsx)("meta",{property:"twitter:description",content:n}),(0,i.jsx)("meta",{property:"og:image",content:r}),(0,i.jsx)("meta",{property:"twitter:image",content:r}),(0,i.jsx)("meta",{name:"twitter:image:alt",content:l}),(0,i.jsx)("meta",{property:"og:type",content:"website"}),(0,i.jsx)("meta",{property:"og:url",content:"https://laion.ai"+d}),(0,i.jsx)("meta",{property:"twitter:url",content:"https://laion.ai"+d}),(0,i.jsx)("meta",{property:"twitter:card",content:"summary_large_image"}),(0,i.jsx)("meta",{name:"viewport",content:"initial-scale=1.0, width=device-width"}),(0,i.jsx)("meta",{name:"theme-color",content:"#1D374E"}),(0,i.jsx)("link",{rel:"icon",type:"image/png",sizes:"32x32",href:"/favicon.png"}),(0,i.jsx)("link",{rel:"icon",href:"/favicon.svg",type:"image/svg+xml"})]})}},4746:function(e,t,a){"use strict";a.r(t),a.d(t,{default:function(){return l}});var i=a(5893),n=a(8396),s=a(1664),o=a.n(s),r=JSON.parse('[{"name":"Datasets","entries":[{"name":"LAION-400M","modality":"image/text","status":"Released","desc":"Formerly known as crawling@home (C@H), an openly accessible 400M image-text-pair dataset.","link":"/blog/laion-400-open-dataset"},{"name":"LAION5B","modality":"image/text","status":"Released","desc":"A dataset consisting of 5.85 billion CLIP-filtered image-text pairs, featuring several nearest neighbor indices, an improved web-interface for exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection.","link":"/blog/laion-5b"},{"name":"LAION5B High-Res","modality":"image/text","status":"Released","desc":"A subset of the LAION5B database, with high resolution images oveer 1024x1024, containing 170 million samples.","link":"https://huggingface.co/datasets/laion/laion-high-resolution"},{"name":"LAION Aesthetics","modality":"image/text","status":"Released","desc":"A subset of LAION5B that has been estimated by a model trained on top of clip embeddings to contain only aestheticly pleasing images.","link":"https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md"},{"name":"LAION-3D","modality":"3d/image/text","status":"Started","desc":"An effort to create a large-scale dataset consisting of 3D models and descriptor pairs.","link":"https://github.com/LAION-AI/laion-3d"},{"name":"Audio Dataset","modality":"text/audio","status":"Started","desc":"An audio dataset for training CLAP and other models, containing a raw and processed dataset, the latter containing .flac files with captions, labels, and other metadata.","link":"https://github.com/LAION-AI/audio-dataset"},{"name":"Watermark Detection","modality":"image/text","kind":"Contrastive","status":"Released","desc":"A repository containing datasets to train a watermark classifier.","link":"https://github.com/LAION-AI/watermark-detection"}]},{"name":"Models","entries":[{"name":"Openclip","modality":"image/text","kind":"Contrastive","status":"released","desc":"An open source implementation of OpenAI\'s CLIP (Contrastive Language-Image Pre-training).","link":"https://github.com/mlfoundations/open_clip"},{"name":"DALLE-2 Prior/Decoder","modality":"image/text","kind":"Generative","status":"Started","desc":"An implementation of DALL-E 2, OpenAI\'s text-to-image synthesis neural network, in Pytorch.","link":"https://github.com/lucidrains/DALLE2-pytorch"},{"name":"ClipCap","modality":"image/text","kind":"Generative","status":"Released","desc":"Generate text from embedding, using pretrained encoder and language models.","link":"https://github.com/TheoCoombes/ClipCap"},{"name":"CLAP","modality":"audio/text","kind":"Contrastive","status":"Started","desc":"A Contrastive Language-Audio Pretraining model, like CLIP, for audio.","link":"https://github.com/LAION-AI/CLAP"},{"name":"Video CLIP","modality":"video/text","kind":"Contrastive","status":"Planning","desc":"A contrastive language pretraining model for videos.","link":"https://github.com/LAION-AI/video-clip"},{"name":"Multilingual-CLIP","modality":"image/text","kind":"Contrastive","status":"In progress","desc":"An implementation of OpenAI\'s CLIP text encoders for any language.","link":"https://github.com/FreddeFrallan/Multilingual-CLIP"},{"name":"NSFW Detection","modality":"image/text","kind":"Contrastive","status":"Released","desc":"A detctor for not safe for work content within images using CLIP.","link":"https://github.com/LAION-AI/CLIP-based-NSFW-Detector"},{"name":"Electric Sheep","modality":"image/text/audio/video","kind":"Contrastive/Generative","status":"Started","desc":"Train Contrastive and generative models on all modalities.","link":"https://github.com/LAION-AI/the-big-plan/blob/main/projects/electric-sheep.md"}]},{"name":"Tools","entries":[{"name":"img2dataset","modality":"image/text","status":"Released","desc":"A tool which allows a user to turn large sets of image urls to an image dataset. Can download, resize and package 100M urls in 20 hours on one machine.","link":"https://github.com/rom1504/img2dataset"},{"name":"Clip Retrieval","modality":"image/text","status":"Released","desc":"Allows a user to easily compute clip embeddings and build a clip retrieval system with them. 100M text+image embeddings can be processed in 20 hours using a RTX 3080.","link":"https://github.com/rom1504/clip-retrieval"},{"name":"Crawlingathome-gpu-hcloud","modality":"image/text","status":"Released","desc":"GPU controlled Hetzner Cloud workers swarm for Crawling@Home project.","link":"https://github.com/rvencu/crawlingathome-gpu-hcloud"},{"name":"Clip Benchmark","modality":"image/text","status":"In Progress","desc":"Evaluating CLIP-like models on a standard set of datasets on different tasks such as zero-shot classification and zero-shot retrieval.","link":"https://github.com/LAION-AI/CLIP_benchmark"}]},{"name":"Papers","entries":[{"name":"LAION-400M","modality":"image/text","status":"Published","link":"https://arxiv.org/abs/2111.02114"},{"name":"LAION-5B","modality":"image/text","status":"started","link":"https://github.com/LAION-AI/laion5B-paper"}]}]');function l(){return(0,i.jsxs)("div",{className:"w-full flex justify-center py-5 pt-16 md:pt-5",children:[(0,i.jsx)(n.Z,{title:"Projects",desc:"A selection of open-source projects maintained by LAION, the Large-scale Artificial Intelligence Open Network, to be used freely in machine learning efforts."}),(0,i.jsxs)("div",{className:"container px-5",children:[(0,i.jsx)("h1",{className:"text-7xl md:text-8xl font-bold",children:"PROJECTS"}),(0,i.jsx)("hr",{className:"mb-5 mt-2 md:hidden"}),r.map((function(e,t){return(0,i.jsxs)("div",{children:[(0,i.jsx)("h3",{className:"pb-4 "+(0===t?"pt-0":"pt-5"),children:e.name.toUpperCase()}),(0,i.jsx)("div",{className:"grid gap-5 grid-cols-2",children:e.entries.map((function(e,t){return"/"===e.link.charAt(0)?(0,i.jsx)("a",{className:"no-underline "+(void 0===e.desc?"col-span-1":"col-span-2"),children:(0,i.jsx)(o(),{href:e.link,children:(0,i.jsx)(d,{item:e})},t)}):(0,i.jsx)("a",{href:e.link,rel:"noopener noreferrer",target:"_blank",className:"no-underline "+(void 0===e.desc?"col-span-1":"col-span-2"),children:(0,i.jsx)(d,{item:e})},t)}))})]},t)}))]})]})}function d(e){var t=e.item;return(0,i.jsxs)("div",{className:" bg-sky border border-paper hover:bg-paper hover:text-sky cursor-pointer transition-colors p-5 shadow-lg shadow-neutral-800/20 flex flex-col sm:flex-row ",children:[(0,i.jsxs)("div",{className:void 0!==t.desc&&"basis-1/4",children:[(0,i.jsx)("p",{className:"text-2xl",children:t.name}),(0,i.jsx)("p",{children:t.modality}),(0,i.jsx)("p",{children:t.kind}),(0,i.jsxs)("p",{children:["Status: ",t.status]})]}),(0,i.jsx)("hr",{className:"mt-4 mb-4 sm:hidden "+(void 0===t.desc&&"hidden")}),(0,i.jsx)("div",{className:"basis-3/4 "+(void 0===t.desc&&"hidden"),children:t.desc})]})}},9008:function(e,t,a){e.exports=a(3121)},1163:function(e,t,a){e.exports=a(880)}},function(e){e.O(0,[774,888,179],(function(){return t=9182,e(e.s=t);var t}));var t=e.O();_N_E=t}]);