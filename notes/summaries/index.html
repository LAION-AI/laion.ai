<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Open Scientific Summaries at Scale: The Inference.net × LAION × Grass Initiative | LAION</title><meta name="title" content="Open Scientific Summaries at Scale: The Inference.net × LAION × Grass Initiative | LAION"/><meta property="og:title" content="Open Scientific Summaries at Scale: The Inference.net × LAION × Grass Initiative | LAION"/><meta name="twitter:title" content="Open Scientific Summaries at Scale: The Inference.net × LAION × Grass Initiative | LAION"/><meta name="description" content="&lt;h2&gt;&lt;a id=&quot;abstract&quot; class=&quot;anchor&quot; href=&quot;#abstract&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewbo..."/><meta property="og:description" content="&lt;h2&gt;&lt;a id=&quot;abstract&quot; class=&quot;anchor&quot; href=&quot;#abstract&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewbo..."/><meta name="twitter:description" content="&lt;h2&gt;&lt;a id=&quot;abstract&quot; class=&quot;anchor&quot; href=&quot;#abstract&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewbo..."/><meta property="og:image" content="https://laion.ai/images/blog/sci3.jpg"/><meta name="twitter:image" content="https://laion.ai/images/blog/sci3.jpg"/><meta name="twitter:image:alt" content="The text: LAION. Large-scale Artificial Intelligence Open Network, TRULY OPEN AI. 100% NON-PROFIT. 100% FREE."/><meta property="og:type" content="website"/><meta property="og:url" content="https://laion.ai/notes/summaries"/><meta name="twitter:url" content="https://laion.ai/notes/summaries"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1D374E"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.png"/><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff2" as="font" type="font/woff2" crossorigin="true"/><meta name="next-head-count" content="25"/><link rel="stylesheet" href="/fonts/load.css"/><link rel="preload" href="/_next/static/css/5357c8cce67e7f29.css" as="style"/><link rel="stylesheet" href="/_next/static/css/5357c8cce67e7f29.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fb0512e25146295.js" defer=""></script><script src="/_next/static/chunks/286-30519d8a3e60551d.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bslug%5D-2b0138ebd04b8900.js" defer=""></script><script src="/_next/static/yC_b85G6wSLJL3iEWziwb/_buildManifest.js" defer=""></script><script src="/_next/static/yC_b85G6wSLJL3iEWziwb/_ssgManifest.js" defer=""></script><script src="/_next/static/yC_b85G6wSLJL3iEWziwb/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-paper md:text-sky md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LAION</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/team/">Team</a><a href="/blog/">Blog</a><a href="/notes/">Notes</a><a href="/press/">Press</a><a href="/about/">About</a><a href="/faq/">FAQ</a><a href="/donations/">Donations</a><a href="/privacy-policy/">Privacy Policy</a><a href="/dataset-requests/">Dataset Requests</a><a href="/impressum/">Impressum</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1D374E;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/team/">Team</a></p><p><a href="/blog/">Blog</a></p><p><a href="/notes/">Notes</a></p><p><a href="/press/">Press</a></p><p><a href="/about/">About</a></p><p><a href="/faq/">FAQ</a></p><p><a href="/donations/">Donations</a></p><p><a href="/privacy-policy/">Privacy Policy</a></p><p><a href="/dataset-requests/">Dataset Requests</a></p><p><a href="/impressum/">Impressum</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-paper grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-8xl md:text-8xl w-full font-bold title-flow break-words">OPEN SCIENTIFIC SUMMARIES AT SCALE: THE INFERENCE.NET × LAION × GRASS INITIATIVE</h1><p class="text-2xl pb-2">by: <!-- -->Christoph Schuhmann, Amarjot Singh, Andrej Radonjic, Sean Smith, and Sam Hogan<!-- -->,<!-- --> <!-- -->07 Nov, 2025<!-- --></p><hr/><div class="pt-2 article"><h2><a id="abstract" class="anchor" href="#abstract" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Abstract</h2>
<p>We present a comprehensive approach to democratizing access to scientific knowledge through large-scale, <strong>structured summarization</strong> of academic literature.</p>
<p align="center">
  <img src="/images/blog/sci5.png"
       alt="LLM-as-a-Judge scores chart"
       style="width:90%; height:auto;">
</p>
<p>We retrieved and processed ~<strong>100 million</strong> research papers from the public internet, leveraging existing datasets from <strong>bethgelab</strong>, <strong>PeS2o</strong>, <strong>Hugging Face</strong>, and <strong>Common Pile</strong>. We designed a standardized <strong>JSON schema</strong> for scientific paper summaries and <strong>post-trained two models</strong>—<strong>Qwen 3 14B</strong> and <strong>Nemotron 12B</strong>—to produce summaries in this format. Our evaluation combines <strong>LLM-as-a-Judge</strong> and a <strong>QA dataset</strong>. Fine-tuned models achieve performance on our evals comparable to leading closed models (e.g., GPT-5, Claude 4.5). <strong>Nemotron 12B</strong> offers ~<strong>2.25×</strong> higher throughput than Qwen 3 14B, making it attractive for large-scale processing.</p>
<p>With this preliminary blog post, we <strong>release a fine-tuned models, 100k paper summaries</strong>.
A live <strong>visualization tool</strong> at <a href="https://laion.inference.net/">https://laion.inference.net/</a> demonstrates the utility of structured summaries. We plan to release structured summaries for the full <strong>100M</strong> paper corpus.</p>
<p align="center">
  <img src="/images/blog/sci4.png"
       alt="LLM-as-a-Judge scores chart"
       style="width:90%; height:auto;">
</p>
<h2><a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h2>
<p>Access to scientific knowledge remains constrained by paywalls, licensing, and copyright, slowing research and education. Our <strong>Project Alexandria</strong> (<a href="https://arxiv.org/abs/2502.19413">arXiv:2502.19413</a>) showed that it is legally and technically feasible to <strong>extract factual knowledge</strong> while respecting copyright via <strong>Knowledge Units</strong>—structured, style-agnostic representations of content. However, research-paper corpora vary in format and structure, making it hard to compare similar claims or retrieve knowledge efficiently. Building on Alexandria, we introduce a <strong>pipeline</strong> to collect, process, and summarize papers into <strong>structured outputs</strong> consumable by humans and AI systems alike. Our aims: * <strong>Create</strong> a massive, openly accessible, well-structured summary dataset of scientific literature * <strong>Develop</strong> models capable of generating <strong>structured, factual</strong> summaries * <strong>Demonstrate</strong> the utility of these summaries for scientific tasks * <strong>Explore</strong> decentralized computing to process at global scale This brief outlines <strong>methodology</strong>, <strong>results</strong>, and <strong>implications</strong> for the scientific community—and humanity.</p>
<h2><a id="methodology" class="anchor" href="#methodology" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Methodology</h2>
<h3><a id="21-dataset-collection--processing" class="anchor" href="#21-dataset-collection--processing" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>2.1 Dataset Collection &amp; Processing</h3>
<p>Primary corpus: ~<strong>100M</strong> research papers retrieved via collaboration with <strong>Wynd Labs</strong> using the <strong>Grass</strong> network. After deduplication, we <strong>supplemented</strong> with: *
<strong>bethgelab</strong>: <em>paper_parsed_jsons</em> (<a href="https://huggingface.co/datasets/bethgelab/paper_parsed_jsons">dataset</a>) *</p>
<p><strong>LAION</strong>: <em>COREX-18text</em> (<a href="https://huggingface.co/datasets/laion/COREX-18text">dataset</a>) *</p>
<p><strong>Common Pile</strong>: <em>PubMed</em> subset (<a href="https://huggingface.co/datasets/common-pile/pubmed">dataset</a>) *</p>
<p><strong>LAION</strong>: <em>PeS2oX-fulltext</em> (<a href="https://huggingface.co/datasets/laion/Pes2oX-fulltext">dataset</a>)</p>
<p><strong>Post-training subset (110k papers)</strong>: 40% from the retrieved corpus, <strong>15% each</strong> from the four sources above. Split: <strong>100k train / 10k val</strong>. <strong>Length stats</strong>: mean <strong>81,334</strong> characters, median <strong>45,025</strong> characters.</p>
<h3><a id="22-structured-summary-schema" class="anchor" href="#22-structured-summary-schema" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>2.2 Structured Summary Schema</h3>
<p>Inspired by Alexandria’s <strong>Knowledge Units</strong>, our <strong>JSON schema</strong> first <strong>classifies</strong> content: * SCIENTIFIC_TEXT — complete research articles * PARTIAL_SCIENTIFIC_TEXT — partial scientific content * NON_SCIENTIFIC_TEXT — non-research content For scientific texts, the schema extracts: <strong>title, authors, year, field/subfield, paper type, executive summary, research context, RQs &amp; hypotheses, methods, procedures/architectures, key results (with numbers), interpretation, contradictions/limitations, claims (with supporting/contradicting evidence), data/code availability, robustness/ablations, ethics, key figures/tables, three takeaways</strong>. (See <strong>Appendix A</strong>.)</p>
<h3><a id="23-model-post-training" class="anchor" href="#23-model-post-training" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>2.3 Model Post-Training</h3>
<p>We post-trained: * <strong>Qwen 3 14B</strong> (dense Transformer) * <strong>Nemotron 12B</strong> (hybrid Mamba-Transformer) Targets were <strong>GPT-5-generated</strong> structured reports. A strict prompt guided <strong>classification</strong>, then <strong>schema-aligned extraction</strong> (executive summary, context, methods, procedures/architectures, key results, interpretations, contradictions, claims, data/code, robustness, ethics, key visuals, and three takeaways). See <strong>Appendix A</strong> for prompt.</p>
<h3><a id="24-evaluation" class="anchor" href="#24-evaluation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>2.4 Evaluation</h3>
<p>We used <strong>two complementary approaches</strong>: 1. <strong>LLM-as-a-Judge</strong> — Ensemble of GPT-5, Gemini 2.5 Pro, and Claude 4.5 Sonnet, rating student outputs vs. GPT-5 references on a <strong>1–5</strong> rubric (accuracy, completeness, structure, clarity; hallucination checks). See survey [6]. 2. <strong>QA Dataset</strong> — For a holdout set, we generated <strong>5 MCQs per paper</strong> with GPT-5 and measured models’ ability to answer <strong>using their own generated summaries</strong> (truncated to <strong>10,000 chars</strong>), providing a proxy for <strong>factual utility</strong> (cf. Alexandria [1]).</p>
<h2><a id="results" class="anchor" href="#results" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Results</h2>
<h3><a id="31-llm-as-a-judge" class="anchor" href="#31-llm-as-a-judge" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>3.1 LLM-as-a-Judge</h3>
<p align="center">
  <img src="/images/blog/sci2.jpg"
       alt="LLM-as-a-Judge scores chart"
       style="width:90%; height:auto;">
<table>
<thead>
<tr>
<th>Model</th>
<th style="text-align:center">Score (1–5)</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-5</td>
<td style="text-align:center"><strong>4.805</strong></td>
</tr>
<tr>
<td><strong>Qwen 3 14B (FT)</strong></td>
<td style="text-align:center"><strong>4.207</strong></td>
</tr>
<tr>
<td><strong>Nemotron 12B (FT)</strong></td>
<td style="text-align:center"><strong>4.095</strong></td>
</tr>
<tr>
<td>Gemini 2.5 Flash</td>
<td style="text-align:center">4.052</td>
</tr>
<tr>
<td>Claude 4.5 Sonnet</td>
<td style="text-align:center">3.521</td>
</tr>
<tr>
<td>GPT OSS 120B</td>
<td style="text-align:center">3.273</td>
</tr>
<tr>
<td>Qwen 3 14B (Base)</td>
<td style="text-align:center">3.015</td>
</tr>
<tr>
<td>GPT OSS 20B</td>
<td style="text-align:center">2.903</td>
</tr>
<tr>
<td>Nemotron 12B (Base)</td>
<td style="text-align:center">2.179</td>
</tr>
</tbody>
</table>
<p><em>Figure 1.</em> Average LLM-as-a-Judge scores; <strong>95% CIs via bootstrap</strong>.</p>
</p>
<h3><a id="32-qa-accuracy" class="anchor" href="#32-qa-accuracy" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>3.2 QA Accuracy</h3>
<p align="center">
  <img src="/images/blog/sci.jpg"
       alt="LLM-as-a-Judge scores chart"
       style="width:90%; height:auto;">
<table>
<thead>
<tr>
<th>Model</th>
<th style="text-align:center">Accuracy (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-5</td>
<td style="text-align:center"><strong>74.6</strong></td>
</tr>
<tr>
<td><strong>Qwen 3 14B (FT)</strong></td>
<td style="text-align:center"><strong>73.9</strong></td>
</tr>
<tr>
<td>Gemini 2.5 Flash</td>
<td style="text-align:center">73.9</td>
</tr>
<tr>
<td>Claude 4.5 Sonnet</td>
<td style="text-align:center">72.9</td>
</tr>
<tr>
<td><strong>Nemotron 12B (FT)</strong></td>
<td style="text-align:center">71.3</td>
</tr>
<tr>
<td>Nemotron 12B (Base)</td>
<td style="text-align:center">70.1</td>
</tr>
<tr>
<td>Qwen 3 14B (Base)</td>
<td style="text-align:center">68.3</td>
</tr>
<tr>
<td>GPT OSS 120B</td>
<td style="text-align:center">63.9</td>
</tr>
<tr>
<td>GPT OSS 20B</td>
<td style="text-align:center">58.8</td>
</tr>
</tbody>
</table>
<p><em>Figure 2.</em> QA evaluation over <strong>1,270 MCQs</strong> (multiple-choice accuracy).</p>
</p>
<h3><a id="33-throughput-on-8h200-tp8-vllm" class="anchor" href="#33-throughput-on-8h200-tp8-vllm" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>3.3 Throughput on 8×H200 (TP=8, vLLM)</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th style="text-align:center">Requests/sec</th>
<th style="text-align:center">Input tok/sec</th>
<th style="text-align:center">Output tok/sec</th>
<th style="text-align:center">Single-req tok/sec</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Nemotron 12B</strong></td>
<td style="text-align:center"><strong>0.97</strong></td>
<td style="text-align:center"><strong>16,943.69</strong></td>
<td style="text-align:center"><strong>4,880.76</strong></td>
<td style="text-align:center"><strong>76.17</strong></td>
</tr>
<tr>
<td><strong>Qwen 3 14B</strong></td>
<td style="text-align:center">0.43</td>
<td style="text-align:center">7,516.54</td>
<td style="text-align:center">2,588.30</td>
<td style="text-align:center">39.59</td>
</tr>
</tbody>
</table>
<p><strong>Nemotron 12B</strong> delivers ~<strong>2.25×</strong> the throughput of <strong>Qwen 3 14B</strong>, favoring <strong>large-scale</strong> runs.</p>
<h3><a id="34-visualization-tool" class="anchor" href="#34-visualization-tool" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>3.4 Visualization Tool</h3>
<p>Explore <strong>100k</strong> structured summaries (Qwen 3 14B FT outputs) at <strong><a href="https://laion.inference.net/">https://laion.inference.net/</a></strong>. We compute <strong>Qwen 3 Embedding 4B</strong> embeddings on summaries and use <strong>UMAP</strong> for clustering; <strong>cosine similarity</strong> supports nearest-neighbor exploration.</p>
<h2><a id="discussion" class="anchor" href="#discussion" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Discussion</h2>
<h3><a id="41-implications" class="anchor" href="#41-implications" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>4.1 Implications</h3>
<p>Structured summaries enable:</p>
<ul>
<li>Faster <strong>retrieval</strong> across the literature</li>
<li>Better <strong>machine reasoning</strong> on scientific content</li>
<li>Improved <strong>accessibility</strong> where full texts are unavailable</li>
<li>Novel <strong>visual analytics</strong> for mapping scientific landscapes</li>
<li><strong>Standardized English</strong> representations to simplify cross-domain search</li>
</ul>
<p>Fine-tuned <strong>open</strong> models, correctly trained and formatted, are <strong>competitive</strong> for this task.</p>
<h3><a id="42-decentralized-compute" class="anchor" href="#42-decentralized-compute" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>4.2 Decentralized Compute</h3>
<p>Processing <strong>100M</strong> papers is compute-intensive. The <strong>Inference.net</strong> <strong>permissionless GPU network</strong> (with <strong>verification</strong>) harnesses <strong>idle global compute</strong> at low cost, offering resilient infrastructure for science. Rough estimates: <strong>&gt;$5M</strong> at current <strong>GPT-5</strong> pricing vs. <strong>&lt; $100k</strong> via decentralized nodes and our models.</p>
<h3><a id="43-limitations" class="anchor" href="#43-limitations" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>4.3 Limitations</h3>
<ul>
<li><strong>Hallucinations</strong> remain possible, especially for fine-grained details (Ns, effect sizes, CIs, dates, units).</li>
<li><strong>LLM-as-a-Judge</strong> compresses multiple desiderata into one score; high scores don’t guarantee <strong>line-by-line fidelity</strong>.</li>
<li><strong>QA</strong> tests whether a smaller model can use a <strong>generated</strong> summary—not whether every atomic claim is exact.</li>
<li><strong>Context limits</strong> (e.g., <strong>128k tokens</strong>) may force <strong>selective reading</strong> on very long papers.</li>
<li><strong>Domain heterogeneity</strong> can reduce recall in specialized subfields without further tuning.</li>
<li><strong>LLM-generated targets</strong> risk <strong>propagating upstream biases</strong>.</li>
</ul>
<p><strong>Appropriate use:</strong> Treat summaries as <strong>high-quality overviews</strong> for search/triage/review—not as substitutes for the source in <strong>high-stakes</strong> contexts. Verify numbers, dates, and terms in the original paper when precision is critical.</p>
<h3><a id="44-outlook--future-work" class="anchor" href="#44-outlook--future-work" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>4.4 Outlook &amp; Future Work</h3>
<ul>
<li><strong>Scale to 100M summaries + metadata</strong>: Join each summary to <strong>OpenAlex</strong> metadata (authors, venues, concepts, references, citations) for <strong>graph-native</strong> exploration at scale (<a href="https://docs.openalex.org/">https://docs.openalex.org/</a>).</li>
<li><strong>Release permissive full texts + summaries</strong>: For permissively licensed papers (e.g., <strong>PeS2o</strong>, <strong>Common Pile PubMed</strong>), pair <strong>full text</strong> with structured summaries to support <strong>long-context</strong> training and <strong>grounded retrieval</strong>.</li>
<li><strong>From summaries to Knowledge Units</strong>: Iteratively convert summaries into <strong>Alexandria-style Knowledge Units</strong> (<a href="https://arxiv.org/abs/2502.19413">arXiv:2502.19413</a>) to create a <strong>shareable factual substrate</strong> suited for open dissemination. (More compute-intensive; we will prioritize scaled summaries + metadata first.)</li>
</ul>
<h2><a id="conclusion" class="anchor" href="#conclusion" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>Open, large-scale <strong>structured summarization</strong> can significantly <strong>accelerate</strong> scientific discovery and education. <strong>Nemotron 12B</strong> provides <strong>superior throughput</strong> for at-scale processing; our <strong>fine-tuned models</strong> and <strong>released datasets</strong> show that open approaches can be both <strong>practical</strong> and <strong>competitive</strong>.</p>
<p>Our <strong>visualizer</strong> demonstrates real applications of structured summaries, and collaboration with <strong>Inference.net</strong> highlights how <strong>decentralized compute</strong> can tackle the processing challenges ahead.</p>
<p><strong>Call to action:</strong></p>
<p>We invite <strong>researchers, librarians, and open-access advocates</strong> to help us <strong>gather more papers</strong> for large-scale knowledge extraction. We also invite <strong>engineers and compute providers</strong> to help <strong>optimize</strong> our paragraph-level pipeline and <strong>contribute GPU capacity</strong> (decentralized nodes, clusters, credits) so we can run inference over the <strong>full corpus</strong> and convert it into <strong>Alexandria-style Knowledge Units</strong>—<strong>freeing factual scientific knowledge</strong> for education and accelerated research.</p>
<h2><a id="acknowledgments" class="anchor" href="#acknowledgments" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgments</h2>
<p>This is a collaboration between <strong>LAION</strong>, <strong>Grass</strong>, and <strong>Inference.net</strong>. We thank all contributors, especially <strong>Tawsif Ratul</strong> for data collection, and <strong>Prof. Sören Auer</strong>, <strong>Dr. Gollam Rabby</strong>, and the <strong>TIB – Leibniz Information Centre for Science and Technology</strong> for scientific advice and support.</p>
<h2><a id="references" class="anchor" href="#references" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>References</h2>
<ol>
<li><strong>Alexandria Project</strong> (2023). <em>Democratizing access to scientific knowledge.</em> <a href="https://projects.laion.ai/project-alexandria/">https://projects.laion.ai/project-alexandria/</a></li>
<li><strong>bethgelab Paper Dataset</strong> (2024). <a href="https://huggingface.co/datasets/bethgelab/paper_parsed_jsons">https://huggingface.co/datasets/bethgelab/paper_parsed_jsons</a></li>
<li><strong>LAION COREX-18text</strong> (2024). <a href="https://huggingface.co/datasets/laion/COREX-18text">https://huggingface.co/datasets/laion/COREX-18text</a></li>
<li><strong>Common Pile PubMed</strong> (2024). <a href="https://huggingface.co/datasets/common-pile/pubmed">https://huggingface.co/datasets/common-pile/pubmed</a></li>
<li><strong>LAION PeS2oX-fulltext</strong> (2024). <a href="https://huggingface.co/datasets/laion/Pes2oX-fulltext">https://huggingface.co/datasets/laion/Pes2oX-fulltext</a></li>
<li><strong>A Survey on LLM-as-a-Judge</strong> (2025). <a href="https://arxiv.org/abs/2411.15594">https://arxiv.org/abs/2411.15594</a></li>
<li><strong>Inference.net Paper Visualizer</strong> (2025). <a href="https://laion.inference.net/">https://laion.inference.net/</a></li>
<li><strong>Qwen 3 Embedding 4B</strong> (2025). <a href="https://huggingface.co/Qwen/Qwen3-Embedding-4B">https://huggingface.co/Qwen/Qwen3-Embedding-4B</a></li>
</ol>
<h2><a id="appendix-a--implementation-details" class="anchor" href="#appendix-a--implementation-details" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Appendix A — Implementation Details</h2>
<h3><a id="a1-llm-as-a-judge-prompt" class="anchor" href="#a1-llm-as-a-judge-prompt" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>A.1 LLM-as-a-Judge Prompt</h3>
<pre><code class="language-text">You are an expert judge evaluating the quality of AI-generated summarizations of scientific research articles. Your task is to evaluate how well a student model's response compares to a teacher model's response (considered the ground truth).

Evaluation Rubric (1-5 scale)
Score 5 - Excellent:
- Matches or exceeds the teacher response in accuracy and completeness
- All key information is present and correctly extracted
- Structure and formatting are clear and well-organized
- No hallucinations or incorrect information
- Demonstrates deep understanding of the scientific content

Score 4 - Good:
- Very similar to teacher response with only minor omissions
- All critical information is captured correctly
- May have slight differences in phrasing or organization
- No significant errors or hallucinations
- Demonstrates good understanding of the scientific content

Score 3 - Average:
- Captures main ideas but missing some important details
- Generally accurate but may have minor inaccuracies
- Structure may be less clear than teacher response
- May have minor inconsistencies
- Demonstrates basic understanding but lacks depth in places

Score 2 - Below Average:
- Missing significant portions of key information
- Contains notable inaccuracies or errors
- Poor structure or organization
- May have some hallucinations or incorrect extrapolations
- Demonstrates limited understanding of the scientific content

Score 1 - Poor:
- Fundamentally incorrect or irrelevant
- Major hallucinations or fabricated information
- Missing most critical information
- Incomprehensible or poorly structured
- Demonstrates little to no understanding of the scientific content

Instructions:
1. Compare the student response against the teacher response
2. Consider the original input as context for what was asked
3. Evaluate accuracy, completeness, structure, and clarity
4. Look for hallucinations or incorrect information
5. Provide specific comments on strengths and weaknesses

Output Format (JSON):
{
  &quot;score&quot;: &lt;integer 1-5&gt;,
  &quot;comments&quot;: &quot;&lt;detailed explanation of your evaluation&gt;&quot;
}
</code></pre>
<h3><a id="a2-json-schema-article-response" class="anchor" href="#a2-json-schema-article-response" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>A.2 JSON Schema (Article Response)</h3>
<pre><code class="language-json">{
  &quot;name&quot;: &quot;article_response&quot;,
  &quot;schema&quot;: {
    &quot;$defs&quot;: {
      &quot;ArticleClassification&quot;: {
        &quot;description&quot;: &quot;Classification of the article content.&quot;,
        &quot;enum&quot;: [&quot;SCIENTIFIC_TEXT&quot;, &quot;PARTIAL_SCIENTIFIC_TEXT&quot;, &quot;NON_SCIENTIFIC_TEXT&quot;],
        &quot;title&quot;: &quot;ArticleClassification&quot;,
        &quot;type&quot;: &quot;string&quot;
      },
      &quot;Claim&quot;: {
        &quot;description&quot;: &quot;Individual research claim with supporting evidence.&quot;,
        &quot;properties&quot;: {
          &quot;details&quot;: {
            &quot;description&quot;: &quot;Testable claim details grounded in specific reported numbers/figures/tables&quot;,
            &quot;title&quot;: &quot;Details&quot;,
            &quot;type&quot;: &quot;string&quot;
          },
          &quot;supporting_evidence&quot;: {
            &quot;description&quot;: &quot;Evidence that supports this claim from the paper&quot;,
            &quot;title&quot;: &quot;Supporting Evidence&quot;,
            &quot;type&quot;: &quot;string&quot;
          },
          &quot;contradicting_evidence&quot;: {
            &quot;description&quot;: &quot;Evidence that contradicts or limits this claim, or empty string if none&quot;,
            &quot;title&quot;: &quot;Contradicting Evidence&quot;,
            &quot;type&quot;: &quot;string&quot;
          },
          &quot;implications&quot;: {
            &quot;description&quot;: &quot;Implications of this claim for the broader field&quot;,
            &quot;title&quot;: &quot;Implications&quot;,
            &quot;type&quot;: &quot;string&quot;
          }
        },
        &quot;required&quot;: [&quot;details&quot;, &quot;supporting_evidence&quot;, &quot;contradicting_evidence&quot;, &quot;implications&quot;],
        &quot;title&quot;: &quot;Claim&quot;,
        &quot;type&quot;: &quot;object&quot;,
        &quot;additionalProperties&quot;: false
      },
      &quot;ScientificSummary&quot;: {
        &quot;description&quot;: &quot;Complete structured summary of a scientific paper.&quot;,
        &quot;properties&quot;: {
          &quot;title&quot;: { &quot;description&quot;: &quot;Exact paper title as it appears in the original paper&quot;, &quot;title&quot;: &quot;Title&quot;, &quot;type&quot;: &quot;string&quot; },
          &quot;authors&quot;: { &quot;description&quot;: &quot;Full list of authors in publication order, including affiliations if provided&quot;, &quot;title&quot;: &quot;Authors&quot;, &quot;type&quot;: &quot;string&quot; },
          &quot;publication_year&quot;: {
            &quot;anyOf&quot;: [{ &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; }],
            &quot;default&quot;: null,
            &quot;description&quot;: &quot;Publication year of the paper if available, must be a valid integer&quot;,
            &quot;title&quot;: &quot;Publication Year&quot;
          },
          &quot;field_subfield&quot;: { &quot;description&quot;: &quot;Academic field and subfield, e.g. 'Computer Science — Vision'&quot;, &quot;title&quot;: &quot;Field Subfield&quot;, &quot;type&quot;: &quot;string&quot; },
          &quot;type_of_paper&quot;: { &quot;description&quot;: &quot;Type: theoretical, empirical, methodological, implementation, review, etc.&quot;, &quot;title&quot;: &quot;Type Of Paper&quot;, &quot;type&quot;: &quot;string&quot; },
          &quot;executive_summary&quot;: { &quot;description&quot;: &quot;Concise narrative: problem, what was done, key findings (with numbers), novelty, significance, limitations&quot;, &quot;title&quot;: &quot;Executive Summary&quot;, &quot;type&quot;: &quot;string&quot; },
          &quot;research_context&quot;: { &quot;description&quot;: &quot;Background gap/controversy, closest prior work, what this addresses&quot;, &quot;title&quot;: &quot;Research Context&quot;, &quot;type&quot;: &quot;string&quot; },
          &quot;research_question_and_hypothesis&quot;: { &quot;description&quot;: &quot;Central research questions, explicit hypotheses/predictions and alternatives&quot;, &quot;title&quot;: &quot;Research Question And Hypothesis&quot;, &quot;type&quot;: &quot;string&quot; },
          &quot;methodological_details&quot;: { &quot;description&quot;: &quot;Design, participants/sample, materials/data, procedure, analysis&quot;, &quot;title&quot;: &quot;Methodological Details&quot;, &quot;type&quot;: &quot;string&quot; },
          &quot;procedures_and_architectures&quot;: { &quot;description&quot;: &quot;Models/systems/apparatus, architectures, hyperparameters, what's new&quot;, &quot;title&quot;: &quot;Procedures And Architectures&quot;, &quot;type&quot;: &quot;string&quot; },
          &quot;key_results&quot;: { &quot;description&quot;: &quot;Quantitative/qualitative findings with actual numbers; baseline/SOTA comparisons; robustness&quot;, &quot;title&quot;: &quot;Key Results&quot;, &quot;type&quot;: &quot;string&quot; },
          &quot;interpretation_and_theoretical_implications&quot;: { &quot;description&quot;: &quot;What findings mean for RQs and theory; mechanisms; scope&quot;, &quot;title&quot;: &quot;Interpretation And Theoretical Implications&quot;, &quot;type&quot;: &quot;string&quot; },
          &quot;contradictions_and_limitations&quot;: { &quot;description&quot;: &quot;Inconsistencies, methodological constraints, external validity, conflicts with prior literature&quot;, &quot;title&quot;: &quot;Contradictions And Limitations&quot;, &quot;type&quot;: &quot;string&quot; },
          &quot;claims&quot;: { &quot;description&quot;: &quot;List of testable claims grounded in specific reported numbers/figures/tables&quot;, &quot;items&quot;: { &quot;$ref&quot;: &quot;#/$defs/Claim&quot; }, &quot;title&quot;: &quot;Claims&quot;, &quot;type&quot;: &quot;array&quot; },
          &quot;data_and_code_availability&quot;: { &quot;description&quot;: &quot;Links, licenses, prereg, supplements, or empty string&quot;, &quot;title&quot;: &quot;Data And Code Availability&quot;, &quot;type&quot;: &quot;string&quot; },
          &quot;robustness_and_ablation_notes&quot;: { &quot;description&quot;: &quot;Ablations/sensitivity/stability analysis, or empty string&quot;, &quot;title&quot;: &quot;Robustness And Ablation Notes&quot;, &quot;type&quot;: &quot;string&quot; },
          &quot;ethical_considerations&quot;: { &quot;description&quot;: &quot;Risks, mitigations, approvals, privacy/consent, dual use, or empty string&quot;, &quot;title&quot;: &quot;Ethical Considerations&quot;, &quot;type&quot;: &quot;string&quot; },
          &quot;key_figures_tables&quot;: { &quot;description&quot;: &quot;Critical figures/tables, what they show, and how they substantiate claims&quot;, &quot;title&quot;: &quot;Key Figures Tables&quot;, &quot;type&quot;: &quot;string&quot; },
          &quot;three_takeaways&quot;: { &quot;description&quot;: &quot;Three short paragraphs: (1) core novelty, (2) strongest evidence with numbers, (3) primary limitation&quot;, &quot;title&quot;: &quot;Three Takeaways&quot;, &quot;type&quot;: &quot;string&quot; }
        },
        &quot;required&quot;: [&quot;title&quot;, &quot;authors&quot;, &quot;publication_year&quot;, &quot;field_subfield&quot;, &quot;type_of_paper&quot;, &quot;executive_summary&quot;, &quot;research_context&quot;, &quot;research_question_and_hypothesis&quot;, &quot;methodological_details&quot;, &quot;procedures_and_architectures&quot;, &quot;key_results&quot;, &quot;interpretation_and_theoretical_implications&quot;, &quot;contradictions_and_limitations&quot;, &quot;claims&quot;, &quot;data_and_code_availability&quot;, &quot;robustness_and_ablation_notes&quot;, &quot;ethical_considerations&quot;, &quot;key_figures_tables&quot;, &quot;three_takeaways&quot;],
        &quot;title&quot;: &quot;ScientificSummary&quot;,
        &quot;type&quot;: &quot;object&quot;,
        &quot;additionalProperties&quot;: false
      }
    },
    &quot;description&quot;: &quot;Top-level response structure for article processing.&quot;,
    &quot;properties&quot;: {
      &quot;article_classification&quot;: { &quot;$ref&quot;: &quot;#/$defs/ArticleClassification&quot; },
      &quot;reason&quot;: {
        &quot;anyOf&quot;: [{ &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; }],
        &quot;default&quot;: null,
        &quot;description&quot;: &quot;Reason when classification is NON_SCIENTIFIC_TEXT&quot;,
        &quot;title&quot;: &quot;Reason&quot;
      },
      &quot;summary&quot;: {
        &quot;anyOf&quot;: [{ &quot;$ref&quot;: &quot;#/$defs/ScientificSummary&quot; }, { &quot;type&quot;: &quot;null&quot; }],
        &quot;default&quot;: null,
        &quot;description&quot;: &quot;Scientific summary if article is SCIENTIFIC_TEXT or PARTIAL_SCIENTIFIC_TEXT&quot;
      }
    },
    &quot;required&quot;: [&quot;article_classification&quot;, &quot;reason&quot;, &quot;summary&quot;],
    &quot;title&quot;: &quot;ArticleResponse&quot;,
    &quot;type&quot;: &quot;object&quot;,
    &quot;additionalProperties&quot;: false
  }
}
</code></pre>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Open Scientific Summaries at Scale: The Inference.net × LAION × Grass Initiative","author":"Christoph Schuhmann, Amarjot Singh, Andrej Radonjic, Sean Smith, and Sam Hogan","date":"November 07 2025","previewImg":"/images/blog/sci3.jpg"},"content":"\n\n\n## Abstract \n\nWe present a comprehensive approach to democratizing access to scientific knowledge through large-scale, **structured summarization** of academic literature. \n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/sci5.png\"\n       alt=\"LLM-as-a-Judge scores chart\"\n       style=\"width:90%; height:auto;\"\u003e\n\u003c/p\u003e\n\nWe retrieved and processed ~**100 million** research papers from the public internet, leveraging existing datasets from **bethgelab**, **PeS2o**, **Hugging Face**, and **Common Pile**. We designed a standardized **JSON schema** for scientific paper summaries and **post-trained two models**—**Qwen 3 14B** and **Nemotron 12B**—to produce summaries in this format. Our evaluation combines **LLM-as-a-Judge** and a **QA dataset**. Fine-tuned models achieve performance on our evals comparable to leading closed models (e.g., GPT-5, Claude 4.5). **Nemotron 12B** offers ~**2.25×** higher throughput than Qwen 3 14B, making it attractive for large-scale processing. \n\nWith this preliminary blog post, we **release a fine-tuned models, 100k paper summaries**. \nA live **visualization tool** at [https://laion.inference.net/](https://laion.inference.net/) demonstrates the utility of structured summaries. We plan to release structured summaries for the full **100M** paper corpus. \n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/sci4.png\"\n       alt=\"LLM-as-a-Judge scores chart\"\n       style=\"width:90%; height:auto;\"\u003e\n\u003c/p\u003e\n\n\n\n## Introduction \n\nAccess to scientific knowledge remains constrained by paywalls, licensing, and copyright, slowing research and education. Our **Project Alexandria** ([arXiv:2502.19413](https://arxiv.org/abs/2502.19413)) showed that it is legally and technically feasible to **extract factual knowledge** while respecting copyright via **Knowledge Units**—structured, style-agnostic representations of content. However, research-paper corpora vary in format and structure, making it hard to compare similar claims or retrieve knowledge efficiently. Building on Alexandria, we introduce a **pipeline** to collect, process, and summarize papers into **structured outputs** consumable by humans and AI systems alike. Our aims: * **Create** a massive, openly accessible, well-structured summary dataset of scientific literature * **Develop** models capable of generating **structured, factual** summaries * **Demonstrate** the utility of these summaries for scientific tasks * **Explore** decentralized computing to process at global scale This brief outlines **methodology**, **results**, and **implications** for the scientific community—and humanity. \n\n\n\n\n## Methodology \n\n### 2.1 Dataset Collection \u0026 Processing \n\nPrimary corpus: ~**100M** research papers retrieved via collaboration with **Wynd Labs** using the **Grass** network. After deduplication, we **supplemented** with: * \n**bethgelab**: *paper_parsed_jsons* ([dataset](https://huggingface.co/datasets/bethgelab/paper_parsed_jsons)) * \n\n**LAION**: *COREX-18text* ([dataset](https://huggingface.co/datasets/laion/COREX-18text)) * \n\n**Common Pile**: *PubMed* subset ([dataset](https://huggingface.co/datasets/common-pile/pubmed)) * \n\n**LAION**: *PeS2oX-fulltext* ([dataset](https://huggingface.co/datasets/laion/Pes2oX-fulltext)) \n\n**Post-training subset (110k papers)**: 40% from the retrieved corpus, **15% each** from the four sources above. Split: **100k train / 10k val**. **Length stats**: mean **81,334** characters, median **45,025** characters. \n\n\n### 2.2 Structured Summary Schema \n\nInspired by Alexandria’s **Knowledge Units**, our **JSON schema** first **classifies** content: * SCIENTIFIC_TEXT — complete research articles * PARTIAL_SCIENTIFIC_TEXT — partial scientific content * NON_SCIENTIFIC_TEXT — non-research content For scientific texts, the schema extracts: **title, authors, year, field/subfield, paper type, executive summary, research context, RQs \u0026 hypotheses, methods, procedures/architectures, key results (with numbers), interpretation, contradictions/limitations, claims (with supporting/contradicting evidence), data/code availability, robustness/ablations, ethics, key figures/tables, three takeaways**. (See **Appendix A**.) \n\n### 2.3 Model Post-Training \n\nWe post-trained: * **Qwen 3 14B** (dense Transformer) * **Nemotron 12B** (hybrid Mamba-Transformer) Targets were **GPT-5-generated** structured reports. A strict prompt guided **classification**, then **schema-aligned extraction** (executive summary, context, methods, procedures/architectures, key results, interpretations, contradictions, claims, data/code, robustness, ethics, key visuals, and three takeaways). See **Appendix A** for prompt. \n\n\n### 2.4 Evaluation \n\nWe used **two complementary approaches**: 1. **LLM-as-a-Judge** — Ensemble of GPT-5, Gemini 2.5 Pro, and Claude 4.5 Sonnet, rating student outputs vs. GPT-5 references on a **1–5** rubric (accuracy, completeness, structure, clarity; hallucination checks). See survey [6]. 2. **QA Dataset** — For a holdout set, we generated **5 MCQs per paper** with GPT-5 and measured models’ ability to answer **using their own generated summaries** (truncated to **10,000 chars**), providing a proxy for **factual utility** (cf. Alexandria [1]). \n\n\n\n\n## Results\n\n### 3.1 LLM-as-a-Judge\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/sci2.jpg\"\n       alt=\"LLM-as-a-Judge scores chart\"\n       style=\"width:90%; height:auto;\"\u003e\n\n\n\n| Model                 | Score (1–5) |\n| --------------------- | :---------: |\n| GPT-5                 |  **4.805**  |\n| **Qwen 3 14B (FT)**   |  **4.207**  |\n| **Nemotron 12B (FT)** |  **4.095**  |\n| Gemini 2.5 Flash      |    4.052    |\n| Claude 4.5 Sonnet     |    3.521    |\n| GPT OSS 120B          |    3.273    |\n| Qwen 3 14B (Base)     |    3.015    |\n| GPT OSS 20B           |    2.903    |\n| Nemotron 12B (Base)   |    2.179    |\n\n*Figure 1.* Average LLM-as-a-Judge scores; **95% CIs via bootstrap**.\n\u003c/p\u003e\n\n### 3.2 QA Accuracy\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/sci.jpg\"\n       alt=\"LLM-as-a-Judge scores chart\"\n       style=\"width:90%; height:auto;\"\u003e\n\n\n| Model                 | Accuracy (%) |\n| --------------------- | :----------: |\n| GPT-5                 |   **74.6**   |\n| **Qwen 3 14B (FT)**   |   **73.9**   |\n| Gemini 2.5 Flash      |     73.9     |\n| Claude 4.5 Sonnet     |     72.9     |\n| **Nemotron 12B (FT)** |     71.3     |\n| Nemotron 12B (Base)   |     70.1     |\n| Qwen 3 14B (Base)     |     68.3     |\n| GPT OSS 120B          |     63.9     |\n| GPT OSS 20B           |     58.8     |\n\n*Figure 2.* QA evaluation over **1,270 MCQs** (multiple-choice accuracy).\n\u003c/p\u003e\n\n### 3.3 Throughput on 8×H200 (TP=8, vLLM)\n\n| Model            | Requests/sec | Input tok/sec | Output tok/sec | Single-req tok/sec |\n| ---------------- | :----------: | :-----------: | :------------: | :----------------: |\n| **Nemotron 12B** |   **0.97**   | **16,943.69** |  **4,880.76**  |      **76.17**     |\n| **Qwen 3 14B**   |     0.43     |    7,516.54   |    2,588.30    |        39.59       |\n\n**Nemotron 12B** delivers ~**2.25×** the throughput of **Qwen 3 14B**, favoring **large-scale** runs.\n\n\n### 3.4 Visualization Tool\n\nExplore **100k** structured summaries (Qwen 3 14B FT outputs) at **[https://laion.inference.net/](https://laion.inference.net/)**. We compute **Qwen 3 Embedding 4B** embeddings on summaries and use **UMAP** for clustering; **cosine similarity** supports nearest-neighbor exploration.\n\n\n\n\n## Discussion\n\n### 4.1 Implications\n\nStructured summaries enable:\n\n* Faster **retrieval** across the literature\n* Better **machine reasoning** on scientific content\n* Improved **accessibility** where full texts are unavailable\n* Novel **visual analytics** for mapping scientific landscapes\n* **Standardized English** representations to simplify cross-domain search\n\nFine-tuned **open** models, correctly trained and formatted, are **competitive** for this task.\n\n\n### 4.2 Decentralized Compute\n\nProcessing **100M** papers is compute-intensive. The **Inference.net** **permissionless GPU network** (with **verification**) harnesses **idle global compute** at low cost, offering resilient infrastructure for science. Rough estimates: **\u003e$5M** at current **GPT-5** pricing vs. **\u003c $100k** via decentralized nodes and our models.\n\n\n### 4.3 Limitations\n\n* **Hallucinations** remain possible, especially for fine-grained details (Ns, effect sizes, CIs, dates, units).\n* **LLM-as-a-Judge** compresses multiple desiderata into one score; high scores don’t guarantee **line-by-line fidelity**.\n* **QA** tests whether a smaller model can use a **generated** summary—not whether every atomic claim is exact.\n* **Context limits** (e.g., **128k tokens**) may force **selective reading** on very long papers.\n* **Domain heterogeneity** can reduce recall in specialized subfields without further tuning.\n* **LLM-generated targets** risk **propagating upstream biases**.\n\n**Appropriate use:** Treat summaries as **high-quality overviews** for search/triage/review—not as substitutes for the source in **high-stakes** contexts. Verify numbers, dates, and terms in the original paper when precision is critical.\n\n\n### 4.4 Outlook \u0026 Future Work\n\n* **Scale to 100M summaries + metadata**: Join each summary to **OpenAlex** metadata (authors, venues, concepts, references, citations) for **graph-native** exploration at scale ([https://docs.openalex.org/](https://docs.openalex.org/)).\n* **Release permissive full texts + summaries**: For permissively licensed papers (e.g., **PeS2o**, **Common Pile PubMed**), pair **full text** with structured summaries to support **long-context** training and **grounded retrieval**.\n* **From summaries to Knowledge Units**: Iteratively convert summaries into **Alexandria-style Knowledge Units** ([arXiv:2502.19413](https://arxiv.org/abs/2502.19413)) to create a **shareable factual substrate** suited for open dissemination. (More compute-intensive; we will prioritize scaled summaries + metadata first.)\n\n\n\n\n\n## Conclusion\n\nOpen, large-scale **structured summarization** can significantly **accelerate** scientific discovery and education. **Nemotron 12B** provides **superior throughput** for at-scale processing; our **fine-tuned models** and **released datasets** show that open approaches can be both **practical** and **competitive**.\n\nOur **visualizer** demonstrates real applications of structured summaries, and collaboration with **Inference.net** highlights how **decentralized compute** can tackle the processing challenges ahead.\n\n**Call to action:**\n\nWe invite **researchers, librarians, and open-access advocates** to help us **gather more papers** for large-scale knowledge extraction. We also invite **engineers and compute providers** to help **optimize** our paragraph-level pipeline and **contribute GPU capacity** (decentralized nodes, clusters, credits) so we can run inference over the **full corpus** and convert it into **Alexandria-style Knowledge Units**—**freeing factual scientific knowledge** for education and accelerated research.\n\n\n\n\n\n## Acknowledgments\n\nThis is a collaboration between **LAION**, **Grass**, and **Inference.net**. We thank all contributors, especially **Tawsif Ratul** for data collection, and **Prof. Sören Auer**, **Dr. Gollam Rabby**, and the **TIB – Leibniz Information Centre for Science and Technology** for scientific advice and support.\n\n\n\n\n## References\n\n1. **Alexandria Project** (2023). *Democratizing access to scientific knowledge.* [https://projects.laion.ai/project-alexandria/](https://projects.laion.ai/project-alexandria/)\n2. **bethgelab Paper Dataset** (2024). [https://huggingface.co/datasets/bethgelab/paper_parsed_jsons](https://huggingface.co/datasets/bethgelab/paper_parsed_jsons)\n3. **LAION COREX-18text** (2024). [https://huggingface.co/datasets/laion/COREX-18text](https://huggingface.co/datasets/laion/COREX-18text)\n4. **Common Pile PubMed** (2024). [https://huggingface.co/datasets/common-pile/pubmed](https://huggingface.co/datasets/common-pile/pubmed)\n5. **LAION PeS2oX-fulltext** (2024). [https://huggingface.co/datasets/laion/Pes2oX-fulltext](https://huggingface.co/datasets/laion/Pes2oX-fulltext)\n6. **A Survey on LLM-as-a-Judge** (2025). [https://arxiv.org/abs/2411.15594](https://arxiv.org/abs/2411.15594)\n7. **Inference.net Paper Visualizer** (2025). [https://laion.inference.net/](https://laion.inference.net/)\n8. **Qwen 3 Embedding 4B** (2025). [https://huggingface.co/Qwen/Qwen3-Embedding-4B](https://huggingface.co/Qwen/Qwen3-Embedding-4B)\n\n\n\n\n## Appendix A — Implementation Details\n\n### A.1 LLM-as-a-Judge Prompt\n\n```text\nYou are an expert judge evaluating the quality of AI-generated summarizations of scientific research articles. Your task is to evaluate how well a student model's response compares to a teacher model's response (considered the ground truth).\n\nEvaluation Rubric (1-5 scale)\nScore 5 - Excellent:\n- Matches or exceeds the teacher response in accuracy and completeness\n- All key information is present and correctly extracted\n- Structure and formatting are clear and well-organized\n- No hallucinations or incorrect information\n- Demonstrates deep understanding of the scientific content\n\nScore 4 - Good:\n- Very similar to teacher response with only minor omissions\n- All critical information is captured correctly\n- May have slight differences in phrasing or organization\n- No significant errors or hallucinations\n- Demonstrates good understanding of the scientific content\n\nScore 3 - Average:\n- Captures main ideas but missing some important details\n- Generally accurate but may have minor inaccuracies\n- Structure may be less clear than teacher response\n- May have minor inconsistencies\n- Demonstrates basic understanding but lacks depth in places\n\nScore 2 - Below Average:\n- Missing significant portions of key information\n- Contains notable inaccuracies or errors\n- Poor structure or organization\n- May have some hallucinations or incorrect extrapolations\n- Demonstrates limited understanding of the scientific content\n\nScore 1 - Poor:\n- Fundamentally incorrect or irrelevant\n- Major hallucinations or fabricated information\n- Missing most critical information\n- Incomprehensible or poorly structured\n- Demonstrates little to no understanding of the scientific content\n\nInstructions:\n1. Compare the student response against the teacher response\n2. Consider the original input as context for what was asked\n3. Evaluate accuracy, completeness, structure, and clarity\n4. Look for hallucinations or incorrect information\n5. Provide specific comments on strengths and weaknesses\n\nOutput Format (JSON):\n{\n  \"score\": \u003cinteger 1-5\u003e,\n  \"comments\": \"\u003cdetailed explanation of your evaluation\u003e\"\n}\n```\n\n### A.2 JSON Schema (Article Response)\n\n```json\n{\n  \"name\": \"article_response\",\n  \"schema\": {\n    \"$defs\": {\n      \"ArticleClassification\": {\n        \"description\": \"Classification of the article content.\",\n        \"enum\": [\"SCIENTIFIC_TEXT\", \"PARTIAL_SCIENTIFIC_TEXT\", \"NON_SCIENTIFIC_TEXT\"],\n        \"title\": \"ArticleClassification\",\n        \"type\": \"string\"\n      },\n      \"Claim\": {\n        \"description\": \"Individual research claim with supporting evidence.\",\n        \"properties\": {\n          \"details\": {\n            \"description\": \"Testable claim details grounded in specific reported numbers/figures/tables\",\n            \"title\": \"Details\",\n            \"type\": \"string\"\n          },\n          \"supporting_evidence\": {\n            \"description\": \"Evidence that supports this claim from the paper\",\n            \"title\": \"Supporting Evidence\",\n            \"type\": \"string\"\n          },\n          \"contradicting_evidence\": {\n            \"description\": \"Evidence that contradicts or limits this claim, or empty string if none\",\n            \"title\": \"Contradicting Evidence\",\n            \"type\": \"string\"\n          },\n          \"implications\": {\n            \"description\": \"Implications of this claim for the broader field\",\n            \"title\": \"Implications\",\n            \"type\": \"string\"\n          }\n        },\n        \"required\": [\"details\", \"supporting_evidence\", \"contradicting_evidence\", \"implications\"],\n        \"title\": \"Claim\",\n        \"type\": \"object\",\n        \"additionalProperties\": false\n      },\n      \"ScientificSummary\": {\n        \"description\": \"Complete structured summary of a scientific paper.\",\n        \"properties\": {\n          \"title\": { \"description\": \"Exact paper title as it appears in the original paper\", \"title\": \"Title\", \"type\": \"string\" },\n          \"authors\": { \"description\": \"Full list of authors in publication order, including affiliations if provided\", \"title\": \"Authors\", \"type\": \"string\" },\n          \"publication_year\": {\n            \"anyOf\": [{ \"type\": \"integer\" }, { \"type\": \"null\" }],\n            \"default\": null,\n            \"description\": \"Publication year of the paper if available, must be a valid integer\",\n            \"title\": \"Publication Year\"\n          },\n          \"field_subfield\": { \"description\": \"Academic field and subfield, e.g. 'Computer Science — Vision'\", \"title\": \"Field Subfield\", \"type\": \"string\" },\n          \"type_of_paper\": { \"description\": \"Type: theoretical, empirical, methodological, implementation, review, etc.\", \"title\": \"Type Of Paper\", \"type\": \"string\" },\n          \"executive_summary\": { \"description\": \"Concise narrative: problem, what was done, key findings (with numbers), novelty, significance, limitations\", \"title\": \"Executive Summary\", \"type\": \"string\" },\n          \"research_context\": { \"description\": \"Background gap/controversy, closest prior work, what this addresses\", \"title\": \"Research Context\", \"type\": \"string\" },\n          \"research_question_and_hypothesis\": { \"description\": \"Central research questions, explicit hypotheses/predictions and alternatives\", \"title\": \"Research Question And Hypothesis\", \"type\": \"string\" },\n          \"methodological_details\": { \"description\": \"Design, participants/sample, materials/data, procedure, analysis\", \"title\": \"Methodological Details\", \"type\": \"string\" },\n          \"procedures_and_architectures\": { \"description\": \"Models/systems/apparatus, architectures, hyperparameters, what's new\", \"title\": \"Procedures And Architectures\", \"type\": \"string\" },\n          \"key_results\": { \"description\": \"Quantitative/qualitative findings with actual numbers; baseline/SOTA comparisons; robustness\", \"title\": \"Key Results\", \"type\": \"string\" },\n          \"interpretation_and_theoretical_implications\": { \"description\": \"What findings mean for RQs and theory; mechanisms; scope\", \"title\": \"Interpretation And Theoretical Implications\", \"type\": \"string\" },\n          \"contradictions_and_limitations\": { \"description\": \"Inconsistencies, methodological constraints, external validity, conflicts with prior literature\", \"title\": \"Contradictions And Limitations\", \"type\": \"string\" },\n          \"claims\": { \"description\": \"List of testable claims grounded in specific reported numbers/figures/tables\", \"items\": { \"$ref\": \"#/$defs/Claim\" }, \"title\": \"Claims\", \"type\": \"array\" },\n          \"data_and_code_availability\": { \"description\": \"Links, licenses, prereg, supplements, or empty string\", \"title\": \"Data And Code Availability\", \"type\": \"string\" },\n          \"robustness_and_ablation_notes\": { \"description\": \"Ablations/sensitivity/stability analysis, or empty string\", \"title\": \"Robustness And Ablation Notes\", \"type\": \"string\" },\n          \"ethical_considerations\": { \"description\": \"Risks, mitigations, approvals, privacy/consent, dual use, or empty string\", \"title\": \"Ethical Considerations\", \"type\": \"string\" },\n          \"key_figures_tables\": { \"description\": \"Critical figures/tables, what they show, and how they substantiate claims\", \"title\": \"Key Figures Tables\", \"type\": \"string\" },\n          \"three_takeaways\": { \"description\": \"Three short paragraphs: (1) core novelty, (2) strongest evidence with numbers, (3) primary limitation\", \"title\": \"Three Takeaways\", \"type\": \"string\" }\n        },\n        \"required\": [\"title\", \"authors\", \"publication_year\", \"field_subfield\", \"type_of_paper\", \"executive_summary\", \"research_context\", \"research_question_and_hypothesis\", \"methodological_details\", \"procedures_and_architectures\", \"key_results\", \"interpretation_and_theoretical_implications\", \"contradictions_and_limitations\", \"claims\", \"data_and_code_availability\", \"robustness_and_ablation_notes\", \"ethical_considerations\", \"key_figures_tables\", \"three_takeaways\"],\n        \"title\": \"ScientificSummary\",\n        \"type\": \"object\",\n        \"additionalProperties\": false\n      }\n    },\n    \"description\": \"Top-level response structure for article processing.\",\n    \"properties\": {\n      \"article_classification\": { \"$ref\": \"#/$defs/ArticleClassification\" },\n      \"reason\": {\n        \"anyOf\": [{ \"type\": \"string\" }, { \"type\": \"null\" }],\n        \"default\": null,\n        \"description\": \"Reason when classification is NON_SCIENTIFIC_TEXT\",\n        \"title\": \"Reason\"\n      },\n      \"summary\": {\n        \"anyOf\": [{ \"$ref\": \"#/$defs/ScientificSummary\" }, { \"type\": \"null\" }],\n        \"default\": null,\n        \"description\": \"Scientific summary if article is SCIENTIFIC_TEXT or PARTIAL_SCIENTIFIC_TEXT\"\n      }\n    },\n    \"required\": [\"article_classification\", \"reason\", \"summary\"],\n    \"title\": \"ArticleResponse\",\n    \"type\": \"object\",\n    \"additionalProperties\": false\n  }\n}\n```\n","slug":"summaries"},"__N_SSG":true},"page":"/notes/[slug]","query":{"slug":"summaries"},"buildId":"yC_b85G6wSLJL3iEWziwb","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>