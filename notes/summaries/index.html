<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Open Scientific Summaries at Scale: The Inference.net × LAION × Grass Initiative | LAION</title><meta name="title" content="Open Scientific Summaries at Scale: The Inference.net × LAION × Grass Initiative | LAION"/><meta property="og:title" content="Open Scientific Summaries at Scale: The Inference.net × LAION × Grass Initiative | LAION"/><meta name="twitter:title" content="Open Scientific Summaries at Scale: The Inference.net × LAION × Grass Initiative | LAION"/><meta name="description" content="&lt;hr&gt;
&lt;h2&gt;&lt;a id=&quot;abstract&quot; class=&quot;anchor&quot; href=&quot;#abstract&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; v..."/><meta property="og:description" content="&lt;hr&gt;
&lt;h2&gt;&lt;a id=&quot;abstract&quot; class=&quot;anchor&quot; href=&quot;#abstract&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; v..."/><meta name="twitter:description" content="&lt;hr&gt;
&lt;h2&gt;&lt;a id=&quot;abstract&quot; class=&quot;anchor&quot; href=&quot;#abstract&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; v..."/><meta property="og:image" content="https://laion.ai[https://github.com/LAION-AI/laion.ai/blob/58dd33f12bf32a95b49fc6a430f9abc3cde127ae/public/images/blog/sci3.jpg](https://github.com/LAION-AI/laion.ai/blob/58dd33f12bf32a95b49fc6a430f9abc3cde127ae/public/images/blog/sci3.jpg)"/><meta name="twitter:image" content="https://laion.ai[https://github.com/LAION-AI/laion.ai/blob/58dd33f12bf32a95b49fc6a430f9abc3cde127ae/public/images/blog/sci3.jpg](https://github.com/LAION-AI/laion.ai/blob/58dd33f12bf32a95b49fc6a430f9abc3cde127ae/public/images/blog/sci3.jpg)"/><meta name="twitter:image:alt" content="The text: LAION. Large-scale Artificial Intelligence Open Network, TRULY OPEN AI. 100% NON-PROFIT. 100% FREE."/><meta property="og:type" content="website"/><meta property="og:url" content="https://laion.ai/notes/summaries"/><meta name="twitter:url" content="https://laion.ai/notes/summaries"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1D374E"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.png"/><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff2" as="font" type="font/woff2" crossorigin="true"/><meta name="next-head-count" content="25"/><link rel="stylesheet" href="/fonts/load.css"/><link rel="preload" href="/_next/static/css/5357c8cce67e7f29.css" as="style"/><link rel="stylesheet" href="/_next/static/css/5357c8cce67e7f29.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fb0512e25146295.js" defer=""></script><script src="/_next/static/chunks/286-30519d8a3e60551d.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bslug%5D-2b0138ebd04b8900.js" defer=""></script><script src="/_next/static/zzQC_GeFBZ1DzFwYZsX-8/_buildManifest.js" defer=""></script><script src="/_next/static/zzQC_GeFBZ1DzFwYZsX-8/_ssgManifest.js" defer=""></script><script src="/_next/static/zzQC_GeFBZ1DzFwYZsX-8/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-paper md:text-sky md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LAION</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/team/">Team</a><a href="/blog/">Blog</a><a href="/notes/">Notes</a><a href="/press/">Press</a><a href="/about/">About</a><a href="/faq/">FAQ</a><a href="/donations/">Donations</a><a href="/privacy-policy/">Privacy Policy</a><a href="/dataset-requests/">Dataset Requests</a><a href="/impressum/">Impressum</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1D374E;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/team/">Team</a></p><p><a href="/blog/">Blog</a></p><p><a href="/notes/">Notes</a></p><p><a href="/press/">Press</a></p><p><a href="/about/">About</a></p><p><a href="/faq/">FAQ</a></p><p><a href="/donations/">Donations</a></p><p><a href="/privacy-policy/">Privacy Policy</a></p><p><a href="/dataset-requests/">Dataset Requests</a></p><p><a href="/impressum/">Impressum</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-paper grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-8xl md:text-8xl w-full font-bold title-flow break-words">OPEN SCIENTIFIC SUMMARIES AT SCALE: THE INFERENCE.NET × LAION × GRASS INITIATIVE</h1><p class="text-2xl pb-2">by: <!-- -->FULL AUTHOR LIST TBD<!-- -->,<!-- --> <!-- -->28 Oct, 2025<!-- --></p><hr/><div class="pt-2 article"><hr>
<h2><a id="abstract" class="anchor" href="#abstract" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Abstract</h2>
<p>We present a comprehensive approach to democratizing access to scientific knowledge through large-scale, <strong>structured summarization</strong> of academic literature. We retrieved and processed ~<strong>100 million</strong> research papers from the public internet, leveraging existing datasets from <strong>bethgelab</strong>, <strong>PeS2o</strong>, <strong>Hugging Face</strong>, and <strong>Common Pile</strong>. We designed a standardized <strong>JSON schema</strong> for scientific paper summaries and <strong>post-trained two models</strong>—<strong>Qwen 3 14B</strong> and <strong>Nemotron 12B</strong>—to produce summaries in this format.</p>
<p>Our evaluation combines <strong>LLM-as-a-Judge</strong> and a <strong>QA dataset</strong>. Fine-tuned models achieve performance on our evals comparable to leading closed models (e.g., GPT-5, Claude 4.5). <strong>Nemotron 12B</strong> offers ~<strong>2.25×</strong> higher throughput than Qwen 3 14B, making it attractive for large-scale processing.</p>
<p>With this preliminary blog post, we <strong>release a fine-tuned model and 100k paper summaries</strong>. A live <strong>visualization tool</strong> at <a href="https://laion.inference.net/">https://laion.inference.net/</a> demonstrates the utility of structured summaries. We plan to release structured summaries for the full <strong>100M</strong> paper corpus.</p>
<hr>
<h2><a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h2>
<p>Access to scientific knowledge remains constrained by paywalls, licensing, and copyright, slowing research and education. Our <strong>Project Alexandria</strong> (<a href="https://arxiv.org/abs/2502.19413">arXiv:2502.19413</a>) showed that it is legally and technically feasible to <strong>extract factual knowledge</strong> while respecting copyright via <strong>Knowledge Units</strong>—structured, style-agnostic representations of content.</p>
<p>However, research-paper corpora vary in format and structure, making it hard to compare similar claims or retrieve knowledge efficiently. Building on Alexandria, we introduce a <strong>pipeline</strong> to collect, process, and summarize papers into <strong>structured outputs</strong> consumable by humans and AI systems alike. Our aims:</p>
<ul>
<li><strong>Create</strong> a massive, openly accessible, well-structured summary dataset of scientific literature</li>
<li><strong>Develop</strong> models capable of generating <strong>structured, factual</strong> summaries</li>
<li><strong>Demonstrate</strong> the utility of these summaries for scientific tasks</li>
<li><strong>Explore</strong> decentralized computing to process at global scale</li>
</ul>
<p>This brief outlines <strong>methodology</strong>, <strong>results</strong>, and <strong>implications</strong> for the scientific community—and humanity.</p>
<hr>
<h2><a id="methodology" class="anchor" href="#methodology" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Methodology</h2>
<h3><a id="21-dataset-collection--processing" class="anchor" href="#21-dataset-collection--processing" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>2.1 Dataset Collection &amp; Processing</h3>
<p>Primary corpus: ~<strong>100M</strong> research papers retrieved via collaboration with <strong>Wynd Labs</strong> using the <strong>Grass</strong> network. After deduplication, we <strong>supplemented</strong> with:</p>
<ul>
<li><strong>bethgelab</strong>: <em>paper_parsed_jsons</em> (<a href="https://huggingface.co/datasets/bethgelab/paper_parsed_jsons">dataset</a>)</li>
<li><strong>LAION</strong>: <em>COREX-18text</em> (<a href="https://huggingface.co/datasets/laion/COREX-18text">dataset</a>)</li>
<li><strong>Common Pile</strong>: <em>PubMed</em> subset (<a href="https://huggingface.co/datasets/common-pile/pubmed">dataset</a>)</li>
<li><strong>LAION</strong>: <em>PeS2oX-fulltext</em> (<a href="https://huggingface.co/datasets/laion/Pes2oX-fulltext">dataset</a>)</li>
</ul>
<p><strong>Post-training subset (110k papers)</strong>: 40% from the retrieved corpus, <strong>15% each</strong> from the four sources above. Split: <strong>100k train / 10k val</strong>.
<strong>Length stats</strong>: mean <strong>81,334</strong> characters, median <strong>45,025</strong> characters.</p>
<h3><a id="22-structured-summary-schema" class="anchor" href="#22-structured-summary-schema" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>2.2 Structured Summary Schema</h3>
<p>Inspired by Alexandria’s <strong>Knowledge Units</strong>, our <strong>JSON schema</strong> first <strong>classifies</strong> content:</p>
<ul>
<li><code>SCIENTIFIC_TEXT</code> — complete research articles</li>
<li><code>PARTIAL_SCIENTIFIC_TEXT</code> — partial scientific content</li>
<li><code>NON_SCIENTIFIC_TEXT</code> — non-research content</li>
</ul>
<p>For scientific texts, the schema extracts: <strong>title, authors, year, field/subfield, paper type, executive summary, research context, RQs &amp; hypotheses, methods, procedures/architectures, key results (with numbers), interpretation, contradictions/limitations, claims (with supporting/contradicting evidence), data/code availability, robustness/ablations, ethics, key figures/tables, three takeaways</strong>. (See <strong>Appendix A</strong>.)</p>
<h3><a id="23-model-post-training" class="anchor" href="#23-model-post-training" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>2.3 Model Post-Training</h3>
<p>We post-trained:</p>
<ul>
<li><strong>Qwen 3 14B</strong> (dense Transformer)</li>
<li><strong>Nemotron 12B</strong> (hybrid Mamba-Transformer)</li>
</ul>
<p>Targets were <strong>GPT-5-generated</strong> structured reports. A strict prompt guided <strong>classification</strong>, then <strong>schema-aligned extraction</strong> (executive summary, context, methods, procedures/architectures, key results, interpretations, contradictions, claims, data/code, robustness, ethics, key visuals, and three takeaways). See <strong>Appendix A</strong> for prompt.</p>
<h3><a id="24-evaluation" class="anchor" href="#24-evaluation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>2.4 Evaluation</h3>
<p>We used <strong>two complementary approaches</strong>:</p>
<ol>
<li><strong>LLM-as-a-Judge</strong> — Ensemble of GPT-5, Gemini 2.5 Pro, and Claude 4.5 Sonnet, rating student outputs vs. GPT-5 references on a <strong>1–5</strong> rubric (accuracy, completeness, structure, clarity; hallucination checks). See survey [6].</li>
<li><strong>QA Dataset</strong> — For a holdout set, we generated <strong>5 MCQs per paper</strong> with GPT-5 and measured models’ ability to answer <strong>using their own generated summaries</strong> (truncated to <strong>10,000 chars</strong>), providing a proxy for <strong>factual utility</strong> (cf. Alexandria [1]).</li>
</ol>
<hr>
<h2><a id="results" class="anchor" href="#results" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Results</h2>
<h3><a id="31-llm-as-a-judge" class="anchor" href="#31-llm-as-a-judge" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>3.1 LLM-as-a-Judge</h3>
<p align="center">
  <img src="https://github.com/LAION-AI/laion.ai/blob/58dd33f12bf32a95b49fc6a430f9abc3cde127ae/public/images/blog/sci2.jpg" alt="LLM-as-a-Judge scores chart" width="600">
</p>
<table>
<thead>
<tr>
<th>Model</th>
<th style="text-align:center">Score (1–5)</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-5</td>
<td style="text-align:center"><strong>4.805</strong></td>
</tr>
<tr>
<td><strong>Qwen 3 14B (FT)</strong></td>
<td style="text-align:center"><strong>4.207</strong></td>
</tr>
<tr>
<td><strong>Nemotron 12B (FT)</strong></td>
<td style="text-align:center"><strong>4.095</strong></td>
</tr>
<tr>
<td>Gemini 2.5 Flash</td>
<td style="text-align:center">4.052</td>
</tr>
<tr>
<td>Claude 4.5 Sonnet</td>
<td style="text-align:center">3.521</td>
</tr>
<tr>
<td>GPT OSS 120B</td>
<td style="text-align:center">3.273</td>
</tr>
<tr>
<td>Qwen 3 14B (Base)</td>
<td style="text-align:center">3.015</td>
</tr>
<tr>
<td>GPT OSS 20B</td>
<td style="text-align:center">2.903</td>
</tr>
<tr>
<td>Nemotron 12B (Base)</td>
<td style="text-align:center">2.179</td>
</tr>
</tbody>
</table>
<p><em>Figure 1.</em> Average LLM-as-a-Judge scores; <strong>95% CIs via bootstrap</strong>.</p>
<h3><a id="32-qa-accuracy" class="anchor" href="#32-qa-accuracy" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>3.2 QA Accuracy</h3>
<p align="center">
  <img src="https://github.com/LAION-AI/laion.ai/blob/58dd33f12bf32a95b49fc6a430f9abc3cde127ae/public/images/blog/sci1.jpg" alt="QA evaluation accuracy chart" width="600">
</p>
<table>
<thead>
<tr>
<th>Model</th>
<th style="text-align:center">Accuracy (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-5</td>
<td style="text-align:center"><strong>74.6</strong></td>
</tr>
<tr>
<td><strong>Qwen 3 14B (FT)</strong></td>
<td style="text-align:center"><strong>73.9</strong></td>
</tr>
<tr>
<td>Gemini 2.5 Flash</td>
<td style="text-align:center">73.9</td>
</tr>
<tr>
<td>Claude 4.5 Sonnet</td>
<td style="text-align:center">72.9</td>
</tr>
<tr>
<td><strong>Nemotron 12B (FT)</strong></td>
<td style="text-align:center">71.3</td>
</tr>
<tr>
<td>Nemotron 12B (Base)</td>
<td style="text-align:center">70.1</td>
</tr>
<tr>
<td>Qwen 3 14B (Base)</td>
<td style="text-align:center">68.3</td>
</tr>
<tr>
<td>GPT OSS 120B</td>
<td style="text-align:center">63.9</td>
</tr>
<tr>
<td>GPT OSS 20B</td>
<td style="text-align:center">58.8</td>
</tr>
</tbody>
</table>
<p><em>Figure 2.</em> QA evaluation over <strong>1,270 MCQs</strong> (multiple-choice accuracy).</p>
<h3><a id="33-throughput-on-8h200-tp8-vllm" class="anchor" href="#33-throughput-on-8h200-tp8-vllm" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>3.3 Throughput on 8×H200 (TP=8, vLLM)</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th style="text-align:center">Requests/sec</th>
<th style="text-align:center">Input tok/sec</th>
<th style="text-align:center">Output tok/sec</th>
<th style="text-align:center">Single-req tok/sec</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Nemotron 12B</strong></td>
<td style="text-align:center"><strong>0.97</strong></td>
<td style="text-align:center"><strong>16,943.69</strong></td>
<td style="text-align:center"><strong>4,880.76</strong></td>
<td style="text-align:center"><strong>76.17</strong></td>
</tr>
<tr>
<td><strong>Qwen 3 14B</strong></td>
<td style="text-align:center">0.43</td>
<td style="text-align:center">7,516.54</td>
<td style="text-align:center">2,588.30</td>
<td style="text-align:center">39.59</td>
</tr>
</tbody>
</table>
<p><strong>Nemotron 12B</strong> delivers ~<strong>2.25×</strong> the throughput of <strong>Qwen 3 14B</strong>, favoring <strong>large-scale</strong> runs.</p>
<h3><a id="34-visualization-tool" class="anchor" href="#34-visualization-tool" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>3.4 Visualization Tool</h3>
<p>Explore <strong>100k</strong> structured summaries (Qwen 3 14B FT outputs) at <strong><a href="https://laion.inference.net/">https://laion.inference.net/</a></strong>. We compute <strong>Qwen 3 Embedding 4B</strong> embeddings on summaries and use <strong>UMAP</strong> for clustering; <strong>cosine similarity</strong> supports nearest-neighbor exploration.</p>
<hr>
<h2><a id="discussion" class="anchor" href="#discussion" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Discussion</h2>
<h3><a id="41-implications" class="anchor" href="#41-implications" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>4.1 Implications</h3>
<p>Structured summaries enable:</p>
<ul>
<li>Faster <strong>retrieval</strong> across the literature</li>
<li>Better <strong>machine reasoning</strong> on scientific content</li>
<li>Improved <strong>accessibility</strong> where full texts are unavailable</li>
<li>Novel <strong>visual analytics</strong> for mapping scientific landscapes</li>
<li><strong>Standardized English</strong> representations to simplify cross-domain search</li>
</ul>
<p>Fine-tuned <strong>open</strong> models, correctly trained and formatted, are <strong>competitive</strong> for this task.</p>
<h3><a id="42-decentralized-compute" class="anchor" href="#42-decentralized-compute" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>4.2 Decentralized Compute</h3>
<p>Processing <strong>100M</strong> papers is compute-intensive. The <strong>Inference.net</strong> <strong>permissionless GPU network</strong> (with <strong>verification</strong>) harnesses <strong>idle global compute</strong> at low cost, offering resilient infrastructure for science. Rough estimates: <strong>&gt;$5M</strong> at current <strong>GPT-5</strong> pricing vs. <strong>&lt; $100k</strong> via decentralized nodes and our models.</p>
<h3><a id="43-limitations" class="anchor" href="#43-limitations" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>4.3 Limitations</h3>
<ul>
<li><strong>Hallucinations</strong> remain possible, especially for fine-grained details (Ns, effect sizes, CIs, dates, units).</li>
<li><strong>LLM-as-a-Judge</strong> compresses multiple desiderata into one score; high scores don’t guarantee <strong>line-by-line fidelity</strong>.</li>
<li><strong>QA</strong> tests whether a smaller model can use a <strong>generated</strong> summary—not whether every atomic claim is exact.</li>
<li><strong>Context limits</strong> (e.g., <strong>128k tokens</strong>) may force <strong>selective reading</strong> on very long papers.</li>
<li><strong>Domain heterogeneity</strong> can reduce recall in specialized subfields without further tuning.</li>
<li><strong>LLM-generated targets</strong> risk <strong>propagating upstream biases</strong>.</li>
</ul>
<p><strong>Appropriate use:</strong> Treat summaries as <strong>high-quality overviews</strong> for search/triage/review—not as substitutes for the source in <strong>high-stakes</strong> contexts. Verify numbers, dates, and terms in the original paper when precision is critical.</p>
<h3><a id="44-outlook--future-work" class="anchor" href="#44-outlook--future-work" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>4.4 Outlook &amp; Future Work</h3>
<ul>
<li><strong>Scale to 100M summaries + metadata</strong>: Join each summary to <strong>OpenAlex</strong> metadata (authors, venues, concepts, references, citations) for <strong>graph-native</strong> exploration at scale (<a href="https://docs.openalex.org/">docs.openalex.org</a>).</li>
<li><strong>Release permissive full texts + summaries</strong>: For permissively licensed papers (e.g., <strong>PeS2o</strong>, <strong>Common Pile PubMed</strong>), pair <strong>full text</strong> with structured summaries to support <strong>long-context</strong> training and <strong>grounded retrieval</strong>.</li>
<li><strong>From summaries to Knowledge Units</strong>: Iteratively convert summaries into <strong>Alexandria-style Knowledge Units</strong> (<a href="https://arxiv.org/abs/2502.19413">arXiv:2502.19413</a>) to create a <strong>shareable factual substrate</strong> suited for open dissemination. (More compute-intensive; we will prioritize scaled summaries + metadata first.)</li>
</ul>
<hr>
<h2><a id="conclusion" class="anchor" href="#conclusion" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>Open, large-scale <strong>structured summarization</strong> can significantly <strong>accelerate</strong> scientific discovery and education. <strong>Nemotron 12B</strong> provides <strong>superior throughput</strong> for at-scale processing; our <strong>fine-tuned models</strong> and <strong>released datasets</strong> show that open approaches can be both <strong>practical</strong> and <strong>competitive</strong>.</p>
<p>Our <strong>visualizer</strong> demonstrates real applications of structured summaries, and collaboration with <strong>Inference.net</strong> highlights how <strong>decentralized compute</strong> can tackle the processing challenges ahead.</p>
<p><strong>Call to action:</strong>
We invite <strong>researchers, librarians, and open-access advocates</strong> to help us <strong>gather more papers</strong> for large-scale knowledge extraction. We also invite <strong>engineers and compute providers</strong> to help <strong>optimize</strong> our paragraph-level pipeline and <strong>contribute GPU capacity</strong> (decentralized nodes, clusters, credits) so we can run inference over the <strong>full corpus</strong> and convert it into <strong>Alexandria-style Knowledge Units</strong>—<strong>freeing factual scientific knowledge</strong> for education and accelerated research.</p>
<hr>
<h2><a id="acknowledgments" class="anchor" href="#acknowledgments" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgments</h2>
<p>This is a collaboration between <strong>LAION</strong>, <strong>Grass</strong>, and <strong>Inference.net</strong>. We thank all contributors, especially <strong>Tawsif Ratul</strong> for data collection, and <strong>Prof. Sören Auer</strong>, <strong>Dr. Gollam Rabby</strong>, and the <strong>TIB – Leibniz Information Centre for Science and Technology</strong> for scientific advice and support.</p>
<hr>
<h2><a id="references" class="anchor" href="#references" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>References</h2>
<ol>
<li><strong>Alexandria Project</strong> (2023). <em>Democratizing access to scientific knowledge.</em> <a href="https://projects.laion.ai/project-alexandria/">https://projects.laion.ai/project-alexandria/</a></li>
<li><strong>bethgelab Paper Dataset</strong> (2024). <a href="https://huggingface.co/datasets/bethgelab/paper_parsed_jsons">https://huggingface.co/datasets/bethgelab/paper_parsed_jsons</a></li>
<li><strong>LAION COREX-18text</strong> (2024). <a href="https://huggingface.co/datasets/laion/COREX-18text">https://huggingface.co/datasets/laion/COREX-18text</a></li>
<li><strong>Common Pile PubMed</strong> (2024). <a href="https://huggingface.co/datasets/common-pile/pubmed">https://huggingface.co/datasets/common-pile/pubmed</a></li>
<li><strong>LAION PeS2oX-fulltext</strong> (2024). <a href="https://huggingface.co/datasets/laion/Pes2oX-fulltext">https://huggingface.co/datasets/laion/Pes2oX-fulltext</a></li>
<li><strong>A Survey on LLM-as-a-Judge</strong> (2025). <a href="https://arxiv.org/abs/2411.15594">https://arxiv.org/abs/2411.15594</a></li>
<li><strong>Inference.net Paper Visualizer</strong> (2025). <a href="https://laion.inference.net/">https://laion.inference.net/</a></li>
<li><strong>Qwen 3 Embedding 4B</strong> (2025). <a href="https://huggingface.co/Qwen/Qwen3-Embedding-4B">https://huggingface.co/Qwen/Qwen3-Embedding-4B</a></li>
</ol>
<hr>
<h2><a id="appendix-a--implementation-details" class="anchor" href="#appendix-a--implementation-details" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Appendix A — Implementation Details</h2>
<h3><a id="a1-llm-as-a-judge-prompt" class="anchor" href="#a1-llm-as-a-judge-prompt" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>A.1 LLM-as-a-Judge Prompt</h3>
<pre><code class="language-text">You are an expert judge evaluating the quality of AI-generated summarizations of scientific research articles...
[truncated for brevity – keep your full prompt here as in the original]
</code></pre>
<h3><a id="a2-json-schema-article-response" class="anchor" href="#a2-json-schema-article-response" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>A.2 JSON Schema (Article Response)</h3>
<pre><code class="language-json">{
  &quot;name&quot;: &quot;article_response&quot;,
  &quot;schema&quot;: {
    &quot;$defs&quot;: {
      &quot;...&quot;: &quot;Keep your full schema here exactly as in the original post&quot;
    }
  }
}
</code></pre>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Open Scientific Summaries at Scale: The Inference.net × LAION × Grass Initiative","author":"FULL AUTHOR LIST TBD","date":"October 28 2025","previewImg":"[https://github.com/LAION-AI/laion.ai/blob/58dd33f12bf32a95b49fc6a430f9abc3cde127ae/public/images/blog/sci3.jpg](https://github.com/LAION-AI/laion.ai/blob/58dd33f12bf32a95b49fc6a430f9abc3cde127ae/public/images/blog/sci3.jpg)"},"content":"-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n## Abstract\n\nWe present a comprehensive approach to democratizing access to scientific knowledge through large-scale, **structured summarization** of academic literature. We retrieved and processed ~**100 million** research papers from the public internet, leveraging existing datasets from **bethgelab**, **PeS2o**, **Hugging Face**, and **Common Pile**. We designed a standardized **JSON schema** for scientific paper summaries and **post-trained two models**—**Qwen 3 14B** and **Nemotron 12B**—to produce summaries in this format.\n\nOur evaluation combines **LLM-as-a-Judge** and a **QA dataset**. Fine-tuned models achieve performance on our evals comparable to leading closed models (e.g., GPT-5, Claude 4.5). **Nemotron 12B** offers ~**2.25×** higher throughput than Qwen 3 14B, making it attractive for large-scale processing.\n\nWith this preliminary blog post, we **release a fine-tuned model and 100k paper summaries**. A live **visualization tool** at [https://laion.inference.net/](https://laion.inference.net/) demonstrates the utility of structured summaries. We plan to release structured summaries for the full **100M** paper corpus.\n\n---\n\n## Introduction\n\nAccess to scientific knowledge remains constrained by paywalls, licensing, and copyright, slowing research and education. Our **Project Alexandria** ([arXiv:2502.19413](https://arxiv.org/abs/2502.19413)) showed that it is legally and technically feasible to **extract factual knowledge** while respecting copyright via **Knowledge Units**—structured, style-agnostic representations of content.\n\nHowever, research-paper corpora vary in format and structure, making it hard to compare similar claims or retrieve knowledge efficiently. Building on Alexandria, we introduce a **pipeline** to collect, process, and summarize papers into **structured outputs** consumable by humans and AI systems alike. Our aims:\n\n* **Create** a massive, openly accessible, well-structured summary dataset of scientific literature\n* **Develop** models capable of generating **structured, factual** summaries\n* **Demonstrate** the utility of these summaries for scientific tasks\n* **Explore** decentralized computing to process at global scale\n\nThis brief outlines **methodology**, **results**, and **implications** for the scientific community—and humanity.\n\n---\n\n## Methodology\n\n### 2.1 Dataset Collection \u0026 Processing\n\nPrimary corpus: ~**100M** research papers retrieved via collaboration with **Wynd Labs** using the **Grass** network. After deduplication, we **supplemented** with:\n\n* **bethgelab**: *paper_parsed_jsons* ([dataset](https://huggingface.co/datasets/bethgelab/paper_parsed_jsons))\n* **LAION**: *COREX-18text* ([dataset](https://huggingface.co/datasets/laion/COREX-18text))\n* **Common Pile**: *PubMed* subset ([dataset](https://huggingface.co/datasets/common-pile/pubmed))\n* **LAION**: *PeS2oX-fulltext* ([dataset](https://huggingface.co/datasets/laion/Pes2oX-fulltext))\n\n**Post-training subset (110k papers)**: 40% from the retrieved corpus, **15% each** from the four sources above. Split: **100k train / 10k val**.\n**Length stats**: mean **81,334** characters, median **45,025** characters.\n\n### 2.2 Structured Summary Schema\n\nInspired by Alexandria’s **Knowledge Units**, our **JSON schema** first **classifies** content:\n\n* `SCIENTIFIC_TEXT` — complete research articles\n* `PARTIAL_SCIENTIFIC_TEXT` — partial scientific content\n* `NON_SCIENTIFIC_TEXT` — non-research content\n\nFor scientific texts, the schema extracts: **title, authors, year, field/subfield, paper type, executive summary, research context, RQs \u0026 hypotheses, methods, procedures/architectures, key results (with numbers), interpretation, contradictions/limitations, claims (with supporting/contradicting evidence), data/code availability, robustness/ablations, ethics, key figures/tables, three takeaways**. (See **Appendix A**.)\n\n### 2.3 Model Post-Training\n\nWe post-trained:\n\n* **Qwen 3 14B** (dense Transformer)\n* **Nemotron 12B** (hybrid Mamba-Transformer)\n\nTargets were **GPT-5-generated** structured reports. A strict prompt guided **classification**, then **schema-aligned extraction** (executive summary, context, methods, procedures/architectures, key results, interpretations, contradictions, claims, data/code, robustness, ethics, key visuals, and three takeaways). See **Appendix A** for prompt.\n\n### 2.4 Evaluation\n\nWe used **two complementary approaches**:\n\n1. **LLM-as-a-Judge** — Ensemble of GPT-5, Gemini 2.5 Pro, and Claude 4.5 Sonnet, rating student outputs vs. GPT-5 references on a **1–5** rubric (accuracy, completeness, structure, clarity; hallucination checks). See survey [6].\n2. **QA Dataset** — For a holdout set, we generated **5 MCQs per paper** with GPT-5 and measured models’ ability to answer **using their own generated summaries** (truncated to **10,000 chars**), providing a proxy for **factual utility** (cf. Alexandria [1]).\n\n---\n\n## Results\n\n### 3.1 LLM-as-a-Judge\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"https://github.com/LAION-AI/laion.ai/blob/58dd33f12bf32a95b49fc6a430f9abc3cde127ae/public/images/blog/sci2.jpg\" alt=\"LLM-as-a-Judge scores chart\" width=\"600\"\u003e\n\u003c/p\u003e\n\n| Model                 | Score (1–5) |\n| --------------------- | :---------: |\n| GPT-5                 |  **4.805**  |\n| **Qwen 3 14B (FT)**   |  **4.207**  |\n| **Nemotron 12B (FT)** |  **4.095**  |\n| Gemini 2.5 Flash      |    4.052    |\n| Claude 4.5 Sonnet     |    3.521    |\n| GPT OSS 120B          |    3.273    |\n| Qwen 3 14B (Base)     |    3.015    |\n| GPT OSS 20B           |    2.903    |\n| Nemotron 12B (Base)   |    2.179    |\n\n*Figure 1.* Average LLM-as-a-Judge scores; **95% CIs via bootstrap**.\n\n### 3.2 QA Accuracy\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"https://github.com/LAION-AI/laion.ai/blob/58dd33f12bf32a95b49fc6a430f9abc3cde127ae/public/images/blog/sci1.jpg\" alt=\"QA evaluation accuracy chart\" width=\"600\"\u003e\n\u003c/p\u003e\n\n| Model                 | Accuracy (%) |\n| --------------------- | :----------: |\n| GPT-5                 |   **74.6**   |\n| **Qwen 3 14B (FT)**   |   **73.9**   |\n| Gemini 2.5 Flash      |     73.9     |\n| Claude 4.5 Sonnet     |     72.9     |\n| **Nemotron 12B (FT)** |     71.3     |\n| Nemotron 12B (Base)   |     70.1     |\n| Qwen 3 14B (Base)     |     68.3     |\n| GPT OSS 120B          |     63.9     |\n| GPT OSS 20B           |     58.8     |\n\n*Figure 2.* QA evaluation over **1,270 MCQs** (multiple-choice accuracy).\n\n### 3.3 Throughput on 8×H200 (TP=8, vLLM)\n\n| Model            | Requests/sec | Input tok/sec | Output tok/sec | Single-req tok/sec |\n| ---------------- | :----------: | :-----------: | :------------: | :----------------: |\n| **Nemotron 12B** |   **0.97**   | **16,943.69** |  **4,880.76**  |      **76.17**     |\n| **Qwen 3 14B**   |     0.43     |    7,516.54   |    2,588.30    |        39.59       |\n\n**Nemotron 12B** delivers ~**2.25×** the throughput of **Qwen 3 14B**, favoring **large-scale** runs.\n\n### 3.4 Visualization Tool\n\nExplore **100k** structured summaries (Qwen 3 14B FT outputs) at **[https://laion.inference.net/](https://laion.inference.net/)**. We compute **Qwen 3 Embedding 4B** embeddings on summaries and use **UMAP** for clustering; **cosine similarity** supports nearest-neighbor exploration.\n\n---\n\n## Discussion\n\n### 4.1 Implications\n\nStructured summaries enable:\n\n* Faster **retrieval** across the literature\n* Better **machine reasoning** on scientific content\n* Improved **accessibility** where full texts are unavailable\n* Novel **visual analytics** for mapping scientific landscapes\n* **Standardized English** representations to simplify cross-domain search\n\nFine-tuned **open** models, correctly trained and formatted, are **competitive** for this task.\n\n### 4.2 Decentralized Compute\n\nProcessing **100M** papers is compute-intensive. The **Inference.net** **permissionless GPU network** (with **verification**) harnesses **idle global compute** at low cost, offering resilient infrastructure for science. Rough estimates: **\u003e$5M** at current **GPT-5** pricing vs. **\u003c $100k** via decentralized nodes and our models.\n\n### 4.3 Limitations\n\n* **Hallucinations** remain possible, especially for fine-grained details (Ns, effect sizes, CIs, dates, units).\n* **LLM-as-a-Judge** compresses multiple desiderata into one score; high scores don’t guarantee **line-by-line fidelity**.\n* **QA** tests whether a smaller model can use a **generated** summary—not whether every atomic claim is exact.\n* **Context limits** (e.g., **128k tokens**) may force **selective reading** on very long papers.\n* **Domain heterogeneity** can reduce recall in specialized subfields without further tuning.\n* **LLM-generated targets** risk **propagating upstream biases**.\n\n**Appropriate use:** Treat summaries as **high-quality overviews** for search/triage/review—not as substitutes for the source in **high-stakes** contexts. Verify numbers, dates, and terms in the original paper when precision is critical.\n\n### 4.4 Outlook \u0026 Future Work\n\n* **Scale to 100M summaries + metadata**: Join each summary to **OpenAlex** metadata (authors, venues, concepts, references, citations) for **graph-native** exploration at scale ([docs.openalex.org](https://docs.openalex.org/)).\n* **Release permissive full texts + summaries**: For permissively licensed papers (e.g., **PeS2o**, **Common Pile PubMed**), pair **full text** with structured summaries to support **long-context** training and **grounded retrieval**.\n* **From summaries to Knowledge Units**: Iteratively convert summaries into **Alexandria-style Knowledge Units** ([arXiv:2502.19413](https://arxiv.org/abs/2502.19413)) to create a **shareable factual substrate** suited for open dissemination. (More compute-intensive; we will prioritize scaled summaries + metadata first.)\n\n---\n\n## Conclusion\n\nOpen, large-scale **structured summarization** can significantly **accelerate** scientific discovery and education. **Nemotron 12B** provides **superior throughput** for at-scale processing; our **fine-tuned models** and **released datasets** show that open approaches can be both **practical** and **competitive**.\n\nOur **visualizer** demonstrates real applications of structured summaries, and collaboration with **Inference.net** highlights how **decentralized compute** can tackle the processing challenges ahead.\n\n**Call to action:**\nWe invite **researchers, librarians, and open-access advocates** to help us **gather more papers** for large-scale knowledge extraction. We also invite **engineers and compute providers** to help **optimize** our paragraph-level pipeline and **contribute GPU capacity** (decentralized nodes, clusters, credits) so we can run inference over the **full corpus** and convert it into **Alexandria-style Knowledge Units**—**freeing factual scientific knowledge** for education and accelerated research.\n\n---\n\n## Acknowledgments\n\nThis is a collaboration between **LAION**, **Grass**, and **Inference.net**. We thank all contributors, especially **Tawsif Ratul** for data collection, and **Prof. Sören Auer**, **Dr. Gollam Rabby**, and the **TIB – Leibniz Information Centre for Science and Technology** for scientific advice and support.\n\n---\n\n## References\n\n1. **Alexandria Project** (2023). *Democratizing access to scientific knowledge.* [https://projects.laion.ai/project-alexandria/](https://projects.laion.ai/project-alexandria/)\n2. **bethgelab Paper Dataset** (2024). [https://huggingface.co/datasets/bethgelab/paper_parsed_jsons](https://huggingface.co/datasets/bethgelab/paper_parsed_jsons)\n3. **LAION COREX-18text** (2024). [https://huggingface.co/datasets/laion/COREX-18text](https://huggingface.co/datasets/laion/COREX-18text)\n4. **Common Pile PubMed** (2024). [https://huggingface.co/datasets/common-pile/pubmed](https://huggingface.co/datasets/common-pile/pubmed)\n5. **LAION PeS2oX-fulltext** (2024). [https://huggingface.co/datasets/laion/Pes2oX-fulltext](https://huggingface.co/datasets/laion/Pes2oX-fulltext)\n6. **A Survey on LLM-as-a-Judge** (2025). [https://arxiv.org/abs/2411.15594](https://arxiv.org/abs/2411.15594)\n7. **Inference.net Paper Visualizer** (2025). [https://laion.inference.net/](https://laion.inference.net/)\n8. **Qwen 3 Embedding 4B** (2025). [https://huggingface.co/Qwen/Qwen3-Embedding-4B](https://huggingface.co/Qwen/Qwen3-Embedding-4B)\n\n---\n\n## Appendix A — Implementation Details\n\n### A.1 LLM-as-a-Judge Prompt\n\n```text\nYou are an expert judge evaluating the quality of AI-generated summarizations of scientific research articles...\n[truncated for brevity – keep your full prompt here as in the original]\n```\n\n### A.2 JSON Schema (Article Response)\n\n```json\n{\n  \"name\": \"article_response\",\n  \"schema\": {\n    \"$defs\": {\n      \"...\": \"Keep your full schema here exactly as in the original post\"\n    }\n  }\n}\n```\n\n","slug":"summaries"},"__N_SSG":true},"page":"/notes/[slug]","query":{"slug":"summaries"},"buildId":"zzQC_GeFBZ1DzFwYZsX-8","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>