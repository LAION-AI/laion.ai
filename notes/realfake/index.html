<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Training a Binary Classifier to Distinguish Images Generated with Stable Diffusion (v1.4) from Real Ones | LAION</title><meta name="title" content="Training a Binary Classifier to Distinguish Images Generated with Stable Diffusion (v1.4) from Real Ones | LAION"/><meta property="og:title" content="Training a Binary Classifier to Distinguish Images Generated with Stable Diffusion (v1.4) from Real Ones | LAION"/><meta name="twitter:title" content="Training a Binary Classifier to Distinguish Images Generated with Stable Diffusion (v1.4) from Real Ones | LAION"/><meta name="description" content="&lt;p&gt;We present the development and assessment of a binary classifier designed to distinguish between authentic images and images generated
using Stable Diffus..."/><meta property="og:description" content="&lt;p&gt;We present the development and assessment of a binary classifier designed to distinguish between authentic images and images generated
using Stable Diffus..."/><meta name="twitter:description" content="&lt;p&gt;We present the development and assessment of a binary classifier designed to distinguish between authentic images and images generated
using Stable Diffus..."/><meta property="og:image" content="https://laion.aihttps://raw.githubusercontent.com/LAION-AI/laion.ai/e095bb080a77443cc6a7e07d97b412af53beebc0/public/images/blog/realfake-classifier-artifacts.png"/><meta name="twitter:image" content="https://laion.aihttps://raw.githubusercontent.com/LAION-AI/laion.ai/e095bb080a77443cc6a7e07d97b412af53beebc0/public/images/blog/realfake-classifier-artifacts.png"/><meta name="twitter:image:alt" content="The text: LAION. Large-scale Artificial Intelligence Open Network, TRULY OPEN AI. 100% NON-PROFIT. 100% FREE."/><meta property="og:type" content="website"/><meta property="og:url" content="https://laion.ai/notes/realfake"/><meta name="twitter:url" content="https://laion.ai/notes/realfake"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1D374E"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.png"/><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff2" as="font" type="font/woff2" crossorigin="true"/><meta name="next-head-count" content="25"/><link rel="stylesheet" href="/fonts/load.css"/><link rel="preload" href="/_next/static/css/5357c8cce67e7f29.css" as="style"/><link rel="stylesheet" href="/_next/static/css/5357c8cce67e7f29.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fb0512e25146295.js" defer=""></script><script src="/_next/static/chunks/286-30519d8a3e60551d.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bslug%5D-2b0138ebd04b8900.js" defer=""></script><script src="/_next/static/I_gppK5zMgnULdE0A6fHF/_buildManifest.js" defer=""></script><script src="/_next/static/I_gppK5zMgnULdE0A6fHF/_ssgManifest.js" defer=""></script><script src="/_next/static/I_gppK5zMgnULdE0A6fHF/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-paper md:text-sky md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LAION</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/team/">Team</a><a href="/blog/">Blog</a><a href="/notes/">Notes</a><a href="/press/">Press</a><a href="/about/">About</a><a href="/faq/">FAQ</a><a href="/donations/">Donations</a><a href="/privacy-policy/">Privacy Policy</a><a href="/dataset-requests/">Dataset Requests</a><a href="/impressum/">Impressum</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1D374E;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/team/">Team</a></p><p><a href="/blog/">Blog</a></p><p><a href="/notes/">Notes</a></p><p><a href="/press/">Press</a></p><p><a href="/about/">About</a></p><p><a href="/faq/">FAQ</a></p><p><a href="/donations/">Donations</a></p><p><a href="/privacy-policy/">Privacy Policy</a></p><p><a href="/dataset-requests/">Dataset Requests</a></p><p><a href="/impressum/">Impressum</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:contact@laion.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.com/invite/eq3cAMZtCC" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/LAION-AI/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-paper grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-8xl md:text-8xl w-full font-bold title-flow break-words">TRAINING A BINARY CLASSIFIER TO DISTINGUISH IMAGES GENERATED WITH STABLE DIFFUSION (V1.4) FROM REAL ONES</h1><p class="text-2xl pb-2">by: <!-- -->Christoph Schuhmann, Ilia Zaitsev<!-- -->,<!-- --> <!-- -->12 Apr, 2023<!-- --></p><hr/><div class="pt-2 article"><p>We present the development and assessment of a binary classifier designed to distinguish between authentic images and images generated
using Stable Diffusion (SD) v1.4. We will discuss the dataset employed, describe the model architecture, outline the training process,
and present the results obtained. Furthermore, we will explore potential future work aimed at enhancing the classifier's performance.
The source code, training parameters, and model weights are <a href="https://huggingface.co/realfakerepo/realfake">available in this repository</a>.</p>
<h3><a id="dataset" class="anchor" href="#dataset" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Dataset</h3>
<p>The training dataset was assembled in two steps. First, four image datasets were merged:</p>
<ol>
<li><a href="https://huggingface.co/datasets/imagenet-1k"><code>imagenet-1k</code></a>: A widely used subset of ImageNet spanning 1,000 object classes.</li>
<li><a href="https://huggingface.co/datasets/laion/laion2B-en-aesthetic"><code>laion2B-en-aesthetic</code></a> (parts 400 to 699): A subset of images from the LAION-5B dataset, estimated to be <a href="https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md">aesthetic</a> by a model trained on top of CLIP embeddings.</li>
<li><a href="https://huggingface.co/datasets/ChristophSchuhmann/Imagenet-1k-SD-1.4"><code>imagenet-1k-SD-1.4</code></a>: A newly-created dataset that serves as a &quot;twin&quot; to the &quot;real&quot; <code>imagenet-1k</code>, containing the same 1,000 classes but generated using Stable Diffusion v1.4 with a variety of prompts per class.</li>
<li><a href="https://huggingface.co/datasets/poloclub/diffusiondb"><code>DiffusionDB 2M</code></a>: The first large-scale text-to-image prompt dataset.</li>
</ol>
<p>Second, two million images were sampled from the merged data, ensuring an equal distribution of real and SD-generated images. Around 10% of that data
is put aside as a validation subset to track the prediction quality during the training process. The following table shows the number of records
assigned to each subset. This diverse and balanced dataset provided a solid foundation for training the model.</p>
<table>
<thead>
<tr>
<th>Label \ Subset</th>
<th>Training</th>
<th>Validation</th>
</tr>
</thead>
<tbody>
<tr>
<td>fake</td>
<td>898785</td>
<td>101215</td>
</tr>
<tr>
<td>real</td>
<td>899986</td>
<td>100014</td>
</tr>
</tbody>
</table>
<p>The specific list of samples used in training is stored in the <a href="https://huggingface.co/realfakerepo/realfake/tree/main/metadata"><code>metadata/prepared.2000k.jsonl</code></a> file available in the repository. Each record includes information about its subset and path to the sample stored on a local disk.
This allows for flexible selection of images for training and validation. Additionally, the folder contains smaller prepared subsets used for debugging purposes. Note that for the <code>imagenet-1k</code> dataset, the training and validation subsets were prepared such that the classes of images do not overlap.</p>
<h3><a id="model-architecture-and-training-process" class="anchor" href="#model-architecture-and-training-process" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model Architecture and Training Process</h3>
<p>We selected a straightforward model architecture utilizing a fine-tuned <a href="https://pytorch.org/vision/main/models/generated/torchvision.models.convnext_large.html">ConvNext Large</a> model with approximately 200 million parameters. This choice was made to obtain quick results using 8x A100 GPUs on the Stability AI cluster.</p>
<p>The training process employed a One-Cycle learning rate scheduler, AdamW optimizer, and basic augmentations such as affine transformations, crops, and cutouts. The model was trained for five epochs starting from pre-trained weights (imagenet-1k) with all layers unfrozen from the beginning. Investigating more sophisticated training strategies is beyond the scope of this work but may be interesting for future research.</p>
<h3><a id="results" class="anchor" href="#results" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Results</h3>
<p>The trained classifier achieved close to 99% accuracy on the validation dataset described in the #Dataset section. Further testing of the model's generalization capability in distinguishing between real and SD-generated images was performed by creating <em>an additional, out-of-sample test set</em>.
It comprised 2,500 images generated with SDv1.4 using a set of prompts proposed by LLM, with each prompt generating 100 different images. In addition,
the test set included 2,500 images from the <code>imagenet-1k</code> validation set. Therefore, none of the test set images is seen during the training process.</p>
<p>The following plots illustrate the model's confidence levels. Analyzing the results, several interesting conclusions can be drawn:</p>
<ul>
<li>Views of nature, construction works, and furniture often cause confusion.</li>
<li>Real images with visual noise or uncommon objects are mistakenly classified as generated images.</li>
<li>Images with visually distinguishable generative artifacts (incorrectly rendered humans, wheels, airplanes, unrealistic lines) are classified as fakes with high confidence.</li>
</ul>
<p><img src="/images/blog/realfake-classifier-real-least-confident.png" alt="">
<img src="/images/blog/realfake-classifier-real-most-confident.png" alt="">
<img src="/images/blog/realfake-classifier-fake-least-confident.png" alt="">
<img src="/images/blog/realfake-classifier-fake-most-confident.png" alt=""></p>
<p>As expected, cases with obvious generative model-produced artifacts are easily classified that . For instance, images with humans often include clear artifacts such as unnatural postures or impossible positions. Another interesting class of images pertains to natural landscapes. In some instances, they are easily recognized as fakes, while others confuse the model. This also holds true for construction works and some furniture images.</p>
<p>The inference notebook is available on <a href="https://colab.research.google.com/drive/1zZR55CpHdKaVQXhZ3yxvOu55jCDkADam">Google's Colab</a>.</p>
<h3><a id="limitations" class="anchor" href="#limitations" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Limitations</h3>
<p>It is important to note that the current model is still a work in progress. The classifier only saw images produced with Stable Diffusion V1.4,
with all possible image artifacts that it produces. (See the example below.)</p>
<p><img src="/images/blog/realfake-classifier-artifacts.png" alt=""></p>
<p>Therefore, it might be the case that the classifier pays attention to those SD-specific artifacts, and wouldn't perform that well on the output
of other generative models.</p>
<p>Another possible limitation is low image resolution. The classifier resizes images to 256px per side, and further crops it to 224px. It might be difficult to effectively classify high-resolution examples.</p>
<p>Finally, the classifier's quality isn't compared against human's performance. As was mentioned before, some fakes have easily recognized artifacts, while others aren't distinguishable by the human eye because of low resolution. Building a testing dataset assets by humans should give a baseline to better estimate model's performance.</p>
<h3><a id="future-work" class="anchor" href="#future-work" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Future Work</h3>
<p>Building on this work, there are several avenues for further exploration:</p>
<ol>
<li>Using various kinds of generative models for building a more challenging dataset to ensure that the classifier works well across
various generative techniques.</li>
<li>Increasing input resolution to ensure that the model can capture fine details.</li>
<li>Creating a test set classified by volunteers to establish a quality baseline for better assessing model's performance.</li>
<li>Investigating whether the classifier can be used to guide SD models (akin to GANs) to steer them towards generating more realistic images. By providing feedback on the realism of generated images, the classifier might help improve the quality of synthesized images.</li>
</ol>
<h3><a id="acknowledgements-and-contributions" class="anchor" href="#acknowledgements-and-contributions" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgements and Contributions</h3>
<ul>
<li>Christoph Schuhmann conceived the initial idea of building a binary classifier to distinguish real vs. generated images, prepared the <code>imagenet-1k-SD</code> dataset, and guided the development process.</li>
<li><a href="https://stability.ai/">Stability AI</a> provided us with compute resources to store the data and train the classifier.</li>
<li>The <a href="https://docs.fast.ai/">fast.ai</a> library was used for quick prototyping of the initial model.</li>
<li>Scalable training was done via <a href="https://lightning.ai/docs/pytorch/stable/">PyTorch-Lightning</a>.</li>
<li>Numerous other open-source tools, models, and datasets made this work possible.</li>
</ul>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Training a Binary Classifier to Distinguish Images Generated with Stable Diffusion (v1.4) from Real Ones","author":"Christoph Schuhmann, Ilia Zaitsev","date":"Apr 12 2023","previewImg":"https://raw.githubusercontent.com/LAION-AI/laion.ai/e095bb080a77443cc6a7e07d97b412af53beebc0/public/images/blog/realfake-classifier-artifacts.png"},"content":"\nWe present the development and assessment of a binary classifier designed to distinguish between authentic images and images generated \nusing Stable Diffusion (SD) v1.4. We will discuss the dataset employed, describe the model architecture, outline the training process, \nand present the results obtained. Furthermore, we will explore potential future work aimed at enhancing the classifier's performance. \nThe source code, training parameters, and model weights are [available in this repository](https://huggingface.co/realfakerepo/realfake).\n\n### Dataset\n\nThe training dataset was assembled in two steps. First, four image datasets were merged:\n\n1. [`imagenet-1k`](https://huggingface.co/datasets/imagenet-1k): A widely used subset of ImageNet spanning 1,000 object classes.\n2. [`laion2B-en-aesthetic`](https://huggingface.co/datasets/laion/laion2B-en-aesthetic) (parts 400 to 699): A subset of images from the LAION-5B dataset, estimated to be [aesthetic](https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md) by a model trained on top of CLIP embeddings.\n3. [`imagenet-1k-SD-1.4`](https://huggingface.co/datasets/ChristophSchuhmann/Imagenet-1k-SD-1.4): A newly-created dataset that serves as a \"twin\" to the \"real\" `imagenet-1k`, containing the same 1,000 classes but generated using Stable Diffusion v1.4 with a variety of prompts per class.\n4. [`DiffusionDB 2M`](https://huggingface.co/datasets/poloclub/diffusiondb): The first large-scale text-to-image prompt dataset.\n\nSecond, two million images were sampled from the merged data, ensuring an equal distribution of real and SD-generated images. Around 10% of that data \nis put aside as a validation subset to track the prediction quality during the training process. The following table shows the number of records \nassigned to each subset. This diverse and balanced dataset provided a solid foundation for training the model.\n\n| Label \\ Subset | Training | Validation |\n|----------------|----------|------------|\n|      fake      |  898785  |   101215   |\n|      real      |  899986  |   100014   |\n\nThe specific list of samples used in training is stored in the [`metadata/prepared.2000k.jsonl`](https://huggingface.co/realfakerepo/realfake/tree/main/metadata) file available in the repository. Each record includes information about its subset and path to the sample stored on a local disk. \nThis allows for flexible selection of images for training and validation. Additionally, the folder contains smaller prepared subsets used for debugging purposes. Note that for the `imagenet-1k` dataset, the training and validation subsets were prepared such that the classes of images do not overlap.\n\n### Model Architecture and Training Process\n\nWe selected a straightforward model architecture utilizing a fine-tuned [ConvNext Large](https://pytorch.org/vision/main/models/generated/torchvision.models.convnext_large.html) model with approximately 200 million parameters. This choice was made to obtain quick results using 8x A100 GPUs on the Stability AI cluster.\n\nThe training process employed a One-Cycle learning rate scheduler, AdamW optimizer, and basic augmentations such as affine transformations, crops, and cutouts. The model was trained for five epochs starting from pre-trained weights (imagenet-1k) with all layers unfrozen from the beginning. Investigating more sophisticated training strategies is beyond the scope of this work but may be interesting for future research.\n\n### Results\n\nThe trained classifier achieved close to 99% accuracy on the validation dataset described in the #Dataset section. Further testing of the model's generalization capability in distinguishing between real and SD-generated images was performed by creating _an additional, out-of-sample test set_. \nIt comprised 2,500 images generated with SDv1.4 using a set of prompts proposed by LLM, with each prompt generating 100 different images. In addition,\nthe test set included 2,500 images from the `imagenet-1k` validation set. Therefore, none of the test set images is seen during the training process.\n\nThe following plots illustrate the model's confidence levels. Analyzing the results, several interesting conclusions can be drawn:\n* Views of nature, construction works, and furniture often cause confusion.\n* Real images with visual noise or uncommon objects are mistakenly classified as generated images.\n* Images with visually distinguishable generative artifacts (incorrectly rendered humans, wheels, airplanes, unrealistic lines) are classified as fakes with high confidence.\n\n![](/images/blog/realfake-classifier-real-least-confident.png)\n![](/images/blog/realfake-classifier-real-most-confident.png)\n![](/images/blog/realfake-classifier-fake-least-confident.png)\n![](/images/blog/realfake-classifier-fake-most-confident.png)\n\nAs expected, cases with obvious generative model-produced artifacts are easily classified that . For instance, images with humans often include clear artifacts such as unnatural postures or impossible positions. Another interesting class of images pertains to natural landscapes. In some instances, they are easily recognized as fakes, while others confuse the model. This also holds true for construction works and some furniture images.\n\nThe inference notebook is available on [Google's Colab](https://colab.research.google.com/drive/1zZR55CpHdKaVQXhZ3yxvOu55jCDkADam).\n\n### Limitations\n\nIt is important to note that the current model is still a work in progress. The classifier only saw images produced with Stable Diffusion V1.4, \nwith all possible image artifacts that it produces. (See the example below.)\n\n![](/images/blog/realfake-classifier-artifacts.png)\n\nTherefore, it might be the case that the classifier pays attention to those SD-specific artifacts, and wouldn't perform that well on the output \nof other generative models.\n\nAnother possible limitation is low image resolution. The classifier resizes images to 256px per side, and further crops it to 224px. It might be difficult to effectively classify high-resolution examples.\n\nFinally, the classifier's quality isn't compared against human's performance. As was mentioned before, some fakes have easily recognized artifacts, while others aren't distinguishable by the human eye because of low resolution. Building a testing dataset assets by humans should give a baseline to better estimate model's performance.\n\n### Future Work\n\nBuilding on this work, there are several avenues for further exploration:\n\n1. Using various kinds of generative models for building a more challenging dataset to ensure that the classifier works well across \nvarious generative techniques.\n1. Increasing input resolution to ensure that the model can capture fine details.\n1. Creating a test set classified by volunteers to establish a quality baseline for better assessing model's performance.\n1. Investigating whether the classifier can be used to guide SD models (akin to GANs) to steer them towards generating more realistic images. By providing feedback on the realism of generated images, the classifier might help improve the quality of synthesized images.\n\n### Acknowledgements and Contributions\n\n* Christoph Schuhmann conceived the initial idea of building a binary classifier to distinguish real vs. generated images, prepared the `imagenet-1k-SD` dataset, and guided the development process.\n* [Stability AI](https://stability.ai/) provided us with compute resources to store the data and train the classifier.\n* The [fast.ai](https://docs.fast.ai/) library was used for quick prototyping of the initial model.\n* Scalable training was done via [PyTorch-Lightning](https://lightning.ai/docs/pytorch/stable/).\n* Numerous other open-source tools, models, and datasets made this work possible.\n","slug":"realfake"},"__N_SSG":true},"page":"/notes/[slug]","query":{"slug":"realfake"},"buildId":"I_gppK5zMgnULdE0A6fHF","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>